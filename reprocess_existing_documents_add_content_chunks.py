"""
æ—¢å­˜æ–‡æ›¸ã«æœ¬æ–‡ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 

ç¾åœ¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚ã‚‹æ–‡æ›¸ã®ã†ã¡ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ã—ã‹æŒã£ã¦ã„ãªã„æ–‡æ›¸ã«å¯¾ã—ã¦ã€
æœ¬æ–‡ï¼ˆattachment_textï¼‰ã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ ã§ä½œæˆã—ã¾ã™ã€‚

å®Ÿè¡Œæ–¹æ³•:
    python reprocess_existing_documents_add_content_chunks.py [--limit 10] [--dry-run]
"""
import sys
from pathlib import Path
import logging
import argparse

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
root_dir = Path(__file__).parent
sys.path.insert(0, str(root_dir))

from A_common.database.client import DatabaseClient
from C_ai_common.llm_client.llm_client import LLMClient
from A_common.utils.chunking import TextChunker


def reprocess_documents(limit: int = None, dry_run: bool = False):
    """
    æ—¢å­˜æ–‡æ›¸ã«æœ¬æ–‡ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 

    Args:
        limit: å‡¦ç†ã™ã‚‹æ–‡æ›¸æ•°ã®ä¸Šé™ï¼ˆNoneã®å ´åˆã¯å…¨ä»¶ï¼‰
        dry_run: Trueã®å ´åˆã¯å®Ÿéš›ã®å‡¦ç†ã¯è¡Œã‚ãšã€å¯¾è±¡ã‚’è¡¨ç¤ºã™ã‚‹ã®ã¿
    """
    logger.info("=" * 80)
    logger.info("æ—¢å­˜æ–‡æ›¸ã®æœ¬æ–‡ãƒãƒ£ãƒ³ã‚¯è¿½åŠ å‡¦ç†é–‹å§‹")
    logger.info("=" * 80)

    db = DatabaseClient(use_service_role=True)
    llm_client = LLMClient()

    # æœ¬æ–‡ãƒãƒ£ãƒ³ã‚¯ãŒå­˜åœ¨ã—ãªã„æ–‡æ›¸ã‚’å–å¾—
    logger.info("æœ¬æ–‡ãƒãƒ£ãƒ³ã‚¯ãŒãªã„æ–‡æ›¸ã‚’æ¤œç´¢ä¸­...")

    # ã¾ãšå…¨æ–‡æ›¸ã‚’å–å¾—
    all_docs_query = db.client.table('Rawdata_FILE_AND_MAIL').select('id, file_name, attachment_text')

    # attachment_textãŒå­˜åœ¨ã™ã‚‹æ–‡æ›¸ã®ã¿
    all_docs_query = all_docs_query.not_.is_('attachment_text', 'null')

    if limit:
        all_docs_query = all_docs_query.limit(limit * 2)  # ä½™è£•ã‚’æŒã£ã¦å–å¾—

    all_docs = all_docs_query.execute()

    logger.info(f"attachment_textæœ‰ã‚Šã®æ–‡æ›¸: {len(all_docs.data)}ä»¶")

    # æœ¬æ–‡ãƒãƒ£ãƒ³ã‚¯ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    docs_to_process = []
    for doc in all_docs.data:
        doc_id = doc['id']

        # ã“ã®æ–‡æ›¸ã®content_smallãƒãƒ£ãƒ³ã‚¯ãŒã‚ã‚‹ã‹ç¢ºèª
        content_chunks = db.client.table('10_ix_search_index').select('id').eq('document_id', doc_id).eq('chunk_type', 'content_small').limit(1).execute()

        if not content_chunks.data:
            # æœ¬æ–‡ãƒãƒ£ãƒ³ã‚¯ãŒãªã„
            docs_to_process.append(doc)

            if limit and len(docs_to_process) >= limit:
                break

    logger.info(f"æœ¬æ–‡ãƒãƒ£ãƒ³ã‚¯ãŒãªã„æ–‡æ›¸: {len(docs_to_process)}ä»¶")

    if not docs_to_process:
        logger.info("âœ… å‡¦ç†å¯¾è±¡ã®æ–‡æ›¸ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    if dry_run:
        logger.warning("ğŸ” DRY RUN ãƒ¢ãƒ¼ãƒ‰: å®Ÿéš›ã®å‡¦ç†ã¯è¡Œã„ã¾ã›ã‚“")
        logger.info("\nå‡¦ç†å¯¾è±¡ã®æ–‡æ›¸:")
        for i, doc in enumerate(docs_to_process[:10], 1):
            text_len = len(doc.get('attachment_text', ''))
            logger.info(f"  {i}. {doc['file_name']} ({text_len}æ–‡å­—)")
        if len(docs_to_process) > 10:
            logger.info(f"  ... ä»– {len(docs_to_process) - 10}ä»¶")
        return

    # æœ¬æ–‡ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
    logger.info(f"\næœ¬æ–‡ãƒãƒ£ãƒ³ã‚¯ä½œæˆé–‹å§‹: {len(docs_to_process)}ä»¶")

    success_count = 0
    failed_count = 0

    for i, doc in enumerate(docs_to_process, 1):
        doc_id = doc['id']
        file_name = doc['file_name']
        attachment_text = doc.get('attachment_text', '')

        if not attachment_text or len(attachment_text.strip()) < 50:
            logger.warning(f"  [{i}/{len(docs_to_process)}] ã‚¹ã‚­ãƒƒãƒ—: {file_name} (æœ¬æ–‡ãŒçŸ­ã„)")
            continue

        logger.info(f"  [{i}/{len(docs_to_process)}] å‡¦ç†ä¸­: {file_name} ({len(attachment_text)}æ–‡å­—)")

        try:
            # æ—¢å­˜ã®ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’å–å¾—ï¼ˆchunk_indexã®é–‹å§‹ä½ç½®ã‚’æ±ºå®šï¼‰
            existing_chunks = db.client.table('10_ix_search_index').select('chunk_index').eq('document_id', doc_id).order('chunk_index', desc=True).limit(1).execute()

            if existing_chunks.data:
                current_chunk_index = existing_chunks.data[0]['chunk_index'] + 1
            else:
                current_chunk_index = 0

            # å°ãƒãƒ£ãƒ³ã‚¯ä½œæˆï¼ˆ150æ–‡å­—ãšã¤ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—30æ–‡å­—ï¼‰
            chunker = TextChunker(chunk_size=150, chunk_overlap=30)
            small_chunks = chunker.split_text(attachment_text)

            logger.info(f"    å°ãƒãƒ£ãƒ³ã‚¯æ•°: {len(small_chunks)}å€‹")

            # å°ãƒãƒ£ãƒ³ã‚¯ã‚’embeddingåŒ–ã—ã¦ä¿å­˜
            small_chunk_count = 0
            for chunk_dict in small_chunks:
                chunk_text = chunk_dict['chunk_text']
                if not chunk_text.strip():
                    continue

                try:
                    # Embeddingç”Ÿæˆ
                    chunk_embedding = llm_client.generate_embedding(chunk_text)

                    # search_indexã«ä¿å­˜
                    chunk_doc = {
                        'document_id': doc_id,
                        'chunk_index': current_chunk_index,
                        'chunk_content': chunk_text,
                        'chunk_size': len(chunk_text),
                        'chunk_type': 'content_small',
                        'embedding': chunk_embedding,
                        'search_weight': 1.0
                    }

                    db.client.table('10_ix_search_index').insert(chunk_doc).execute()
                    current_chunk_index += 1
                    small_chunk_count += 1
                except Exception as e:
                    logger.error(f"    å°ãƒãƒ£ãƒ³ã‚¯ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

            logger.info(f"    âœ… å®Œäº†: å°ãƒãƒ£ãƒ³ã‚¯{small_chunk_count}å€‹")
            success_count += 1

        except Exception as e:
            logger.error(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            failed_count += 1

    logger.info("\n" + "=" * 80)
    logger.info(f"âœ… å‡¦ç†å®Œäº†: æˆåŠŸ{success_count}ä»¶ / å¤±æ•—{failed_count}ä»¶")
    logger.info("=" * 80)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ—¢å­˜æ–‡æ›¸ã«æœ¬æ–‡ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ ')
    parser.add_argument('--limit', type=int, help='å‡¦ç†ã™ã‚‹æ–‡æ›¸æ•°ã®ä¸Šé™')
    parser.add_argument('--dry-run', action='store_true', help='å®Ÿéš›ã®å‡¦ç†ã¯è¡Œã‚ãšã€å¯¾è±¡ã‚’è¡¨ç¤ºã™ã‚‹ã®ã¿')
    args = parser.parse_args()

    reprocess_documents(limit=args.limit, dry_run=args.dry_run)
