# Pipeline Snapshot (Lightweight)

Generated: 2026-02-01T06:16:11.100978
Files: 32

## Files

- shared/pipeline/__init__.py (24 lines)
- shared/pipeline/archive/stage_h_structuring.py (496 lines)
- shared/pipeline/archive/stage_i_synthesis.py (262 lines)
- shared/pipeline/config_loader.py (222 lines)
- shared/pipeline/constants.py (79 lines)
- shared/pipeline/image_preprocessing.py (215 lines)
- shared/pipeline/ocr_config.py (219 lines)
- shared/pipeline/ocr_report.py (161 lines)
- shared/pipeline/pipeline.py (978 lines)
- shared/pipeline/prompts/__init__.py (171 lines)
- shared/pipeline/stage_e_preprocessing.py (369 lines)
- shared/pipeline/stage_f_visual.py (4149 lines)
- shared/pipeline/stage_g1_table_refiner.py (892 lines)
- shared/pipeline/stage_g2_text_refiner.py (890 lines)
- shared/pipeline/stage_g_gate.py (336 lines)
- shared/pipeline/stage_g_refiner.py (870 lines)
- shared/pipeline/stage_h1_table.py (971 lines)
- shared/pipeline/stage_h2_text.py (574 lines)
- shared/pipeline/stage_h_kakeibo.py (617 lines)
- shared/pipeline/stage_hi_combined.py (432 lines)
- shared/pipeline/stage_j_chunking.py (112 lines)
- shared/pipeline/stage_k_embedding.py (144 lines)
- shared/pipeline/utils/__init__.py (7 lines)
- shared/pipeline/utils/table_parser.py (144 lines)
- shared/ai/__init__.py (1 lines)
- shared/ai/embeddings/__init__.py (1 lines)
- shared/ai/embeddings/embeddings.py (63 lines)
- shared/ai/llm_client/__init__.py (1 lines)
- shared/ai/llm_client/exceptions.py (19 lines)
- shared/ai/llm_client/llm_client.py (668 lines)
- shared/__init__.py (2 lines)
- pyproject.toml (17 lines)

Total: 14106 lines

---

## shared/pipeline/__init__.py

```python
"""
G_unified_pipeline: 統合ドキュメント処理パイプライン

Stage E-K を統合した、堅牢かつ高精度なドキュメント処理フロー

使用方法:
    from shared.pipeline import UnifiedDocumentPipeline

    pipeline = UnifiedDocumentPipeline()
    result = await pipeline.process_document(
        file_path=Path("document.pdf"),
        file_name="document.pdf",
        doc_type="invoice",
        workspace="personal",
        mime_type="application/pdf",
        source_id="drive_file_id"
    )
"""

from .pipeline import UnifiedDocumentPipeline

__all__ = ['UnifiedDocumentPipeline']
__version__ = '1.0.0'
```

## shared/pipeline/archive/stage_h_structuring.py

```python
"""
Stage H: Structuring (構造化)

非構造化テキストから、意味のあるデータ(JSON)を抽出
- 役割: 指定スキーマに基づくJSON抽出、表データの正規化
- モデル: 設定ファイルで指定（Gemini, OpenAI等のLLM）
- 重要: doc_typeを判定するのではなく、渡されたdoc_typeのスキーマを使ってデータを抽出

F_stage_c_extractor から完全移行
"""
import re
import json
import json_repair
from typing import Dict, Any, Optional
from pathlib import Path
from string import Template
from loguru import logger
from datetime import datetime

from shared.ai.llm_client.llm_client import LLMClient
from .constants import STAGE_H_INPUT_SCHEMA_VERSION


class StageHStructuring:
    """Stage H: 構造化（設定ベース版）"""

    def __init__(self, llm_client: LLMClient):
        """
        Args:
            llm_client: LLMクライアント
        """
        self.llm = llm_client

    def process(
        self,
        file_name: str,
        doc_type: str,
        workspace: str,
        combined_text: str,
        prompt: str,
        model: str,
        stage_f_structure: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        構造化（設定ベース版）

        Args:
            file_name: ファイル名
            doc_type: ドキュメントタイプ
            workspace: ワークスペース
            combined_text: 統合テキスト（Stage E + G の結果）
            prompt: プロンプト（config/prompts/stage_h/*.md から読み込み）
            model: モデル名
            stage_f_structure: Stage F の構造化情報（tables, sections, visual_elements）

        Returns:
            {
                'document_date': str,
                'tags': List[str],
                'metadata': Dict[str, Any]
            }
        """
        logger.info(f"[Stage H] 構造化開始... (doc_type={doc_type}, model={model})")

        # ============================================
        # Stage H 入力スキーマ v1.1 検証（契約固定）
        # ============================================
        h_warnings = []
        if stage_f_structure:
            # 1. schema_version 検証
            schema_ver = stage_f_structure.get('schema_version', 'unknown')
            if schema_ver != STAGE_H_INPUT_SCHEMA_VERSION:
                h_warnings.append(f"H_SCHEMA_VERSION_MISMATCH: expected stage_h_input.v1.1, got {schema_ver}")
                logger.warning(f"[Stage H] schema_version不一致: {schema_ver}")
            else:
                logger.info(f"[Stage H] schema_version=stage_h_input.v1.1 ✓")

            # 2. post_body.text 検証
            post_body = stage_f_structure.get('post_body', {})
            post_body_text = post_body.get('text', '')
            if not post_body_text or not post_body_text.strip():
                h_warnings.append("H_POST_BODY_EMPTY: post_body.text is empty")
                logger.warning("[Stage H] post_body.text が空です")
            else:
                logger.info(f"[Stage H] post_body: {len(post_body_text)}文字 (source: {post_body.get('source', 'unknown')})")

            # 3. text_blocks[0].block_type == "post_body" 検証
            text_blocks = stage_f_structure.get('text_blocks', [])
            if text_blocks:
                first_block = text_blocks[0]
                if first_block.get('block_type') != 'post_body':
                    h_warnings.append(f"H_FIRST_BLOCK_NOT_POST_BODY: text_blocks[0].block_type={first_block.get('block_type')}")
                    logger.warning(f"[Stage H] text_blocks[0]がpost_bodyではありません: {first_block.get('block_type')}")
                else:
                    logger.info(f"[Stage H] text_blocks[0]=post_body ({first_block.get('char_count', 0)}文字) ✓")

            # 構造化情報ログ
            logger.info(f"[Stage H] Stage F 構造化情報: tables={len(stage_f_structure.get('tables', []))}, text_blocks={len(text_blocks)}")
            if h_warnings:
                logger.warning(f"[Stage H] 入力検証warnings: {len(h_warnings)}件")
        else:
            logger.info("[Stage H] stage_f_structure なし（レガシーモード）")

        if not combined_text or not combined_text.strip():
            logger.warning("[Stage H] 入力テキストが空です")
            return self._get_fallback_result(doc_type)

        try:
            # プロンプト構築
            logger.info("[Stage H] プロンプト構築中...")
            full_prompt = self._build_prompt(
                prompt_template=prompt,
                file_name=file_name,
                doc_type=doc_type,
                workspace=workspace,
                combined_text=combined_text
            )
            logger.info(f"[Stage H] プロンプト構築完了 ({len(full_prompt)}文字)")

            # LLM呼び出し
            logger.info(f"[Stage H] LLM呼び出し中... (model={model})")
            response = self.llm.call_model(
                tier="default",
                prompt=full_prompt,
                model_name=model
            )
            logger.info(f"[Stage H] LLM応答受信: success={response.get('success')}")

            if not response.get("success"):
                logger.error(f"[Stage H エラー] LLM呼び出し失敗: {response.get('error')}")
                return self._get_fallback_result(doc_type)

            # JSON抽出（リトライ機能付き）
            content = response.get("content", "")
            logger.info(f"[Stage H] ===== LLMレスポンス全文 =====\n{content}\n[Stage H] ===== レスポンス終了 =====")
            result = self._extract_json_with_retry(content, model=model, max_retries=2)

            # Stage F の構造化情報をマージ
            if stage_f_structure:
                result = self._merge_stage_f_structure(result, stage_f_structure)

            # 結果の整形
            final_metadata = result.get('metadata', {})
            # h_warnings を metadata に合流（ログだけでなく実データとして保持）
            if h_warnings:
                final_metadata.setdefault('warnings', []).extend(h_warnings)
                final_metadata['schema_validation'] = {
                    'version_checked': STAGE_H_INPUT_SCHEMA_VERSION,
                    'warnings_count': len(h_warnings)
                }
            return {
                'document_date': result.get('document_date'),
                'tags': result.get('tags', []),
                'metadata': final_metadata
            }

        except Exception as e:
            logger.error(f"[Stage H エラー] 構造化失敗: {e}", exc_info=True)
            return self._get_fallback_result(doc_type)

    def _build_prompt(
        self,
        prompt_template: str,
        file_name: str,
        doc_type: str,
        workspace: str,
        combined_text: str
    ) -> str:
        """
        プロンプトを構築

        Args:
            prompt_template: プロンプトテンプレート
            file_name: ファイル名
            doc_type: ドキュメントタイプ
            workspace: ワークスペース
            combined_text: 統合テキスト

        Returns:
            構築されたプロンプト
        """
        # string.Templateを使用してテンプレート変数を置換（JSONの{}と競合しない）
        template = Template(prompt_template)
        prompt = template.substitute(
            file_name=file_name,
            doc_type=doc_type,
            workspace=workspace,
            combined_text=combined_text,
            current_date=datetime.now().strftime("%Y-%m-%d")
        )

        return prompt

    def _extract_json_with_retry(
        self,
        content: str,
        model: str,
        max_retries: int = 2
    ) -> Dict[str, Any]:
        """
        JSON抽出（リトライ機能付き）

        Args:
            content: LLMの出力
            model: モデル名
            max_retries: 最大リトライ回数

        Returns:
            抽出されたJSON
        """
        for attempt in range(max_retries + 1):
            try:
                result = self._extract_json(content)
                logger.debug(f"[Stage H] JSON抽出成功 (試行{attempt + 1}/{max_retries + 1})")
                return result

            except Exception as e:
                if attempt < max_retries:
                    logger.warning(f"[Stage H] JSON抽出失敗 (試行{attempt + 1}/{max_retries + 1}): {e}")
                    # リトライ: LLMにJSON修正を依頼
                    content = self._retry_json_extraction(content, str(e), model)
                else:
                    logger.error(f"[Stage H] JSON抽出失敗（最終試行）: {e}")
                    raise

        # ここには到達しないはずだが、念のため
        return {}

    def _extract_json(self, content: str) -> Dict[str, Any]:
        """
        コンテンツからJSONを抽出

        Args:
            content: LLMの出力

        Returns:
            抽出されたJSON

        Raises:
            Exception: JSON抽出に失敗した場合
        """
        # 複数のパターンでJSONブロックを探す
        patterns = [
            r'```json\s*(.*?)```',  # ```json ... ``` (改行を柔軟に)
            r'```\s*(.*?)```',      # ``` ... ```
            r'\{[\s\S]*?\}',        # { ... } (非貪欲)
        ]

        json_str = None
        for pattern in patterns:
            match = re.search(pattern, content, re.DOTALL)
            if match:
                json_str = match.group(1) if match.lastindex else match.group(0)
                json_str = json_str.strip()
                # { で始まるか確認
                if json_str.startswith('{'):
                    break
                else:
                    json_str = None

        if not json_str:
            # パターンマッチしない場合、{ ... } を直接探す
            match = re.search(r'\{[\s\S]*\}', content, re.DOTALL)
            if match:
                json_str = match.group(0).strip()
            else:
                json_str = content.strip()

        # JSONパース
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            # json_repair で修復を試みる
            logger.error(f"[Stage H] JSON解析失敗: {e}")
            logger.error(f"[Stage H] 抽出されたJSON（最初の500文字）:\n{json_str[:500]}")
            try:
                return json_repair.loads(json_str)
            except Exception as repair_error:
                logger.error(f"[Stage H] JSON修復も失敗: {repair_error}")
                logger.error(f"[Stage H] 修復失敗したJSON全文:\n{json_str}")
                raise

    def _retry_json_extraction(
        self,
        failed_content: str,
        error_message: str,
        model: str
    ) -> str:
        """
        JSON抽出失敗時、LLMにJSON修正を依頼

        Args:
            failed_content: 失敗したコンテンツ
            error_message: エラーメッセージ
            model: モデル名

        Returns:
            修正されたJSON文字列
        """
        prompt = f"""以下のJSONにエラーがあります。修正してください。

エラー: {error_message}

元のJSON:
```
{failed_content}
```

修正されたJSONを ```json ブロックで出力してください。
"""

        try:
            response = self.llm.call_model(
                tier="default",
                prompt=prompt,
                model_name=model
            )

            if response.get("success"):
                return response.get("content", "")
            else:
                logger.error(f"[Stage H] JSON修正失敗: {response.get('error')}")
                return failed_content

        except Exception as e:
            logger.error(f"[Stage H] JSON修正エラー: {e}")
            return failed_content

    def _merge_stage_f_structure(self, result: Dict[str, Any], stage_f_structure: Dict[str, Any]) -> Dict[str, Any]:
        """
        Stage F の構造化情報を Stage H の結果にマージ

        Args:
            result: Stage H の抽出結果
            stage_f_structure: Stage F の構造化情報

        Returns:
            マージ済みの結果
        """
        metadata = result.get('metadata', {})

        # v1.1契約の判定
        schema_ver = stage_f_structure.get('schema_version', '')
        is_v1_1 = (schema_ver == STAGE_H_INPUT_SCHEMA_VERSION)

        # v1.1: tables/sections の取得場所を適切に
        if is_v1_1:
            # v1.1: tables はトップレベル、sections は layout_info
            stage_f_tables = stage_f_structure.get('tables', [])
            stage_f_sections = stage_f_structure.get('layout_info', {}).get('sections', [])
            # v1.1では text_blocks が Stage F で生成済み → _raw_text_blocks に保存（LLM出力を優先）
            stage_f_text_blocks = stage_f_structure.get('text_blocks', [])
            if stage_f_text_blocks:
                # 生のtext_blocksは隠しフィールドに保存
                metadata['_raw_text_blocks'] = stage_f_text_blocks
                logger.info(f"[Stage H] v1.1: Stage F text_blocks を _raw_text_blocks に保存 ({len(stage_f_text_blocks)}ブロック)")

                # LLMが articles を生成していればそれを優先、なければ text_blocks をチェック
                llm_articles = metadata.get('articles', [])
                llm_text_blocks = metadata.get('text_blocks', [])

                if llm_articles:
                    # LLMが articles を生成済み → text_blocks は不要
                    logger.info(f"[Stage H] LLM生成の articles を優先 ({len(llm_articles)}件)")
                    if 'text_blocks' in metadata:
                        del metadata['text_blocks']
                elif llm_text_blocks:
                    # LLMが text_blocks を生成済み → そのまま使用
                    logger.info(f"[Stage H] LLM生成の text_blocks を使用 ({len(llm_text_blocks)}件)")
                else:
                    # LLM出力が空 → フォールバックとして _raw_text_blocks を text_blocks に
                    metadata['text_blocks'] = stage_f_text_blocks
                    logger.info(f"[Stage H] フォールバック: _raw_text_blocks を text_blocks に採用")
        else:
            # レガシー: 直下または layout_info から取得
            stage_f_tables = stage_f_structure.get('tables', []) or stage_f_structure.get('layout_info', {}).get('tables', [])
            stage_f_sections = stage_f_structure.get('sections', []) or stage_f_structure.get('layout_info', {}).get('sections', [])

        # Stage F の tables を優先的に使用（Stage H で tables が抽出されていない場合、または少ない場合）
        stage_h_tables = metadata.get('structured_tables', [])

        if stage_f_tables and len(stage_f_tables) > len(stage_h_tables):
            logger.info(f"[Stage H] Stage F の tables を優先使用: {len(stage_f_tables)}個（Stage H: {len(stage_h_tables)}個）")

            # Stage F の tables を Stage H の形式に変換
            converted_tables = []
            for i, table in enumerate(stage_f_tables):
                rows = table.get('rows', [])
                if rows:
                    # 最初の行をヘッダーとして扱う
                    headers = rows[0] if rows else []
                    data_rows = rows[1:] if len(rows) > 1 else []

                    # 辞書形式に変換
                    converted_rows = []
                    for row in data_rows:
                        row_dict = {}
                        for j, cell in enumerate(row):
                            header = headers[j] if j < len(headers) else f"列{j+1}"
                            row_dict[header] = cell
                        converted_rows.append(row_dict)

                    converted_tables.append({
                        'table_title': table.get('caption', f'表{i+1}'),
                        'table_type': 'ocr_extracted',
                        'headers': headers,
                        'rows': converted_rows
                    })

            metadata['structured_tables'] = converted_tables

        # Stage F の sections を text_blocks に統合（text_blocks が少ない場合）
        # v1.1では text_blocks が正なのでこの変換はスキップ
        stage_h_text_blocks = metadata.get('text_blocks', [])

        if not is_v1_1 and stage_f_sections and len(stage_h_text_blocks) < len(stage_f_sections):
            logger.info(f"[Stage H] Stage F の sections を text_blocks に変換: {len(stage_f_sections)}個")

            converted_blocks = []
            current_heading = None
            current_content_parts = []

            for section in stage_f_sections:
                section_type = section.get('type', '')
                content = section.get('content', '')

                if section_type == 'heading':
                    # 前のブロックを保存
                    if current_heading and current_content_parts:
                        converted_blocks.append({
                            'title': current_heading,
                            'content': '\n'.join(current_content_parts)
                        })
                    # 新しいヘッディング開始
                    current_heading = content
                    current_content_parts = []
                else:
                    # paragraph, list など
                    if content:
                        # contentがlistの場合は改行で結合して文字列化
                        if isinstance(content, list):
                            current_content_parts.append('\n'.join(str(item) for item in content))
                        else:
                            current_content_parts.append(str(content))

            # 最後のブロックを保存
            if current_heading and current_content_parts:
                converted_blocks.append({
                    'title': current_heading,
                    'content': '\n'.join(current_content_parts)
                })

            # Stage H の text_blocks より多ければ置き換え
            if len(converted_blocks) > len(stage_h_text_blocks):
                metadata['text_blocks'] = converted_blocks

        # Stage F の visual_elements からデッドライン情報を抽出
        visual_elements = stage_f_structure.get('visual_elements', {})
        if visual_elements:
            deadline_info = visual_elements.get('deadline_info')
            if deadline_info and not result.get('document_date'):
                logger.info(f"[Stage H] Stage F の deadline_info を document_date として使用: {deadline_info}")
                result['document_date'] = deadline_info

            # 強調されたテキストをタグに追加
            emphasized_text = visual_elements.get('emphasized_text', [])
            if emphasized_text:
                existing_tags = result.get('tags', [])
                for text in emphasized_text:
                    if text and text not in existing_tags:
                        existing_tags.append(text)
                result['tags'] = existing_tags
                logger.info(f"[Stage H] Stage F の emphasized_text をタグに追加: {emphasized_text}")

        result['metadata'] = metadata
        return result

    def _get_fallback_result(self, doc_type: str) -> Dict[str, Any]:
        """
        フォールバック結果を返す

        Args:
            doc_type: ドキュメントタイプ

        Returns:
            最小限の結果
        """
        return {
            'document_date': None,
            'tags': [],
            'metadata': {
                'doc_type': doc_type,
                'extraction_failed': True
            }
        }
```

## shared/pipeline/archive/stage_i_synthesis.py

```python
"""
Stage I: Synthesis (統合・要約)

抽出されたデータと元のテキストを統合し、人間と検索エンジンに最適な形に整形
- 役割: 全情報を統合し、要約・タグ生成・基準日付抽出
- モデル: 設定ファイルで指定（デフォルト: Gemini 2.5 Flash）

D_stage_a_classifier から完全移行
"""
import json
from typing import Dict, Any, Optional
from pathlib import Path
from string import Template
from loguru import logger

from shared.ai.llm_client.llm_client import LLMClient


class StageISynthesis:
    """Stage I: 統合・要約（設定ベース版）"""

    def __init__(self, llm_client: LLMClient):
        """
        Args:
            llm_client: LLMクライアント
        """
        self.llm = llm_client

    def process(
        self,
        combined_text: str,
        stageH_result: Dict[str, Any],
        prompt: str,
        model: str
    ) -> Dict[str, Any]:
        """
        統合・要約（設定ベース版）

        Args:
            combined_text: 統合テキスト
            stageH_result: Stage H の結果
            prompt: プロンプト（config/prompts/stage_i/*.md から読み込み）
            model: モデル名

        Returns:
            {
                'summary': str,
                'relevant_date': str,
                'tags': List[str]
            }
        """
        logger.info(f"[Stage I] 統合・要約開始... (model={model})")

        if not combined_text or not combined_text.strip():
            logger.warning("[Stage I] 入力テキストが空です")
            return self._get_fallback_result(stageH_result)

        try:
            # プロンプト構築
            full_prompt = self._build_prompt(
                prompt_template=prompt,
                combined_text=combined_text,
                stageH_result=stageH_result
            )

            # LLM呼び出し
            response = self.llm.call_model(
                tier="default",
                prompt=full_prompt,
                model_name=model
            )

            if not response.get("success"):
                logger.error(f"[Stage I エラー] LLM呼び出し失敗: {response.get('error')}")
                return self._get_fallback_result(stageH_result)

            # 結果をJSON形式で取得
            content = response.get("content", "")
            logger.info(f"[Stage I] ===== LLMレスポンス全文 ===== {content[:500]}")
            result = self._parse_result(content)

            # Stage H のタグとマージ
            stageH_tags = stageH_result.get('tags', [])
            stageI_tags = result.get('tags', [])
            merged_tags = list(set(stageH_tags + stageI_tags))  # 重複削除

            return {
                'title': result.get('title', ''),
                'summary': result.get('summary', ''),
                'relevant_date': result.get('relevant_date') or stageH_result.get('document_date'),
                'tags': merged_tags,
                'calendar_events': result.get('calendar_events', []),
                'tasks': result.get('tasks', [])
            }

        except Exception as e:
            logger.error(f"[Stage I エラー] 統合・要約失敗: {e}", exc_info=True)
            return self._get_fallback_result(stageH_result)

    def _build_prompt(
        self,
        prompt_template: str,
        combined_text: str,
        stageH_result: Dict[str, Any]
    ) -> str:
        """
        プロンプトを構築

        Args:
            prompt_template: プロンプトテンプレート
            combined_text: 統合テキスト
            stageH_result: Stage H の結果

        Returns:
            構築されたプロンプト
        """
        # Stage H の結果をJSON文字列化
        stageH_json = json.dumps(stageH_result, ensure_ascii=False, indent=2)

        # string.Templateを使用してテンプレート変数を置換（JSONの{}と競合しない）
        template = Template(prompt_template)
        prompt = template.substitute(
            combined_text=combined_text,
            stageH_result=stageH_json
        )

        return prompt

    def _sanitize_llm_json(self, text: str) -> str:
        """
        P1-2: LLM出力をJSONパース前にサニタイズ

        処理順序:
        1. None → "" / strip()
        2. コードフェンス除去 (```json ... ```)
        3. 先頭の json / JSON: ラベル除去
        4. 先頭の {{ → { に縮退
        5. 最初の { から最後の } までを切り出し
        """
        import re

        # 1. None対策とstrip
        if text is None:
            return ""
        text = text.strip()
        if not text:
            return ""

        # 2. コードフェンス除去
        # ```json や ```JSON など
        text = re.sub(r'^```(?:json|JSON)?\s*\n?', '', text)
        text = re.sub(r'\n?```\s*$', '', text)

        # 3. 先頭の json / JSON: ラベル除去
        text = re.sub(r'^(?:json|JSON)\s*[:：]?\s*', '', text.strip())

        # 4. 先頭の {{ → { に縮退（先頭のみ）
        if text.startswith('{{'):
            text = text[1:]

        # 5. 最初の { から最後の } までを切り出し
        first_brace = text.find('{')
        last_brace = text.rfind('}')
        if first_brace >= 0 and last_brace > first_brace:
            text = text[first_brace:last_brace + 1]

        return text

    def _parse_result(self, content: str, doc_id: str = None) -> Dict[str, Any]:
        """
        P1-2/P1-3: LLM出力から結果を抽出（サニタイズ + ログ強化）

        Args:
            content: LLMの出力
            doc_id: ドキュメントID（ログ用）

        Returns:
            抽出された結果
        """
        raw = content or ""
        raw_head = raw[:200] if raw else "(empty)"

        # P1-2: サニタイズ適用
        clean = self._sanitize_llm_json(raw)
        clean_head = clean[:200] if clean else "(empty)"

        # JSON形式で出力されている場合
        try:
            if clean:
                result = json.loads(clean)
                logger.info(f"[Stage I] JSON解析成功 (doc_id={doc_id})")
                return result
        except json.JSONDecodeError as e:
            logger.warning(f"[P1-2] JSON parse failed attempt=1 (doc_id={doc_id}): {e}")
            logger.warning(f"  raw_head: {raw_head}")
            logger.warning(f"  clean_head: {clean_head}")

        # P1-3: フォールバック（テキストから抽出）
        logger.info(f"[Stage I] JSONパース失敗 → テキスト抽出フォールバック (doc_id={doc_id})")
        return self._extract_from_text(raw)

    def _extract_from_text(self, content: str) -> Dict[str, Any]:
        """
        JSON形式でない場合、テキストから情報を抽出（フォールバック）
        """
        import re

        result = {
            'title': '',
            'summary': '',
            'tags': [],
            'relevant_date': None,
            'calendar_events': [],
            'tasks': []
        }

        if not content:
            return result

        # 要約を抽出（最初の段落または全体）
        lines = content.split('\n')
        summary_lines = []
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#') and not line.startswith('-'):
                summary_lines.append(line)
                if len(summary_lines) >= 3:  # 最大3行
                    break

        result['summary'] = ' '.join(summary_lines) if summary_lines else content[:200]

        # タグを抽出（例: タグ: tag1, tag2, tag3）
        tags_match = re.search(r'タグ[:：]\s*(.+)', content, re.IGNORECASE)
        if tags_match:
            tags_str = tags_match.group(1)
            result['tags'] = [t.strip() for t in tags_str.split(',')]

        # 日付を抽出（YYYY-MM-DD形式）
        date_match = re.search(r'(\d{4}-\d{2}-\d{2})', content)
        if date_match:
            result['relevant_date'] = date_match.group(1)

        return result

    def _get_fallback_result(self, stageH_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        フォールバック結果を返す

        Args:
            stageH_result: Stage H の結果

        Returns:
            最小限の結果
        """
        return {
            'summary': '処理に失敗しました',
            'relevant_date': stageH_result.get('document_date'),
            'tags': stageH_result.get('tags', []),
            'calendar_events': [],
            'tasks': []
        }
```

## shared/pipeline/config_loader.py

```python
"""
設定ローダー

models.yaml と pipeline_routes.yaml を読み込み、
doc_type や workspace に応じて適切なプロンプトとモデルを返す
"""
from pathlib import Path
from typing import Dict, Any, Optional
import yaml
from loguru import logger


class ConfigLoader:
    """パイプライン設定ローダー"""

    def __init__(self, config_dir: Optional[Path] = None):
        """
        初期化

        Args:
            config_dir: 設定ディレクトリ（デフォルト: G_unified_pipeline/config/）
        """
        if config_dir is None:
            config_dir = Path(__file__).parent / "config"

        self.config_dir = Path(config_dir)
        self.models_config = self._load_yaml(self.config_dir / "models.yaml")

        # パイプラインルーティング設定を読み込み
        pipeline_routing = self.config_dir / "pipeline_routing.yaml"
        if pipeline_routing.exists():
            self.routes_config = self._load_yaml(pipeline_routing)
            logger.info(f"✅ pipeline_routing.yaml を読み込みました")
        else:
            # フォールバック: 旧 pipeline_routes.yaml
            self.routes_config = self._load_yaml(self.config_dir / "pipeline_routes.yaml")
            logger.warning(f"⚠️ pipeline_routing.yaml が見つかりません。pipeline_routes.yaml を使用します")

        # プロンプト設定を読み込み
        prompts_file = self.config_dir / "prompts.yaml"
        if prompts_file.exists():
            prompts_data = self._load_yaml(prompts_file)
            self.prompts_config = prompts_data.get('prompts', {})
            logger.info(f"✅ prompts.yaml を読み込みました")
        else:
            self.prompts_config = {}
            logger.warning(f"⚠️ prompts.yaml が見つかりません。MDファイルから読み込みます")

        logger.info(f"✅ 設定ローダー初期化完了: {self.config_dir}")

    def _load_yaml(self, file_path: Path) -> Dict[str, Any]:
        """YAML ファイルを読み込む"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"YAML読み込みエラー: {file_path} - {e}")
            return {}

    def get_route_config(self, doc_type: str, workspace: Optional[str] = None) -> Dict[str, Any]:
        """
        doc_type と workspace に基づいてルート設定を取得

        Args:
            doc_type: ドキュメントタイプ
            workspace: ワークスペース（オプション）

        Returns:
            ルート設定（stages ごとの prompt_key, model_key）
        """
        routing = self.routes_config.get('routing', {})

        # 優先順位1: workspace ベースのルート
        if workspace:
            by_workspace = routing.get('by_workspace', {})
            if workspace in by_workspace:
                logger.debug(f"ルート選択: workspace={workspace}")
                return by_workspace[workspace]

        # 優先順位2: doc_type ベースのルート
        by_doc_type = routing.get('by_doc_type', {})
        if doc_type in by_doc_type:
            logger.debug(f"ルート選択: doc_type={doc_type}")
            return by_doc_type[doc_type]

        # 優先順位3: デフォルト
        logger.debug("ルート選択: default")
        return by_doc_type.get('default', {})

    def get_prompt(self, stage: str, prompt_key: str) -> str:
        """
        プロンプトを取得（prompts.yaml または MDファイルから）

        Args:
            stage: ステージ名 (stage_f, stage_g, stage_h, stage_i)
            prompt_key: プロンプトキー (default, flyer, classroom など)

        Returns:
            プロンプトテキスト
        """
        # prompts.yaml から読み込み
        if self.prompts_config and stage in self.prompts_config:
            if prompt_key in self.prompts_config[stage]:
                prompt = self.prompts_config[stage][prompt_key]
                logger.debug(f"プロンプト読み込み: {stage}/{prompt_key} ({len(prompt)}文字)")
                return prompt
            elif prompt_key != 'default' and 'default' in self.prompts_config[stage]:
                # フォールバック: default プロンプトを試す
                logger.warning(f"プロンプトキー '{prompt_key}' が見つかりません。default を使用します: {stage}")
                prompt = self.prompts_config[stage]['default']
                logger.debug(f"プロンプト読み込み: {stage}/default ({len(prompt)}文字)")
                return prompt

        # フォールバック: MDファイルから読み込み（後方互換性）
        prompt_file = self.config_dir / "prompts" / stage / f"{stage}_{prompt_key}.md"
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                prompt = f.read()
                logger.debug(f"プロンプト読み込み（MDファイル）: {stage}/{stage}_{prompt_key}.md ({len(prompt)}文字)")
                return prompt
        except FileNotFoundError:
            logger.warning(f"プロンプトが見つかりません: {stage}/{prompt_key}")
            # 最後のフォールバック: default プロンプトを試す
            if prompt_key != 'default':
                return self.get_prompt(stage, 'default')
            return ""
        except Exception as e:
            logger.error(f"プロンプト読み込みエラー: {prompt_file} - {e}")
            return ""

    def get_model(self, stage: str, model_key: str) -> str:
        """
        モデル名を取得

        Args:
            stage: ステージ名 (stage_f, stage_g, stage_h, stage_i, stage_k)
            model_key: モデルキー (default, flyer, classroom など)

        Returns:
            モデル名
        """
        models = self.models_config.get('models', {})
        stage_models = models.get(stage, {})

        model = stage_models.get(model_key)
        if model:
            logger.debug(f"モデル選択: {stage}/{model_key} → {model}")
            return model

        # フォールバック: default モデル
        default_model = stage_models.get('default')
        if default_model:
            logger.debug(f"モデル選択（フォールバック）: {stage}/default → {default_model}")
            return default_model

        logger.error(f"モデルが見つかりません: {stage}/{model_key}")
        return ""

    def get_stage_config(
        self,
        stage: str,
        doc_type: str,
        workspace: Optional[str] = None
    ) -> Dict[str, str]:
        """
        特定ステージの設定を取得（プロンプト + モデル + custom_handler）

        Args:
            stage: ステージ名 (stage_f, stage_g, stage_h, stage_i)
            doc_type: ドキュメントタイプ
            workspace: ワークスペース（オプション）

        Returns:
            {'prompt': str, 'model': str, 'custom_handler': str (optional), 'skip': bool (optional)}
        """
        route = self.get_route_config(doc_type, workspace)
        stages_config = route.get('stages', {})
        stage_config = stages_config.get(stage, {})

        prompt_key = stage_config.get('prompt_key', 'default')
        model_key = stage_config.get('model_key', 'default')

        result = {
            'prompt': self.get_prompt(stage, prompt_key),
            'model': self.get_model(stage, model_key)
        }

        # custom_handler がある場合は追加
        if 'custom_handler' in stage_config:
            result['custom_handler'] = stage_config['custom_handler']

        # skip フラグがある場合は追加
        if 'skip' in stage_config:
            result['skip'] = stage_config['skip']

        return result

    def get_hybrid_ocr_enabled(self, doc_type: str, workspace: Optional[str] = None) -> bool:
        """
        ハイブリッドOCR（Surya + PaddleOCR）が有効かどうかを取得

        Args:
            doc_type: ドキュメントタイプ
            workspace: ワークスペース（オプション）

        Returns:
            True: ハイブリッドOCR有効
            False: ハイブリッドOCR無効（Gemini Visionのみ）
        """
        hybrid_ocr_config = self.models_config.get('hybrid_ocr', {})

        # workspace ベースの設定を確認
        if workspace and workspace in hybrid_ocr_config:
            return hybrid_ocr_config[workspace]

        # doc_type ベースの設定を確認
        if doc_type in hybrid_ocr_config:
            return hybrid_ocr_config[doc_type]

        # デフォルト設定
        return hybrid_ocr_config.get('default', False)
```

## shared/pipeline/constants.py

```python
"""
Stage F / Stage H 共通定数

v1.1 契約で使用するスキーマバージョンおよび定数を一元管理

【設計 2026-01-31】Ver 6.1 CSV型JSON超軽量モード対応
"""

# Stage H 入力スキーマバージョン
STAGE_H_INPUT_SCHEMA_VERSION = "stage_h_input.v1.1"

# Stage F 出力スキーマバージョン
STAGE_F_OUTPUT_SCHEMA_VERSION = "stage_f_output.v2.0"

# block_type の許可値（v1.1）
BLOCK_TYPES_V1_1 = [
    "post_body",      # 投稿本文（最優先文脈、必ず先頭）
    "heading",        # 見出し
    "paragraph",      # 段落
    "list_item",      # 箇条書き
    "table",          # 表（Markdown形式）
    "table_text",     # 表内テキスト
    "note",           # 注記
]

# ============================================
# Stage F: 10ステップ構成の定数
# ============================================

# F-1: Image Normalization
F1_TARGET_DPI = 300  # 統一DPI

# F-2: Surya Block Detection
SURYA_MAX_DIM = 2000  # Suryaリサイズ上限

# F-3: Coordinate Quantization（座標量子化）
QUANTIZE_GRID_SIZE = 1000  # 1000×1000 グリッド

# F-6 OCR 上限（レガシー互換）
MAX_OCR_CALLS = 20
MAX_CROP_LONG_EDGE = 1000  # リサイズ閾値
PER_PAGE_MAX_UNION_ROI = 3
MIN_ROI_AREA = 2000  # 最小ROI面積
UNION_PADDING = 20  # union ROIのpadding (px)

# F-7: Dual Read - Path A
F7_MODEL_IMAGE = "gemini-2.5-flash-lite"  # 【Ver 6.1】画像用（CSV型JSON超軽量モード）
F7_MODEL_AV = "gemini-2.5-flash-lite"  # 音声/動画用

# F-8: Dual Read - Path B
F8_MODEL = "gemini-2.5-flash"  # 構造解析（視覚の鬼）

# F-9.5: AI Rescue（曖昧データ救済）
F95_MODEL = "gemini-2.5-flash-lite"

# F-10: Stage E Scrubbing（正本化）
F10_MODEL = "gemini-2.5-flash-lite"

# F-7/F-8 共通
F7_F8_MAX_TOKENS = 65536
F7_F8_TEMPERATURE = 0.0

# チャンク処理
# gemini-2.5-flash-lite: 出力上限 65,536 トークン（十分な余裕あり）
CHUNK_SIZE_PAGES = 1  # 1ページごとに分割処理（安定性のため維持）

# ============================================
# Stage G / H1 / H2 モデル定義
# ============================================

# Stage G: Integration Refiner
G_MODEL = "gemini-2.5-flash-lite"

# Stage H1: Table Specialist
H1_MODEL = "gemini-2.5-flash-lite"

# Stage H2: Text Specialist
H2_MODEL = "gemini-2.5-flash"
```

## shared/pipeline/image_preprocessing.py

```python
"""
画像前処理ユーティリティ

PaddleOCRの認識精度向上のための画像前処理機能を提供
"""
import cv2
import numpy as np
from typing import Tuple
from loguru import logger


def preprocess_image_for_ocr(
    image: np.ndarray,
    apply_clahe: bool = True,
    apply_denoise: bool = True,
    apply_sharpen: bool = True,
    apply_binarize: bool = False
) -> Tuple[np.ndarray, dict]:
    """
    OCR認識精度向上のための画像前処理

    Args:
        image: 入力画像（numpy array、RGB or Grayscale）
        apply_clahe: CLAHEによるコントラスト調整を適用
        apply_denoise: ノイズ除去を適用
        apply_sharpen: シャープ化を適用
        apply_binarize: 二値化を適用（低品質画像向け）

    Returns:
        (processed_image, stats): 前処理済み画像と統計情報
    """
    stats = {
        'original_shape': image.shape,
        'applied_operations': []
    }

    # RGB → Grayscale 変換
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    else:
        gray = image.copy()

    processed = gray.copy()

    # 1. CLAHE（コントラスト制限適応ヒストグラム均等化）
    if apply_clahe:
        try:
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            processed = clahe.apply(processed)
            stats['applied_operations'].append('CLAHE')
        except Exception as e:
            logger.warning(f"CLAHE処理失敗: {e}")

    # 2. ノイズ除去（Non-local Means Denoising）
    if apply_denoise:
        try:
            processed = cv2.fastNlMeansDenoising(processed, None, h=10, templateWindowSize=7, searchWindowSize=21)
            stats['applied_operations'].append('Denoise')
        except Exception as e:
            logger.warning(f"ノイズ除去処理失敗: {e}")

    # 3. シャープ化（Unsharp Masking）
    if apply_sharpen:
        try:
            # ガウシアンブラーでぼかし画像を作成
            blurred = cv2.GaussianBlur(processed, (0, 0), 3)
            # オリジナル - ぼかし = シャープマスク
            processed = cv2.addWeighted(processed, 1.5, blurred, -0.5, 0)
            stats['applied_operations'].append('Sharpen')
        except Exception as e:
            logger.warning(f"シャープ化処理失敗: {e}")

    # 4. 二値化（Otsuの閾値処理） - オプション
    if apply_binarize:
        try:
            _, processed = cv2.threshold(processed, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            stats['applied_operations'].append('Binarize')
        except Exception as e:
            logger.warning(f"二値化処理失敗: {e}")

    # 5. PaddleOCR互換性のためRGB形式に変換
    # PaddleOCRはRGB/カラー画像を期待するため、グレースケールから戻す
    if len(processed.shape) == 2:
        processed = cv2.cvtColor(processed, cv2.COLOR_GRAY2RGB)
        stats['applied_operations'].append('GrayToRGB')

    stats['final_shape'] = processed.shape
    stats['operations_count'] = len(stats['applied_operations'])

    return processed, stats


def adaptive_preprocess(
    image: np.ndarray,
    confidence_threshold: float = 0.7
) -> np.ndarray:
    """
    低信頼度領域に対する適応的前処理

    通常の前処理で認識精度が低い場合に、より強力な前処理を適用

    Args:
        image: 入力画像
        confidence_threshold: この閾値以下の場合、強力な前処理を適用

    Returns:
        前処理済み画像
    """
    # より強力な前処理: 二値化 + ノイズ除去 + シャープ化
    processed, _ = preprocess_image_for_ocr(
        image,
        apply_clahe=True,
        apply_denoise=True,
        apply_sharpen=True,
        apply_binarize=True  # 二値化を有効化
    )

    return processed


def preprocess_for_ppstructure(
    image: np.ndarray,
    enhance_contrast: bool = True,
    sharpen: bool = True
) -> Tuple[np.ndarray, dict]:
    """
    P2-1: PPStructure表検出用の画像前処理

    表罫線を強調し、PPStructureの検出精度を向上させる

    Args:
        image: 入力画像（numpy array、RGB）
        enhance_contrast: コントラスト強調を適用
        sharpen: シャープ化を適用

    Returns:
        (processed_image, stats): 前処理済み画像と統計情報
    """
    stats = {
        'original_shape': image.shape,
        'applied_operations': []
    }

    processed = image.copy()

    # 1. コントラスト強調（LABカラースペースで輝度のみ調整）
    if enhance_contrast:
        try:
            # RGB → LAB
            lab = cv2.cvtColor(processed, cv2.COLOR_RGB2LAB)
            l, a, b = cv2.split(lab)

            # CLAHE を L チャンネルに適用
            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
            l = clahe.apply(l)

            # LAB → RGB
            lab = cv2.merge([l, a, b])
            processed = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
            stats['applied_operations'].append('CLAHE_LAB')
        except Exception as e:
            logger.warning(f"[P2-1] コントラスト強調失敗: {e}")

    # 2. シャープ化（表罫線を強調）
    if sharpen:
        try:
            # Unsharp Masking
            blurred = cv2.GaussianBlur(processed, (0, 0), 2)
            processed = cv2.addWeighted(processed, 1.8, blurred, -0.8, 0)
            stats['applied_operations'].append('Sharpen')
        except Exception as e:
            logger.warning(f"[P2-1] シャープ化失敗: {e}")

    stats['final_shape'] = processed.shape
    stats['operations_count'] = len(stats['applied_operations'])
    stats['preproc'] = 'on' if stats['operations_count'] > 0 else 'off'

    return processed, stats


def calculate_image_quality_score(image: np.ndarray) -> float:
    """
    画像品質スコアを計算（0.0～1.0）

    ぼやけ具合やコントラストを評価し、前処理の必要性を判定

    Args:
        image: 入力画像

    Returns:
        品質スコア（高いほど高品質）
    """
    try:
        # グレースケール変換
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image

        # 1. ぼやけ検出（Laplacian分散）
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        blur_score = min(laplacian_var / 100.0, 1.0)  # 正規化

        # 2. コントラスト評価（標準偏差）
        contrast_score = min(gray.std() / 50.0, 1.0)

        # 総合スコア
        quality_score = (blur_score * 0.6 + contrast_score * 0.4)

        return quality_score

    except Exception as e:
        logger.warning(f"画像品質評価失敗: {e}")
        return 0.5  # デフォルト値
```

## shared/pipeline/ocr_config.py

```python
"""
OCRエンジン設定とキャッシング

PaddleOCRの設定、バージョン検出、結果キャッシング機能を提供
"""
import hashlib
import json
import pickle
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger
import time


class OCRConfig:
    """OCRエンジンの設定管理"""

    # 信頼度閾値
    CONFIDENCE_THRESHOLD_LOW = 0.5      # これ以下は無視
    CONFIDENCE_THRESHOLD_MID = 0.7      # これ以下は再処理
    CONFIDENCE_THRESHOLD_HIGH = 0.85    # これ以上は高品質

    # 並列処理設定
    ENABLE_PARALLEL = True               # 並列処理を有効化
    MAX_WORKERS = 4                      # 最大ワーカー数

    # キャッシング設定
    ENABLE_CACHE = True                  # 結果キャッシングを有効化
    CACHE_DIR = Path("cache/ocr_results")  # キャッシュディレクトリ
    CACHE_TTL = 86400                    # キャッシュ有効期限（秒）

    # OCRエンジン優先順位
    OCR_ENGINE_PRIORITY = [
        'paddleocr',  # 1. PaddleOCR（日本語に強い）
        'surya',      # 2. Surya（レイアウト解析）
        'gemini'      # 3. Gemini Vision（フォールバック）
    ]

    # 画像前処理設定
    ENABLE_PREPROCESSING = True          # 画像前処理を有効化
    PREPROCESSING_QUALITY_THRESHOLD = 0.7  # この品質以下で前処理適用

    # Geminiフォールバック設定
    ENABLE_GEMINI_FALLBACK = True        # 低信頼度時Geminiフォールバック
    GEMINI_FALLBACK_THRESHOLD = 0.6      # この信頼度以下でフォールバック


class OCRResultCache:
    """OCR結果のキャッシング"""

    def __init__(self, cache_dir: Path = OCRConfig.CACHE_DIR, ttl: int = OCRConfig.CACHE_TTL):
        """
        Args:
            cache_dir: キャッシュディレクトリ
            ttl: キャッシュ有効期限（秒）
        """
        self.cache_dir = cache_dir
        self.ttl = ttl
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.hits = 0
        self.misses = 0

    def _get_cache_key(self, image_data: bytes, config: Dict[str, Any]) -> str:
        """
        画像とconfigからキャッシュキーを生成

        Args:
            image_data: 画像のバイトデータ
            config: OCR設定

        Returns:
            SHA256ハッシュキー
        """
        config_str = json.dumps(config, sort_keys=True)
        combined = image_data + config_str.encode('utf-8')
        return hashlib.sha256(combined).hexdigest()

    def get(self, image_data: bytes, config: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        キャッシュから結果を取得

        Args:
            image_data: 画像のバイトデータ
            config: OCR設定

        Returns:
            キャッシュされた結果、または None
        """
        if not OCRConfig.ENABLE_CACHE:
            return None

        cache_key = self._get_cache_key(image_data, config)
        cache_file = self.cache_dir / f"{cache_key}.pkl"

        if not cache_file.exists():
            self.misses += 1
            return None

        try:
            # キャッシュファイルの有効期限確認
            file_age = time.time() - cache_file.stat().st_mtime
            if file_age > self.ttl:
                cache_file.unlink()  # 期限切れ削除
                self.misses += 1
                return None

            # キャッシュ読み込み
            with open(cache_file, 'rb') as f:
                result = pickle.load(f)

            self.hits += 1
            logger.debug(f"[Cache] HIT: {cache_key[:8]}... (age: {file_age:.0f}s)")
            return result

        except Exception as e:
            logger.warning(f"[Cache] 読み込み失敗: {e}")
            self.misses += 1
            return None

    def set(self, image_data: bytes, config: Dict[str, Any], result: Dict[str, Any]):
        """
        結果をキャッシュに保存

        Args:
            image_data: 画像のバイトデータ
            config: OCR設定
            result: OCR結果
        """
        if not OCRConfig.ENABLE_CACHE:
            return

        cache_key = self._get_cache_key(image_data, config)
        cache_file = self.cache_dir / f"{cache_key}.pkl"

        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(result, f)
            logger.debug(f"[Cache] SAVE: {cache_key[:8]}...")
        except Exception as e:
            logger.warning(f"[Cache] 保存失敗: {e}")

    def clear(self):
        """キャッシュをクリア"""
        try:
            for cache_file in self.cache_dir.glob("*.pkl"):
                cache_file.unlink()
            logger.info(f"[Cache] クリア完了: {self.cache_dir}")
        except Exception as e:
            logger.warning(f"[Cache] クリア失敗: {e}")

    def get_stats(self) -> Dict[str, Any]:
        """キャッシュ統計を取得"""
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0

        cache_files = list(self.cache_dir.glob("*.pkl"))
        total_size = sum(f.stat().st_size for f in cache_files)

        return {
            'hits': self.hits,
            'misses': self.misses,
            'total_requests': total,
            'hit_rate': hit_rate,
            'cache_files': len(cache_files),
            'total_size_mb': total_size / (1024 * 1024)
        }


class PaddleOCRVersionAdapter:
    """PaddleOCRバージョン互換性アダプター"""

    @staticmethod
    def detect_version() -> str:
        """PaddleOCRのバージョンを検出"""
        try:
            import paddleocr
            version = getattr(paddleocr, '__version__', 'unknown')
            logger.info(f"[PaddleOCR] Version detected: {version}")
            return version
        except Exception as e:
            logger.warning(f"[PaddleOCR] Version detection failed: {e}")
            return 'unknown'

    @staticmethod
    def extract_result(ocr_result: Any) -> tuple:
        """
        PaddleOCRの結果から統一的にテキストと信頼度を抽出

        Args:
            ocr_result: PaddleOCRの結果オブジェクト

        Returns:
            (texts: List[str], confidences: List[float])
        """
        texts = []
        confidences = []

        try:
            # PaddleOCR 3.x: 辞書ライクオブジェクト
            if isinstance(ocr_result, dict) or hasattr(ocr_result, '__getitem__'):
                rec_texts = ocr_result.get('rec_texts', []) if hasattr(ocr_result, 'get') else ocr_result.get('rec_texts', [])
                rec_scores = ocr_result.get('rec_scores', []) if hasattr(ocr_result, 'get') else ocr_result.get('rec_scores', [])

                if rec_texts:
                    texts = list(rec_texts)
                    confidences = list(rec_scores) if rec_scores else []

            # PaddleOCR 2.x: リスト形式
            elif isinstance(ocr_result, list):
                for line in ocr_result:
                    if line and len(line) >= 2 and line[1]:
                        texts.append(line[1][0])
                        confidences.append(line[1][1])

        except Exception as e:
            logger.warning(f"[PaddleOCR] Result extraction failed: {e}")

        return texts, confidences
```

## shared/pipeline/ocr_report.py

```python
"""
OCR認識精度レポート生成

OCR処理の詳細な統計とレポートを生成
"""
from typing import Dict, Any, List
from dataclasses import dataclass, field
from datetime import datetime
import json


@dataclass
class OCRRegionStats:
    """領域ごとの統計"""
    region_id: int
    bbox: List[int]
    text_length: int
    confidence: float
    preprocessing_applied: bool
    reprocessed: bool = False
    improvement: float = 0.0


@dataclass
class OCRProcessingReport:
    """OCR処理レポート"""
    # 基本情報
    file_name: str
    processing_time: float
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

    # 領域統計
    total_regions: int = 0
    recognized_regions: int = 0
    low_confidence_regions: int = 0
    reprocessed_regions: int = 0
    improved_regions: int = 0

    # 認識統計
    total_chars: int = 0
    avg_confidence: float = 0.0
    min_confidence: float = 1.0
    max_confidence: float = 0.0

    # 前処理統計
    preprocessed_regions: int = 0
    preprocessing_time: float = 0.0

    # 再処理統計
    reprocessing_time: float = 0.0
    avg_improvement: float = 0.0

    # キャッシュ統計
    cache_hits: int = 0
    cache_misses: int = 0

    # 領域詳細
    regions: List[OCRRegionStats] = field(default_factory=list)

    def add_region(self, region_stats: OCRRegionStats):
        """領域統計を追加"""
        self.regions.append(region_stats)
        self.total_regions += 1

        if region_stats.text_length > 0:
            self.recognized_regions += 1
            self.total_chars += region_stats.text_length

        if region_stats.confidence < 0.7:
            self.low_confidence_regions += 1

        if region_stats.preprocessing_applied:
            self.preprocessed_regions += 1

        if region_stats.reprocessed:
            self.reprocessed_regions += 1
            if region_stats.improvement > 0:
                self.improved_regions += 1

        # 信頼度統計更新
        if region_stats.confidence > 0:
            self.min_confidence = min(self.min_confidence, region_stats.confidence)
            self.max_confidence = max(self.max_confidence, region_stats.confidence)

    def calculate_final_stats(self):
        """最終統計を計算"""
        if self.regions:
            confidences = [r.confidence for r in self.regions if r.confidence > 0]
            self.avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0

            improvements = [r.improvement for r in self.regions if r.reprocessed and r.improvement > 0]
            self.avg_improvement = sum(improvements) / len(improvements) if improvements else 0.0

    def to_dict(self) -> Dict[str, Any]:
        """辞書形式に変換"""
        return {
            'file_name': self.file_name,
            'processing_time': self.processing_time,
            'timestamp': self.timestamp,
            'summary': {
                'total_regions': self.total_regions,
                'recognized_regions': self.recognized_regions,
                'recognition_rate': self.recognized_regions / self.total_regions if self.total_regions > 0 else 0,
                'low_confidence_regions': self.low_confidence_regions,
                'total_chars': self.total_chars,
            },
            'confidence': {
                'average': self.avg_confidence,
                'min': self.min_confidence,
                'max': self.max_confidence,
            },
            'preprocessing': {
                'preprocessed_regions': self.preprocessed_regions,
                'preprocessing_rate': self.preprocessed_regions / self.total_regions if self.total_regions > 0 else 0,
                'preprocessing_time': self.preprocessing_time,
            },
            'reprocessing': {
                'reprocessed_regions': self.reprocessed_regions,
                'improved_regions': self.improved_regions,
                'improvement_rate': self.improved_regions / self.reprocessed_regions if self.reprocessed_regions > 0 else 0,
                'avg_improvement': self.avg_improvement,
                'reprocessing_time': self.reprocessing_time,
            },
            'cache': {
                'cache_hits': self.cache_hits,
                'cache_misses': self.cache_misses,
                'cache_hit_rate': self.cache_hits / (self.cache_hits + self.cache_misses) if (self.cache_hits + self.cache_misses) > 0 else 0,
            }
        }

    def to_json(self, indent: int = 2) -> str:
        """JSON文字列に変換"""
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=indent)

    def print_summary(self):
        """サマリーを表示"""
        print("\n" + "=" * 60)
        print(f"OCR Processing Report: {self.file_name}")
        print("=" * 60)
        print(f"Total Regions: {self.total_regions}")
        print(f"Recognized: {self.recognized_regions}/{self.total_regions} ({self.recognized_regions/self.total_regions*100:.1f}%)")
        print(f"Total Characters: {self.total_chars}")
        print(f"Average Confidence: {self.avg_confidence:.2%}")
        print(f"Low Confidence Regions: {self.low_confidence_regions} (< 0.7)")
        print(f"\nPreprocessing:")
        print(f"  Preprocessed: {self.preprocessed_regions}/{self.total_regions} ({self.preprocessed_regions/self.total_regions*100:.1f}%)")
        print(f"  Time: {self.preprocessing_time:.2f}s")
        print(f"\nReprocessing:")
        print(f"  Reprocessed: {self.reprocessed_regions}")
        print(f"  Improved: {self.improved_regions}/{self.reprocessed_regions}")
        if self.reprocessed_regions > 0:
            print(f"  Improvement Rate: {self.improved_regions/self.reprocessed_regions*100:.1f}%")
            print(f"  Avg Improvement: +{self.avg_improvement:.2%}")
        print(f"  Time: {self.reprocessing_time:.2f}s")
        print(f"\nCache:")
        total_cache = self.cache_hits + self.cache_misses
        if total_cache > 0:
            print(f"  Hits: {self.cache_hits}/{total_cache} ({self.cache_hits/total_cache*100:.1f}%)")
        print(f"\nTotal Processing Time: {self.processing_time:.2f}s")
        print("=" * 60 + "\n")
```

## shared/pipeline/pipeline.py

```python
"""
統合ドキュメント処理パイプライン (Stage E-K) - 設定ベース版

設計書: DESIGN_UNIFIED_PIPELINE.md v2.0 に準拠
処理順序: Stage E → F → G → H1 → H2 → J → K

Stage概要:
- Stage E: Pre-processing（テキスト抽出）
- Stage F: Visual Analysis（視覚解析、gemini-2.5-pro）
         - 物理的OCR抽出、JSON出力（カラムナ形式）
- Stage G: Logical Refinement（論理的精錬、gemini-2.0-flash-lite）
         - 重複排除、REF_ID付与、unified_text生成
- Stage H1: Table Specialist（表処理専門）
         - 定型表・構造化表を先に処理
         - カラムナ形式→辞書リスト変換
         - H2への入力量削減のため表テキストを抽出
- Stage H2: Text Specialist（テキスト処理専門、gemini-2.0-flash）
         - 軽量化されたテキストで構造化 + 要約
         - calendar_events, tasks, title, summary を生成
         - audit_canonical_text（監査用正本）を生成
- Stage J: Chunking（チャンク化）
- Stage K: Embedding（ベクトル化）

特徴:
- doc_type / workspace に応じて自動的にプロンプトとモデルを切り替え
- config/ 内の YAML と Markdown ファイルで設定管理
- Stage G で REF_ID付き目録を生成し、後続ステージが参照可能
- H1 + H2 分割によりトークン消費を削減
"""
import asyncio
import json
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger

from shared.ai.llm_client.llm_client import LLMClient
from shared.common.database.client import DatabaseClient
from shared.common.connectors.google_drive import GoogleDriveConnector

from .config_loader import ConfigLoader
from .stage_e_preprocessing import StageEPreprocessor
from .stage_f_visual import StageFVisualAnalyzer
from .stage_g_refiner import StageGRefiner  # Stage G: 論理的精錬
from .stage_hi_combined import StageHICombined  # Stage H+I: 統合版（後方互換）
from .stage_h1_table import StageH1Table  # Stage H1: 表処理専門
from .stage_h2_text import StageH2Text  # Stage H2: テキスト処理専門
from .stage_h_kakeibo import StageHKakeibo  # 家計簿専用
from .stage_j_chunking import StageJChunking
from .stage_k_embedding import StageKEmbedding
from .constants import STAGE_H_INPUT_SCHEMA_VERSION

# Phase 5: Execution versioning
from shared.processing.execution_manager import ExecutionManager, ExecutionContext

# 家計簿専用のDB保存ハンドラー (オプショナル)
try:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from shared.kakeibo.kakeibo_db_handler import KakeiboDBHandler
    KAKEIBO_AVAILABLE = True
except ImportError:
    logger.warning("K_kakeibo module not available, kakeibo features will be disabled")
    KakeiboDBHandler = None
    KAKEIBO_AVAILABLE = False


# ============================================
# v1.1 契約: post_body は Rawdata_FILE_AND_MAIL.display_post_text から取得
# ============================================
def _build_post_body(raw_doc: dict | None) -> dict:
    """
    post_body を Rawdata_FILE_AND_MAIL.display_post_text から直接取得。
    GAS で classroom/gmail/drive 全てこのカラムに本文を保存している。

    Returns:
        { "text": str, "source": str, "char_count": int }
    """
    if not isinstance(raw_doc, dict):
        return {"text": "", "source": "no_raw_doc", "char_count": 0}

    text = (raw_doc.get("display_post_text") or "").strip()
    if text:
        return {"text": text, "source": "rawdata.display_post_text", "char_count": len(text)}

    return {"text": "", "source": "empty", "char_count": 0}


class UnifiedDocumentPipeline:
    """統合ドキュメント処理パイプライン (Stage E-K) - 設定ベース版"""

    @staticmethod
    def _sanitize_text(text: str) -> str:
        """
        テキストからnull文字を除去

        Args:
            text: 入力テキスト

        Returns:
            サニタイズ済みテキスト
        """
        if not text:
            return text
        # null文字 (\u0000) を除去
        return text.replace('\u0000', '')

    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        db_client: Optional[DatabaseClient] = None,
        config_dir: Optional[Path] = None,
        enable_hybrid_ocr: Optional[bool] = None
    ):
        """
        Args:
            llm_client: LLMクライアント（Noneの場合は新規作成）
            db_client: データベースクライアント（Noneの場合は新規作成）
            config_dir: 設定ディレクトリ（デフォルト: G_unified_pipeline/config/）
            enable_hybrid_ocr: ハイブリッドOCR（Surya + PaddleOCR）を有効化（Noneの場合は設定ファイルから取得）
        """
        self.llm_client = llm_client or LLMClient()
        self.db = db_client or DatabaseClient(use_service_role=True)  # RLSバイパスのためService Role使用
        self.drive_connector = GoogleDriveConnector()  # Google Drive ファイル名更新用

        # 設定ローダーを初期化
        self.config = ConfigLoader(config_dir)

        # ハイブリッドOCRの有効/無効を決定
        if enable_hybrid_ocr is None:
            # 設定ファイルから取得（デフォルトの設定）
            enable_hybrid_ocr = self.config.get_hybrid_ocr_enabled('default')

        # 各ステージを初期化
        self.stage_e = StageEPreprocessor(self.llm_client)
        self.stage_f = StageFVisualAnalyzer(self.llm_client, enable_surya=enable_hybrid_ocr)
        self.stage_g = StageGRefiner(self.llm_client)  # Stage G: 論理的精錬
        self.stage_hi = StageHICombined(self.llm_client)  # Stage H+I: 統合版（後方互換）
        self.stage_h1 = StageH1Table(self.llm_client)  # Stage H1: 表処理専門
        self.stage_h2 = StageH2Text(self.llm_client)  # Stage H2: テキスト処理専門
        self.stage_h_kakeibo = StageHKakeibo(self.db)  # 家計簿専用
        self.stage_j = StageJChunking()
        self.stage_k = StageKEmbedding(self.llm_client, self.db)

        # 家計簿専用のDB保存ハンドラー
        self.kakeibo_db_handler = KakeiboDBHandler(self.db) if KAKEIBO_AVAILABLE else None

        logger.info(f"✅ UnifiedDocumentPipeline 初期化完了（E→F→G→H1→H2→J→K, ハイブリッドOCR={'有効' if enable_hybrid_ocr else '無効'}）")

    async def process_document(
        self,
        file_path: Path,
        file_name: str,
        doc_type: str,
        workspace: str,
        mime_type: str,
        source_id: str,
        existing_document_id: Optional[str] = None,
        extra_metadata: Optional[Dict[str, Any]] = None,
        progress_callback=None,
        owner_id: Optional[str] = None,
        enable_execution_tracking: bool = False
    ) -> Dict[str, Any]:
        """
        ドキュメントを処理（Stage E-K）

        Args:
            file_path: ファイルパス
            file_name: ファイル名
            doc_type: ドキュメントタイプ（設定ルーティングに使用）
            workspace: ワークスペース
            mime_type: MIMEタイプ
            source_id: ソースID
            existing_document_id: 更新する既存ドキュメントID（Noneの場合は新規作成）
            extra_metadata: 追加メタデータ（Classroom固有フィールドなど）
            progress_callback: 進捗コールバック
            owner_id: オーナーID（Phase 3 必須 for kakeibo）
            enable_execution_tracking: Phase 5 execution versioning を有効化

        Returns:
            処理結果 {'success': bool, 'document_id': str, ...}
        """
        # Phase 5: Execution tracking 初期化
        execution_context: Optional[ExecutionContext] = None
        execution_manager: Optional[ExecutionManager] = None
        start_time = None

        if enable_execution_tracking:
            import time
            start_time = time.time()
            execution_manager = ExecutionManager(self.db)

        try:
            logger.info(f"📄 ドキュメント処理開始: {file_name} (doc_type={doc_type}, workspace={workspace})")

            # ============================================
            # Stage E: Pre-processing
            # ============================================
            logger.info("[Stage E] Pre-processing開始...")
            if progress_callback:
                progress_callback("E1")

            # extra_metadata から既に抽出済みのテキスト（attachment_text）を取得
            # HTMLファイル等、Ingestion時にテキスト抽出済みの場合に使用
            pre_extracted_text = extra_metadata.get('attachment_text', '') if extra_metadata else ''

            stage_e_result = self.stage_e.extract_text(
                file_path,
                mime_type,
                pre_extracted_text=pre_extracted_text,
                workspace=workspace,
                progress_callback=progress_callback
            )

            # イベントループに制御を返す（並列タスク実行のため）
            await asyncio.sleep(0)

            # Stage E の結果をチェック
            if not stage_e_result.get('success'):
                error_msg = f"Stage E失敗: {stage_e_result.get('error', 'テキスト抽出エラー')}"
                logger.error(f"[Stage E失敗] {error_msg}")
                return {'success': False, 'error': error_msg}

            extracted_text = stage_e_result.get('content', '')
            # P2-2: E-2で検出した表のbbox情報を取得
            stage_e_metadata = stage_e_result.get('metadata', {})
            e2_table_bboxes = stage_e_metadata.get('table_bboxes', [])
            # ログ出力は Stage E 内で既に実施済み

            # ============================================
            # Stage F: Visual Analysis (gemini-2.5-pro で完璧に仕上げる)
            # ============================================
            # post_body 作成（投稿本文 = Stage H 最優先文脈）
            # 【v1.1契約】Rawdata_FILE_AND_MAIL から本文を優先取得
            raw_doc = None
            if existing_document_id:
                try:
                    r = self.db.client.table("Rawdata_FILE_AND_MAIL").select(
                        "id, display_post_text, attachment_text"
                    ).eq("id", existing_document_id).limit(1).execute()
                    if r and getattr(r, "data", None):
                        raw_doc = r.data[0]
                        logger.info(f"[Stage F] raw_doc取得: id={existing_document_id}")
                except Exception as e:
                    logger.warning(f"[Stage F] raw_doc取得失敗: {e.__class__.__name__}: {e}")

            post_body = _build_post_body(raw_doc)
            logger.info(f"[Stage F] post_body作成: {post_body['char_count']}文字 (source: {post_body['source']})")

            # P0-4: Stage F 直前の存在チェック（ファイルがある場合のみ）
            if file_path is not None and not file_path.exists():
                error_msg = f"[P0-4] TEMP_PDF_MISSING: Stage F 入力ファイルが存在しません: {file_path}"
                logger.error(error_msg)
                return {
                    'success': False,
                    'error': error_msg,
                    'failure_stage': 'F',
                    'failure_reason': 'TEMP_PDF_MISSING'
                }

            # 設定から Stage F のプロンプトとモデルを取得
            stage_f_config = self.config.get_stage_config('stage_f', doc_type, workspace)
            prompt_f = stage_f_config['prompt']
            model_f = stage_f_config['model']

            # P0-1: 明示的に file_path を渡す（state 参照禁止）
            logger.info(f"[Stage F] Visual Analysis開始... (model={model_f})")
            if file_path is not None:
                logger.info(f"[P0-1] 入力ファイル: {file_path} (exists={file_path.exists()})")
            else:
                logger.info("[P0-1] 入力ファイル: なし（テキストのみ）")
            # P2-2: E-2のtable_bboxes情報をログ出力
            if e2_table_bboxes:
                logger.info(f"[P2-2] Stage Fへ渡す E-2 table_bboxes: {len(e2_table_bboxes)}個")

            if progress_callback:
                progress_callback("F")

            # Stage Eの判定結果を直接使用（再計算しない）
            requires_vision = stage_e_result.get('requires_vision', False)
            requires_transcription = stage_e_result.get('requires_transcription', False)

            # Stage F 呼び出し（正攻法: 全引数を正しく渡す）
            stage_f_result = self.stage_f.process(
                file_path=file_path,
                mime_type=mime_type or '',
                requires_vision=requires_vision,
                requires_transcription=requires_transcription,
                post_body=post_body,
                progress_callback=progress_callback,
                # YAMLから読み込んだ設定を渡す
                prompt=prompt_f,
                model=model_f,
                extracted_text=extracted_text,
                workspace=workspace,
                e2_table_bboxes=e2_table_bboxes,
                stage_e_metadata=stage_e_metadata  # 【Ver 6.4】座標付き文字情報
            )
            logger.info(f"[Stage F完了] Vision結果: {type(stage_f_result).__name__}")

            # イベントループに制御を返す（並列タスク実行のため）
            await asyncio.sleep(0)

            # ============================================
            # Stage F 結果処理（Dict型を直接使用 - dumps/loads排除）
            # ============================================
            try:
                # Stage F は Dict を直接返す（JSONの往復変換を排除）
                vision_json = stage_f_result
                # DB保存用にJSON文字列も保持（vision_raw）
                vision_raw = json.dumps(stage_f_result, ensure_ascii=False)

                # v1.1契約: Stage F payload をそのまま stage_f_structure として使用（再構成禁止）
                stage_f_structure = vision_json
                schema_ver = vision_json.get('schema_version', '')
                is_v1_1 = (schema_ver == STAGE_H_INPUT_SCHEMA_VERSION)

                if is_v1_1:
                    # v1.1: full_text をそのまま使用（混ぜ物合成禁止）
                    combined_text = vision_json.get('full_text', '')
                    post_body = vision_json.get('post_body', {})
                    text_blocks = vision_json.get('text_blocks', [])

                    logger.info(f"[Stage F→H] v1.1契約モード:")
                    logger.info(f"  ├─ schema_version: {schema_ver}")
                    logger.info(f"  ├─ full_text: {len(combined_text)}文字")
                    logger.info(f"  ├─ post_body: {post_body.get('char_count', 0)}文字 (source: {post_body.get('source', 'unknown')})")
                    logger.info(f"  ├─ text_blocks: {len(text_blocks)}ブロック")
                    logger.info(f"  ├─ text_blocks[0]: {text_blocks[0].get('block_type') if text_blocks else 'N/A'}")
                    logger.info(f"  └─ tables: {len(vision_json.get('tables', []))}個")
                else:
                    # レガシー: 従来の合成ロジック（後方互換）
                    ocr_text = vision_json.get('full_text', '')
                    text_parts = []

                    # 1. 投稿文テキスト（Classroom等のメタデータから）
                    if extra_metadata:
                        display_post_text = extra_metadata.get('display_post_text', '')
                        if display_post_text and display_post_text.strip():
                            text_parts.append(f"[投稿文]\n{display_post_text}")
                            logger.info(f"[Stage F→H] display_post_text追加: {len(display_post_text)}文字")

                    # 2. OCR抽出テキスト
                    if ocr_text and ocr_text.strip():
                        text_parts.append(f"[OCR抽出テキスト]\n{ocr_text}")

                    # 3. 画像の視覚的説明（visual_elements.notes）
                    visual_elements = vision_json.get('visual_elements', {})
                    notes = visual_elements.get('notes', [])
                    if notes:
                        notes_text = '\n'.join(notes)
                        text_parts.append(f"[画像の視覚的説明]\n{notes_text}")
                        logger.info(f"[Stage F→H] visual_elements.notes追加: {len(notes_text)}文字")

                    combined_text = '\n\n'.join(text_parts)

                    logger.info(f"[Stage F→H] レガシーモード:")
                    logger.info(f"  ├─ combined_text: {len(combined_text)}文字")
                    logger.info(f"  ├─ OCR full_text: {len(ocr_text)}文字")
                    logger.info(f"  ├─ sections: {len(vision_json.get('layout_info', {}).get('sections', []))}個")
                    logger.info(f"  └─ tables: {len(vision_json.get('layout_info', {}).get('tables', []))}個")
            except json.JSONDecodeError as e:
                logger.warning(f"[Stage F→H] JSON解析失敗: {e}")
                combined_text = vision_raw
                stage_f_structure = None

            # 空のコンテンツをチェック（空のドキュメントは警告のみ、エラーではない）
            if not combined_text or not combined_text.strip():
                logger.warning(f"[Stage F→H] 統合テキストが空です（テキストのないドキュメントの可能性）")
                combined_text = ""  # 空文字列として継続

            # ============================================
            # Stage G: 論理的精錬（Logical Refinement）
            # ============================================
            # Stage F の出力を整理し、REF_ID付き目録を作成
            stage_g_result = None
            stage_g_config = self.config.get_stage_config('stage_g', doc_type, workspace)

            # Stage G スキップ判定（家計簿や skip 設定がある場合）
            skip_stage_g = stage_g_config.get('skip', False) or doc_type == 'kakeibo'

            if stage_f_structure and not skip_stage_g:
                model_g = stage_g_config.get('model', 'gemini-2.0-flash-lite')
                logger.info(f"[Stage G] 論理的精錬開始... (model={model_g})")
                if progress_callback:
                    progress_callback("G")

                try:
                    # 【Ver 6.4】Stage F に file_path を追加（G1/G2 での画像再読用）
                    if file_path is not None:
                        stage_f_structure['file_path'] = file_path

                    # Stage E + Stage F を Stage G に渡す（v2.0: G-Gate + G1 + G2）
                    stage_g_result = self.stage_g.process(
                        stage_e_result=stage_e_result,
                        stage_f_payload=stage_f_structure,
                        post_body=post_body,
                        model=model_g,
                        workspace=workspace
                    )

                    # Stage G の出力をログ
                    logger.info(f"[Stage G完了] ref_count={stage_g_result.get('ref_count', 0)}, mode={stage_g_result.get('processing_mode', 'unknown')}")

                    # 【Ver 6.4】監査ログをログ出力
                    audit_log = stage_g_result.get('audit_log', {})
                    if audit_log:
                        logger.info(f"[Stage G] 【Ver 6.4】監査ログ: 洗替{audit_log.get('total_scrubbed', 0)}件, 異常解決{audit_log.get('total_anomalies_resolved', 0)}件")

                    # Stage G の unified_text を combined_text として使用（後続に渡す）
                    if stage_g_result.get('unified_text'):
                        combined_text = stage_g_result['unified_text']
                        logger.info(f"[Stage G→H] unified_text: {len(combined_text)}文字")

                    # Stage G の source_inventory を stage_f_structure に追加（Stage Hで参照可能に）
                    if stage_g_result.get('source_inventory'):
                        stage_f_structure['source_inventory'] = stage_g_result['source_inventory']
                        logger.info(f"[Stage G→H] source_inventory: {len(stage_g_result['source_inventory'])}件")

                    if stage_g_result.get('table_inventory'):
                        stage_f_structure['table_inventory'] = stage_g_result['table_inventory']
                        logger.info(f"[Stage G→H] table_inventory: {len(stage_g_result['table_inventory'])}件")

                    # 警告があれば出力
                    for warning in stage_g_result.get('warnings', []):
                        logger.warning(f"[Stage G警告] {warning}")

                except Exception as e:
                    logger.warning(f"[Stage G] 処理失敗、スキップして続行: {e}")
                    # Stage G が失敗しても Stage H は続行可能

                # イベントループに制御を返す（並列タスク実行のため）
                await asyncio.sleep(0)
            elif skip_stage_g:
                logger.info(f"[Stage G] スキップ (doc_type={doc_type}, skip={stage_g_config.get('skip', False)})")

            # ============================================
            # Stage H+I: 構造化 + 統合・要約
            # ============================================
            # custom_handler の確認（ルート設定から直接取得、model は取得しない）
            route_config = self.config.get_route_config(doc_type, workspace)
            stage_h_routing = route_config.get('stages', {}).get('stage_h', {})
            custom_handler = stage_h_routing.get('custom_handler')

            # 家計簿専用処理の場合（統合版は使わない）
            if custom_handler == 'kakeibo':
                # 家計簿の場合のみ stage_h_config を取得
                stage_h_config = self.config.get_stage_config('stage_h', doc_type, workspace)
                logger.info(f"[Stage H] 家計簿構造化開始... (custom_handler=kakeibo)")
                if progress_callback:
                    progress_callback("H")

                # Stage F の出力を辞書に変換（combined_text が JSON 文字列の場合）
                # ※ json, re はモジュールレベルでインポート済み
                try:
                    # Markdownのコードブロック (```json ... ```) を除去
                    json_text = combined_text.strip()
                    if json_text.startswith('```'):
                        # 最初と最後の```を除去
                        json_text = re.sub(r'^```(?:json)?\s*\n', '', json_text)
                        json_text = re.sub(r'\n```\s*$', '', json_text)

                    logger.debug(f"[Stage H] JSON パース前の最初の500文字:\n{json_text[:500]}")
                    stage_f_output = json.loads(json_text)
                except (json.JSONDecodeError, TypeError) as e:
                    logger.error(f"[Stage H] combined_text が JSON 形式ではありません: {e}")
                    logger.error(f"[Stage H] combined_text の内容:\n{combined_text[:1000]}")
                    raise ValueError("Stage F output must be JSON for kakeibo processing")

                # 家計簿専用 Stage H で処理
                stageH_result = self.stage_h_kakeibo.process(stage_f_output)

                # イベントループに制御を返す（並列タスク実行のため）
                await asyncio.sleep(0)

                # 家計簿専用のDB保存
                if self.kakeibo_db_handler:
                    # Phase 3: owner_id 必須チェック
                    if not owner_id:
                        raise ValueError("owner_id is required for kakeibo processing (Phase 3)")

                    logger.info("[DB保存] 家計簿データをDBに保存...")
                    kakeibo_save_result = self.kakeibo_db_handler.save_receipt(
                        stage_h_output=stageH_result,
                        file_name=file_name,
                        drive_file_id=source_id,
                        model_name=stage_h_config['model'],
                        source_folder=workspace,
                        owner_id=owner_id
                    )
                    logger.info(f"[DB保存完了] receipt_id={kakeibo_save_result['receipt_id']}")
                else:
                    logger.warning("K_kakeibo module not available, skipping kakeibo DB save")

                # 家計簿は Rawdata_FILE_AND_MAIL に保存せず、ここで終了
                return {
                    'success': True,
                    'receipt_id': kakeibo_save_result['receipt_id'],
                    'transaction_ids': kakeibo_save_result['transaction_ids'],
                    'log_id': kakeibo_save_result['log_id'],
                    'doc_type': 'kakeibo'
                }

            # ============================================
            # Stage H1 + H2: 分割処理（トークン消費削減版）
            # ============================================
            else:
                stage_hi_config = self.config.get_stage_config('stage_hi', doc_type, workspace)
                prompt_hi = stage_hi_config['prompt']
                model_hi = stage_hi_config['model']

                # -----------------------------------------
                # アンカーベースのH1/H2ルーティング
                # -----------------------------------------
                # Stage F からアンカー配列を取得（新形式）
                anchors = stage_f_structure.get('anchors', []) if stage_f_structure else []

                routing_result = self.stage_g.route_anchors_to_stages(
                    stage_g_result=stage_g_result or {},
                    anchors=anchors
                )

                h1_payload = routing_result.get('h1_payload', {})
                h2_payload = routing_result.get('h2_payload', {})
                anchor_map = routing_result.get('anchor_map', {})

                logger.info(f"[Stage G→H] ルーティング完了: H1={len(h1_payload.get('heavy_tables', []))}表, H2テキスト={len(h2_payload.get('text_anchors', []))}件")

                # -----------------------------------------
                # Stage H1: 表処理専門（重い表のみ）
                # -----------------------------------------
                heavy_tables = h1_payload.get('heavy_tables', [])
                table_inventory = stage_g_result.get('table_inventory', []) if stage_g_result else []

                logger.info(f"[Stage H1] 表処理開始... (重い表: {len(heavy_tables)}件, 全表: {len(table_inventory)}件)")
                if progress_callback:
                    progress_callback("H1")

                h1_result = self.stage_h1.process(
                    table_inventory=table_inventory,
                    doc_type=doc_type,
                    workspace=workspace,
                    unified_text=combined_text
                )

                # H1の結果をログ
                h1_stats = h1_result.get('statistics', {})
                logger.info(f"[Stage H1完了] processed={h1_stats.get('processed', 0)}, "
                           f"extracted_meta_keys={list(h1_result.get('extracted_metadata', {}).keys())}")

                # イベントループに制御を返す
                await asyncio.sleep(0)

                # -----------------------------------------
                # H2用にテキストを軽量化（アンカーベース）
                # -----------------------------------------
                # 方法1: Stage G のルーティング結果を使用
                reduced_text = h2_payload.get('reduced_text', '')

                # 方法2: フォールバック（従来のフラグメントベース）
                if not reduced_text:
                    reduced_text = combined_text
                    table_text_fragments = h1_result.get('table_text_fragments', [])

                    if table_text_fragments:
                        reduced_text = self.stage_h1.remove_table_text_from_unified(
                            combined_text, table_text_fragments
                        )

                logger.info(f"[Stage H1→H2] テキスト軽量化: {len(combined_text)}→{len(reduced_text)}文字 (-{(len(combined_text) - len(reduced_text)) * 100 // max(len(combined_text), 1)}%)")

                # -----------------------------------------
                # Stage H2: テキスト処理専門
                # -----------------------------------------
                logger.info(f"[Stage H2] テキスト処理開始... (model={model_hi})")
                if progress_callback:
                    progress_callback("H2")

                stageHI_result = self.stage_h2.process(
                    file_name=file_name,
                    doc_type=doc_type,
                    workspace=workspace,
                    reduced_text=reduced_text,
                    prompt=prompt_hi,
                    model=model_hi,
                    h1_result=h1_result,
                    stage_f_structure=stage_f_structure,
                    stage_g_result=stage_g_result
                )

                # イベントループに制御を返す（並列タスク実行のため）
                await asyncio.sleep(0)

                # Stage H2 の結果をチェック
                if not stageHI_result or not isinstance(stageHI_result, dict):
                    error_msg = "Stage H2失敗: 結果が不正です"
                    logger.error(f"[Stage H2失敗] {error_msg}")
                    return {'success': False, 'error': error_msg}

                # 結果を変数に展開（後続処理との互換性のため）
                document_date = stageHI_result.get('document_date')
                tags = stageHI_result.get('tags', [])
                stageH_metadata = stageHI_result.get('metadata', {})
                title = stageHI_result.get('title', '')
                summary = stageHI_result.get('summary', '')
                relevant_date = stageHI_result.get('document_date')

                # カレンダーイベントとタスクを取得
                calendar_events = stageHI_result.get('calendar_events', [])
                tasks = stageHI_result.get('tasks', [])

                # metadataに追加
                stageH_metadata['calendar_events'] = calendar_events
                stageH_metadata['tasks'] = tasks

                # audit_canonical_text があれば metadata に追加
                audit_text = stageHI_result.get('audit_canonical_text', '')
                if audit_text:
                    stageH_metadata['audit_canonical_text'] = audit_text

                # H1処理統計を追加
                stageH_metadata['_h1_h2_split'] = True
                stageH_metadata['_h1_statistics'] = h1_stats

                # 【Ver 6.4】監査ログをメタデータに追加
                if stage_g_result and stage_g_result.get('audit_log'):
                    stageH_metadata['_v64_audit_log'] = stage_g_result['audit_log']

                logger.info(f"[Stage H1+H2完了] title={title[:30] if title else 'N/A'}..., "
                           f"calendar_events={len(calendar_events)}件, tasks={len(tasks)}件")

                # Stage H 互換の結果オブジェクトを作成
                stageH_result = {
                    'document_date': document_date,
                    'tags': tags,
                    'metadata': stageH_metadata
                }

            # ============================================
            # Google Drive ファイル名更新（タイトルに基づく）
            # ============================================
            if title and source_id and file_name:
                # ファイル名から拡張子を抽出
                import os
                file_extension = os.path.splitext(file_name)[1]  # 例: ".pdf"

                # 新しいファイル名を生成（タイトル + 拡張子）
                new_file_name = title + file_extension

                # Google Drive のファイル名を更新
                try:
                    self.drive_connector.rename_file(source_id, new_file_name)
                    logger.info(f"[Google Drive] ファイル名更新成功: {new_file_name}")
                except Exception as e:
                    # ファイル名更新失敗はエラーログのみ（処理は継続）
                    logger.warning(f"[Google Drive] ファイル名更新失敗: {e}")

            # ============================================
            # Stage J: Chunking
            # ============================================
            logger.info("[Stage J] チャンク化開始...")
            if progress_callback:
                progress_callback("J")
            chunks = self.stage_j.process(
                display_subject=extra_metadata.get('display_subject', file_name) if extra_metadata else file_name,
                summary=summary,
                tags=tags,
                document_date=document_date,
                metadata=stageH_metadata
            )
            logger.info(f"[Stage J完了] チャンク数: {len(chunks)}")

            # イベントループに制御を返す（並列タスク実行のため）
            await asyncio.sleep(0)

            # ============================================
            # DB保存: Rawdata_FILE_AND_MAIL
            # ============================================
            document_id = existing_document_id
            try:
                # 既存ドキュメントの attachment_text, metadata, display_* フィールドを取得（nullで上書きしないため）
                existing_attachment_text = None
                existing_metadata = {}
                existing_display_fields = {}
                if existing_document_id:
                    try:
                        existing_doc = self.db.client.table('Rawdata_FILE_AND_MAIL').select(
                            'attachment_text, metadata, display_sender, display_sender_email, display_subject, display_sent_at, display_post_text'
                        ).eq('id', existing_document_id).execute()
                        if existing_doc.data and len(existing_doc.data) > 0:
                            doc = existing_doc.data[0]
                            existing_attachment_text = doc.get('attachment_text', '')
                            # 既存 metadata を保持（message_id, thread_id, subject など）
                            existing_metadata = doc.get('metadata', {})
                            if isinstance(existing_metadata, str):
                                existing_metadata = json.loads(existing_metadata)
                            # display_* フィールドを保持
                            existing_display_fields = {
                                'display_sender': doc.get('display_sender'),
                                'display_sender_email': doc.get('display_sender_email'),
                                'display_subject': doc.get('display_subject'),
                                'display_sent_at': doc.get('display_sent_at'),
                                'display_post_text': doc.get('display_post_text')
                            }
                            logger.debug(f"[DB保存] 既存attachment_text取得: {len(existing_attachment_text or '')}文字")
                            logger.debug(f"[DB保存] 既存metadata取得: {list(existing_metadata.keys())}")
                            logger.debug(f"[DB保存] 既存display_*フィールド取得: sender={existing_display_fields.get('display_sender')}, subject={existing_display_fields.get('display_subject')}")
                    except Exception as e:
                        logger.warning(f"[DB保存警告] 既存フィールド取得失敗: {e}")

                # テキストフィールドをサニタイズ（null文字を除去）
                sanitized_combined_text = self._sanitize_text(combined_text)
                sanitized_summary = self._sanitize_text(summary)
                sanitized_extracted_text = self._sanitize_text(extracted_text)

                # Stage F の出力をパース（JSONから各要素を抽出）
                stage_f_text_ocr = None
                stage_f_layout_ocr = None
                stage_f_visual_elements = None
                try:
                    if vision_raw and stage_f_structure:
                        # full_text を text OCR として保存
                        stage_f_text_ocr = self._sanitize_text(stage_f_structure.get('full_text', ''))

                        # v1.1契約: sections/tables の取得場所を適切に
                        sf_schema = stage_f_structure.get('schema_version', '')
                        if sf_schema == STAGE_H_INPUT_SCHEMA_VERSION:
                            # v1.1: layout_info.sections, トップレベル tables
                            layout_info = stage_f_structure.get('layout_info', {})
                            sections = layout_info.get('sections', [])
                            tables = stage_f_structure.get('tables', [])
                        else:
                            # レガシー: 直下または layout_info から取得（後方互換）
                            sections = stage_f_structure.get('sections', []) or stage_f_structure.get('layout_info', {}).get('sections', [])
                            tables = stage_f_structure.get('tables', []) or stage_f_structure.get('layout_info', {}).get('tables', [])

                        stage_f_layout_ocr = json.dumps({
                            'sections': sections,
                            'tables': tables
                        }, ensure_ascii=False, indent=2)

                        # visual_elements をそのまま保存
                        stage_f_visual_elements = json.dumps(
                            stage_f_structure.get('visual_elements', {}),
                            ensure_ascii=False,
                            indent=2
                        )
                except Exception as e:
                    logger.warning(f"[DB保存警告] Stage F出力のパースに失敗: {e}")

                # Stage Eが空の場合、Stage Fのfull_textを使用
                if not sanitized_extracted_text and stage_f_text_ocr:
                    logger.info("[DB保存] Stage Eが空のため、Stage Fのfull_textを使用")
                    sanitized_extracted_text = stage_f_text_ocr

                # Stage F アンカー配列を取得
                stage_f_anchors = None
                if stage_f_structure and 'anchors' in stage_f_structure:
                    stage_f_anchors = stage_f_structure.get('anchors', [])

                # Stage G 結果を取得
                stage_g_result_json = None
                if stage_g_result:
                    stage_g_result_json = {
                        'source_inventory': stage_g_result.get('source_inventory', []),
                        'table_inventory': stage_g_result.get('table_inventory', []),
                        'cross_validation': stage_g_result.get('cross_validation', {}),
                        'processing_mode': stage_g_result.get('processing_mode', '')
                    }

                # Stage H1 結果を取得
                stage_h1_tables_json = None
                if 'h1_result' in dir() and h1_result:
                    stage_h1_tables_json = {
                        'processed_tables': h1_result.get('processed_tables', []),
                        'extracted_metadata': h1_result.get('extracted_metadata', {}),
                        'statistics': h1_result.get('statistics', {})
                    }

                # titleをサニタイズ
                sanitized_title = self._sanitize_text(title)

                # attachment_text の決定ロジック
                # - Stage Eが正当にテキストを抽出した場合（sanitized_combined_text が空でない）→ 使用（正当な上書き）
                # - Stage Eが失敗した場合（sanitized_combined_text が空）→ 既存値を保持（nullで上書きしない）
                final_attachment_text = sanitized_combined_text
                if not sanitized_combined_text and existing_attachment_text:
                    final_attachment_text = existing_attachment_text
                    logger.info(f"[DB保存] Stage Eが空のため、既存attachment_textを保持: {len(final_attachment_text)}文字")

                # metadata のマージロジック
                # 既存の metadata（message_id, thread_id, subject など）を保持しつつ、
                # Stage H の metadata（LLMが生成した構造化データ）を追加
                final_metadata = {}
                if existing_document_id and existing_metadata:
                    # 既存の metadata をベースにする
                    final_metadata = existing_metadata.copy()
                    logger.info(f"[DB保存] 既存metadataを保持: {list(existing_metadata.keys())}")
                # Stage H の metadata を追加・更新
                if stageH_metadata:
                    final_metadata.update(stageH_metadata)
                    logger.info(f"[DB保存] Stage H metadataをマージ: {list(stageH_metadata.keys())}")

                doc_data = {
                    'source_id': source_id,
                    'source_type': 'unified_pipeline',
                    'file_name': file_name,
                    'workspace': workspace,
                    'doc_type': doc_type,
                    'title': sanitized_title,
                    'attachment_text': final_attachment_text,
                    'summary': sanitized_summary,
                    'tags': tags,
                    'document_date': document_date,
                    'metadata': final_metadata,
                    'processing_status': 'completed',
                    # 各ステージの出力を保存（新スキーマ 2026-01-27）
                    'stage_e_text': sanitized_extracted_text,  # Stage E: 物理抽出テキスト（E-1〜E-3統合）
                    'stage_f_text_ocr': stage_f_text_ocr,        # Stage F: Path A テキスト抽出
                    'stage_f_layout_ocr': stage_f_layout_ocr,    # Stage F: レイアウト情報
                    'stage_f_visual_elements': stage_f_visual_elements,  # Stage F: 視覚要素
                    'stage_f_anchors': json.dumps(stage_f_anchors, ensure_ascii=False) if stage_f_anchors else None,  # Stage F: アンカー配列
                    'stage_g_result': json.dumps(stage_g_result_json, ensure_ascii=False) if stage_g_result_json else None,  # Stage G: 統合精錬結果
                    'stage_h_normalized': reduced_text if 'reduced_text' in dir() else sanitized_combined_text,  # Stage H2: 軽量化済み入力
                    'stage_h1_tables': json.dumps(stage_h1_tables_json, ensure_ascii=False) if stage_h1_tables_json else None,  # Stage H1: 処理済み表
                    'stage_h_result': json.dumps(stageH_result, ensure_ascii=False, indent=2) if stageH_result else None,  # Stage H2: 構造化結果
                    'stage_j_chunks_json': json.dumps(chunks, ensure_ascii=False, indent=2)  # Stage J: チャンク化結果
                }

                # 既存ドキュメントの場合、display_* フィールドを保持（Gmail ingestion時に設定された値を上書きしないため）
                if existing_document_id and existing_display_fields:
                    for key, value in existing_display_fields.items():
                        if value is not None:  # Noneでない値のみ保持
                            doc_data[key] = value
                    logger.debug(f"[DB保存] display_*フィールドを保持: {list(existing_display_fields.keys())}")

                # extra_metadata をマージ
                if extra_metadata:
                    # display_*フィールドは最上位フィールドとして保存
                    display_fields = ['display_subject', 'display_sender', 'display_sender_email', 'display_sent_at', 'display_post_text', 'display_type']
                    for field in display_fields:
                        if field in extra_metadata and extra_metadata[field] is not None:
                            doc_data[field] = extra_metadata[field]
                            logger.debug(f"[DB保存] extra_metadataから{field}を設定: {extra_metadata[field]}")

                    # display_*以外のフィールドはmetadataにマージ
                    other_metadata = {k: v for k, v in extra_metadata.items() if k not in display_fields}
                    if other_metadata:
                        if isinstance(doc_data['metadata'], dict):
                            doc_data['metadata'].update(other_metadata)
                        else:
                            doc_data['metadata'] = other_metadata

                # 既存ドキュメントを更新 or 新規作成
                if existing_document_id:
                    logger.info(f"[DB更新] 既存ドキュメント更新: {existing_document_id}")
                    # IDを除外してUPDATE（IDは変更不可）
                    update_data = {k: v for k, v in doc_data.items() if k != 'id'}
                    result = self.db.client.table('Rawdata_FILE_AND_MAIL').update(update_data).eq('id', existing_document_id).execute()
                    if result.data and len(result.data) > 0:
                        document_id = result.data[0]['id']
                        logger.info(f"[DB更新完了] Rawdata_FILE_AND_MAIL ID: {document_id}")
                    else:
                        logger.error("[DB更新エラー] ドキュメント更新失敗")
                        return {'success': False, 'error': 'Document update failed'}
                else:
                    logger.info("[DB保存] 新規ドキュメント作成")
                    result = self.db.client.table('Rawdata_FILE_AND_MAIL').insert(doc_data).execute()
                    if result.data and len(result.data) > 0:
                        document_id = result.data[0]['id']
                        logger.info(f"[DB保存] Rawdata_FILE_AND_MAIL ID: {document_id}")
                    else:
                        logger.error("[DB保存エラー] ドキュメント作成失敗")
                        return {'success': False, 'error': 'Document creation failed'}

            except Exception as e:
                logger.error(f"[DB保存エラー] {e}")
                return {'success': False, 'error': str(e)}

            # ============================================
            # Stage K: Embedding
            # ============================================
            logger.info("[Stage K] ベクトル化開始...")
            if progress_callback:
                progress_callback("K")

            # 既存ドキュメントの場合は、古いチャンクを削除
            if existing_document_id:
                try:
                    logger.info(f"[Stage K] 既存チャンク削除: document_id={document_id}")
                    self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
                except Exception as e:
                    logger.warning(f"[Stage K 警告] 既存チャンク削除エラー（継続）: {e}")

            # 新しいチャンクを保存
            stage_k_result = self.stage_k.embed_and_save(document_id, chunks)

            # Stage K の結果をチェック（厳格モード: 1つでも失敗したら全体失敗）
            if not stage_k_result.get('success'):
                error_msg = f"Stage K失敗: {stage_k_result.get('failed_count', 0)}/{len(chunks)}チャンク保存失敗"
                logger.error(f"[Stage K失敗] {error_msg}")
                return {'success': False, 'error': error_msg}

            # 部分的失敗は警告として扱う（一部のチャンクは保存済み）
            failed_count = stage_k_result.get('failed_count', 0)
            saved_count = stage_k_result.get('saved_count', 0)
            if failed_count > 0:
                logger.warning(f"[Stage K警告] 部分的な失敗: {failed_count}/{len(chunks)}チャンク保存失敗（{saved_count}チャンクは保存済み）")
                # 失敗したが、一部は成功しているので継続

            logger.info(f"[Stage K完了] {stage_k_result.get('saved_count', 0)}/{len(chunks)}チャンク保存")

            # Phase 5: Execution tracking - 成功時
            if enable_execution_tracking and execution_manager and owner_id and document_id:
                import time
                duration_ms = int((time.time() - start_time) * 1000) if start_time else None

                # execution 作成（処理完了後に作成、即座に succeeded）
                try:
                    exec_ctx = execution_manager.create_execution(
                        document_id=document_id,
                        owner_id=owner_id,
                        input_text=combined_text if 'combined_text' in dir() else '',
                        model_version=stage_h_config.get('model') if 'stage_h_config' in dir() else None,
                        normalized_text=combined_text if 'combined_text' in dir() else ''
                    )
                    execution_manager.mark_succeeded(
                        execution_id=exec_ctx.execution_id,
                        result_data={
                            'summary': summary,
                            'tags': tags,
                            'document_date': document_date if 'document_date' in dir() else None,
                            'metadata': stageH_metadata if 'stageH_metadata' in dir() else {},
                            'chunks_count': stage_k_result.get('saved_count', 0)
                        },
                        processing_duration_ms=duration_ms
                    )
                    logger.info(f"[Phase 5] Execution 記録完了: {exec_ctx.execution_id[:8]}...")
                except Exception as exec_e:
                    logger.warning(f"[Phase 5] Execution 記録エラー（継続）: {exec_e}")

            return {
                'success': True,
                'document_id': document_id,
                'summary': summary,
                'tags': tags,
                'chunks_count': stage_k_result.get('saved_count', 0)
            }

        except Exception as e:
            logger.error(f"[パイプラインエラー] {e}", exc_info=True)

            # Phase 5: Execution tracking - 失敗時
            if enable_execution_tracking and execution_manager and owner_id:
                import time
                duration_ms = int((time.time() - start_time) * 1000) if start_time else None
                try:
                    # 既存 document_id がある場合のみ execution を記録
                    doc_id = existing_document_id or (document_id if 'document_id' in dir() else None)
                    if doc_id:
                        exec_ctx = execution_manager.create_execution(
                            document_id=doc_id,
                            owner_id=owner_id,
                            input_text='',  # 失敗時は入力が不明な場合がある
                            model_version=None
                        )
                        execution_manager.mark_failed(
                            execution_id=exec_ctx.execution_id,
                            error_code='PIPELINE_ERROR',
                            error_message=str(e),
                            processing_duration_ms=duration_ms
                        )
                        logger.info(f"[Phase 5] 失敗 Execution 記録: {exec_ctx.execution_id[:8]}...")
                except Exception as exec_e:
                    logger.warning(f"[Phase 5] 失敗 Execution 記録エラー（継続）: {exec_e}")

            return {'success': False, 'error': str(e)}
```

## shared/pipeline/prompts/__init__.py

```python
"""
G_unified_pipeline prompts module
"""

# Stage G: レシート構造化プロンプト（家計簿用）
STAGE_G_FORMATTING_PROMPT = """あなたはレシートのOCR結果から構造化データを生成する専門家です。

以下は Stage F（OCR）で抽出されたレシート全文です：

---
{vision_raw}
---

このテキストから、レシート情報を構造化してJSON形式で出力してください。

## 構造化タスク

### 1. 店舗情報の抽出
- 店舗名
- 住所
- 電話番号
- 店舗コード（記載されていれば）

### 2. 取引情報の抽出
- 日付（YYYY-MM-DD形式）
- 時刻（HH:MM:SS形式、記載されていれば）
- レジ番号
- レシート番号
- 取引番号

### 3. 商品明細の抽出
各商品について：
- 行番号（レシート記載の順序）
- 商品名（レシート記載のまま）
- 数量
- 単価
- 金額
- **税率マーク（最重要！）**
  - 商品名の横や行内にある記号を必ず抽出してください
  - 例: `*`, `※`, `★`, `◆`, `8%`, `10%`, `(軽)` など
  - マークがない場合は null
  - **注意**: `*`（アスタリスク）も`※`（米印）もすべて抽出すること

**重要な注意事項（数量情報行の処理）**：
- 括弧で囲まれた数量情報（例: `(2個 X 単128)`, `(3パック X 単200)`）は**独立した商品ではない**
- これらは**直前の商品の数量詳細**なので、独立した行として抽出しないこと
- 数量情報は直前の商品の quantity と unit_price に反映させること

**重要な注意事項（小計・合計行の識別）**：
- 以下の行は**商品ではなく、集計行**です。独立した商品として抽出しないでください：
  - 「小計」「小 計」
  - 「合計」「合 計」
  - 「お買上」「お買上げ」「お買い上げ」
  - 「計」
  - これらの行には `line_type: "SUBTOTAL"` または `line_type: "TOTAL"` を設定してください

### 4. 金額情報の抽出
**重要：レシートに記載されている数値をそのまま抽出（計算・推測禁止）**

- 小計（税抜小計、記載されていれば）
- 8%対象額（税抜）
- 8%消費税額
- 10%対象額（税抜）
- 10%消費税額
- 消費税合計
- 合計（支払額）
- お預かり（記載されていれば）
- お釣り（記載されていれば）

### 5. 支払情報
- 支払方法（現金/カード/電子マネー等）
- カード情報（記載されていれば）

### 6. その他情報
- ポイント情報
- キャンペーン情報
- バーコード番号
- **税率説明**（重要！）
  - レシート下部の税率に関する説明文を抽出
  - 例: 「*印は軽減税率（8%）適用商品」「※は軽減税率対象」など
  - 記載がない場合は null
- その他の特記事項

## 出力形式

以下のJSON形式で出力してください：

```json
{{
  "shop_info": {{
    "name": "店舗名",
    "address": "住所（記載があれば）",
    "phone": "電話番号（記載があれば）",
    "store_code": "店舗コード（記載があれば）"
  }},
  "transaction_info": {{
    "date": "YYYY-MM-DD",
    "time": "HH:MM:SS（記載があれば）",
    "register_number": "レジ番号（記載があれば）",
    "receipt_number": "レシート番号（記載があれば）",
    "transaction_number": "取引番号（記載があれば）"
  }},
  "items": [
    {{
      "line_number": 1,
      "line_text": "レシートのこの行のテキストそのまま",
      "product_name": "商品名",
      "quantity": 1,
      "unit_price": 100,
      "amount": 100,
      "tax_mark": "※または★またはなし"
    }}
  ],
  "amounts": {{
    "subtotal": 1377,
    "tax_8_base": 0,
    "tax_8_amount": 0,
    "tax_10_base": 1377,
    "tax_10_amount": 123,
    "total_tax": 123,
    "total": 1500,
    "received": 2000,
    "change": 500
  }},
  "payment": {{
    "method": "現金",
    "card_info": null
  }},
  "other_info": {{
    "points": "ポイント情報",
    "campaign": "キャンペーン情報",
    "barcode": "バーコード番号",
    "tax_rate_note": "*印は軽減税率（8%）適用商品",
    "notes": "その他特記事項"
  }}
}}
```

## 重要な注意事項

1. **数値は必ずレシート記載のまま**
   - 計算しない、推測しない
   - 記載がない項目は null にする

2. **8%/10%税率の区分（最重要！）**
   - **各商品行の税率マークを必ず確認して抽出すること**
   - マークの例: `*`, `※`, `★`, `◆`, `8%`, `10%`, `(軽)` など
   - **`*`（アスタリスク）と`※`（米印）は別の文字として扱うこと**
   - 商品名の横、行の末尾、数量の後など、どこにあっても抽出すること
   - レシート下部に「*印は軽減税率（8%）適用商品」のような説明があれば、other_info.tax_rate_note に抽出すること
   - 記載がない場合のみ null にする（推測しない）

3. **小計・消費税・合計の関係**
   - レシートに「小計」「消費税」「合計」が明記されている場合、その数値をそのまま使う
   - 外税レシート：小計 + 消費税 = 合計
   - 内税レシート：合計のみ（小計は記載なしの場合あり）

4. **税額サマリー**
   - 「8%対象 ○○円（税 △△円）」のような記載があれば必ず抽出
   - 「10%対象 ○○円（税 △△円）」のような記載があれば必ず抽出

5. **記載がない項目**
   - 記載がない項目は null にする
   - 空文字列や0ではなく null を使う

## エラー処理

- OCR結果が不完全な場合：`{{"error": "incomplete_ocr", "details": "不完全な箇所の説明"}}`
- レシートとして認識できない場合：`{{"error": "not_a_receipt"}}`
"""
```

## shared/pipeline/stage_e_preprocessing.py

```python
"""
Stage E: Pre-processing (前処理) - 物理抽出専用（AI排除版）

【設計 2026-01-26】コスト最適化のためAIを完全排除

役割: ファイルからの「物理的テキスト抽出」のみ
- PDF: pdfplumber / PyMuPDF でテキスト抽出
- Office: OfficeProcessor でテキスト抽出
- テキスト: エンコーディング検出して読み込み
- 画像/音声/動画: raw_text="" で返す（Stage F-7で処理）

処理フロー（E-3で終了）:
- E-1: ファイル検証
- E-2: MIMEタイプルーティング
- E-3: 物理テキスト抽出（AI不使用）

【重要】
- 添付なしの場合は Stage E をスキップし、Stage G からスタート
- 画像/音声/動画は raw_text="" で返し、Stage F-7 で AI 処理
"""
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger

from shared.common.processors.pdf import PDFProcessor
from shared.common.processors.office import OfficeProcessor


class StageEPreprocessor:
    """Stage E: 前処理（物理抽出専用、AI排除版）"""

    # MIMEタイプ分類
    DOCUMENT_MIME_TYPES = {
        'application/pdf',
        'text/plain',
        'text/html',
        'text/csv',
        'text/markdown',
        'application/vnd.openxmlformats-officedocument.wordprocessingml.document',  # .docx
        'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',        # .xlsx
        'application/vnd.openxmlformats-officedocument.presentationml.presentation',  # .pptx
        'application/msword',  # .doc
        'application/vnd.ms-excel',  # .xls
        'application/vnd.ms-powerpoint',  # .ppt
    }

    IMAGE_MIME_TYPES = {
        'image/png',
        'image/jpeg',
        'image/jpg',
        'image/gif',
        'image/webp',
        'image/bmp',
        'image/tiff',
    }

    AUDIO_MIME_TYPES = {
        'audio/mpeg',
        'audio/mp3',
        'audio/wav',
        'audio/x-wav',
        'audio/ogg',
        'audio/flac',
        'audio/aac',
        'audio/m4a',
        'audio/x-m4a',
        'audio/webm',
    }

    VIDEO_MIME_TYPES = {
        'video/mp4',
        'video/mpeg',
        'video/quicktime',
        'video/x-msvideo',
        'video/webm',
        'video/x-matroska',
        'video/avi',
        'video/mov',
    }

    def __init__(self, llm_client=None):
        """
        Args:
            llm_client: LLMクライアント（後方互換性のため残すが使用しない）
        """
        # AI排除版: llm_client は使用しない
        self.pdf_processor = PDFProcessor(llm_client=None)  # AI不使用
        self.office_processor = OfficeProcessor()

    def extract_text(
        self,
        file_path: Path,
        mime_type: str,
        pre_extracted_text: Optional[str] = None,
        workspace: Optional[str] = None,
        progress_callback=None
    ) -> Dict[str, Any]:
        """
        ファイルからテキストを物理的に抽出（AI不使用）

        Args:
            file_path: ファイルパス
            mime_type: MIMEタイプ
            pre_extracted_text: 既に抽出済みのテキスト（HTML→PNG等の場合）
            workspace: ワークスペース（未使用、後方互換性のため残す）
            progress_callback: 進捗コールバック

        Returns:
            {
                'success': bool,
                'content': str,  # 物理抽出テキスト（画像/音声/動画は空）
                'char_count': int,
                'method': str,
                'metadata': dict,
                'requires_vision': bool,  # Stage F で Vision 処理が必要か
                'requires_transcription': bool  # Stage F で音声書き起こしが必要か
            }
        """
        logger.info("=" * 60)
        logger.info("[Stage E] 物理抽出開始（AI排除版）")

        # ============================================
        # [E-1] ファイル検証
        # ============================================
        if progress_callback:
            progress_callback("E-1")

        # ファイルなしの場合
        if file_path is None:
            logger.info("  └─ ファイルなし → Stage E スキップ推奨")
            content = pre_extracted_text or ""
            return {
                'success': True,
                'content': content,
                'char_count': len(content),
                'method': 'no_file',
                'metadata': {'skip_stage_e': True},
                'requires_vision': False,
                'requires_transcription': False
            }

        file_path = Path(file_path)
        logger.info(f"  ├─ ファイル: {file_path.name}")
        logger.info(f"  └─ MIMEタイプ: {mime_type}")

        if not file_path.exists():
            logger.error(f"[Stage E] ファイルが存在しません: {file_path}")
            return {
                'success': False,
                'content': '',
                'char_count': 0,
                'method': 'error',
                'error': f'File not found: {file_path}',
                'requires_vision': False,
                'requires_transcription': False
            }

        # ============================================
        # [E-2] MIMEタイプルーティング
        # ============================================
        if progress_callback:
            progress_callback("E-2")

        try:
            # ============================================
            # [E-3] 物理テキスト抽出
            # ============================================
            if progress_callback:
                progress_callback("E-3")

            # --- ドキュメント処理（物理抽出） ---
            if mime_type in self.DOCUMENT_MIME_TYPES or mime_type == 'application/pdf':
                content, method, metadata = self._extract_document(
                    file_path, mime_type, pre_extracted_text
                )
                return {
                    'success': True,
                    'content': content,
                    'char_count': len(content),
                    'method': method,
                    'metadata': metadata,
                    'requires_vision': True,  # PDF/Officeも画像ページがある可能性
                    'requires_transcription': False
                }

            # --- 画像処理（物理抽出不可 → Stage F-7へ） ---
            elif mime_type in self.IMAGE_MIME_TYPES or mime_type.startswith('image/'):
                logger.info("[E-3] 画像ファイル → 物理抽出不可、Stage F-7 で処理")
                content = pre_extracted_text or ""
                return {
                    'success': True,
                    'content': content,
                    'char_count': len(content),
                    'method': 'image_passthrough',
                    'metadata': {'mime_type': mime_type, 'file_path': str(file_path)},
                    'requires_vision': True,
                    'requires_transcription': False
                }

            # --- 音声処理（物理抽出不可 → Stage F-7へ） ---
            elif mime_type in self.AUDIO_MIME_TYPES or mime_type.startswith('audio/'):
                logger.info("[E-3] 音声ファイル → 物理抽出不可、Stage F-7 で処理")
                return {
                    'success': True,
                    'content': '',
                    'char_count': 0,
                    'method': 'audio_passthrough',
                    'metadata': {'mime_type': mime_type, 'file_path': str(file_path)},
                    'requires_vision': False,
                    'requires_transcription': True
                }

            # --- 動画処理（物理抽出不可 → Stage F-7へ） ---
            elif mime_type in self.VIDEO_MIME_TYPES or mime_type.startswith('video/'):
                logger.info("[E-3] 動画ファイル → 物理抽出不可、Stage F-7 で処理")
                return {
                    'success': True,
                    'content': '',
                    'char_count': 0,
                    'method': 'video_passthrough',
                    'metadata': {'mime_type': mime_type, 'file_path': str(file_path)},
                    'requires_vision': True,  # 動画のフレーム解析
                    'requires_transcription': True  # 音声書き起こし
                }

            # --- 未対応MIMEタイプ ---
            else:
                logger.warning(f"[E-3] 未対応のMIMEタイプ: {mime_type}")
                return {
                    'success': True,
                    'content': pre_extracted_text or '',
                    'char_count': len(pre_extracted_text or ''),
                    'method': 'unsupported',
                    'metadata': {'mime_type': mime_type},
                    'requires_vision': False,
                    'requires_transcription': False
                }

        except Exception as e:
            logger.error(f"[Stage E エラー] 物理抽出失敗: {e}", exc_info=True)
            return {
                'success': False,
                'content': '',
                'char_count': 0,
                'method': 'error',
                'error': str(e),
                'requires_vision': False,
                'requires_transcription': False
            }

    def _extract_document(
        self,
        file_path: Path,
        mime_type: str,
        pre_extracted_text: Optional[str]
    ) -> tuple:
        """
        ドキュメントから物理的にテキストを抽出（AI不使用）

        Returns:
            (content, method, metadata)
        """
        logger.info("[E-3] ドキュメント物理抽出開始")

        content = ""
        method = "document"
        metadata = {}

        # PDF処理
        if mime_type == 'application/pdf':
            logger.info("  ├─ PDF処理（pdfplumber/PyMuPDF）")
            result = self.pdf_processor.extract_text(str(file_path), progress_callback=None)
            if result.get('success'):
                content = result.get('content', '')
                method = 'pdf_physical'
                metadata = result.get('metadata', {})

            # 【Ver 6.4】座標付き1文字リストを抽出（物理証拠）
            try:
                import pdfplumber
                physical_chars = []
                with pdfplumber.open(str(file_path)) as pdf:
                    for page_idx, page in enumerate(pdf.pages):
                        w, h = float(page.width), float(page.height)
                        for char in page.chars:
                            # Stage F と同じ 1000x1000 グリッドに正規化
                            physical_chars.append({
                                "text": char.get("text", ""),
                                "bbox": [
                                    int(char.get("x0", 0) * 1000 / w) if w > 0 else 0,
                                    int(char.get("top", 0) * 1000 / h) if h > 0 else 0,
                                    int(char.get("x1", 0) * 1000 / w) if w > 0 else 0,
                                    int(char.get("bottom", 0) * 1000 / h) if h > 0 else 0
                                ],
                                "page": page_idx
                            })
                metadata['physical_chars'] = physical_chars
                logger.info(f"  ├─ 座標付き文字: {len(physical_chars)} 文字")
            except Exception as e:
                logger.warning(f"  ├─ 座標抽出失敗: {e}")

            logger.info(f"  └─ PDF抽出完了: {len(content)} 文字")

        # Office文書処理
        elif 'openxmlformats' in mime_type or 'msword' in mime_type or 'ms-excel' in mime_type or 'ms-powerpoint' in mime_type:
            # ファイルタイプ判定
            if 'word' in mime_type:
                file_type = 'docx'
            elif 'sheet' in mime_type or 'excel' in mime_type:
                file_type = 'xlsx'
            elif 'presentation' in mime_type or 'powerpoint' in mime_type:
                file_type = 'pptx'
            else:
                file_type = 'office'

            logger.info(f"  ├─ Office処理 (type: {file_type})")
            result = self.office_processor.extract_text(str(file_path))
            if result.get('success'):
                content = result.get('content', '')
            method = f'{file_type}_physical'
            logger.info(f"  └─ Office抽出完了: {len(content)} 文字")

        # テキスト系ファイル処理
        elif mime_type.startswith('text/'):
            logger.info("  ├─ テキストファイル処理")
            try:
                # 複数のエンコーディングを試行
                encodings = ['utf-8', 'shift_jis', 'cp932', 'euc-jp', 'iso-2022-jp']
                for encoding in encodings:
                    try:
                        content = file_path.read_text(encoding=encoding)
                        logger.info(f"  ├─ エンコーディング: {encoding}")
                        break
                    except UnicodeDecodeError:
                        continue
                method = 'text_physical'
            except Exception as e:
                logger.error(f"  └─ テキスト読み込みエラー: {e}")
            logger.info(f"  └─ テキスト抽出完了: {len(content)} 文字")

        # pre_extracted_textがある場合は追加
        if pre_extracted_text:
            if content:
                content = pre_extracted_text + "\n\n---\n\n" + content
            else:
                content = pre_extracted_text

        logger.info("=" * 60)
        logger.info(f"[Stage E完了] 物理抽出結果:")
        logger.info(f"  ├─ 処理方式: {method}")
        logger.info(f"  └─ テキスト: {len(content)} 文字")
        logger.info("=" * 60)

        return content, method, metadata

    def process(self, file_path: Path, mime_type: str) -> str:
        """
        ファイルからテキストを抽出（process() エイリアス）

        Args:
            file_path: ファイルパス
            mime_type: MIMEタイプ

        Returns:
            extracted_text: 抽出されたテキスト
        """
        result = self.extract_text(file_path, mime_type)
        return result.get('content', '') if result.get('success') else ''
```

## shared/pipeline/stage_f_visual.py

```python
"""
Stage F: Dual-Vision Analysis (独立読解 10段階)

【設計 2026-01-31】Ver 6.4 物理座標ID溶接モード

核心: 「FはEの答えを知らない状態で、ゼロから視覚情報を暴き出す」

============================================
F-1〜F-5: 構造の下地作り（AIなし、Surya + スクリプト）
  - F-1: Image Normalization (正規化)
  - F-2: Surya Block Detection (領域検出)
  - F-3: Coordinate Quantization (座標量子化) ← トークン削減の肝
  - F-4: Logical Reading Order (読む順序の確定)
  - F-5: Block Classification (構造ラベル付与)

F-6〜F-10: 独立・二重読解（AIの本番）
  - F-6: Blind Prompting (プロンプト注入) ← Stage E結果を遮断
  - F-7: Dual Read - Path A (テキストの鬼 / 2.5 Flash-Lite) ← 先行
  - F-8: Dual Read - Path B (視覚の鬼 / 2.5 Flash)
  - F-9: Physical Sorting + Address Tagging (PROGRAM)
  - F-9.5: AI Rescue for Ambiguous Data (2.5 Flash-Lite)
  - F-10: Stage E Scrubbing (正本化)

【Ver 6.4 変更点】
  - F-7 先行: [ID, 文字] ペアのみ読み取り（位置判断は一切させない）
  - F-8 後行: 構造解析でヘッダー座標を確定
  - F-9 (PROGRAM): Surya bbox + F-8ヘッダーで物理仕分け + 住所タグ付け
  - F-9.5: 低信頼度データのみ AI レスキュー
  - F-10: Stage E の正確なテキストで洗い替え
============================================
"""
import json
import time
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from loguru import logger
from PIL import Image

from shared.ai.llm_client.llm_client import LLMClient
from shared.ai.llm_client.exceptions import MaxTokensExceededError
from .constants import (
    STAGE_F_OUTPUT_SCHEMA_VERSION,
    F1_TARGET_DPI,
    SURYA_MAX_DIM,
    QUANTIZE_GRID_SIZE,
    F7_MODEL_IMAGE,
    F7_MODEL_AV,
    F8_MODEL,
    F95_MODEL,
    F10_MODEL,
    F7_F8_MAX_TOKENS,
    F7_F8_TEMPERATURE,
    CHUNK_SIZE_PAGES,
)

# Surya のインポート（オプショナル）
try:
    from surya.detection import DetectionPredictor
    SURYA_AVAILABLE = True
except ImportError:
    SURYA_AVAILABLE = False
    logger.warning("[Stage F] Surya not installed - F-2 will be skipped")


class StageFVisualAnalyzer:
    """Stage F: 独立読解 10段階（Dual-Vision Analysis）"""

    def __init__(self, llm_client: LLMClient, enable_surya: bool = True):
        """
        Args:
            llm_client: LLMクライアント
            enable_surya: Suryaを有効化（デフォルト: True）
        """
        self.llm_client = llm_client
        self.enable_surya = enable_surya and SURYA_AVAILABLE

        # Surya detector (lazy loading)
        self._surya_detector = None

        # トークン使用量の収集用
        self._f7_usage: List[Dict[str, Any]] = []
        self._f8_usage: List[Dict[str, Any]] = []

        # 列境界情報の保存用（F-3.5で検出、F-7で使用）
        # {page_idx: [0, 250, 500, 750, 1000]} の形式
        self._page_column_boundaries: Dict[int, List[int]] = {}

    @property
    def surya_detector(self):
        """Surya detector の遅延初期化"""
        if self._surya_detector is None and self.enable_surya:
            try:
                self._surya_detector = DetectionPredictor()
                logger.info("[Stage F] Surya detector initialized")
            except Exception as e:
                logger.warning(f"[Stage F] Surya initialization failed: {e}")
                self.enable_surya = False
        return self._surya_detector

    def process(
        self,
        file_path: Optional[Path],
        mime_type: str,
        requires_vision: bool = False,
        requires_transcription: bool = False,
        post_body: Optional[Dict[str, Any]] = None,
        progress_callback=None,
        # 以下: pipeline.py から渡される追加引数（2026-01-28 統合）
        prompt: str = None,
        model: str = None,
        extracted_text: str = None,
        workspace: str = None,
        e2_table_bboxes: List[Dict] = None,
        stage_e_metadata: Dict[str, Any] = None  # 【Ver 6.4】座標付き文字情報
    ) -> Dict[str, Any]:
        """
        Stage F メイン処理（10ステップ）

        Args:
            file_path: ファイルパス（添付なしの場合はNone）
            mime_type: MIMEタイプ
            requires_vision: Vision処理が必要か（Stage Eから）
            requires_transcription: 音声書き起こしが必要か（Stage Eから）
            post_body: 投稿本文
            progress_callback: 進捗コールバック
            prompt: YAMLから読み込んだプロンプト（F7/F8で使用）
            model: YAMLから読み込んだモデル（F7/F8で使用）
            extracted_text: Stage Eで抽出済みのテキスト
            workspace: ワークスペース
            e2_table_bboxes: Stage Eで検出した表のbbox座標

        Returns:
            Stage F 出力（Stage G への入力）
        """
        total_start = time.time()

        # トークン使用量をリセット
        self._f7_usage = []
        self._f8_usage = []

        # 【Ver 6.4】Stage E の座標付き文字情報を保存（F-10で使用）
        self._e_content = extracted_text or ''
        self._e_physical_chars = []
        if stage_e_metadata:
            self._e_physical_chars = stage_e_metadata.get('physical_chars', [])
            if self._e_physical_chars:
                logger.info(f"  ├─ Stage E物理証拠: {len(self._e_physical_chars)}文字")

        # 予備知識を保存（プロンプトで使用、Stage E内容は排除）
        self._file_name = file_path.name if file_path else None
        self._doc_type = self._infer_doc_type(mime_type, file_path)
        self._workspace = workspace

        logger.info("=" * 60)
        logger.info("[Stage F] 独立読解 10段階 開始")
        logger.info(f"  ├─ ファイル: {file_path.name if file_path else 'なし'}")
        logger.info(f"  ├─ MIMEタイプ: {mime_type}")
        logger.info(f"  ├─ requires_vision: {requires_vision}")
        logger.info(f"  └─ requires_transcription: {requires_transcription}")
        logger.info("=" * 60)

        # 添付なし or 処理不要 → 空のpayloadを返す
        if file_path is None or (not requires_vision and not requires_transcription):
            logger.info("[Stage F] スキップ（添付なし or 処理不要）")
            return self._create_empty_payload(post_body)

        # ファイル存在確認
        if not file_path.exists():
            logger.error(f"[Stage F] ファイルが存在しません: {file_path}")
            return self._create_empty_payload(post_body, error=f"File not found: {file_path}")

        # メディアタイプ判定
        is_image = mime_type.startswith('image/') if mime_type else False
        is_audio = mime_type.startswith('audio/') if mime_type else False
        is_video = mime_type.startswith('video/') if mime_type else False
        is_document = mime_type in {'application/pdf'} or mime_type.startswith('text/')

        try:
            # ============================================
            # 音声/動画: F-7 のみ実行（Transcription特化）
            # ============================================
            if is_audio or is_video:
                return self._process_audio_video(
                    file_path, mime_type, is_video, post_body, progress_callback
                )

            # ============================================
            # 画像/PDF: F-1〜F-10 フル実行
            # ============================================
            return self._process_image_document(
                file_path, mime_type, is_document, post_body, progress_callback
            )

        except Exception as e:
            logger.error(f"[Stage F] 処理エラー: {e}", exc_info=True)
            return self._create_empty_payload(post_body, error=str(e))

        finally:
            total_elapsed = time.time() - total_start
            logger.info(f"[Stage F] 総処理時間: {total_elapsed:.2f}秒")

    def _process_audio_video(
        self,
        file_path: Path,
        mime_type: str,
        is_video: bool,
        post_body: Optional[Dict],
        progress_callback
    ) -> Dict[str, Any]:
        """音声/動画処理（F-7のみ）"""
        logger.info("[Stage F] 音声/動画モード → F-7 Transcription のみ実行")

        if progress_callback:
            progress_callback("F-7")

        # F-7: Transcription（gemini-2.5-flash-lite）
        f7_result = self._f7_transcription(file_path, mime_type, is_video)

        # F-9: 結果集約（音声/動画は F-7 のみ）
        if progress_callback:
            progress_callback("F-9")

        return {
            "schema_version": STAGE_F_OUTPUT_SCHEMA_VERSION,
            "post_body": post_body or {},
            "path_a_result": f7_result,
            "path_b_result": {},  # 音声/動画は Path B なし
            "media_type": "video" if is_video else "audio",
            "processing_mode": "transcription_only",
            "warnings": []
        }

    def _process_image_document(
        self,
        file_path: Path,
        mime_type: str,
        is_document: bool,
        post_body: Optional[Dict],
        progress_callback
    ) -> Dict[str, Any]:
        """
        画像/ドキュメント処理（F-1〜F-10）

        【チャンク処理】
        MAX_TOKENSエラー回避のため、5ページごとに分割処理を行う。
        これにより100ページ超のPDFでも安定して処理可能。
        """

        # ============================================
        # F-1: Image Normalization（全ページ）
        # ============================================
        if progress_callback:
            progress_callback("F-1")
        all_page_images = self._f1_normalize(file_path, is_document)
        total_pages = len(all_page_images)

        logger.info(f"[Stage F] 総ページ数: {total_pages}, チャンクサイズ: {CHUNK_SIZE_PAGES}")

        # ============================================
        # 5ページごとのチャンクに分割
        # ============================================
        page_chunks = [
            all_page_images[i:i + CHUNK_SIZE_PAGES]
            for i in range(0, total_pages, CHUNK_SIZE_PAGES)
        ]
        total_chunks = len(page_chunks)

        logger.info(f"[Stage F] チャンク数: {total_chunks}")

        # チャンク処理結果の蓄積用
        aggregated_full_texts = []
        aggregated_blocks = []
        aggregated_tables = []
        aggregated_diagrams = []
        aggregated_charts = []
        aggregated_structured_candidates = []
        chunk_warnings = []

        # ============================================
        # チャンクごとに F-2〜F-8 を実行
        # ============================================
        for chunk_idx, chunk_pages in enumerate(page_chunks):
            chunk_start_page = chunk_idx * CHUNK_SIZE_PAGES
            chunk_end_page = chunk_start_page + len(chunk_pages) - 1

            logger.info("=" * 50)
            logger.info(f"[Stage F] チャンク {chunk_idx + 1}/{total_chunks} 処理中")
            logger.info(f"  ├─ ページ範囲: {chunk_start_page + 1}〜{chunk_end_page + 1}")
            logger.info(f"  └─ ページ数: {len(chunk_pages)}")

            # F-2: Surya Block Detection（このチャンクのみ）
            if progress_callback:
                progress_callback(f"F-2 ({chunk_idx + 1}/{total_chunks})")
            surya_blocks = self._f2_detect_blocks(chunk_pages)

            # F-3: Coordinate Quantization
            if progress_callback:
                progress_callback(f"F-3 ({chunk_idx + 1}/{total_chunks})")
            quantized_blocks = self._f3_quantize(surya_blocks, chunk_pages)

            # F-3.5: Intelligent Filtering & Column Detection（トークン削減の肝）
            if progress_callback:
                progress_callback(f"F-3.5 ({chunk_idx + 1}/{total_chunks})")
            filtered_blocks = self._f35_filter_and_columnize(quantized_blocks)

            # F-4: Logical Reading Order
            if progress_callback:
                progress_callback(f"F-4 ({chunk_idx + 1}/{total_chunks})")
            ordered_blocks = self._f4_reading_order(filtered_blocks)

            # F-5: Block Classification
            if progress_callback:
                progress_callback(f"F-5 ({chunk_idx + 1}/{total_chunks})")
            classified_blocks = self._f5_classify(ordered_blocks)

            # F-6: ID焼き込み画像生成（Ver 4.0 - 座標はAIに渡さない）
            if progress_callback:
                progress_callback(f"F-6 ({chunk_idx + 1}/{total_chunks})")

            # IDマッピングをシステム内部に保持（AIには渡さない）
            id_mapping = self._f6_store_id_mapping(classified_blocks)
            self._id_mapping = id_mapping  # インスタンス変数に保存

            # ============================================
            # 【Ver 6.2】F-8 → F-7 順次実行（構造先行モデル）
            # F-8: 構造の鬼（ヘッダー位置・セル結合を先に確定）
            # F-7: 文字の鬼（確定した枠に文字をハメ込むだけ）
            # ============================================
            if progress_callback:
                progress_callback(f"F-8 ({chunk_idx + 1}/{total_chunks})")

            path_a_result = {}
            path_b_result = {}

            # このチャンク（ページ）の列境界情報を取得
            page_column_boundaries = self._page_column_boundaries.get(chunk_start_page, [0, QUANTIZE_GRID_SIZE])

            # Step 1: F-7 先行（文字読み取り - [ID, 文字]のみ）
            try:
                path_a_result = self._f7_path_a_chunk_extraction_v62(
                    chunk_pages, classified_blocks, chunk_idx, chunk_start_page,
                    page_column_boundaries
                )
                logger.info(f"[F-7] チャンク{chunk_idx + 1} 文字読み取り完了")

            except MaxTokensExceededError as mte:
                logger.error(f"[F-7] チャンク{chunk_idx + 1} MAX_TOKENS到達")
                logger.info(f"[F-7] ===== MAX_TOKENS部分出力ログ開始 =====")
                logger.info(f"[F-7] 部分出力（全文）:\n{mte.partial_output}")
                logger.info(f"[F-7] ===== MAX_TOKENS部分出力ログ終了 =====")
                chunk_warnings.append(f"CHUNK_{chunk_idx}_F7_MAX_TOKENS: {len(mte.partial_output)}文字で切断")
                # 部分出力からでもパースを試みる
                try:
                    import json_repair
                    path_a_result = json_repair.repair_json(mte.partial_output, return_objects=True)
                    if isinstance(path_a_result, dict):
                        path_a_result['_max_tokens_partial'] = True
                        logger.info(f"[F-7] 部分出力から復元成功")
                except Exception as parse_err:
                    logger.warning(f"[F-7] 部分出力パース失敗: {parse_err}")
                    path_a_result = {"error": str(mte), "_max_tokens_partial": True}

            except Exception as e:
                logger.error(f"[F-7] チャンク{chunk_idx + 1} エラー: {e}")
                chunk_warnings.append(f"CHUNK_{chunk_idx}_F7_ERROR: {str(e)}")

            # Step 2: F-8 後行（構造解析）
            if progress_callback:
                progress_callback(f"F-8 ({chunk_idx + 1}/{total_chunks})")

            try:
                path_b_result = self._f8_path_b_chunk_analysis(
                    chunk_pages, classified_blocks, chunk_idx, chunk_start_page
                )
                logger.info(f"[F-8] チャンク{chunk_idx + 1} 構造解析完了")

                # F-8 からヘッダー情報を抽出（F-9で使用）
                f8_headers = self._extract_headers_from_f8(path_b_result)
                logger.info(f"[F-8] ヘッダー情報: X={len(f8_headers.get('x_headers', []))}個, Y={len(f8_headers.get('y_headers', []))}個")

            except Exception as e:
                logger.error(f"[F-8] チャンク{chunk_idx + 1} エラー: {e}")
                chunk_warnings.append(f"CHUNK_{chunk_idx}_F8_ERROR: {str(e)}")
                f8_headers = {'x_headers': [], 'y_headers': [], 'header_coords': {}}

            except Exception as e:
                logger.error(f"[F-7] チャンク{chunk_idx + 1} エラー: {e}")
                chunk_warnings.append(f"CHUNK_{chunk_idx}_F7_ERROR: {str(e)}")

            # チャンク結果を蓄積
            chunk_text = path_a_result.get("full_text_ordered", "")
            aggregated_full_texts.append(chunk_text)
            logger.info(f"[F-9蓄積] チャンク{chunk_idx + 1}: {len(chunk_text)}文字を蓄積 (累計{sum(len(t) for t in aggregated_full_texts)}文字)")

            # 【Ver 6.4】F-7の生テキスト + 座標情報を蓄積（F-9でマッピング）
            raw_texts = path_a_result.get("raw_texts", [])
            block_coords = path_a_result.get("block_coords", {})
            for item in raw_texts:
                if isinstance(item, (list, tuple)) and len(item) >= 2:
                    block_id = str(item[0]).strip()
                    text = str(item[1]).strip()
                    coords = block_coords.get(block_id, {})
                    aggregated_blocks.append({
                        "block_id": block_id,
                        "text": text,
                        "chunk_idx": chunk_idx,
                        "original_page": chunk_start_page,
                        "coords": coords
                    })

            # 表データをマージして蓄積
            merged_tables = self._merge_chunk_tables(path_a_result, path_b_result, chunk_idx, chunk_start_page)
            aggregated_tables.extend(merged_tables)

            # 構造化データ候補を蓄積
            for candidate in path_b_result.get("structured_data_candidates", []):
                candidate["chunk_idx"] = chunk_idx
                aggregated_structured_candidates.append(candidate)

            # ダイアグラム・チャートを蓄積
            aggregated_diagrams.extend(path_b_result.get("diagrams", []))
            aggregated_charts.extend(path_b_result.get("charts", []))

            logger.info(f"[Stage F] チャンク{chunk_idx + 1} 完了: テキスト{len(path_a_result.get('full_text_ordered', ''))}文字, 表{len(merged_tables)}件")

        # ============================================
        # 【Ver 6.2】F-9: 物理仕分け + 住所タグの幾何学的割当
        # ============================================
        if progress_callback:
            progress_callback("F-9")

        logger.info("=" * 50)
        logger.info("[F-9] 【Ver 6.2】物理仕分け + 住所タグ割当開始")

        # F-8 からヘッダー座標を収集（最後のチャンクの結果を使用）
        f8_headers = self._extract_headers_from_f8(path_b_result) if path_b_result else {}

        # F-9: 物理仕分け + 住所タグ
        f9_result, low_confidence_items = self._f9_physical_sorting(
            aggregated_blocks,
            aggregated_tables,
            f8_headers,
            aggregated_structured_candidates
        )

        logger.info(f"[F-9] 物理仕分け完了: {len(f9_result.get('tagged_texts', []))}テキスト, 低信頼度{len(low_confidence_items)}件")

        # ============================================
        # 【Ver 6.2】F-9.5: 低信頼度住所のAIレスキュー
        # ============================================
        if low_confidence_items and progress_callback:
            progress_callback("F-9.5")

        rescued_count = 0
        if low_confidence_items:
            logger.info(f"[F-9.5] 【Ver 6.2】AIレスキュー開始: {len(low_confidence_items)}件")

            rescued_items = self._f95_ai_rescue(
                low_confidence_items,
                f8_headers,
                chunk_pages[0]['image'] if chunk_pages else None
            )
            rescued_count = len(rescued_items)

            # 救済されたアイテムをマージ
            f9_result['tagged_texts'].extend(rescued_items)

            logger.info(f"[F-9.5] AIレスキュー完了: {rescued_count}件 (IDs: {[i.get('id', '?') for i in rescued_items[:5]]}...)")

        # 最終蓄積テキストを構築
        final_full_text = "\n\n".join(aggregated_full_texts)
        logger.info(f"[F-9] 結合テキスト: {len(final_full_text)}文字")

        # 後方互換のため従来形式も保持
        merged_result = {
            # Ver 6.2: tagged_texts を直接格納
            "tagged_texts": f9_result.get('tagged_texts', []),
            "x_headers": f9_result.get('x_headers', []),
            "y_headers": f9_result.get('y_headers', []),
            # 新形式: アンカーベース
            "anchors": self._build_anchor_packets(
                aggregated_blocks,
                aggregated_tables,
                aggregated_structured_candidates
            ),
            # 旧形式: 互換性のため残す
            "text_source": {
                "full_text": final_full_text,
                "blocks": aggregated_blocks,
                "missed_texts": []
            },
            "tables": aggregated_tables,
            "structured_data_candidates": aggregated_structured_candidates,
            "visual_source": {
                "diagrams": aggregated_diagrams,
                "charts": aggregated_charts,
                "layout": {}
            },
            "metadata": {
                "path_a_model": F7_MODEL_IMAGE,
                "path_b_model": F8_MODEL,
                "table_count": len(aggregated_tables),
                "total_table_rows": sum(t.get("row_count", 0) for t in aggregated_tables),
                "total_pages": total_pages,
                "chunk_count": total_chunks,
                "chunk_size": CHUNK_SIZE_PAGES,
                "f9_physical_count": len(f9_result.get('tagged_texts', [])) - rescued_count,
                "f95_rescued_count": rescued_count
            }
        }

        total_text_len = len(merged_result["text_source"]["full_text"])
        logger.info(f"[F-9] 完了: 総テキスト{total_text_len}文字, 物理決定{len(f9_result.get('tagged_texts', [])) - rescued_count}件, AI救済{rescued_count}件")

        # ============================================
        # 【Ver 6.2】F-10: Stage Eによる最終洗い替え
        # ============================================
        if progress_callback:
            progress_callback("F-10")

        # Stage E のテキストを取得（pipeline から渡される）
        e_content = getattr(self, '_e_content', '') or ''

        validated_payload = self._f10_stage_e_scrubbing(merged_result, e_content, post_body)

        # チャンク処理情報を追加
        validated_payload["processing_mode"] = "dual_vision_chunked"
        validated_payload["chunk_info"] = {
            "total_pages": total_pages,
            "chunk_size": CHUNK_SIZE_PAGES,
            "chunk_count": total_chunks
        }
        validated_payload["warnings"].extend(chunk_warnings)

        return validated_payload

    # ============================================
    # F-1: Image Normalization
    # ============================================
    def _f1_normalize(self, file_path: Path, is_document: bool) -> List[Dict]:
        """
        F-1: ページ画像正規化
        PDF → 各ページを300dpiで画像化
        画像 → そのまま読み込み
        """
        f1_start = time.time()
        logger.info("[F-1] Image Normalization 開始")

        page_images = []
        DPI = F1_TARGET_DPI

        file_ext = file_path.suffix.lower()

        if file_ext == '.pdf':
            import fitz  # PyMuPDF
            doc = fitz.open(file_path)
            total_pages = len(doc)
            logger.info(f"  ├─ PDF: {total_pages}ページ")

            for page_num in range(total_pages):
                page = doc[page_num]
                mat = fitz.Matrix(DPI / 72, DPI / 72)
                pix = page.get_pixmap(matrix=mat)
                img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                page_images.append({
                    'page_index': page_num,
                    'image': img,
                    'width': pix.width,
                    'height': pix.height,
                    'dpi': DPI
                })
            doc.close()
        else:
            # 画像ファイル
            img = Image.open(file_path).convert("RGB")
            page_images.append({
                'page_index': 0,
                'image': img,
                'width': img.size[0],
                'height': img.size[1],
                'dpi': DPI
            })

        f1_elapsed = time.time() - f1_start
        logger.info(f"[F-1完了] {len(page_images)}ページ, {f1_elapsed:.2f}秒")

        return page_images

    # ============================================
    # F-2: Surya Block Detection
    # ============================================
    def _f2_detect_blocks(self, page_images: List[Dict]) -> List[Dict]:
        """
        F-2: Suryaブロック検出
        """
        f2_start = time.time()
        surya_blocks = []

        if not self.enable_surya or not page_images:
            logger.info("[F-2] Surya スキップ（無効 or ページなし）")
            return surya_blocks

        logger.info("[F-2] Surya Block Detection 開始")

        for page_data in page_images:
            page_idx = page_data['page_index']
            img = page_data['image']

            # リサイズ（Suryaメモリ対策）
            w, h = img.size
            scale = 1.0
            if max(w, h) > SURYA_MAX_DIM:
                scale = SURYA_MAX_DIM / max(w, h)
                img = img.resize((int(w * scale), int(h * scale)), Image.Resampling.LANCZOS)

            # Surya検出
            try:
                detection_results = self.surya_detector([img])
                if detection_results and detection_results[0].bboxes:
                    for block_id, box in enumerate(detection_results[0].bboxes):
                        raw_bbox = box.bbox
                        # 元の座標系に復元
                        restored_bbox = [
                            raw_bbox[0] / scale,
                            raw_bbox[1] / scale,
                            raw_bbox[2] / scale,
                            raw_bbox[3] / scale
                        ]
                        surya_blocks.append({
                            'page': page_idx,
                            'bbox': restored_bbox,
                            'block_id': f"p{page_idx}_b{block_id}",
                            'confidence': getattr(box, 'confidence', 1.0)
                        })
            except Exception as e:
                logger.warning(f"[F-2] Surya検出エラー (page {page_idx}): {e}")

        f2_elapsed = time.time() - f2_start
        logger.info(f"[F-2完了] {len(surya_blocks)}ブロック検出, {f2_elapsed:.2f}秒")

        return surya_blocks

    # ============================================
    # F-3: Coordinate Quantization
    # ============================================
    def _f3_quantize(self, surya_blocks: List[Dict], page_images: List[Dict]) -> List[Dict]:
        """
        F-3: 座標量子化（1000×1000グリッド）
        トークン削減の肝
        """
        f3_start = time.time()
        logger.info("[F-3] Coordinate Quantization 開始")

        page_dims = {p['page_index']: (p['width'], p['height']) for p in page_images}
        quantized = []

        for block in surya_blocks:
            page_idx = block['page']
            bbox = block['bbox']
            w, h = page_dims.get(page_idx, (1000, 1000))

            # 1000×1000 グリッドに量子化
            q_bbox = [
                int(bbox[0] * QUANTIZE_GRID_SIZE / w),
                int(bbox[1] * QUANTIZE_GRID_SIZE / h),
                int(bbox[2] * QUANTIZE_GRID_SIZE / w),
                int(bbox[3] * QUANTIZE_GRID_SIZE / h)
            ]

            quantized.append({
                **block,
                'bbox_original': bbox,
                'bbox': q_bbox,  # 量子化後の座標
                'page_width': w,
                'page_height': h
            })

        f3_elapsed = time.time() - f3_start
        logger.info(f"[F-3完了] {len(quantized)}ブロック量子化, {f3_elapsed:.2f}秒")

        return quantized

    # ============================================
    # F-3.5: 三つの物理的根拠（空白・罫線・密度）による動的分離
    # ============================================

    def _f35_filter_and_columnize(self, blocks: List[Dict]) -> List[Dict]:
        """
        F-3.5: 物理的根拠に基づく動的分離（Ver 3.2）

        【設計原則】
        1. ノイズ除去: 極小ブロックを物理削除（190→90程度）
        2. 分割軸の決定: 空白→罫線→密度の順に、縦OR横の一方を選択
        3. 排他的分割: 網の目状禁止、一方向のスライスのみ
        4. 横方向パッキング: 分割パッチ内で横のみ結合（縦結合禁止）
        5. ヘッダー/インデックス複製: 全パッチに基準列/行を随伴
        """
        import numpy as np

        f35_start = time.time()
        original_count = len(blocks)
        logger.info(f"[F-3.5] 動的分離開始: {original_count}ブロック")

        if not blocks:
            return []

        # ============================================
        # 【Ver 3.5】ページ別に全ブロックを保持（密度分析用）
        # ============================================
        all_pages = {}  # フィルタ前の全ブロック（密度分析用）
        for block in blocks:
            page = block.get('page', 0)
            if page not in all_pages:
                all_pages[page] = []
            all_pages[page].append(block)

        # ============================================
        # Step 1: ノイズブロックの物理削除（出力用）
        # 【Ver 3.5】閾値を大幅緩和 - 小さい文字ブロックも保持
        # ============================================
        MIN_BLOCK_SIZE = 5   # 最小サイズ（量子化後）← 15から緩和
        MIN_BLOCK_AREA = 50  # 最小面積 ← 300から緩和（偏差値「65」等を保持）

        filtered = []
        garbage_count = 0

        for block in blocks:
            bbox = block['bbox']
            w = bbox[2] - bbox[0]
            h = bbox[3] - bbox[1]
            area = w * h

            # 本当のゴミ（点やドット）だけ除外
            if w < MIN_BLOCK_SIZE and h < MIN_BLOCK_SIZE:
                garbage_count += 1
                continue
            if area < MIN_BLOCK_AREA:
                garbage_count += 1
                continue

            filtered.append(block)

        logger.info(f"  ├─ ノイズ除去: {garbage_count}ブロック削除 → 残り{len(filtered)}ブロック")

        # フィルタ後をページ別に整理（後処理用）
        pages = {}
        for block in filtered:
            page = block.get('page', 0)
            if page not in pages:
                pages[page] = []
            pages[page].append(block)

        all_processed = []

        for page_idx in all_pages.keys():
            # フィルタ後のブロック（後処理用）
            page_blocks = pages.get(page_idx, [])
            # フィルタ前の全ブロック（密度分析用）
            all_page_blocks = all_pages[page_idx]

            # ============================================
            # Step 2: 分割軸の決定（全ブロックで密度分析）
            # 【Ver 4.0】F-3.5の決定は「神の宣告」- F-7で上書き不可
            # ============================================
            split_axis, split_positions, split_reason = self._determine_split_axis(all_page_blocks)
            logger.info(f"  ├─ 密度分析: {len(all_page_blocks)}ブロック使用（フィルタ前）")
            logger.info(f"  ├─ ページ{page_idx}: 分割軸={split_axis}, 理由={split_reason}")

            # 【Ver 4.0】神の宣告をインスタンス変数に保存（F-7で参照、上書き禁止）
            self._current_split_axis = split_axis
            self._current_split_positions = split_positions
            self._current_split_reason = split_reason

            # 列境界情報を保存（互換性のため維持）
            if split_axis == 'vertical':
                self._page_column_boundaries[page_idx] = [0] + split_positions + [QUANTIZE_GRID_SIZE]
            else:
                self._page_column_boundaries[page_idx] = [0, QUANTIZE_GRID_SIZE]

            # ============================================
            # Step 3: 排他的分割（縦か横の一方のみ）
            # 【Ver 6.4】ブロック座標死守: どのパスでも全ブロック維持
            # ============================================
            logger.info(f"  ├─ 【Ver 6.4診断】Step 3開始: page_blocks={len(page_blocks)}個, split_axis={split_axis}")

            if split_axis == 'none':
                # 【Ver 6.4】分割なし = 全ブロックそのまま維持（パージ禁止）
                packed = self._horizontal_packing(page_blocks, page_idx)
                logger.info(f"  ├─ 【Ver 6.4診断】split_axis=none: {len(page_blocks)}個 → {len(packed)}個（パージなし）")
                all_processed.extend(packed)

            elif split_axis == 'vertical':
                # 縦分割（左右に分ける）: インデックス列を複製
                split_result = self._vertical_split_with_index(
                    page_blocks, split_positions, page_idx
                )
                logger.info(f"  ├─ 【Ver 6.4診断】split_axis=vertical: {len(page_blocks)}個 → {len(split_result)}個")
                all_processed.extend(split_result)

            else:  # horizontal
                # 横分割（上下に分ける）: ヘッダー行を複製
                split_result = self._horizontal_split_with_header(
                    page_blocks, split_positions, page_idx
                )
                logger.info(f"  ├─ 【Ver 6.4診断】split_axis=horizontal: {len(page_blocks)}個 → {len(split_result)}個")
                all_processed.extend(split_result)

        f35_elapsed = time.time() - f35_start
        reduction_rate = (1 - len(all_processed) / original_count) * 100 if original_count > 0 else 0
        logger.info(f"[F-3.5完了] {original_count} → {len(all_processed)}ブロック "
                    f"({reduction_rate:.1f}%削減), {f35_elapsed:.2f}秒")
        logger.info(f"  └─ 【Ver 6.4確認】ブロック座標死守: {len(all_processed)}個のSuryaブロックを維持")

        return all_processed

    def _determine_split_axis(self, blocks: List[Dict]) -> tuple:
        """
        【Ver 4.0】汎用グリッド制圧エンジン

        設計原則:
        1. 80%ルール: 表専有面積が80%以下なら分割しない
        2. ヘッダー最小化: ヘッダー数が少ない軸で分割
        3. 6本制限: 境界線は両端含め最大6本（5領域）

        Returns:
            (axis, positions, reason)
            axis: 'vertical' | 'horizontal' | 'none'
            positions: 分割位置のリスト（内部境界のみ、両端除く）
            reason: 決定理由
        """
        import numpy as np

        if not blocks:
            return ('none', [], 'ブロックなし')

        # ============================================
        # STEP 0: 表の専有面積を計算（80%ルール）
        # ============================================
        all_x = [b['bbox'][0] for b in blocks] + [b['bbox'][2] for b in blocks]
        all_y = [b['bbox'][1] for b in blocks] + [b['bbox'][3] for b in blocks]

        table_x_min, table_x_max = min(all_x), max(all_x)
        table_y_min, table_y_max = min(all_y), max(all_y)

        table_width = table_x_max - table_x_min
        table_height = table_y_max - table_y_min
        table_area = table_width * table_height
        page_area = QUANTIZE_GRID_SIZE * QUANTIZE_GRID_SIZE

        occupancy = table_area / page_area if page_area > 0 else 0

        logger.debug(f"    表専有率: {occupancy*100:.1f}% (W:{table_width}, H:{table_height})")

        # 80%以下なら分割不要
        if occupancy <= 0.80:
            return ('none', [], f'専有率{occupancy*100:.0f}%≤80%、分割不要')

        # ============================================
        # STEP 1: ヒストグラム構築（密度分析）
        # ============================================
        MIN_GAP = 20  # 最小ガター幅（緩和）
        MAX_INTERNAL_BOUNDARIES = 4  # 内部境界は最大4本（両端含め6本）

        x_histogram = np.zeros(QUANTIZE_GRID_SIZE)
        y_histogram = np.zeros(QUANTIZE_GRID_SIZE)
        x_line_height = np.zeros(QUANTIZE_GRID_SIZE)
        y_line_width = np.zeros(QUANTIZE_GRID_SIZE)

        for block in blocks:
            bbox = block['bbox']
            x_start, y_start = max(0, int(bbox[0])), max(0, int(bbox[1]))
            x_end, y_end = min(QUANTIZE_GRID_SIZE, int(bbox[2])), min(QUANTIZE_GRID_SIZE, int(bbox[3]))
            w, h = x_end - x_start, y_end - y_start

            x_histogram[x_start:x_end] += 1
            y_histogram[y_start:y_end] += 1

            if w <= 10:
                x_line_height[x_start:x_end] = np.maximum(x_line_height[x_start:x_end], h)
            if h <= 10:
                y_line_width[y_start:y_end] = np.maximum(y_line_width[y_start:y_end], w)

        # 【診断】X軸ヒストグラム全出力
        hist_str = "".join([f"[{i}]{int(x_histogram[i])}" for i in range(QUANTIZE_GRID_SIZE)])
        logger.info(f"  ├─ [X軸ヒストグラム] {hist_str}")

        # ============================================
        # 【Ver 5.6】STEP 2: 純粋空白（Gap）のみで分割
        # 密度分析・罫線検出を完全廃止。histogram=0の領域のみを分割軸とする。
        # ============================================
        v_gaps = self._find_gaps(x_histogram, MIN_GAP, MAX_INTERNAL_BOUNDARIES)
        h_gaps = self._find_gaps(y_histogram, MIN_GAP, MAX_INTERNAL_BOUNDARIES)

        # 【Ver 5.6】罫線・密度谷は使用しない
        logger.info(f"  ├─ [Ver 5.6] 純粋空白のみで分割判定")
        logger.info(f"  ├─ [診断] v_gaps（空白のみ）: {v_gaps}")
        logger.info(f"  ├─ [診断] h_gaps（空白のみ）: {h_gaps}")

        # 【Ver 5.6】Gapのみを候補として使用（罫線・密度谷は無視）
        v_candidates = [pos for pos, width in v_gaps]
        h_candidates = [pos for pos, width in h_gaps]

        logger.info(f"  ├─ [診断] v_candidates（純粋空白）: {v_candidates}")
        logger.info(f"  ├─ [診断] h_candidates（純粋空白）: {h_candidates}")

        # ============================================
        # 【Ver 5.6】STEP 3: 空白がなければ分割しない
        # AIに大きな表を1枚で渡し、ヘッダーを自律的に特定させる
        # ============================================
        has_v = len(v_candidates) > 0
        has_h = len(h_candidates) > 0

        if not has_v and not has_h:
            # 【Ver 5.6】空白なし = 分割なし = AIに丸投げ
            logger.info(f"  ├─ [Ver 5.6] 純粋空白なし → 分割せずAIに丸投げ")
            return ('none', [], '純粋空白なし（Ver5.6: AI自律モード）')

        # 空白がある場合のみ分割
        if has_v and has_h:
            # 両方ある場合は垂直優先（列方向の空白を使う）
            axis = 'vertical'
            positions = v_candidates
            reason = f'純粋空白検出（垂直{len(v_candidates)}本）'
        elif has_v:
            axis = 'vertical'
            positions = v_candidates
            reason = f'純粋空白検出（垂直{len(v_candidates)}本）'
        else:
            axis = 'horizontal'
            positions = h_candidates
            reason = f'純粋空白検出（水平{len(h_candidates)}本）'

        return (axis, positions, reason)

    def _merge_boundary_candidates(
        self,
        gaps: List[tuple],
        lines: List[int],
        valleys: List[tuple],
        max_count: int
    ) -> List[int]:
        """空白・罫線・密度谷の候補を統合（重複除去・優先順位付き）"""
        candidates = []
        used_positions = set()
        MERGE_THRESHOLD = 30  # この距離以内は同一境界とみなす

        def add_if_new(pos):
            for used in used_positions:
                if abs(pos - used) < MERGE_THRESHOLD:
                    return False
            used_positions.add(pos)
            candidates.append(pos)
            return True

        # 優先1: 空白ガター（最も信頼性が高い）
        for pos, width in gaps:
            if len(candidates) >= max_count:
                break
            add_if_new(pos)

        # 優先2: 罫線
        for pos in lines:
            if len(candidates) >= max_count:
                break
            add_if_new(pos)

        # 優先3: 密度谷
        for pos, depth in valleys:
            if len(candidates) >= max_count:
                break
            add_if_new(pos)

        return sorted(candidates)

    def _count_density_peaks(self, histogram) -> int:
        """ヒストグラムからピーク（情報の柱）の数を数える"""
        import numpy as np

        if len(histogram) == 0 or np.max(histogram) == 0:
            return 0

        # スムージング
        kernel_size = 20
        kernel = np.ones(kernel_size) / kernel_size
        smoothed = np.convolve(histogram, kernel, mode='same')

        # 閾値以上の連続区間をカウント
        threshold = np.max(smoothed) * 0.2  # 最大値の20%以上
        in_peak = False
        peak_count = 0

        for val in smoothed:
            if val >= threshold and not in_peak:
                in_peak = True
                peak_count += 1
            elif val < threshold:
                in_peak = False

        return peak_count

    def _find_gaps(self, histogram, min_gap: int, max_count: int) -> List[tuple]:
        """ヒストグラムから空白ギャップを検出"""
        import numpy as np
        gaps = []
        in_gap = False
        gap_start = 0

        for i, val in enumerate(histogram):
            if val == 0 and not in_gap:
                in_gap = True
                gap_start = i
            elif val > 0 and in_gap:
                gap_width = i - gap_start
                if gap_width >= min_gap:
                    center = gap_start + gap_width // 2
                    if 50 < center < QUANTIZE_GRID_SIZE - 50:
                        gaps.append((center, gap_width))
                in_gap = False

        # 幅が広い順にソート
        gaps.sort(key=lambda x: x[1], reverse=True)
        return gaps[:max_count]

    def _find_lines(self, line_sizes, min_size: int) -> List[int]:
        """罫線位置を検出"""
        lines = []
        in_line = False
        line_start = 0

        for i, size in enumerate(line_sizes):
            if size >= min_size and not in_line:
                in_line = True
                line_start = i
            elif size < min_size and in_line:
                center = (line_start + i) // 2
                if 50 < center < QUANTIZE_GRID_SIZE - 50:
                    lines.append(center)
                in_line = False

        return lines

    def _find_density_valleys(self, histogram, max_count: int) -> List[tuple]:
        """
        【Ver 3.7修正】密度の谷を検出（隣接マージ版）

        問題: 同じ谷の隣接ピクセル(85,86,87,88)を別々に検出してしまう
        解決: 検出後に隣接する谷をマージし、真に独立した谷のみ返す
        """
        import numpy as np

        if len(histogram) < 100:
            return []

        # 移動平均でスムージング
        kernel_size = 30  # 大きめのカーネルで安定化
        smoothed = np.convolve(histogram, np.ones(kernel_size)/kernel_size, mode='same')

        # 全体の統計を取得
        mean_val = np.mean(smoothed)
        max_val = np.max(smoothed)

        # 局所最小値を検出（閾値を緩和）
        raw_valleys = []
        MIN_VALLEY_SEPARATION = 80  # 谷と谷の最小間隔（量子化座標）

        for i in range(60, len(smoothed) - 60):
            # 前後40ピクセルより低い = より広い範囲で谷を検出
            if smoothed[i] < smoothed[i-40] and smoothed[i] < smoothed[i+40]:
                # 深さ = 周囲との差
                depth = min(smoothed[i-40], smoothed[i+40]) - smoothed[i]
                # 相対的な深さも考慮（平均値の10%以上の落ち込み）
                relative_depth = (mean_val - smoothed[i]) / mean_val if mean_val > 0 else 0

                if depth > 0.3 or relative_depth > 0.1:
                    raw_valleys.append((i, depth, relative_depth))

        # 【重要】隣接する谷をマージ
        merged_valleys = []
        raw_valleys.sort(key=lambda x: x[0])  # 位置順でソート

        for pos, depth, rel_depth in raw_valleys:
            # 既存の谷と近すぎないかチェック
            is_new = True
            for j, (existing_pos, existing_depth, _) in enumerate(merged_valleys):
                if abs(pos - existing_pos) < MIN_VALLEY_SEPARATION:
                    # 近い谷がある場合、深い方を採用
                    if depth > existing_depth:
                        merged_valleys[j] = (pos, depth, rel_depth)
                    is_new = False
                    break

            if is_new:
                merged_valleys.append((pos, depth, rel_depth))

        # 深さ順でソートして返す
        merged_valleys.sort(key=lambda x: x[1], reverse=True)

        logger.debug(f"    [密度谷] raw={len(raw_valleys)}個 → merged={len(merged_valleys)}個")

        return [(pos, depth) for pos, depth, _ in merged_valleys[:max_count]]

    def _horizontal_packing(self, blocks: List[Dict], page_idx: int) -> List[Dict]:
        """
        【Ver 6.4】横パッキング完全廃止 → パススルー

        ============================================
        【廃止理由】2026-01-31
        横パッキングは246個のブロックを9個の巨大な塊に統合し、
        以下の致命的な問題を引き起こしていた:
          1. 情報の消失（AIが巨大な塊を読み飛ばす）
          2. MAX_TOKENS 回避の失敗
          3. 「空白の通り道」判定の誤り

        【新方針】
        Suryaが検出した246個のブロックを1つも統合せず、
        そのままの粒度で後続処理に渡す。
        ============================================

        Args:
            blocks: Suryaが検出したブロック（統合禁止）
            page_idx: ページ番号

        Returns:
            入力ブロックをそのまま返す（メタデータのみ補完）
        """
        result = []

        for idx, block in enumerate(blocks):
            # 必要なメタデータを補完（既存があれば維持）
            processed = block.copy()

            # block_id がなければ生成
            if not processed.get('block_id'):
                processed['block_id'] = f"p{page_idx}_b{idx}"

            # ページ情報を補完
            processed['page'] = page_idx
            if 'page_width' not in processed:
                processed['page_width'] = 1000
            if 'page_height' not in processed:
                processed['page_height'] = 1000

            # パッキングフラグは False（統合していない）
            processed['is_row_packed'] = False
            processed['block_count'] = 1

            result.append(processed)

        logger.info(f"  ├─ 【Ver 6.4】パッキング廃止: {len(blocks)}ブロック → {len(result)}ブロック（そのまま）")
        return result

    def _vertical_split_with_index(self, blocks: List[Dict], positions: List[int], page_idx: int) -> List[Dict]:
        """縦分割（インデックス列複製付き）"""
        boundaries = [0] + positions + [QUANTIZE_GRID_SIZE]
        num_columns = len(boundaries) - 1

        # 列ごとにブロックを分類
        columns = {i: [] for i in range(num_columns)}

        for block in blocks:
            x_center = (block['bbox'][0] + block['bbox'][2]) / 2
            for i in range(num_columns):
                if boundaries[i] <= x_center < boundaries[i + 1]:
                    columns[i].append(block)
                    break

        # インデックス列（左端）を特定
        index_column = columns.get(0, [])

        result = []
        for col_id in range(num_columns):
            col_blocks = columns[col_id]
            if not col_blocks:
                continue

            # 横方向パッキング
            packed = self._horizontal_packing(col_blocks, page_idx)

            for block in packed:
                block['column_id'] = col_id
                block['num_columns'] = num_columns
                block['has_index_column'] = col_id > 0  # 2列目以降はインデックス列を随伴
                block['col_boundaries'] = [boundaries[col_id], boundaries[col_id + 1]]

            result.extend(packed)

        # 列境界を保存
        self._page_column_boundaries[page_idx] = boundaries

        logger.info(f"  ├─ 縦分割: {num_columns}列, インデックス列={len(index_column)}ブロック")
        return result

    def _horizontal_split_with_header(self, blocks: List[Dict], positions: List[int], page_idx: int) -> List[Dict]:
        """横分割（ヘッダー行複製付き）"""
        boundaries = [0] + positions + [QUANTIZE_GRID_SIZE]
        num_sections = len(boundaries) - 1

        # セクションごとにブロックを分類
        sections = {i: [] for i in range(num_sections)}

        for block in blocks:
            y_center = (block['bbox'][1] + block['bbox'][3]) / 2
            for i in range(num_sections):
                if boundaries[i] <= y_center < boundaries[i + 1]:
                    sections[i].append(block)
                    break

        # ヘッダー行（最上段）を特定
        header_section = sections.get(0, [])

        result = []
        for sec_id in range(num_sections):
            sec_blocks = sections[sec_id]
            if not sec_blocks:
                continue

            # 横方向パッキング
            packed = self._horizontal_packing(sec_blocks, page_idx)

            for block in packed:
                block['section_id'] = sec_id
                block['num_sections'] = num_sections
                block['has_header_row'] = sec_id > 0  # 2セクション目以降はヘッダー行を随伴
                block['section_boundaries'] = [boundaries[sec_id], boundaries[sec_id + 1]]

            result.extend(packed)

        # 水平分割の場合、列境界は1列扱い
        self._page_column_boundaries[page_idx] = [0, QUANTIZE_GRID_SIZE]

        logger.info(f"  ├─ 横分割: {num_sections}セクション, ヘッダー行={len(header_section)}ブロック")
        return result

    def _merge_column_blocks(
        self,
        col_blocks: List[Dict],
        page_idx: int,
        column_id: int,
        num_columns: int,
        col_x_start: int,
        col_x_end: int
    ) -> Dict:
        """
        【ページ完結型】列内の全ブロックを1つのテーブルブロックに統合

        Args:
            col_blocks: この列に属する全ブロック（Y座標でソート済み）
            page_idx: ページ番号
            column_id: 列番号
            num_columns: このページの総列数
            col_x_start: 列の左端X座標
            col_x_end: 列の右端X座標

        Returns:
            統合されたブロック（1ページ1列1ブロック）
        """
        if len(col_blocks) == 1:
            block = col_blocks[0]
            block['table_id'] = f"TBL_P{page_idx}_C{column_id}"
            block['is_page_complete'] = True
            block['row_count'] = 1
            return block

        # 全ブロックのバウンディングボックスを統合
        x_min = min(b['bbox'][0] for b in col_blocks)
        y_min = min(b['bbox'][1] for b in col_blocks)
        x_max = max(b['bbox'][2] for b in col_blocks)
        y_max = max(b['bbox'][3] for b in col_blocks)

        # 行データを構築（Y座標順）
        rows = []
        for block in col_blocks:
            rows.append({
                'block_id': block['block_id'],
                'y': block['y_center'],
                'bbox': block['bbox']
            })

        # ページ跨ぎの判定（下端に近いか）
        is_at_page_bottom = y_max > 900  # 1000グリッド中900以上
        is_at_page_top = y_min < 100     # 1000グリッド中100以下

        return {
            'page': page_idx,
            'block_id': f"col_P{page_idx}_C{column_id}",
            'table_id': f"TBL_P{page_idx}_C{column_id}",
            'bbox': [x_min, y_min, x_max, y_max],
            'bbox_original': col_blocks[0].get('bbox_original'),
            'page_width': col_blocks[0].get('page_width'),
            'page_height': col_blocks[0].get('page_height'),
            'column_id': column_id,
            'num_columns': num_columns,
            'col_x_range': [col_x_start, col_x_end],
            'x_center': (x_min + x_max) / 2,
            'y_center': (y_min + y_max) / 2,
            # ページ完結型メタデータ
            'is_page_complete': True,
            'row_count': len(col_blocks),
            'rows': rows,
            'original_block_ids': [b['block_id'] for b in col_blocks],
            # ページ跨ぎ用フラグ（Stage Hで使用）
            'is_continued_from_prev': is_at_page_top,  # 前ページから続く可能性
            'is_continued_to_next': is_at_page_bottom,  # 次ページに続く可能性
            'stitch_hint': {
                'prev_page': page_idx - 1 if is_at_page_top else None,
                'next_page': page_idx + 1 if is_at_page_bottom else None,
                'column_id': column_id
            }
        }

    def _merge_row_blocks(self, row_blocks: List[Dict], page_idx: int, column_id: int) -> Dict:
        """
        同じ行のブロックを1つにマージ（レガシー互換用）
        """
        if len(row_blocks) == 1:
            return row_blocks[0]

        # X座標でソート
        row_blocks.sort(key=lambda b: b['bbox'][0])

        # バウンディングボックスを統合
        x_min = min(b['bbox'][0] for b in row_blocks)
        y_min = min(b['bbox'][1] for b in row_blocks)
        x_max = max(b['bbox'][2] for b in row_blocks)
        y_max = max(b['bbox'][3] for b in row_blocks)

        # 元のblock_idを結合
        merged_id = "+".join(b['block_id'] for b in row_blocks)

        return {
            'page': page_idx,
            'block_id': f"merged_{merged_id}",
            'bbox': [x_min, y_min, x_max, y_max],
            'bbox_original': row_blocks[0].get('bbox_original'),
            'page_width': row_blocks[0].get('page_width'),
            'page_height': row_blocks[0].get('page_height'),
            'confidence': min(b.get('confidence', 1.0) for b in row_blocks),
            'column_id': column_id,
            'x_center': (x_min + x_max) / 2,
            'y_center': (y_min + y_max) / 2,
            'merged_count': len(row_blocks),
            'original_blocks': [b['block_id'] for b in row_blocks]
        }

    # ============================================
    # F-4: Logical Reading Order
    # ============================================
    def _f4_reading_order(self, blocks: List[Dict]) -> List[Dict]:
        """
        F-4: 読む順序の確定
        F-3.5で検出された列を使用して正確にソート
        """
        f4_start = time.time()
        logger.info("[F-4] Logical Reading Order 開始")

        if not blocks:
            return []

        # F-3.5で既にcolumn_id, x_center, y_centerが設定されている場合はそのまま使用
        for block in blocks:
            if 'x_center' not in block:
                bbox = block['bbox']
                block['x_center'] = (bbox[0] + bbox[2]) / 2
            if 'y_center' not in block:
                bbox = block['bbox']
                block['y_center'] = (bbox[1] + bbox[3]) / 2
            if 'column_id' not in block:
                # フォールバック: 左右2分割
                block['column_id'] = 0 if block['x_center'] < QUANTIZE_GRID_SIZE / 2 else 1

        # ソート: page → column → y → x
        sorted_blocks = sorted(
            blocks,
            key=lambda b: (
                b.get('page', 0),
                b.get('column_id', 0),
                b.get('y_center', 0),
                b.get('x_center', 0)
            )
        )

        # reading_order を付与
        for order, block in enumerate(sorted_blocks):
            block['reading_order'] = order

        f4_elapsed = time.time() - f4_start
        logger.info(f"[F-4完了] {len(sorted_blocks)}ブロック順序確定, {f4_elapsed:.2f}秒")

        return sorted_blocks

    # ============================================
    # F-5: Block Classification
    # ============================================
    def _f5_classify(self, blocks: List[Dict]) -> List[Dict]:
        """
        F-5: 構造ラベル付与
        ルールベースでブロックタイプを推定
        """
        f5_start = time.time()
        logger.info("[F-5] Block Classification 開始")

        for block in blocks:
            bbox = block['bbox']
            w = bbox[2] - bbox[0]
            h = bbox[3] - bbox[1]
            y = block.get('y_center', 500)
            area = w * h
            aspect_ratio = w / h if h > 0 else 1.0

            # ブロックタイプ推定
            block_type = 'body_hint'

            # 表の検出
            if aspect_ratio > 3.0 and 20 < h < 150:
                block_type = 'table_hint'
            elif 10000 < area < 500000 and 0.5 < aspect_ratio < 3.0:
                block_type = 'table_hint'
            # 見出し
            elif h < 80 and w > 200 and y < 200:
                block_type = 'heading_hint'
            # ヘッダー
            elif y < 80:
                block_type = 'header_hint'
            # フッター
            elif y > 920:
                block_type = 'footer_hint'
            # 注記
            elif area < 5000:
                block_type = 'note_hint'

            block['block_type_hint'] = block_type

        f5_elapsed = time.time() - f5_start
        type_counts = {}
        for b in blocks:
            t = b.get('block_type_hint', 'unknown')
            type_counts[t] = type_counts.get(t, 0) + 1
        logger.info(f"[F-5完了] ラベル分布: {type_counts}, {f5_elapsed:.2f}秒")

        return blocks

    # ============================================
    # F-6: ID焼き込み画像生成（Ver 4.0 - 座標排除）
    # ============================================
    def _f6_burn_ids_to_image(
        self,
        image: Image.Image,
        blocks: List[Dict],
        page_idx: int = 0
    ) -> Image.Image:
        """
        F-6: ブロックIDを画像に直接焼き込む

        【Ver 4.0】座標データはAIに渡さない。
        代わりに、IDを視覚的に画像に描画し、AIは「見たまま」判断する。

        Args:
            image: 元画像（PIL Image）
            blocks: ブロックリスト（block_id, bbox含む）
            page_idx: ページ番号

        Returns:
            ID焼き込み済み画像
        """
        from PIL import ImageDraw, ImageFont
        import io

        f6_start = time.time()
        logger.info(f"[F-6] ID焼き込み開始: {len(blocks)}ブロック")

        # 画像をコピー（元画像を変更しない）
        img_with_ids = image.copy()
        draw = ImageDraw.Draw(img_with_ids)

        # フォント設定（システムフォントを使用）
        try:
            font = ImageFont.truetype("arial.ttf", 14)
        except:
            try:
                font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 14)
            except:
                font = ImageFont.load_default()

        img_width, img_height = image.size

        # 各ブロックにIDを描画
        for block in blocks:
            block_id = block.get('block_id', '')
            bbox = block.get('bbox', [0, 0, 0, 0])

            # 量子化座標を実座標に変換
            page_width = block.get('page_width', QUANTIZE_GRID_SIZE)
            page_height = block.get('page_height', QUANTIZE_GRID_SIZE)

            x1 = int(bbox[0] * img_width / QUANTIZE_GRID_SIZE)
            y1 = int(bbox[1] * img_height / QUANTIZE_GRID_SIZE)
            x2 = int(bbox[2] * img_width / QUANTIZE_GRID_SIZE)
            y2 = int(bbox[3] * img_height / QUANTIZE_GRID_SIZE)

            # IDラベル（短縮形: p0_r5 → #5）
            short_id = block_id.split('_')[-1] if '_' in block_id else block_id
            label = f"#{short_id}"

            # 背景付きでIDを描画（視認性確保）
            text_bbox = draw.textbbox((x1, y1), label, font=font)
            padding = 2
            draw.rectangle(
                [text_bbox[0] - padding, text_bbox[1] - padding,
                 text_bbox[2] + padding, text_bbox[3] + padding],
                fill='yellow'
            )
            draw.text((x1, y1), label, fill='red', font=font)

            # ブロック境界を薄く描画（デバッグ用、オプション）
            # draw.rectangle([x1, y1, x2, y2], outline='blue', width=1)

        f6_elapsed = time.time() - f6_start
        logger.info(f"[F-6完了] {len(blocks)}個のID焼き込み, {f6_elapsed:.2f}秒")
        logger.info(f"  └─ 【Ver 6.4確認】AIに渡すID数: {len(blocks)}個（Surya検出ブロック全数）")

        return img_with_ids

    def _f6_store_id_mapping(self, blocks: List[Dict]) -> Dict[str, Dict]:
        """
        ID→座標マッピングをシステム内部に保持（AIには渡さない）

        Returns:
            {block_id: {bbox, page, column_id, ...}}
        """
        mapping = {}
        for block in blocks:
            block_id = block.get('block_id', '')
            mapping[block_id] = {
                'bbox': block.get('bbox'),
                'page': block.get('page', 0),
                'column_id': block.get('column_id', 0),
                'row_id': block.get('row_id', 0),
            }
        return mapping

    # ============================================
    # F-7: Dual Read - Path A (Text Extraction)
    # 【Ver 4.0】座標排除 - ID焼き込み画像のみ使用
    # ============================================
    def _f7_path_a_text_extraction(
        self,
        file_path: Path,
        page_images: List[Dict] = None
    ) -> Dict[str, Any]:
        """
        F-7 Path A: 構造マッピング（gemini-2.0-flash）

        【Ver 4.0】座標データは渡さない。ID焼き込み画像を見て判断。
        """
        f7_start = time.time()
        logger.info("[F-7] Path A - 構造マッピング開始（座標排除版）")

        prompt = self._build_f7_prompt()

        try:
            response = self.llm_client.generate_with_vision(
                prompt=prompt,
                image_path=str(file_path),
                model=F7_MODEL_IMAGE,
                max_tokens=F7_F8_MAX_TOKENS,
                temperature=F7_F8_TEMPERATURE,
                response_format="json"
            )

            # 【Ver 6.4】生成物ログ出力（MAX_TOKENS途切れ対応）
            logger.info(f"[F-7] ===== 生成物ログ開始 =====")
            logger.info(f"[F-7] レスポンス長: {len(response) if response else 0}文字")
            logger.info(f"[F-7] 生レスポンス:\n{response if response else '(empty)'}")
            logger.info(f"[F-7] ===== 生成物ログ終了 =====")

            # JSON パース
            try:
                result = json.loads(response)
            except json.JSONDecodeError as jde:
                logger.warning(f"[F-7] JSONパース失敗（MAX_TOKENS?）: {jde}")
                logger.warning(f"[F-7] 途切れたレスポンス末尾: ...{response[-500:] if response and len(response) > 500 else response}")
                import json_repair
                result = json_repair.repair_json(response, return_objects=True)

            f7_elapsed = time.time() - f7_start
            logger.info(f"[F-7完了] Path A: {len(response)}文字, {f7_elapsed:.2f}秒")

            return result

        except MaxTokensExceededError as mte:
            logger.error(f"[F-7] Path A MAX_TOKENS到達: {mte}")
            logger.info(f"[F-7] ===== MAX_TOKENS部分出力ログ開始 =====")
            logger.info(f"[F-7] 部分出力（全文）:\n{mte.partial_output}")
            logger.info(f"[F-7] ===== MAX_TOKENS部分出力ログ終了 =====")
            try:
                import json_repair
                result = json_repair.repair_json(mte.partial_output, return_objects=True)
                result['_max_tokens_partial'] = True
                return result
            except:
                return {"error": str(mte), "extracted_texts": [], "tables": [], "_max_tokens_partial": True}

        except Exception as e:
            logger.error(f"[F-7] Path A エラー: {e}")
            return {"error": str(e), "extracted_texts": [], "tables": []}

    def _parse_id_text_header(self, id_text_str: str) -> str:
        """
        【Ver 6.2】"ID:テキスト" 形式からテキスト部分を抽出

        Args:
            id_text_str: "1:2/1" 形式の文字列、または "none"

        Returns:
            テキスト部分（例: "2/1"）、または空文字
        """
        if not id_text_str or id_text_str.lower() == "none":
            return ""

        if ":" in id_text_str:
            # "ID:テキスト" 形式 → テキスト部分を返す
            parts = id_text_str.split(":", 1)
            return parts[1].strip() if len(parts) > 1 else ""
        else:
            # ":" がない場合はそのまま返す（互換性のため）
            return id_text_str.strip()

    def _infer_doc_type(self, mime_type: str, file_path: Path) -> str:
        """MIMEタイプとファイル名からドキュメントタイプを推測"""
        if not mime_type:
            return "unknown"

        if mime_type == 'application/pdf':
            # ファイル名からヒントを得る
            name = file_path.name.lower() if file_path else ""
            if '成績' in name or 'score' in name or '偏差値' in name:
                return "成績表・偏差値表"
            elif '時間割' in name or 'schedule' in name or 'timetable' in name:
                return "時間割・スケジュール"
            elif '通信' in name or 'news' in name or 'letter' in name:
                return "お知らせ・通信"
            return "PDF文書"
        elif mime_type.startswith('image/'):
            return "画像"
        elif mime_type.startswith('text/'):
            return "テキスト"
        return "その他"

    def _build_f7_prompt(self) -> str:
        """
        【Ver 6.4】AIは文字を読むだけ。位置判断は一切させない。
        """
        return """画像内の各ID（赤いラベル）に対応するテキストを紐づけてください。

出力形式: {"texts": [["ID番号", "テキスト"], ...]}

ルール:
- 画像内の文字を全て拾う
- 各文字を最寄りのIDと紐づける
- 対応するIDが見つからない文字は ["null", "テキスト"] とする
- 位置判断・分類は不要（後続処理で行う）
"""

    # ============================================
    # F-7: Transcription (音声/動画用)
    # ============================================
    def _f7_transcription(
        self,
        file_path: Path,
        mime_type: str,
        is_video: bool
    ) -> Dict[str, Any]:
        """
        F-7: 音声/動画の書き起こし（gemini-2.5-flash-lite）
        """
        f7_start = time.time()
        media_type = "動画" if is_video else "音声"
        logger.info(f"[F-7] Transcription ({media_type}) 開始")

        prompt = self._build_transcription_prompt(is_video)

        try:
            import google.generativeai as genai

            # ファイルアップロード
            logger.info(f"  ├─ ファイルアップロード中: {file_path.name}")
            uploaded_file = genai.upload_file(path=str(file_path), mime_type=mime_type)

            # 処理完了待機
            while uploaded_file.state.name == "PROCESSING":
                time.sleep(2)
                uploaded_file = genai.get_file(uploaded_file.name)

            if uploaded_file.state.name == "FAILED":
                raise ValueError(f"ファイル処理失敗: {uploaded_file.state.name}")

            # モデル初期化
            model = genai.GenerativeModel(F7_MODEL_AV)

            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]

            generation_config = genai.GenerationConfig(
                max_output_tokens=F7_F8_MAX_TOKENS,
                temperature=F7_F8_TEMPERATURE
            )

            # 生成
            response = model.generate_content(
                [prompt, uploaded_file],
                generation_config=generation_config,
                safety_settings=safety_settings,
                request_options={"timeout": 600}
            )

            transcript = ""
            visual_log = ""

            if response.candidates and response.candidates[0].content.parts:
                raw_text = response.candidates[0].content.parts[0].text

                # JSON パース試行
                try:
                    result = json.loads(raw_text)
                    transcript = result.get('transcript', raw_text)
                    visual_log = result.get('visual_log', '')
                except:
                    transcript = raw_text

            # ファイル削除
            try:
                genai.delete_file(name=uploaded_file.name)
            except:
                pass

            f7_elapsed = time.time() - f7_start
            logger.info(f"[F-7完了] Transcription: {len(transcript)}文字, {f7_elapsed:.2f}秒")

            return {
                "transcript": transcript,
                "visual_log": visual_log if is_video else "",
                "media_type": media_type,
                "model": F7_MODEL_AV
            }

        except Exception as e:
            logger.error(f"[F-7] Transcription エラー: {e}")
            return {"error": str(e), "transcript": "", "visual_log": ""}

    def _build_transcription_prompt(self, is_video: bool) -> str:
        """Transcription用プロンプト"""
        base = """# 音声/映像 完全書き起こし

## Mission
一言一句完全な書き起こしを行ってください。

## 重要な指示
- 「あー」「えー」「うーん」などのフィラーも全て書き起こす
- 言い淀み、言い直しもそのまま記録
- 笑い声、咳払いは [笑い]、[咳払い] のように記録
- 沈黙が長い場合は [沈黙 約5秒] のように記録
- 複数人の場合は話者を識別（話者A、話者B）
- 聞き取れない部分は [聞き取り不明] と記載

## 禁止事項
- 要約は絶対に禁止
- 文章の整理や言い換えは禁止
- 内容の省略は禁止

## 出力形式
```json
{
  "transcript": "[00:00] 話者A: えー、本日は...",
  "visual_log": ""
}
```
"""
        if is_video:
            base += """
## 動画の場合: visual_log も記録
```json
{
  "transcript": "...",
  "visual_log": "[00:00] 黒背景、中央にロゴ\\n[00:03] オフィス会議室が映る..."
}
```
"""
        return base

    # ============================================
    # F-8: Dual Read - Path B (Visual Analysis)
    # ============================================
    def _f8_path_b_visual_analysis(
        self,
        file_path: Path,
        page_images: List[Dict] = None
    ) -> Dict[str, Any]:
        """
        【Ver 4.0】F-8 Path B: 構造解析（座標排除版）
        """
        f8_start = time.time()
        logger.info("[F-8] Path B - Visual Analysis 開始（座標排除版）")

        prompt = self._build_f8_prompt()

        try:
            response = self.llm_client.generate_with_vision(
                prompt=prompt,
                image_path=str(file_path),
                model=F8_MODEL,
                max_tokens=F7_F8_MAX_TOKENS,
                temperature=F7_F8_TEMPERATURE,
                response_format="json"
            )

            # 【Ver 6.4】生成物ログ出力（MAX_TOKENS途切れ対応）
            logger.info(f"[F-8] ===== 生成物ログ開始 =====")
            logger.info(f"[F-8] レスポンス長: {len(response) if response else 0}文字")
            logger.info(f"[F-8] 生レスポンス:\n{response if response else '(empty)'}")
            logger.info(f"[F-8] ===== 生成物ログ終了 =====")

            # JSON パース
            try:
                result = json.loads(response)
            except json.JSONDecodeError as jde:
                logger.warning(f"[F-8] JSONパース失敗（MAX_TOKENS?）: {jde}")
                logger.warning(f"[F-8] 途切れたレスポンス末尾: ...{response[-500:] if response and len(response) > 500 else response}")
                import json_repair
                result = json_repair.repair_json(response, return_objects=True)

            f8_elapsed = time.time() - f8_start
            logger.info(f"[F-8完了] Path B: {len(response)}文字, {f8_elapsed:.2f}秒")

            return result

        except Exception as e:
            logger.error(f"[F-8] Path B エラー: {e}")
            return {"error": str(e), "tables": [], "diagrams": [], "layout_analysis": {}}

    def _build_f8_prompt(self) -> str:
        """【Ver 4.0】F-8用プロンプト構築（座標排除版）"""
        return """# F-8: 構造解析

## Mission
画像内の表・図解の構造を解析せよ。
文字の書き起こしはPath Aが担当。あなたは**構造・関係性**に集中。

## 解析対象

### 1. 表の構造解析（最重要）

#### 検出すべき情報
- **X軸ヘッダー（x_headers）**: 表の列見出し（例: 偏差値、2/1、2/2、2/3）
- **Y軸ヘッダー（y_headers）**: 表の行見出し（例: 開成、麻布、武蔵）
- **セル結合**: colspan（横結合）、rowspan（縦結合）の正確な位置と範囲
- **ヘッダー構造**: 何行目までがヘッダーか、多段ヘッダーの場合その構造
- **データ型推定**: 各列が以下のいずれか
  - `text`: テキスト（氏名、項目名など）
  - `number`: 数値（個数、順位など）
  - `currency`: 金額（円、ドルなど）
  - `date`: 日付
  - `time`: 時刻
  - `percentage`: パーセンテージ
- **小計・合計行**: 太字や背景色で強調された集計行の位置（row index）
- **空白セルの意味**: 「データなし」か「上と同じ（ditto）」か

#### 表の種類（table_type）を判定
- `list_type`: リスト型（縦方向にデータが並ぶ）
  - ランキング表、名簿、成績一覧、商品リスト
  - 各行が1つのエントリ（人、商品、項目）を表す
- `matrix_type`: マトリクス型（横軸に日付や項目が並ぶ）
  - 時間割、月間予定表、週間スケジュール
  - 行と列の交差点にデータがある

### 2. 構造化可能なデータの検出
視覚的に表として表示されていなくても、以下のパターンを検出：
- **Key-Valueペア**: 「項目名: 値」の繰り返し
- **ランキング・順位表**: 「1位: ○○」「2位: △△」
- **カンマ区切りデータ**: 「A, B, C」のような並列データ
- **罫線なしの表**: 空白/タブで区切られた列データ

### 3. 図解・フローチャート
- 要素間の接続、矢印の向き
- 階層構造、グループ化
- 条件分岐（Yes/No）

### 4. グラフ・チャート
- グラフ種類（棒、折れ線、円、散布図）
- 軸ラベル、凡例
- データ傾向（増加、減少、ピーク位置）

### 5. レイアウト
- 段組構造、強調パターン
- セクション区切り

## 出力形式
```json
{{
  "tables": [
    {{
      "block_id": "p0_b5",
      "table_type": "list_type",
      "x_headers": ["偏差値", "2/1", "2/2", "2/3"],
      "y_headers": ["開成", "麻布", "武蔵", "駒場東邦"],
      "structure": {{
        "header_rows": 1,
        "total_rows": 10,
        "total_cols": 5,
        "merged_cells": [
          {{"row": 0, "col": 0, "rowspan": 2, "colspan": 1, "content_hint": "項目名"}}
        ],
        "column_types": ["text", "text", "currency", "currency", "percentage"],
        "summary_rows": [9],
        "has_footer": true
      }},
      "semantic_role": "四半期売上比較表",
      "data_quality": {{
        "empty_cells": 2,
        "ditto_cells": 0,
        "needs_verification": false
      }}
    }}
  ],
  "x_headers": ["偏差値", "2/1", "2/2", "2/3"],
  "y_headers": ["開成", "麻布", "武蔵", "駒場東邦"],
  "structured_data_candidates": [
    {{
      "location": "本文中段",
      "pattern": "key_value_pairs",
      "suggested_headers": ["項目", "内容"],
      "estimated_rows": 5,
      "source_text_hint": "提出期限: 2025-01-15..."
    }}
  ],
  "diagrams": [
    {{
      "block_id": "p0_b8",
      "type": "flowchart",
      "elements_count": 5,
      "connections_count": 4,
      "has_conditions": true,
      "semantic_role": "申請承認フロー"
    }}
  ],
  "charts": [
    {{
      "block_id": "p0_b12",
      "type": "bar_chart",
      "x_axis": "月",
      "y_axis": "売上（万円）",
      "data_points_approx": 12,
      "trend": "Q3で急増、Q4で減少"
    }}
  ],
  "layout_analysis": {{
    "column_structure": "2-column",
    "sections": [
      {{"name": "ヘッダー", "blocks": ["p0_b0"], "purpose": "タイトル"}},
      {{"name": "本文", "blocks": ["p0_b1", "p0_b2"], "purpose": "説明文"}},
      {{"name": "表エリア", "blocks": ["p0_b5"], "purpose": "データ表示"}}
    ],
    "emphasis_patterns": ["見出しは青色太字", "重要数値は赤色"]
  }}
}}
```

## 禁止事項
- 文字の書き起こし禁止（それはPath Aの仕事）
- 推測による補完禁止
- **表の行数・列数を間違えることは許されない（正確にカウント）**
"""

    # ============================================
    # チャンク処理用メソッド（MAX_TOKENSエラー回避）
    # ============================================

    def _save_chunk_as_temp_image(self, chunk_pages: List[Dict], chunk_idx: int) -> Path:
        """
        チャンク内の画像を一時ファイルとして保存
        複数ページの場合は縦に結合した1枚の画像として保存
        """
        import tempfile

        if len(chunk_pages) == 1:
            # 1ページのみ: そのまま保存
            img = chunk_pages[0]['image']
        else:
            # 複数ページ: 縦に結合
            images = [p['image'] for p in chunk_pages]
            total_height = sum(img.size[1] for img in images)
            max_width = max(img.size[0] for img in images)

            combined = Image.new('RGB', (max_width, total_height), (255, 255, 255))
            y_offset = 0
            for img in images:
                combined.paste(img, (0, y_offset))
                y_offset += img.size[1]

            img = combined

        # 一時ファイルに保存
        temp_file = tempfile.NamedTemporaryFile(
            suffix=f'_chunk{chunk_idx}.png',
            delete=False
        )
        img.save(temp_file.name, 'PNG')
        temp_file.close()

        return Path(temp_file.name)

    # ============================================
    # ============================================
    # 【Ver 4.0】泥棒ロジック（_detect_gutters）完全削除済み
    # F-7は自律判断を行わない。F-3.5の決定を無条件実行するのみ。
    # ============================================

    def _smart_crop_patches(
        self,
        image: Image.Image,
        column_boundaries: List[int],
        overlap: int = 50
    ) -> List[Dict]:
        """
        【Ver 4.0】F-3.5の決定を「神の宣告」として無条件実行

        ============================================
        泥棒ロジック完全排除版
        ============================================

        このメソッドは「自律判断」を一切行わない。
        F-3.5が決定した split_axis と boundaries だけを信じ、
        ただ切る。それ以外のことは1行もしない。
        """
        img_width, img_height = image.size
        patches = []

        # ============================================
        # 【Ver 3.8】F-3.5の決定を取得（絶対命令・上書き禁止）
        # ============================================
        split_axis = getattr(self, '_current_split_axis', 'none')
        split_positions = getattr(self, '_current_split_positions', [])

        # 【Ver 3.8】フォールバック削除 - F-3.5の決定のみを信じる
        # column_boundaries引数は無視。お節介な再計算は一切しない。

        logger.info(f"[SmartCrop] 【Ver 3.8】F-3.5絶対命令: split_axis={split_axis}")
        logger.info(f"[SmartCrop] split_positions: {split_positions}")
        logger.info(f"[SmartCrop] column_boundaries（引数）: {column_boundaries}")

        # ============================================
        # 分割なしの場合: 画像全体を1枚で返す
        # ============================================
        if split_axis == 'none' or not split_positions:
            logger.info("[SmartCrop] 分割なし → 全体を1枚で返却")
            return [{
                'image': image,
                'type': 'full',
                'info': '全体（F-3.5: 分割不要）',
                'patch_index': 0,
                'total_patches': 1,
                'is_continuation': False
            }]

        # ============================================
        # 【Ver 5.0】垂直分割 - 単純矩形スライス
        # 座標の歪みを排除。シンプルに縦に切るだけ。
        # ============================================
        if split_axis == 'vertical':
            # 量子化座標を実座標に変換
            boundaries_px = [0]
            for pos in split_positions:
                px = int(pos * img_width / QUANTIZE_GRID_SIZE)
                # 重複排除・有効範囲チェック
                if px > boundaries_px[-1] + 50 and px < img_width - 50:
                    boundaries_px.append(px)
            boundaries_px.append(img_width)

            num_columns = len(boundaries_px) - 1

            # 【診断ログ】全境界線を表示
            logger.info(f"[SmartCrop] ======== 【Ver 5.0】単純矩形垂直分割 ========")
            logger.info(f"[SmartCrop] split_positions（量子化）: {split_positions}")
            logger.info(f"[SmartCrop] boundaries_px（実座標）: {boundaries_px}")
            logger.info(f"[SmartCrop] 画像サイズ: {img_width}x{img_height}")
            logger.info(f"[SmartCrop] 生成予定列数: {num_columns}")

            # 最低2列は必要（1列なら分割なしと同じ）
            if num_columns < 2:
                logger.warning(f"[SmartCrop] 列数不足({num_columns}) → 全体を1枚で返却")
                return [{
                    'image': image,
                    'type': 'full',
                    'info': '全体（境界線不足）',
                    'patch_index': 0,
                    'total_patches': 1,
                    'is_continuation': False
                }]

            # ============================================
            # 【Ver 5.0】単純矩形スライス
            # 各パッチはフルハイトの縦帯として切り出す
            # ヘッダーもインデックスも切らない（全部見える状態で渡す）
            # ============================================
            for col_idx in range(num_columns):
                x_start = boundaries_px[col_idx]
                x_end = boundaries_px[col_idx + 1]

                # のりしろ付きで切り出し
                crop_start = max(0, x_start - overlap) if col_idx > 0 else 0
                crop_end = min(img_width, x_end + overlap) if col_idx < num_columns - 1 else img_width

                # 単純な矩形切り出し（フルハイト）
                col_img = image.crop((crop_start, 0, crop_end, img_height))
                col_width = crop_end - crop_start

                logger.info(f"[SmartCrop] 列{col_idx + 1}/{num_columns}: x={x_start}〜{x_end} (crop:{crop_start}〜{crop_end}, {col_width}x{img_height})")

                patches.append({
                    'image': col_img,
                    'type': 'v_column_simple',
                    'info': f'垂直列{col_idx + 1}/{num_columns} (x:{x_start}-{x_end})',
                    'patch_index': col_idx,
                    'total_patches': num_columns,
                    'is_continuation': col_idx > 0,
                    'x_range': (x_start, x_end),
                    'crop_range': (crop_start, crop_end)
                })

            logger.info(f"[SmartCrop] ======== 単純矩形分割完了: {len(patches)}パッチ ========")
            return patches

        # ============================================
        # 水平分割（横に切る = 上下に分ける）
        # 縦方向の検討は1行も許さない
        # ============================================
        if split_axis == 'horizontal':
            # 量子化座標を実座標に変換
            boundaries_px = [0]
            for pos in split_positions:
                px = int(pos * img_height / QUANTIZE_GRID_SIZE)
                boundaries_px.append(px)
            boundaries_px.append(img_height)

            num_sections = len(boundaries_px) - 1
            logger.info(f"[SmartCrop] 水平分割: {num_sections}セクション at {boundaries_px}")

            # ヘッダー行（最上部）
            header_height = boundaries_px[1] if len(boundaries_px) > 1 else img_height
            header_row = image.crop((0, 0, img_width, min(header_height + overlap, img_height)))

            for sec_idx in range(num_sections):
                y_start = max(0, boundaries_px[sec_idx] - overlap)
                y_end = min(img_height, boundaries_px[sec_idx + 1] + overlap)

                sec_img = image.crop((0, y_start, img_width, y_end))
                sec_height = y_end - y_start

                if sec_idx == 0:
                    # 最初のセクションはそのまま
                    patches.append({
                        'image': sec_img,
                        'type': 'h_section',
                        'info': f'水平セクション{sec_idx + 1}/{num_sections}',
                        'patch_index': sec_idx,
                        'total_patches': num_sections,
                        'is_continuation': False
                    })
                else:
                    # 2セクション目以降: ヘッダー行を上に結合
                    # ヘッダーの高さをセクションに合わせてスケール（最大20%まで）
                    scaled_header_height = min(int(sec_height * 0.2), header_row.height)
                    scaled_header = header_row.resize(
                        (img_width, scaled_header_height),
                        Image.Resampling.LANCZOS
                    )

                    combined_height = scaled_header.height + sec_img.height
                    combined = Image.new('RGB', (img_width, combined_height), (255, 255, 255))
                    combined.paste(scaled_header, (0, 0))
                    combined.paste(sec_img, (0, scaled_header.height))

                    patches.append({
                        'image': combined,
                        'type': 'h_section_with_header',
                        'info': f'水平セクション{sec_idx + 1}/{num_sections}（ヘッダー付き）',
                        'patch_index': sec_idx,
                        'total_patches': num_sections,
                        'is_continuation': True
                    })

            logger.info(f"[SmartCrop] 水平分割完了: {len(patches)}パッチ")
            return patches

        # ここに到達することはない（安全弁）
        logger.warning(f"[SmartCrop] 不明なsplit_axis: {split_axis}")
        return [{
            'image': image,
            'type': 'full',
            'info': '全体（フォールバック）',
            'patch_index': 0,
            'total_patches': 1,
            'is_continuation': False
        }]

    # ============================================
    # 【Ver 6.2】F-8 → F-7 構造先行モデル
    # ============================================

    def _extract_headers_from_f8(self, f8_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        【Ver 6.2】F-8 の結果からヘッダー情報を抽出

        F-8 が検出した表構造からヘッダー（X軸・Y軸）を抽出し、
        各ヘッダーの物理座標（BBox）も含めて返す。

        Args:
            f8_result: F-8 の出力

        Returns:
            {
                'x_headers': ['2/1', '2/2', ...],
                'y_headers': ['74', '73', ...],
                'header_coords': {
                    '2/1': {'x': 100, 'y': 50, 'bbox': [...]},
                    '74': {'x': 30, 'y': 200, 'bbox': [...]},
                    ...
                }
            }
        """
        result = {
            'x_headers': [],
            'y_headers': [],
            'header_coords': {}
        }

        # 【Ver 6.4】トップレベルのヘッダーを優先取得
        top_x = f8_result.get('x_headers', [])
        top_y = f8_result.get('y_headers', [])
        if top_x:
            for xh in top_x:
                if isinstance(xh, str) and xh and xh not in result['x_headers']:
                    result['x_headers'].append(xh)
        if top_y:
            for yh in top_y:
                if isinstance(yh, str) and yh and yh not in result['y_headers']:
                    result['y_headers'].append(yh)

        # F-8 の表構造からも抽出（トップレベルがなければこちらを使用）
        tables = f8_result.get('tables', [])
        for table in tables:
            # X軸ヘッダー（列見出し）
            x_headers = table.get('column_headers', []) or table.get('x_headers', [])
            for xh in x_headers:
                if isinstance(xh, dict):
                    text = xh.get('text', '')
                    bbox = xh.get('bbox')
                    if text and text not in result['x_headers']:
                        result['x_headers'].append(text)
                        if bbox:
                            x_center = (bbox[0] + bbox[2]) / 2 if isinstance(bbox, (list, tuple)) else 0
                            y_center = (bbox[1] + bbox[3]) / 2 if isinstance(bbox, (list, tuple)) else 0
                            result['header_coords'][text] = {'x': x_center, 'y': y_center, 'bbox': bbox}
                elif isinstance(xh, str) and xh:
                    if xh not in result['x_headers']:
                        result['x_headers'].append(xh)

            # Y軸ヘッダー（行見出し）
            y_headers = table.get('row_headers', []) or table.get('y_headers', [])
            for yh in y_headers:
                if isinstance(yh, dict):
                    text = yh.get('text', '')
                    bbox = yh.get('bbox')
                    if text and text not in result['y_headers']:
                        result['y_headers'].append(text)
                        if bbox:
                            x_center = (bbox[0] + bbox[2]) / 2 if isinstance(bbox, (list, tuple)) else 0
                            y_center = (bbox[1] + bbox[3]) / 2 if isinstance(bbox, (list, tuple)) else 0
                            result['header_coords'][text] = {'x': x_center, 'y': y_center, 'bbox': bbox}
                elif isinstance(yh, str) and yh:
                    if yh not in result['y_headers']:
                        result['y_headers'].append(yh)

        logger.debug(f"[F-8→F-7] ヘッダー抽出: X={result['x_headers']}, Y={result['y_headers']}")
        return result

    def _f7_path_a_chunk_extraction_v62(
        self,
        chunk_pages: List[Dict],
        blocks: List[Dict],
        chunk_idx: int,
        chunk_start_page: int,
        column_boundaries: List[int] = None
    ) -> Dict[str, Any]:
        """
        【Ver 6.4】F-7: 文字読み取りのみ

        AIの仕事: 黄色ラベル(#ID)の横にある文字を読み取るだけ
        座標マッピングはF-9(PROGRAM)で実行

        Args:
            chunk_pages: ページ画像リスト
            blocks: Suryaで検出したブロック（座標情報付き）
            chunk_idx: チャンクインデックス
            chunk_start_page: 開始ページ番号
            column_boundaries: 列境界

        Returns:
            抽出結果（raw_texts + block_coords）
        """
        import tempfile
        f7_start = time.time()
        logger.info(f"[F-7] 【Ver 6.4】チャンク{chunk_idx + 1} 文字読み取り開始")
        logger.info(f"  ├─ 【Ver 6.4確認】受け取ったブロック数: {len(blocks)}個")

        if not chunk_pages or 'image' not in chunk_pages[0]:
            logger.error(f"[F-7] チャンク{chunk_idx + 1} 画像データなし")
            return {"error": "No image data", "raw_texts": [], "block_coords": {}, "full_text_ordered": ""}

        original_image: Image.Image = chunk_pages[0]['image']

        # ID焼き込み画像を生成
        id_burned_image = self._f6_burn_ids_to_image(original_image, blocks, chunk_start_page)

        # ブロックの座標マップを構築（F-9で使用）
        block_coords = {}
        for block in blocks:
            block_id = block.get('block_id', '')
            bbox = block.get('bbox')
            if block_id and bbox:
                if isinstance(bbox, (list, tuple)) and len(bbox) >= 4:
                    x_center = (bbox[0] + bbox[2]) / 2
                    y_center = (bbox[1] + bbox[3]) / 2
                    block_coords[block_id] = {'x': x_center, 'y': y_center, 'bbox': bbox}

        # 一時ファイルに保存してAIに渡す
        with tempfile.NamedTemporaryFile(suffix=f'_chunk{chunk_idx}_v64.png', delete=False) as f:
            id_burned_image.save(f, format='PNG')
            temp_path = Path(f.name)

        try:
            # Ver 6.4 プロンプト: AIは文字を読むだけ
            prompt = self._build_f7_prompt()

            response = self.llm_client.generate_with_vision(
                prompt=prompt,
                image_path=str(temp_path),
                model=F7_MODEL_IMAGE,
                max_tokens=F7_F8_MAX_TOKENS,
                temperature=F7_F8_TEMPERATURE,
                response_format="json"
            )

            # 【Ver 6.4】生成物ログ出力（MAX_TOKENS途切れ対応）
            logger.info(f"[F-7] ===== 生成物ログ開始 =====")
            logger.info(f"[F-7] レスポンス長: {len(response) if response else 0}文字")
            logger.info(f"[F-7] 生レスポンス:\n{response if response else '(empty)'}")
            logger.info(f"[F-7] ===== 生成物ログ終了 =====")

            # JSONパース
            try:
                result = json.loads(response)
            except json.JSONDecodeError as jde:
                logger.warning(f"[F-7] JSONパース失敗（MAX_TOKENS?）: {jde}")
                logger.warning(f"[F-7] 途切れたレスポンス末尾: ...{response[-500:] if response and len(response) > 500 else response}")
                import json_repair
                result = json_repair.repair_json(response, return_objects=True)

            f7_elapsed = time.time() - f7_start
            raw_texts = result.get('texts', [])
            logger.info(f"[F-7] 【Ver 6.4】AIテキスト読み取り完了: {f7_elapsed:.2f}秒, {len(raw_texts)}件")

            # F-7は読み取り結果と座標情報を返す（マッピングはF-9で実行）
            result['raw_texts'] = raw_texts
            result['block_coords'] = block_coords
            result['_v64_raw_only'] = True

            # メインループ互換: extracted_texts と full_text_ordered をセット
            extracted_texts = []
            full_text_parts = []
            for item in raw_texts:
                if isinstance(item, (list, tuple)) and len(item) >= 2:
                    block_id = str(item[0]).strip()
                    text = str(item[1]).strip()
                    coords = block_coords.get(block_id, {})
                    extracted_texts.append({
                        'block_id': block_id,
                        'text': text,
                        'coords': coords,
                        'page': chunk_start_page
                    })
                    if text:
                        full_text_parts.append(text)

            result['extracted_texts'] = extracted_texts
            result['full_text_ordered'] = '\n'.join(full_text_parts)

            logger.info(f"[F-7] 【Ver 6.4】データ整形完了: {len(extracted_texts)}ブロック, {len(result['full_text_ordered'])}文字")

            return result

        except MaxTokensExceededError as mte:
            # 【Ver 6.4】MAX_TOKENS到達時も部分出力をログに記録
            logger.error(f"[F-7] MAX_TOKENS到達: {mte}")
            logger.info(f"[F-7] ===== MAX_TOKENS部分出力ログ開始 =====")
            logger.info(f"[F-7] 部分出力長: {len(mte.partial_output)}文字")
            logger.info(f"[F-7] 部分出力（全文）:\n{mte.partial_output}")
            logger.info(f"[F-7] ===== MAX_TOKENS部分出力ログ終了 =====")

            # 部分出力でもパースを試みる
            try:
                import json_repair
                result = json_repair.repair_json(mte.partial_output, return_objects=True)
                if isinstance(result, dict) and 'texts' in result:
                    logger.info(f"[F-7] 部分出力からテキスト復元成功: {len(result.get('texts', []))}件")
                    result['block_coords'] = block_coords
                    result['_max_tokens_partial'] = True
                    return result
            except Exception as parse_err:
                logger.warning(f"[F-7] 部分出力のパース失敗: {parse_err}")

            return {"error": str(mte), "raw_texts": [], "block_coords": block_coords, "full_text_ordered": "", "_max_tokens_partial": True}

        except Exception as e:
            logger.error(f"[F-7] 【Ver 6.4】エラー: {e}")
            return {"error": str(e), "raw_texts": [], "block_coords": {}, "full_text_ordered": ""}

        finally:
            try:
                temp_path.unlink()
            except:
                pass

    def _map_texts_to_headers_by_coords(
        self,
        ai_texts: List[List[str]],
        block_coords: Dict[str, Dict],
        f8_headers: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        【Ver 6.2 核心】プログラムによる座標マッピング

        AIが読んだテキストを、Surya座標を使って最短距離のヘッダーに紐付ける。
        AIの判断は一切信用しない。物理座標のみを信じる。

        Args:
            ai_texts: AIが読んだ [[ID, テキスト], ...] の配列
            block_coords: {block_id: {x, y, bbox}} の座標マップ
            f8_headers: F-8 から抽出したヘッダー情報

        Returns:
            tagged_texts: [{text, x_header, y_header, ...}, ...]
        """
        tagged_texts = []
        header_coords = f8_headers.get('header_coords', {})
        x_headers = f8_headers.get('x_headers', [])
        y_headers = f8_headers.get('y_headers', [])

        for item in ai_texts:
            if not isinstance(item, (list, tuple)) or len(item) < 2:
                continue

            block_id = str(item[0]).strip()
            text = str(item[1]).strip()

            if not text:
                continue

            # 短縮ID（#5 → p0_r5 など）を正規化
            normalized_id = self._normalize_block_id(block_id)

            # このブロックの座標を取得
            coords = block_coords.get(normalized_id) or block_coords.get(block_id)
            if not coords:
                # 座標がない場合は untagged として扱う
                tagged_texts.append({
                    'id': block_id,
                    'text': text,
                    'x_header': '',
                    'y_header': '',
                    'type': 'untagged',
                    '_no_coords': True
                })
                continue

            text_x = coords['x']
            text_y = coords['y']

            # ============================================
            # 最短距離でX軸ヘッダーを決定
            # ============================================
            nearest_x_header = ''
            min_x_distance = float('inf')

            for xh in x_headers:
                if xh in header_coords:
                    xh_x = header_coords[xh]['x']
                    distance = abs(text_x - xh_x)
                    if distance < min_x_distance:
                        min_x_distance = distance
                        nearest_x_header = xh

            # ============================================
            # 最短距離でY軸ヘッダーを決定
            # ============================================
            nearest_y_header = ''
            min_y_distance = float('inf')

            for yh in y_headers:
                if yh in header_coords:
                    yh_y = header_coords[yh]['y']
                    distance = abs(text_y - yh_y)
                    if distance < min_y_distance:
                        min_y_distance = distance
                        nearest_y_header = yh

            # ヘッダーが見つかったかどうかで type を決定
            if nearest_x_header or nearest_y_header:
                item_type = 'cell'
            else:
                item_type = 'untagged'

            tagged_texts.append({
                'id': block_id,
                'text': text,
                'x_header': nearest_x_header,
                'y_header': nearest_y_header,
                'type': item_type,
                '_x_distance': min_x_distance if nearest_x_header else None,
                '_y_distance': min_y_distance if nearest_y_header else None
            })

            logger.debug(f"[F-7座標] #{block_id}='{text}' → X='{nearest_x_header}'(d={min_x_distance:.1f}), Y='{nearest_y_header}'(d={min_y_distance:.1f})")

        return tagged_texts

    def _normalize_block_id(self, short_id: str) -> str:
        """
        短縮ID（#5）を正規化されたblock_id（p0_r5）に変換

        現在の _id_mapping を使用して逆引き
        """
        if not hasattr(self, '_id_mapping') or not self._id_mapping:
            return short_id

        # _id_mapping は {block_id: {bbox, page, ...}} 形式
        # short_id が "5" の場合、"p0_r5" や "r5" を探す
        for block_id in self._id_mapping.keys():
            if block_id.endswith(f'_{short_id}') or block_id.endswith(f'r{short_id}'):
                return block_id

        return short_id

    def _f7_path_a_chunk_extraction(
        self,
        chunk_pages: List[Dict],
        blocks: List[Dict],
        chunk_idx: int,
        chunk_start_page: int,
        column_boundaries: List[int] = None
    ) -> Dict[str, Any]:
        """
        【Ver 4.0】F-7 Path A: 座標排除・ID焼き込み版

        【設計原則】
        1. 座標データはAIに渡さない
        2. ID焼き込み画像を使用
        3. AIは「見たまま」構造をマッピング
        """
        import tempfile
        f7_start = time.time()
        logger.info(f"[F-7] Path A - チャンク{chunk_idx + 1} ID焼き込み版開始")

        # 元画像を取得
        if not chunk_pages or 'image' not in chunk_pages[0]:
            logger.error(f"[F-7] チャンク{chunk_idx + 1} 画像データなし")
            return {"error": "No image data", "extracted_texts": [], "tables": [], "full_text_ordered": ""}

        original_image: Image.Image = chunk_pages[0]['image']

        # 【Ver 4.0】ID焼き込み画像を生成
        id_burned_image = self._f6_burn_ids_to_image(original_image, blocks, chunk_start_page)

        # スマートクロッピングでパッチを生成（ID焼き込み済み画像から）
        patches = self._smart_crop_patches(id_burned_image, column_boundaries, overlap=50)

        # 各パッチを処理
        all_extracted_texts = []
        all_tables = []
        all_full_texts = []
        patch_errors = []

        for patch_idx, patch in enumerate(patches):
            patch_img = patch['image']
            patch_type = patch['type']
            patch_info = patch['info']
            is_continuation = patch.get('is_continuation', False)

            logger.info(f"[F-7] パッチ{patch_idx + 1}/{len(patches)}: {patch_info} ({patch_img.size[0]}x{patch_img.size[1]}) 継続={is_continuation}")

            # 一時ファイルに保存
            with tempfile.NamedTemporaryFile(suffix=f'_chunk{chunk_idx}_patch{patch_idx}.png', delete=False) as f:
                patch_img.save(f, format='PNG')
                temp_path = Path(f.name)

            try:
                # 【Ver 4.0】座標なしプロンプト
                prompt = self._build_f7_smart_prompt(
                    chunk_idx, chunk_start_page, patch_info, patch_type, len(patches),
                    patch_index=patch_idx, is_continuation=is_continuation
                )

                response = self.llm_client.generate_with_vision(
                    prompt=prompt,
                    image_path=str(temp_path),
                    model=F7_MODEL_IMAGE,
                    max_tokens=F7_F8_MAX_TOKENS,
                    temperature=F7_F8_TEMPERATURE,
                    response_format="json"
                )

                # 【Ver 6.4】生成物ログ出力（MAX_TOKENS途切れ対応）
                logger.info(f"[F-7] ===== 生成物ログ開始（パッチ{patch_idx + 1}） =====")
                logger.info(f"[F-7] レスポンス長: {len(response) if response else 0}文字")
                logger.info(f"[F-7] 生レスポンス:\n{response if response else '(empty)'}")
                logger.info(f"[F-7] ===== 生成物ログ終了 =====")

                # JSON パース
                try:
                    result = json.loads(response)
                except json.JSONDecodeError as jde:
                    logger.warning(f"[F-7] JSONパース失敗（MAX_TOKENS?）: {jde}")
                    logger.warning(f"[F-7] 途切れたレスポンス末尾: ...{response[-500:] if response and len(response) > 500 else response}")
                    import json_repair
                    result = json_repair.repair_json(response, return_objects=True)

                # 【診断】Gemini応答の中身を確認
                logger.info(f"[F-7完了] パッチ{patch_idx + 1}/{len(patches)}: {len(response)}文字")
                logger.info(f"[F-7診断] result keys: {list(result.keys()) if isinstance(result, dict) else 'NOT A DICT'}")

                # ============================================
                # 【Ver 6.4】最軽量形式の展開（texts のみ）
                # AIは [ID, テキスト] ペアのみを出力
                # ヘッダー割り当てはF-9が座標ベースで実行
                # ============================================
                if 'texts' in result and isinstance(result.get('texts'), list):
                    raw_texts = result['texts']
                    extracted_texts = []
                    full_text_parts = []

                    for item in raw_texts:
                        if isinstance(item, (list, tuple)) and len(item) >= 2:
                            block_id = str(item[0]).strip()
                            text = str(item[1]).strip()
                            coords = block_coords.get(block_id, {})
                            extracted_texts.append({
                                'block_id': block_id,
                                'text': text,
                                'coords': coords,
                                'page': chunk_start_page,
                                'patch_idx': patch_idx,
                                'type': 'cell'  # F-9 で座標ベースで再分類される
                            })
                            if text:
                                full_text_parts.append(text)

                    result['extracted_texts'] = extracted_texts
                    result['raw_texts'] = raw_texts
                    result['full_text_ordered'] = '\n'.join(full_text_parts)
                    result['_v64_format'] = True

                    all_extracted_texts.extend(extracted_texts)
                    all_full_texts.append(result['full_text_ordered'])

                    logger.info(f"[F-7] 【Ver 6.4】パッチ{patch_idx + 1}: {len(extracted_texts)}ブロック抽出")

                    # 【Ver 6.4】Ver 5.0 の後続処理をスキップ（重複防止）
                    continue

                # ============================================
                # 【Ver 6.2 互換】CSV型JSON形式の展開（ID:テキスト溶接対応）
                # cols + rows 形式を tagged_texts 形式に変換
                # ============================================
                elif 'cols' in result and 'rows' in result:
                    cols = result.get('cols', [])
                    rows = result.get('rows', [])

                    # Ver 6.2 形式かどうかを判定（id, xh, yh 列の存在）
                    is_v62 = 'id' in cols and 'xh' in cols and 'yh' in cols
                    logger.info(f"[F-7診断] 【Ver 6.2】CSV型JSON検出: {len(cols)}列 x {len(rows)}行 (v6.2={is_v62})")

                    if is_v62:
                        # ============================================
                        # 【Ver 6.2】ID:テキスト溶接形式の展開
                        # ============================================
                        id_idx = cols.index('id')
                        text_idx = cols.index('text')
                        xh_idx = cols.index('xh')
                        yh_idx = cols.index('yh')
                        type_idx = cols.index('type') if 'type' in cols else -1

                        tagged_texts = []
                        untagged_texts = []
                        x_headers = []
                        y_headers = []

                        for row in rows:
                            if not isinstance(row, list) or len(row) < 4:
                                continue

                            item_id = str(row[id_idx]).strip() if len(row) > id_idx else ""
                            text_val = str(row[text_idx]).strip() if len(row) > text_idx else ""
                            xh_val = str(row[xh_idx]).strip() if len(row) > xh_idx else ""
                            yh_val = str(row[yh_idx]).strip() if len(row) > yh_idx else ""
                            type_val = str(row[type_idx]).strip().lower() if type_idx >= 0 and len(row) > type_idx else "cell"

                            if not text_val:
                                continue

                            # ヘッダーを収集
                            if type_val == "x_header":
                                x_headers.append(text_val)
                                continue
                            elif type_val == "y_header":
                                y_headers.append(text_val)
                                continue
                            elif type_val in ("title", "note"):
                                untagged_texts.append({
                                    "id": item_id,
                                    "text": text_val,
                                    "type": type_val
                                })
                                continue

                            # セルデータ: "ID:テキスト" 形式をパース
                            x_header = self._parse_id_text_header(xh_val)
                            y_header = self._parse_id_text_header(yh_val)

                            tagged_texts.append({
                                "id": item_id,
                                "text": text_val,
                                "x_header": x_header,
                                "y_header": y_header,
                                "xh_raw": xh_val,  # 生の "ID:テキスト" を保持（デバッグ用）
                                "yh_raw": yh_val
                            })

                        # ヘッダーをresultに追加
                        if x_headers:
                            result['x_headers'] = x_headers
                        if y_headers:
                            result['y_headers'] = y_headers
                        result['tagged_texts'] = tagged_texts
                        result['untagged_texts'] = untagged_texts
                        result['_v62_format'] = True  # Ver 6.2 フラグ

                        logger.info(f"[F-7診断] 【Ver 6.2】展開完了: x_headers={len(x_headers)}, y_headers={len(y_headers)}, tagged={len(tagged_texts)}, untagged={len(untagged_texts)}")

                    else:
                        # ============================================
                        # 【Ver 6.1 互換】従来形式の展開
                        # ============================================
                        text_idx = cols.index('text') if 'text' in cols else 0
                        x_h_idx = cols.index('x_h') if 'x_h' in cols else 1
                        y_h_idx = cols.index('y_h') if 'y_h' in cols else 2
                        type_idx = cols.index('type') if 'type' in cols else 3

                        tagged_texts = []
                        untagged_texts = []

                        for row in rows:
                            if not isinstance(row, list) or len(row) < 1:
                                continue

                            text_val = row[text_idx] if len(row) > text_idx else ""
                            x_h_val = row[x_h_idx] if len(row) > x_h_idx else ""
                            y_h_val = row[y_h_idx] if len(row) > y_h_idx else ""
                            type_val = row[type_idx] if len(row) > type_idx else "tagged"

                            if not text_val:
                                continue

                            if type_val == "tagged" and (x_h_val or y_h_val):
                                tagged_texts.append({
                                    "text": str(text_val),
                                    "x_header": str(x_h_val),
                                    "y_header": str(y_h_val)
                                })
                            else:
                                untagged_texts.append({
                                    "text": str(text_val),
                                    "type": str(type_val) if type_val != "tagged" else "note"
                                })

                        result['tagged_texts'] = tagged_texts
                        result['untagged_texts'] = untagged_texts

                        logger.info(f"[F-7診断] 【Ver 6.1】展開完了: tagged={len(tagged_texts)}, untagged={len(untagged_texts)}")

                logger.info(f"[F-7診断] x_headers: {len(result.get('x_headers', []))}個")
                logger.info(f"[F-7診断] y_headers: {len(result.get('y_headers', []))}個")
                logger.info(f"[F-7診断] tagged_texts: {len(result.get('tagged_texts', []))}個")
                logger.info(f"[F-7診断] untagged_texts: {len(result.get('untagged_texts', []))}個")
                logger.info(f"[F-7診断] cells(旧形式): {len(result.get('cells', []))}個")
                logger.info(f"[F-7診断] texts(旧形式): {len(result.get('texts', []))}個")

                # トークン使用量を収集
                if hasattr(self.llm_client, 'last_usage') and self.llm_client.last_usage:
                    usage = self.llm_client.last_usage.copy()
                    usage['chunk_idx'] = chunk_idx
                    usage['patch_idx'] = patch_idx
                    self._f7_usage.append(usage)

                # ============================================
                # 【Ver 5.0】ヘッダータグ付きテキストの収集
                # グリッド思考を完全廃止。各テキストにヘッダーをタグ付け。
                # ============================================

                # ヘッダー情報を収集
                x_headers = result.get("x_headers", [])
                y_headers = result.get("y_headers", [])

                # タグ付きテキストを収集
                for tagged in result.get("tagged_texts", []):
                    if isinstance(tagged, dict) and tagged.get("text"):
                        text_block = {
                            "id": tagged.get("id", ""),
                            "text": str(tagged["text"]),
                            "x_header": tagged.get("x_header", ""),
                            "y_header": tagged.get("y_header", ""),
                            "patch_idx": patch_idx,
                            "patch_info": patch_info,
                            "type": "tagged"
                        }
                        all_extracted_texts.append(text_block)

                # タグなしテキスト（タイトル、注釈など）を収集
                for untagged in result.get("untagged_texts", []):
                    if isinstance(untagged, dict) and untagged.get("text"):
                        text_block = {
                            "id": untagged.get("id", ""),
                            "text": str(untagged["text"]),
                            "x_header": "",
                            "y_header": "",
                            "patch_idx": patch_idx,
                            "patch_info": patch_info,
                            "type": untagged.get("type", "note")
                        }
                        all_extracted_texts.append(text_block)

                # 【後方互換】旧形式(cells/texts)もサポート
                for cell in result.get("cells", []):
                    if isinstance(cell, dict) and cell.get("value"):
                        text_block = {
                            "id": cell.get("id", ""),
                            "text": str(cell["value"]),
                            "x_header": cell.get("h_header", ""),
                            "y_header": cell.get("v_header", ""),
                            "patch_idx": patch_idx,
                            "patch_info": patch_info,
                            "type": "cell"
                        }
                        all_extracted_texts.append(text_block)

                for text_item in result.get("texts", []):
                    if isinstance(text_item, dict) and text_item.get("value"):
                        text_block = {
                            "id": text_item.get("id", ""),
                            "text": str(text_item["value"]),
                            "x_header": "",
                            "y_header": "",
                            "patch_idx": patch_idx,
                            "patch_info": patch_info,
                            "type": text_item.get("type", "text")
                        }
                        all_extracted_texts.append(text_block)

                # 【Ver 5.6】ヘッダー情報とtagged_textsを統合テーブルとして保存
                if x_headers or y_headers:
                    # このパッチのtagged_textsを収集
                    patch_tagged_texts = [
                        t for t in all_extracted_texts
                        if t.get("patch_idx") == patch_idx and t.get("type") == "tagged"
                    ]
                    all_tables.append({
                        "type": "ver5_tagged_table",
                        "block_id": f"ver5_patch{patch_idx}",
                        "x_headers": x_headers,
                        "y_headers": y_headers,
                        "tagged_texts": patch_tagged_texts,
                        "patch_idx": patch_idx,
                        "patch_info": patch_info,
                        "table_type": "deviation_table"  # 偏差値表
                    })

                # full_text_ordered の構築
                patch_text_parts = [t.get("text", "") for t in all_extracted_texts if t.get("patch_idx") == patch_idx]
                patch_full_text = "\n".join(patch_text_parts)
                all_full_texts.append(patch_full_text)

                # 【診断】収集結果
                patch_texts_count = len([t for t in all_extracted_texts if t.get('patch_idx') == patch_idx])
                patch_tables_count = len([t for t in all_tables if t.get('patch_idx') == patch_idx])
                logger.info(f"[F-7診断] パッチ{patch_idx + 1} 収集完了:")
                logger.info(f"  ├─ all_extracted_texts に追加: {patch_texts_count}個")
                logger.info(f"  ├─ all_tables に追加: {patch_tables_count}個")
                logger.info(f"  ├─ x_headers: {x_headers}")
                logger.info(f"  └─ y_headers: {y_headers[:5]}... (計{len(y_headers)}個)" if len(y_headers) > 5 else f"  └─ y_headers: {y_headers}")

            except Exception as e:
                logger.error(f"[F-7] パッチ{patch_idx + 1}/{len(patches)} エラー: {e}")
                patch_errors.append(f"patch{patch_idx}: {str(e)}")

            finally:
                try:
                    temp_path.unlink()
                except:
                    pass

        f7_elapsed = time.time() - f7_start
        logger.info(f"[F-7完了] チャンク{chunk_idx + 1} 全{len(patches)}パッチ完了: {f7_elapsed:.2f}秒")

        # 【診断】最終集計
        logger.info(f"[F-7診断] === 最終集計 ===")
        logger.info(f"  ├─ all_extracted_texts 総数: {len(all_extracted_texts)}")
        logger.info(f"  ├─ all_tables 総数: {len(all_tables)}")
        logger.info(f"  ├─ all_full_texts 総文字数: {sum(len(t) for t in all_full_texts)}")
        tagged_count = len([t for t in all_extracted_texts if t.get('type') == 'tagged'])
        logger.info(f"  └─ うち tagged タイプ: {tagged_count}")

        return {
            "extracted_texts": all_extracted_texts,
            "tables": all_tables,
            "full_text_ordered": "\n\n".join(all_full_texts),
            "patch_count": len(patches),
            "errors": patch_errors if patch_errors else None
        }

    def _build_f7_smart_prompt(
        self,
        chunk_idx: int,
        start_page: int,
        patch_info: str,
        patch_type: str,
        total_patches: int,
        patch_index: int = 0,
        is_continuation: bool = False
    ) -> str:
        """
        【Ver 6.4】F-7 軽量化プロンプト

        AIの仕事: 黄色ラベル(#ID)の横の文字を読むだけ
        プログラムの仕事: 座標からヘッダーを自動計算
        """
        base_prompt = self._build_f7_prompt()

        # 【Ver 6.4】L字型パッチの複雑な説明を削除
        # ヘッダー特定はプログラムが座標ベースで行うため、AIには不要

        context = f"""
## 画像情報
- ページ: {start_page + 1}
- パッチ: {patch_index + 1}/{total_patches}
"""
        return base_prompt + context

    def _f7_process_single_image(
        self,
        image: Image.Image,
        chunk_idx: int,
        chunk_start_page: int,
        f7_start: float
    ) -> Dict[str, Any]:
        """【Ver 4.0】F-7: 単一画像処理（座標排除版）"""
        import tempfile
        with tempfile.NamedTemporaryFile(suffix=f'_chunk{chunk_idx}.png', delete=False) as f:
            image.save(f, format='PNG')
            temp_image_path = Path(f.name)

        prompt = self._build_f7_chunk_prompt(chunk_idx, chunk_start_page, 1)

        try:
            response = self.llm_client.generate_with_vision(
                prompt=prompt,
                image_path=str(temp_image_path),
                model=F7_MODEL_IMAGE,
                max_tokens=F7_F8_MAX_TOKENS,
                temperature=F7_F8_TEMPERATURE,
                response_format="json"
            )

            # 【Ver 6.4】生成物ログ出力（MAX_TOKENS途切れ対応）
            logger.info(f"[F-7] ===== 生成物ログ開始（チャンク{chunk_idx + 1}） =====")
            logger.info(f"[F-7] レスポンス長: {len(response) if response else 0}文字")
            logger.info(f"[F-7] 生レスポンス:\n{response if response else '(empty)'}")
            logger.info(f"[F-7] ===== 生成物ログ終了 =====")

            try:
                result = json.loads(response)
            except json.JSONDecodeError as jde:
                logger.warning(f"[F-7] JSONパース失敗（MAX_TOKENS?）: {jde}")
                logger.warning(f"[F-7] 途切れたレスポンス末尾: ...{response[-500:] if response and len(response) > 500 else response}")
                import json_repair
                result = json_repair.repair_json(response, return_objects=True)

            f7_elapsed = time.time() - f7_start
            logger.info(f"[F-7完了] チャンク{chunk_idx + 1} Path A: {len(response)}文字, {f7_elapsed:.2f}秒")

            if hasattr(self.llm_client, 'last_usage') and self.llm_client.last_usage:
                usage = self.llm_client.last_usage.copy()
                usage['chunk_idx'] = chunk_idx
                self._f7_usage.append(usage)

            return result

        except Exception as e:
            logger.error(f"[F-7] チャンク{chunk_idx + 1} Path A エラー: {e}")
            return {"error": str(e), "extracted_texts": [], "tables": [], "full_text_ordered": ""}

        finally:
            try:
                temp_image_path.unlink()
            except:
                pass

    def _build_f7_column_prompt(self, chunk_idx: int, start_page: int, col_idx: int, total_cols: int) -> str:
        """【Ver 6.4】F-7列用プロンプト（最小化版）"""
        base_prompt = self._build_f7_prompt()

        # 【Ver 6.4】最小限のコンテキストのみ
        column_info = f"""
## 画像情報
- ページ: {start_page + 1}, 列: {col_idx + 1}/{total_cols}
"""
        return base_prompt + column_info

    def _build_f7_chunk_prompt(self, chunk_idx: int, start_page: int, page_count: int) -> str:
        """【Ver 6.4】F-7チャンク用プロンプト（最小化版）"""
        base_prompt = self._build_f7_prompt()

        # 【Ver 6.4】最小限のコンテキストのみ
        chunk_info = f"""
## 画像情報
- ページ: {start_page + 1}〜{start_page + page_count}
"""
        return base_prompt + chunk_info

    def _f8_path_b_chunk_analysis(
        self,
        chunk_pages: List[Dict],
        blocks: List[Dict],
        chunk_idx: int,
        chunk_start_page: int
    ) -> Dict[str, Any]:
        """
        【Ver 4.0】F-8 Path B: 座標排除版・構造解析

        Args:
            chunk_pages: このチャンクのページ画像リスト
            blocks: ブロックリスト（座標はAIに渡さない）
            chunk_idx: チャンクインデックス
            chunk_start_page: このチャンクの開始ページ番号
        """
        f8_start = time.time()
        logger.info(f"[F-8] Path B - チャンク{chunk_idx + 1} Visual Analysis 開始（座標排除版）")

        # チャンク画像を一時ファイルに保存
        temp_image_path = self._save_chunk_as_temp_image(chunk_pages, chunk_idx)

        prompt = self._build_f8_chunk_prompt(chunk_idx, chunk_start_page, len(chunk_pages))

        try:
            response = self.llm_client.generate_with_vision(
                prompt=prompt,
                image_path=str(temp_image_path),
                model=F8_MODEL,
                max_tokens=F7_F8_MAX_TOKENS,
                temperature=F7_F8_TEMPERATURE,
                response_format="json"
            )

            # 【Ver 6.4】生成物ログ出力（MAX_TOKENS途切れ対応）
            logger.info(f"[F-8] ===== 生成物ログ開始（チャンク{chunk_idx + 1}） =====")
            logger.info(f"[F-8] レスポンス長: {len(response) if response else 0}文字")
            logger.info(f"[F-8] 生レスポンス:\n{response if response else '(empty)'}")
            logger.info(f"[F-8] ===== 生成物ログ終了 =====")

            # JSON パース
            try:
                result = json.loads(response)
            except json.JSONDecodeError as jde:
                logger.warning(f"[F-8] JSONパース失敗（MAX_TOKENS?）: {jde}")
                logger.warning(f"[F-8] 途切れたレスポンス末尾: ...{response[-500:] if response and len(response) > 500 else response}")
                import json_repair
                result = json_repair.repair_json(response, return_objects=True)

            f8_elapsed = time.time() - f8_start
            logger.info(f"[F-8完了] チャンク{chunk_idx + 1} Path B: {len(response)}文字, {f8_elapsed:.2f}秒")

            # トークン使用量を収集
            if hasattr(self.llm_client, 'last_usage') and self.llm_client.last_usage:
                usage = self.llm_client.last_usage.copy()
                usage['chunk_idx'] = chunk_idx
                self._f8_usage.append(usage)
                logger.info(f"[F-8] トークン使用量: prompt={usage.get('prompt_tokens', 0)}, completion={usage.get('completion_tokens', 0)}")

            return result

        except Exception as e:
            logger.error(f"[F-8] チャンク{chunk_idx + 1} Path B エラー: {e}")
            return {"error": str(e), "tables": [], "diagrams": [], "charts": [], "structured_data_candidates": []}

        finally:
            # 一時ファイル削除
            try:
                temp_image_path.unlink()
            except:
                pass

    def _build_f8_chunk_prompt(self, chunk_idx: int, start_page: int, page_count: int) -> str:
        """【Ver 4.0】F-8チャンク用プロンプト構築（座標排除版）"""
        base_prompt = self._build_f8_prompt()

        chunk_info = f"""
## チャンク情報
- チャンク番号: {chunk_idx + 1}
- ページ範囲: {start_page + 1}〜{start_page + page_count}ページ目
"""
        return base_prompt + chunk_info

    def _merge_chunk_tables(
        self,
        path_a_result: Dict[str, Any],
        path_b_result: Dict[str, Any],
        chunk_idx: int,
        chunk_start_page: int
    ) -> List[Dict[str, Any]]:
        """
        チャンク内の表データをマージ

        Path A（テキスト内容）と Path B（構造情報）を統合し、
        チャンク情報を付加して返す
        """
        path_a_tables = path_a_result.get("tables", [])
        path_b_tables = path_b_result.get("tables", [])

        merged_tables = []
        for a_table in path_a_tables:
            block_id = a_table.get("block_id", "")

            # Path B から対応する構造情報を探す
            b_structure = {}
            for b_table in path_b_tables:
                if b_table.get("block_id") == block_id:
                    b_structure = b_table
                    break

            # columns/headers どちらも受け付ける（カラムナ形式優先）
            columns = a_table.get("columns") or a_table.get("headers", [])
            rows = a_table.get("rows", [])

            merged_table = {
                "block_id": f"chunk{chunk_idx}_{block_id}",  # チャンク情報を付加
                "chunk_idx": chunk_idx,
                "chunk_start_page": chunk_start_page,
                "table_title": a_table.get("table_title", ""),
                "table_type": a_table.get("table_type", b_structure.get("table_type", "visual_table")),
                "columns": columns,
                "rows": rows,
                "row_count": a_table.get("row_count", len(rows)),
                "col_count": a_table.get("col_count", len(columns)),
                "caption": a_table.get("caption", ""),
                # Path B からの構造情報
                "structure": b_structure.get("structure", {}),
                "semantic_role": b_structure.get("semantic_role", ""),
                "data_quality": b_structure.get("data_quality", {}),
                # 【Ver 5.6】ヘッダータグ付きテキストを保持
                "x_headers": a_table.get("x_headers", []),
                "y_headers": a_table.get("y_headers", []),
                "tagged_texts": a_table.get("tagged_texts", [])
            }
            merged_tables.append(merged_table)

        return merged_tables

    # ============================================
    # F-9: Result Convergence
    # ============================================
    def _f9_merge_results(
        self,
        path_a: Dict[str, Any],
        path_b: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        F-9: 抽出結果の集約
        Path AとPath Bの結果をマージ
        特に表データは Path A (テキスト) + Path B (構造) を統合
        """
        f9_start = time.time()
        logger.info("[F-9] Result Convergence 開始")

        # Path A の表データ（テキスト内容）
        path_a_tables = path_a.get("tables", [])
        # Path B の表データ（構造情報）
        path_b_tables = path_b.get("tables", [])
        # Path B の構造化データ候補
        structured_candidates = path_b.get("structured_data_candidates", [])

        # 表データの統合: Path A のテキスト + Path B の構造
        merged_tables = []
        for a_table in path_a_tables:
            block_id = a_table.get("block_id", "")

            # Path B から対応する構造情報を探す
            b_structure = {}
            for b_table in path_b_tables:
                if b_table.get("block_id") == block_id:
                    b_structure = b_table
                    break

            # columns/headers どちらも受け付ける（カラムナ形式優先）
            columns = a_table.get("columns") or a_table.get("headers", [])
            rows = a_table.get("rows", [])

            merged_table = {
                "block_id": block_id,
                "table_title": a_table.get("table_title", ""),
                "table_type": a_table.get("table_type", b_structure.get("table_type", "visual_table")),
                "columns": columns,  # カラムナ形式で統一
                "rows": rows,
                "row_count": a_table.get("row_count", len(rows)),
                "col_count": a_table.get("col_count", len(columns)),
                "caption": a_table.get("caption", ""),
                # Path B からの構造情報
                "structure": b_structure.get("structure", {}),
                "semantic_role": b_structure.get("semantic_role", ""),
                "data_quality": b_structure.get("data_quality", {})
            }
            merged_tables.append(merged_table)

        # 表の統計
        total_rows = sum(t.get("row_count", 0) for t in merged_tables)
        total_tables = len(merged_tables)

        logger.info(f"[F-9] 表統合: {total_tables}テーブル, 合計{total_rows}行")

        merged = {
            "text_source": {
                "full_text": path_a.get("full_text_ordered", ""),
                "blocks": path_a.get("extracted_texts", []),
                "missed_texts": path_a.get("missed_texts", [])
            },
            "tables": merged_tables,  # 統合済み表データ
            "structured_data_candidates": structured_candidates,
            "visual_source": {
                "diagrams": path_b.get("diagrams", []),
                "charts": path_b.get("charts", []),
                "layout": path_b.get("layout_analysis", {})
            },
            "metadata": {
                "path_a_model": F7_MODEL_IMAGE,
                "path_b_model": F8_MODEL,
                "table_count": total_tables,
                "total_table_rows": total_rows
            }
        }

        f9_elapsed = time.time() - f9_start
        logger.info(f"[F-9完了] マージ完了, {f9_elapsed:.2f}秒")

        return merged

    # ============================================
    # F-9 Helper: アンカーパケット生成
    # ============================================
    def _build_anchor_packets(
        self,
        text_blocks: List[Dict],
        tables: List[Dict],
        structured_candidates: List[Dict]
    ) -> List[Dict[str, Any]]:
        """
        アンカーベースのパケット配列を生成

        テキストブロックと表を統一的なアンカー形式に変換し、
        Stage Gでの振り分け（H1/H2）を容易にする

        Args:
            text_blocks: F-7から抽出されたテキストブロック
            tables: F-7/F-8からマージされた表データ
            structured_candidates: F-8で検出された構造化候補

        Returns:
            アンカーパケット配列:
            [
                {"anchor_id": "B-001", "type": "text", "content": "...", "page": 1},
                {"anchor_id": "B-002", "type": "table", "title": "...", "columns": [...], "rows": [...], "is_heavy": true}
            ]
        """
        anchors = []
        anchor_index = 1

        # 表のblock_idを収集（テキストから除外するため）
        table_block_ids = set(t.get("block_id", "") for t in tables)

        # テキストブロックをアンカー化
        for block in text_blocks:
            block_id = block.get("block_id", "")

            # 表として既に処理されているブロックはスキップ
            if block_id in table_block_ids:
                continue

            text = block.get("text", "")
            if not text or len(text.strip()) < 3:
                continue

            anchors.append({
                "anchor_id": f"B-{anchor_index:03d}",
                "original_block_id": block_id,
                "type": "text",
                "block_type": block.get("block_type", "paragraph"),
                "content": text,
                "page": block.get("original_page", block.get("page", 0)),
                "reading_order": block.get("reading_order", 0),
                "confidence": block.get("confidence", "medium"),
                "is_heavy": False  # テキストは常に軽量
            })
            anchor_index += 1

        # 表をアンカー化
        for table in tables:
            block_id = table.get("block_id", "")
            rows = table.get("rows", [])
            columns = table.get("columns", []) or table.get("headers", [])

            # 重い表の判定（20行以上 or 5列以上）
            is_heavy = len(rows) >= 20 or len(columns) >= 5

            anchors.append({
                "anchor_id": f"B-{anchor_index:03d}",
                "original_block_id": block_id,
                "type": "table",
                "table_type": table.get("table_type", "visual_table"),
                "title": table.get("table_title", ""),
                "columns": columns,
                "rows": rows,
                "row_count": len(rows),
                "col_count": len(columns),
                "page": table.get("chunk_start_page", 0),
                "is_heavy": is_heavy,
                "structure": table.get("structure", {}),
                "semantic_role": table.get("semantic_role", "")
            })
            anchor_index += 1

        # 構造化候補をアンカー化（表として検出されなかったが構造化可能なデータ）
        for candidate in structured_candidates:
            anchors.append({
                "anchor_id": f"B-{anchor_index:03d}",
                "type": "structured_candidate",
                "candidate_type": candidate.get("type", "key_value"),
                "content": candidate.get("content", {}),
                "page": candidate.get("page", 0),
                "is_heavy": False
            })
            anchor_index += 1

        # reading_order でソート（テキストの場合）
        anchors.sort(key=lambda x: (x.get("page", 0), x.get("reading_order", 0)))

        logger.info(f"[F-9] アンカー生成: text={sum(1 for a in anchors if a['type'] == 'text')}, "
                   f"table={sum(1 for a in anchors if a['type'] == 'table')}, "
                   f"heavy={sum(1 for a in anchors if a.get('is_heavy', False))}")

        return anchors

    # ============================================
    # 【Ver 6.2】F-9: 物理仕分け + 住所タグの幾何学的割当
    # ============================================
    def _f9_physical_sorting(
        self,
        blocks: List[Dict],
        tables: List[Dict],
        f8_headers: Dict[str, Any],
        structured_candidates: List[Dict]
    ) -> Tuple[Dict[str, Any], List[Dict]]:
        """
        【Ver 6.2】F-9: 物理仕分け + 住所タグの幾何学的割当

        プログラムによる数学的な座標計算でヘッダーを決定。
        閾値以下の曖昧なデータは low_confidence_items として返す。

        Args:
            blocks: テキストブロックリスト
            tables: 表リスト
            f8_headers: F-8 から抽出したヘッダー情報
            structured_candidates: 構造化候補

        Returns:
            (result, low_confidence_items)
            result: {tagged_texts, x_headers, y_headers}
            low_confidence_items: 閾値以下の曖昧なデータ
        """
        CONFIDENCE_THRESHOLD = 50  # ピクセル距離の閾値

        x_headers = f8_headers.get('x_headers', [])
        y_headers = f8_headers.get('y_headers', [])
        header_coords = f8_headers.get('header_coords', {})

        tagged_texts = []
        low_confidence_items = []

        for block in blocks:
            text = block.get('text', '').strip()
            block_id = block.get('block_id', block.get('id', ''))

            if not text:
                continue

            # ヘッダー自体はスキップ
            if text in x_headers or text in y_headers:
                continue

            # 【Ver 6.4】座標取得: coords (新形式) または bbox (旧形式)
            coords = block.get('coords', {})
            bbox = coords.get('bbox') or block.get('bbox')

            if coords and 'x' in coords and 'y' in coords:
                # 新形式: 中心座標が既に計算済み
                text_x = coords['x']
                text_y = coords['y']
            elif bbox and isinstance(bbox, (list, tuple)) and len(bbox) >= 4:
                # 旧形式: bboxから中心座標を計算
                text_x = (bbox[0] + bbox[2]) / 2
                text_y = (bbox[1] + bbox[3]) / 2
            else:
                # 座標がない場合は低信頼度
                low_confidence_items.append({
                    'id': block_id,
                    'text': text,
                    'reason': 'no_coords'
                })
                continue

            # 最短距離でX軸ヘッダーを決定
            nearest_x = ''
            min_x_dist = float('inf')
            second_x_dist = float('inf')

            for xh in x_headers:
                if xh in header_coords:
                    dist = abs(text_x - header_coords[xh]['x'])
                    if dist < min_x_dist:
                        second_x_dist = min_x_dist
                        min_x_dist = dist
                        nearest_x = xh
                    elif dist < second_x_dist:
                        second_x_dist = dist

            # 最短距離でY軸ヘッダーを決定
            nearest_y = ''
            min_y_dist = float('inf')
            second_y_dist = float('inf')

            for yh in y_headers:
                if yh in header_coords:
                    dist = abs(text_y - header_coords[yh]['y'])
                    if dist < min_y_dist:
                        second_y_dist = min_y_dist
                        min_y_dist = dist
                        nearest_y = yh
                    elif dist < second_y_dist:
                        second_y_dist = dist

            # 信頼度判定: 1位と2位の距離差が閾値以下なら低信頼度
            x_confidence = second_x_dist - min_x_dist if second_x_dist != float('inf') else float('inf')
            y_confidence = second_y_dist - min_y_dist if second_y_dist != float('inf') else float('inf')

            if (x_confidence < CONFIDENCE_THRESHOLD and nearest_x) or \
               (y_confidence < CONFIDENCE_THRESHOLD and nearest_y):
                # 低信頼度: AIレスキュー対象
                low_confidence_items.append({
                    'id': block_id,
                    'text': text,
                    'bbox': bbox,
                    'x_candidates': [nearest_x, ''] if nearest_x else [],
                    'y_candidates': [nearest_y, ''] if nearest_y else [],
                    'x_confidence': x_confidence,
                    'y_confidence': y_confidence,
                    'reason': 'low_confidence'
                })
            else:
                # 高信頼度: 物理決定
                tagged_texts.append({
                    'id': block_id,
                    'text': text,
                    'x_header': nearest_x,
                    'y_header': nearest_y,
                    'type': 'cell' if (nearest_x and nearest_y) else 'untagged',
                    '_x_distance': min_x_dist,
                    '_y_distance': min_y_dist,
                    '_physical_decision': True
                })

        logger.info(f"[F-9] 物理決定: {len(tagged_texts)}件, 低信頼度: {len(low_confidence_items)}件")

        return {
            'tagged_texts': tagged_texts,
            'x_headers': x_headers,
            'y_headers': y_headers
        }, low_confidence_items

    # ============================================
    # 【Ver 6.2】F-9.5: 低信頼度住所のAIレスキュー
    # ============================================
    def _f95_ai_rescue(
        self,
        low_confidence_items: List[Dict],
        f8_headers: Dict[str, Any],
        image: Image.Image = None
    ) -> List[Dict]:
        """
        【Ver 6.2】F-9.5: 低信頼度住所のAIレスキュー

        プログラムで決定できなかった曖昧なデータを、
        AIに画像を見せて最終判断させる。
        介入件数を透明にログ出力。

        Args:
            low_confidence_items: 低信頼度アイテムリスト
            f8_headers: F-8 から抽出したヘッダー情報
            image: 元画像（AIに見せる用）

        Returns:
            救済されたtagged_textsリスト
        """
        if not low_confidence_items:
            return []

        rescued_items = []
        x_headers = f8_headers.get('x_headers', [])
        y_headers = f8_headers.get('y_headers', [])

        # AIに渡すプロンプト
        rescue_prompt = f"""# 住所特定レスキュー（Ver 6.2）

以下のテキストについて、画像上の位置関係からX軸・Y軸ヘッダーを特定してください。

## 利用可能なヘッダー
- X軸ヘッダー: {json.dumps(x_headers, ensure_ascii=False)}
- Y軸ヘッダー: {json.dumps(y_headers, ensure_ascii=False)}

## 判定対象
{json.dumps([{'id': i['id'], 'text': i['text']} for i in low_confidence_items], ensure_ascii=False, indent=2)}

## 出力形式
```json
{{"rescued": [{{"id": "22", "x_header": "2/1", "y_header": "74"}}]}}
```
特定できない場合は null。説明不要。
"""

        try:
            if image:
                import tempfile
                with tempfile.NamedTemporaryFile(suffix='_f95_rescue.png', delete=False) as f:
                    image.save(f, format='PNG')
                    temp_path = Path(f.name)

                response = self.llm_client.generate_with_vision(
                    prompt=rescue_prompt,
                    image_path=str(temp_path),
                    model=F95_MODEL,  # Ver 6.2: gemini-2.5-flash-lite
                    max_tokens=2000,
                    temperature=0.1,
                    response_format="json"
                )

                try:
                    temp_path.unlink()
                except:
                    pass
            else:
                # 画像がない場合はテキストのみで判断
                response = self.llm_client.generate(
                    prompt=rescue_prompt,
                    model=F95_MODEL,  # Ver 6.2: gemini-2.5-flash-lite
                    max_tokens=2000,
                    temperature=0.1,
                    response_format="json"
                )

            # 【Ver 6.4】生成物ログ出力（MAX_TOKENS途切れ対応）
            logger.info(f"[F-9.5] ===== 生成物ログ開始 =====")
            logger.info(f"[F-9.5] レスポンス長: {len(response) if response else 0}文字")
            logger.info(f"[F-9.5] 生レスポンス:\n{response if response else '(empty)'}")
            logger.info(f"[F-9.5] ===== 生成物ログ終了 =====")

            # JSONパース
            try:
                result = json.loads(response)
            except json.JSONDecodeError as jde:
                logger.warning(f"[F-9.5] JSONパース失敗（MAX_TOKENS?）: {jde}")
                logger.warning(f"[F-9.5] 途切れたレスポンス末尾: ...{response[-500:] if response and len(response) > 500 else response}")
                import json_repair
                result = json_repair.repair_json(response, return_objects=True)

            # 救済結果を処理
            for rescued in result.get('rescued', []):
                item_id = str(rescued.get('id', ''))
                x_h = rescued.get('x_header', '')
                y_h = rescued.get('y_header', '')

                # 元のアイテムを探す
                original = next((i for i in low_confidence_items if str(i.get('id', '')) == item_id), None)
                if original:
                    rescued_items.append({
                        'id': item_id,
                        'text': original.get('text', ''),
                        'x_header': x_h,
                        'y_header': y_h,
                        'type': 'cell' if (x_h and y_h) else 'untagged',
                        '_ai_rescued': True,
                        '_original_reason': original.get('reason', '')
                    })

            logger.info(f"[F-9.5] AI介入: {len(low_confidence_items)}件中{len(rescued_items)}件を救済")

        except Exception as e:
            logger.warning(f"[F-9.5] AIレスキュー失敗: {e}")
            # フォールバック: 最も近いヘッダーを採用
            for item in low_confidence_items:
                rescued_items.append({
                    'id': item.get('id', ''),
                    'text': item.get('text', ''),
                    'x_header': item.get('x_candidates', [''])[0] if item.get('x_candidates') else '',
                    'y_header': item.get('y_candidates', [''])[0] if item.get('y_candidates') else '',
                    'type': 'untagged',
                    '_ai_rescued': False,
                    '_fallback': True
                })

        return rescued_items

    # ============================================
    # 【Ver 6.4】F-10: 正本化 + 異常座標特定
    # ============================================
    def _f10_stage_e_scrubbing(
        self,
        merged_result: Dict[str, Any],
        e_content: str,
        post_body: Optional[Dict]
    ) -> Dict[str, Any]:
        """
        【Ver 6.4】F-10: 正本化と異常座標特定

        役割1: Stage E のデジタル文字で F7 の読みを強制上書き（洗い替え）
        役割2: 証拠がない/矛盾する箇所を anomaly_report として Stage G に渡す

        Args:
            merged_result: F-9 の出力
            e_content: Stage E のテキスト（互換性用）
            post_body: 投稿本文

        Returns:
            {
                scrubbed_data: 洗い替え成功データ（信頼度100%）,
                anomaly_report: Stage G への外科手術指示書
            }
        """
        f10_start = time.time()
        logger.info("[F-10] 【Ver 6.4】正本化 + 異常検知開始")

        tagged_texts = merged_result.get('tagged_texts', [])
        x_headers = merged_result.get('x_headers', [])
        y_headers = merged_result.get('y_headers', [])
        header_coords = merged_result.get('header_coords', {})

        # Stage E から座標付き文字リストを取得
        physical_chars = getattr(self, '_e_physical_chars', [])

        scrubbed_data = []
        anomaly_report = []
        PROXIMITY_THRESHOLD = 20  # ピクセル距離の閾値

        # ============================================
        # Step 1: 物理証拠による自動洗い替え（第一防衛線）
        # ============================================
        if physical_chars:
            logger.info(f"[F-10] 物理証拠あり: {len(physical_chars)}文字 → 洗い替え実行")

            # E文字の座標インデックスを構築
            e_char_index = {}  # {(page, x_bucket, y_bucket): [chars]}
            BUCKET_SIZE = 50  # 座標のバケットサイズ
            for ec in physical_chars:
                page = ec.get('page', 0)
                bbox = ec.get('bbox', [0, 0, 0, 0])
                cx = (bbox[0] + bbox[2]) // 2
                cy = (bbox[1] + bbox[3]) // 2
                bucket_key = (page, cx // BUCKET_SIZE, cy // BUCKET_SIZE)
                if bucket_key not in e_char_index:
                    e_char_index[bucket_key] = []
                e_char_index[bucket_key].append(ec)

            # 各 tagged_text を検証・洗い替え
            for tt in tagged_texts:
                tt_text = tt.get('text', '')
                tt_coords = tt.get('coords', {})
                tt_bbox = tt_coords.get('bbox') or tt.get('bbox')
                page = tt.get('page', 0)

                if not tt_bbox:
                    # 座標がない → 異常
                    anomaly_report.append({
                        'id': tt.get('id', ''),
                        'text': tt_text,
                        'reason': 'no_bbox',
                        'page': page,
                        'bbox': None
                    })
                    continue

                # この tagged_text の座標範囲
                if isinstance(tt_bbox, (list, tuple)) and len(tt_bbox) >= 4:
                    tt_x1, tt_y1, tt_x2, tt_y2 = tt_bbox[:4]
                    tt_cx = (tt_x1 + tt_x2) / 2
                    tt_cy = (tt_y1 + tt_y2) / 2
                else:
                    anomaly_report.append({
                        'id': tt.get('id', ''),
                        'text': tt_text,
                        'reason': 'invalid_bbox',
                        'page': page,
                        'bbox': tt_bbox
                    })
                    continue

                # 近傍のE文字を収集
                nearby_e_chars = []
                for dx in [-1, 0, 1]:
                    for dy in [-1, 0, 1]:
                        bucket_key = (page, int(tt_cx) // BUCKET_SIZE + dx, int(tt_cy) // BUCKET_SIZE + dy)
                        nearby_e_chars.extend(e_char_index.get(bucket_key, []))

                # 枠内に収まるE文字をフィルタ
                chars_in_bbox = []
                for ec in nearby_e_chars:
                    ec_bbox = ec.get('bbox', [0, 0, 0, 0])
                    ec_cx = (ec_bbox[0] + ec_bbox[2]) / 2
                    ec_cy = (ec_bbox[1] + ec_bbox[3]) / 2

                    # 枠内判定（または近傍判定）
                    in_x = tt_x1 - PROXIMITY_THRESHOLD <= ec_cx <= tt_x2 + PROXIMITY_THRESHOLD
                    in_y = tt_y1 - PROXIMITY_THRESHOLD <= ec_cy <= tt_y2 + PROXIMITY_THRESHOLD

                    if in_x and in_y:
                        chars_in_bbox.append({
                            'text': ec.get('text', ''),
                            'x': ec_bbox[0],
                            'bbox': ec_bbox
                        })

                if chars_in_bbox:
                    # 洗い替え成功: E文字で上書き
                    sorted_chars = sorted(chars_in_bbox, key=lambda c: c['x'])
                    scrubbed_text = ''.join(c['text'] for c in sorted_chars)

                    scrubbed_data.append({
                        'id': tt.get('id', ''),
                        'text': scrubbed_text,
                        'original_ocr': tt_text,
                        'x_header': tt.get('x_header', ''),
                        'y_header': tt.get('y_header', ''),
                        'type': tt.get('type', 'cell'),
                        '_scrubbed': True,
                        '_e_char_count': len(chars_in_bbox),
                        'bbox': tt_bbox
                    })
                else:
                    # 証拠欠落: E文字が枠内にない → 異常
                    anomaly_report.append({
                        'id': tt.get('id', ''),
                        'text': tt_text,
                        'reason': 'no_evidence_in_bbox',
                        'page': page,
                        'bbox': tt_bbox,
                        'x_header': tt.get('x_header', ''),
                        'y_header': tt.get('y_header', '')
                    })

            logger.info(f"[F-10] 洗い替え完了: 成功{len(scrubbed_data)}件, 異常{len(anomaly_report)}件")

        else:
            # ============================================
            # フォールバック: 物理証拠なし（画像PDF等）
            # ============================================
            logger.info("[F-10] 物理証拠なし → 全件を異常扱い（Stage G で再読）")

            for tt in tagged_texts:
                # 類似度マッチングで暫定洗い替えを試みる
                tt_text = tt.get('text', '')
                if e_content:
                    best_match = self._find_best_e_match(tt_text, set(e_content.split()))
                    if best_match and best_match != tt_text:
                        tt['text'] = best_match
                        tt['_fallback_scrubbed'] = True

                scrubbed_data.append(tt)

                # 物理証拠がない場合は全て要検証
                if not tt.get('_fallback_scrubbed'):
                    anomaly_report.append({
                        'id': tt.get('id', ''),
                        'text': tt_text,
                        'reason': 'no_physical_evidence',
                        'page': tt.get('page', 0),
                        'bbox': tt.get('bbox')
                    })

        # ============================================
        # Step 2: 最終payload構築
        # ============================================
        f10_elapsed = time.time() - f10_start

        payload = {
            "schema_version": STAGE_F_OUTPUT_SCHEMA_VERSION,
            "post_body": post_body or {},
            "path_a_result": {
                "tagged_texts": scrubbed_data,
                "x_headers": x_headers,
                "y_headers": y_headers,
                "full_text_ordered": merged_result.get("text_source", {}).get("full_text", ""),
                "extracted_texts": merged_result.get("text_source", {}).get("blocks", []),
                "tables": merged_result.get("tables", []),
                "_v64_scrubbed": True
            },
            "path_b_result": merged_result.get("visual_source", {}),
            "anchors": merged_result.get("anchors", []),
            # 【Ver 6.4】Stage G への外科手術指示書
            "anomaly_report": anomaly_report,
            "metadata": {
                **merged_result.get("metadata", {}),
                "f10_physical_chars": len(physical_chars) if physical_chars else 0,
                "f10_scrubbed_count": len(scrubbed_data),
                "f10_anomaly_count": len(anomaly_report),
                "f10_elapsed": f10_elapsed
            },
            "warnings": []
        }

        logger.info(f"[F-10] 完了: {f10_elapsed:.2f}秒, 洗替{len(scrubbed_data)}件, 異常{len(anomaly_report)}件")

        return payload

    def _find_nearest_header(
        self,
        coord: float,
        headers: List[str],
        header_coords: Dict[str, Dict],
        axis: str
    ) -> str:
        """
        座標から最も近いヘッダーを見つける

        Args:
            coord: 対象座標（x または y）
            headers: ヘッダーリスト
            header_coords: ヘッダー座標マップ
            axis: 'x' または 'y'

        Returns:
            最も近いヘッダー名
        """
        nearest = ''
        min_dist = float('inf')

        for h in headers:
            if h in header_coords:
                h_coord = header_coords[h].get(axis, 0)
                dist = abs(coord - h_coord)
                if dist < min_dist:
                    min_dist = dist
                    nearest = h

        # ヘッダー座標がない場合は、ヘッダーリストの順序で推定
        if not nearest and headers:
            # 簡易的に等間隔で配置されていると仮定
            interval = 1000 / (len(headers) + 1)
            for i, h in enumerate(headers):
                h_coord = interval * (i + 1)
                dist = abs(coord - h_coord)
                if dist < min_dist:
                    min_dist = dist
                    nearest = h

        return nearest

    def _f10_fallback_scrubbing(self, tagged_texts: List[Dict], e_content: str) -> int:
        """
        フォールバック: 従来の類似度マッチングによる洗い替え
        """
        if not e_content:
            return 0

        e_words = set()
        for line in e_content.split('\n'):
            line = line.strip()
            if line:
                e_words.add(line)
                for word in line.split():
                    if len(word) >= 2:
                        e_words.add(word)

        scrub_count = 0
        for tt in tagged_texts:
            ocr_text = tt.get('text', '')
            if not ocr_text:
                continue

            best_match = self._find_best_e_match(ocr_text, e_words)
            if best_match and best_match != ocr_text:
                tt['original_ocr'] = ocr_text
                tt['text'] = best_match
                tt['_scrubbed'] = True
                scrub_count += 1

        return scrub_count

    def _find_best_e_match(self, ocr_text: str, e_words: set) -> Optional[str]:
        """
        OCRテキストに最も類似するStage E単語を見つける

        Args:
            ocr_text: OCRテキスト
            e_words: Stage Eの単語セット

        Returns:
            最も類似する単語、または None
        """
        if not ocr_text or not e_words:
            return None

        best_match = None
        best_score = 0

        ocr_chars = set(ocr_text)

        for e_word in e_words:
            # 完全一致
            if ocr_text == e_word:
                return e_word

            # 部分一致
            if ocr_text in e_word:
                score = len(ocr_text) / len(e_word)
                if score > best_score and score >= 0.5:
                    best_score = score
                    best_match = e_word
                continue

            if e_word in ocr_text:
                score = len(e_word) / len(ocr_text) * 0.9
                if score > best_score and score >= 0.5:
                    best_score = score
                    best_match = e_word
                continue

            # 文字の重複率（Jaccard係数）
            e_chars = set(e_word)
            intersection = len(ocr_chars & e_chars)
            union = len(ocr_chars | e_chars)
            if union > 0:
                score = intersection / union * 0.8
                if score > best_score and score >= 0.6:
                    best_score = score
                    best_match = e_word

        return best_match

    # ============================================
    # F-10: Payload Validation（後方互換）
    # ============================================
    def _f10_validate(
        self,
        merged_result: Dict[str, Any],
        post_body: Optional[Dict]
    ) -> Dict[str, Any]:
        """
        F-10: 契約保証（Payload Validation）
        - 必須項目チェック
        - 表データの完全性検証
        - 文字数検証
        """
        f10_start = time.time()
        logger.info("[F-10] Payload Validation 開始")

        warnings = []

        # 1. full_text の存在確認
        full_text = merged_result.get("text_source", {}).get("full_text", "")
        full_text_len = len(full_text)
        if not full_text:
            warnings.append("F10_WARN: full_text is empty")
        logger.info(f"[F-10] full_text: {full_text_len}文字")

        # 2. 表データの完全性検証
        tables = merged_result.get("tables", [])
        table_warnings = self._validate_tables(tables)
        warnings.extend(table_warnings)

        # 3. ブロックの検証
        blocks = merged_result.get("text_source", {}).get("blocks", [])
        blocks_text_len = sum(len(b.get("text", "")) for b in blocks)
        logger.info(f"[F-10] blocks: {len(blocks)}個, 合計{blocks_text_len}文字")

        # 4. 文字数の整合性チェック（警告のみ、エラーにはしない）
        if blocks_text_len > 0 and full_text_len > 0:
            # full_text は blocks の統合なので、概ね同じ長さになるはず
            diff_ratio = abs(full_text_len - blocks_text_len) / max(full_text_len, blocks_text_len)
            if diff_ratio > 0.5:  # 50%以上の差異は警告
                warnings.append(f"F10_WARN: full_text({full_text_len}) と blocks合計({blocks_text_len}) の差異が大きい")

        # 5. 表データの統計
        table_count = len(tables)
        total_rows = sum(t.get("row_count", 0) for t in tables)
        tables_with_columns = sum(1 for t in tables if t.get("columns") or t.get("headers"))

        logger.info(f"[F-10] 表統計: {table_count}テーブル, {total_rows}行, columns付き={tables_with_columns}")

        # 6. トークン使用量の集計
        f7_total = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0, "model": F7_MODEL_IMAGE}
        for u in self._f7_usage:
            f7_total["prompt_tokens"] += u.get("prompt_tokens", 0)
            f7_total["completion_tokens"] += u.get("completion_tokens", 0)
            f7_total["total_tokens"] += u.get("total_tokens", 0)

        f8_total = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0, "model": F8_MODEL}
        for u in self._f8_usage:
            f8_total["prompt_tokens"] += u.get("prompt_tokens", 0)
            f8_total["completion_tokens"] += u.get("completion_tokens", 0)
            f8_total["total_tokens"] += u.get("total_tokens", 0)

        logger.info(f"[F-10] F7トークン合計: prompt={f7_total['prompt_tokens']}, completion={f7_total['completion_tokens']}")
        logger.info(f"[F-10] F8トークン合計: prompt={f8_total['prompt_tokens']}, completion={f8_total['completion_tokens']}")

        # 7. 最終payload構築
        payload = {
            "schema_version": STAGE_F_OUTPUT_SCHEMA_VERSION,
            "post_body": post_body or {},
            "full_text": full_text,
            "text_blocks": blocks,
            "tables": tables,
            "structured_data_candidates": merged_result.get("structured_data_candidates", []),
            "visual_elements": {
                "diagrams": merged_result.get("visual_source", {}).get("diagrams", []),
                "charts": merged_result.get("visual_source", {}).get("charts", []),
            },
            "layout_analysis": merged_result.get("visual_source", {}).get("layout", {}),
            "metadata": {
                **merged_result.get("metadata", {}),
                "full_text_char_count": full_text_len,
                "blocks_count": len(blocks),
                "table_count": table_count,
                "total_table_rows": total_rows,
            },
            "media_type": "image",
            "processing_mode": "dual_vision",
            "warnings": warnings,
            "llm_usage": {
                "F7": f7_total,
                "F8": f8_total
            }
        }

        f10_elapsed = time.time() - f10_start
        logger.info(f"[F-10完了] Validation完了, warnings={len(warnings)}, {f10_elapsed:.2f}秒")

        return payload

    def _validate_tables(self, tables: List[Dict]) -> List[str]:
        """表データの完全性を検証（カラムナ形式対応）"""
        warnings = []

        for i, table in enumerate(tables):
            table_id = table.get("block_id", f"table_{i}")

            # columns/headers どちらも受け付ける（カラムナ形式優先）
            columns = table.get("columns") or table.get("headers", [])
            rows = table.get("rows", [])

            if not columns and not rows:
                warnings.append(f"F10_TABLE_WARN: {table_id} has no columns and no rows")
                continue

            # 列数の整合性チェック
            if columns:
                col_count = len(columns)
                for row_idx, row in enumerate(rows):
                    if isinstance(row, list) and len(row) != col_count:
                        warnings.append(f"F10_TABLE_WARN: {table_id} row {row_idx} has {len(row)} cols, expected {col_count}")

            # data_summary の検出（禁止パターン）
            if "data_summary" in table:
                warnings.append(f"F10_TABLE_ERROR: {table_id} uses data_summary (PROHIBITED)")

            # 辞書リスト形式の検出（禁止パターン）
            if rows and isinstance(rows[0], dict):
                warnings.append(f"F10_TABLE_ERROR: {table_id} uses dict rows (PROHIBITED - use columnar format)")

            # 空の rows チェック
            if not rows:
                warnings.append(f"F10_TABLE_WARN: {table_id} has columns but no rows")

            # セル内のカンマ検出（構造化不十分の可能性）
            for row_idx, row in enumerate(rows):
                if isinstance(row, list):
                    for col_idx, cell in enumerate(row):
                        if isinstance(cell, str) and ", " in cell and len(cell.split(", ")) > 2:
                            warnings.append(f"F10_TABLE_HINT: {table_id} row {row_idx} col {col_idx} may need further expansion (contains comma-separated data)")

        return warnings

    # ============================================
    # ユーティリティ
    # ============================================
    def _create_empty_payload(
        self,
        post_body: Optional[Dict],
        error: str = None
    ) -> Dict[str, Any]:
        """空のpayloadを生成"""
        payload = {
            "schema_version": STAGE_F_OUTPUT_SCHEMA_VERSION,
            "post_body": post_body or {},
            "path_a_result": {},
            "path_b_result": {},
            "media_type": "none",
            "processing_mode": "skipped",
            "warnings": []
        }
        if error:
            payload["warnings"].append(f"F_ERROR: {error}")
        return payload
```

## shared/pipeline/stage_g1_table_refiner.py

```python
"""
Stage G1: Table Refiner（表専用整理）- Ver 6.4 事実陳列専用

【設計 2026-01-31】Ver 6.4: JSON生成でのAI使用を完全禁止

============================================
【Ver 6.4】絶対禁止事項（時限爆弾の除去）:
  ┌─────────────────────────────────────────┐
  │ ✗ 名寄せ: 表記揺れの勝手な清書         │
  │ ✗ 穴埋め: 読み取れない値の推測補完     │
  │ ✗ 重複判断: AIが「同じ」と判断した行の統合 │
  │                                         │
  │ これらは「善意の改ざん」であり          │
  │ 100%正確な抽出を破壊する時限爆弾である  │
  └─────────────────────────────────────────┘

【Ver 6.4】役割:
  - F-10 洗い替え済みデータを「そのまま」JSON配列に格納
  - AIの「作文」ではなく「事実の陳列」
  - 文字の書き換え・推測による補完・項目の結合は一切禁止

【Ver 6.4】許可されるAI使用:
  - anomaly_report にある異常セルのみのピンポイント画像再読
  - これは「読み直し」であり「推測」ではない

入力:
  - tables: 表データリスト（F-10 洗い替え済み）
  - anomaly_report: 異常セルリスト（外科手術対象）
  - file_path: 画像ファイルパス（画像再読用）

出力:
  - tables: 整形済み表データ（H1 へ直送）
  - token_usage: トークン使用量
  - audit_log: 監査ログ
============================================
"""
import json
import time
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass, field
from loguru import logger
import re

# 画像処理ライブラリ（オプション）
try:
    import cv2
    import numpy as np
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    logger.warning("[G1] cv2 not available, image enhancement disabled")

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    logger.warning("[G1] PIL not available, image cropping disabled")

# G1 で使用するモデル
G1_MODEL = "gemini-2.5-flash-lite"
G1_REREAD_MODEL = "gemini-2.5-flash"  # 異常再読には高精度モデル


@dataclass
class TableValidationResult:
    """表の検算結果"""
    anchor_id: str
    is_valid: bool
    warnings: List[str] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    auto_fixed: bool = False
    ai_polished: bool = False
    fix_description: str = ""


class StageG1TableRefiner:
    """G1: 表データの統合・検算・整形（AI研磨対応）"""

    # 検算の閾値
    MAX_EMPTY_CELL_RATIO = 0.5  # 空セル率がこれ以上なら警告
    MAX_DUPLICATE_ROW_RATIO = 0.3  # 重複行率がこれ以上なら警告

    # AI研磨の投入条件
    POLISH_THRESHOLD_CELL_MISMATCH = 2  # セル数不一致がこれ以上なら研磨
    POLISH_THRESHOLD_EMPTY_RATIO = 0.3  # 空セル率がこれ以上なら研磨

    def __init__(self, llm_client=None):
        """
        Args:
            llm_client: LLMクライアント（AI研磨に使用）
        """
        self.llm = llm_client
        self._token_usage: Dict[str, Any] = {
            'prompt_tokens': 0,
            'completion_tokens': 0,
            'total_tokens': 0,
            'model': G1_MODEL,
            'polished_tables': 0
        }

    def process(self, g1_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        表データを検証・整形（Ver 6.4: Fから届いた文字の最終研磨）

        Args:
            g1_input: G-Gate からの入力
                - tables: 表データリスト
                - table_page_context: 表ページのテキストコンテキスト

        Returns:
            H1 用の整形済み JSON
        """
        # ============================================
        # 【Ver 6.2】物理分離フローの場合は専用処理
        # ============================================
        if g1_input.get('_v62_cell_only'):
            return self._process_v62(g1_input)

        # 以下は従来処理
        logger.info("[G1] 表整理開始（Ver 6.4: 研磨専念）...")

        tables = g1_input.get('tables', [])
        table_page_context = g1_input.get('table_page_context', {})

        if not tables:
            logger.info("[G1] 表なし → スキップ")
            logger.info(f"[G1] トークン使用量: 0 (表なし)")
            return {
                'tables': [],
                'validation_results': [],
                'table_page_context': {},
                'statistics': {
                    'total_tables': 0,
                    'valid_tables': 0,
                    'fixed_tables': 0,
                    'polished_tables': 0,
                    'total_rows': 0
                },
                'token_usage': self._token_usage.copy()
            }

        # 各表を検証・整形
        validated_tables = []
        validation_results = []
        total_rows = 0
        fixed_count = 0
        polished_count = 0

        for table in tables:
            anchor_id = table.get('anchor_id', '')
            logger.debug(f"[G1] 検証: {anchor_id}")

            # Step 1: ルールベース検算
            validation = self._validate_table(table)

            # Step 2: 自動修正（セル数調整等）
            if validation.errors and not validation.is_valid:
                fixed_table, fix_result = self._auto_fix_table(table, validation)
                if fix_result.is_valid:
                    table = fixed_table
                    validation = fix_result
                    fixed_count += 1

            # 【Ver 6.4】AI研磨: 無条件実行を廃止
            # 異常がある場合のみ _process_v62 で anomaly_report ベースの再読を実行
            # ここでは検証のみ行い、無駄なAI呼び出しを排除

            validation_results.append(validation)

            # Step 4: 最終整形
            formatted_table = self._format_table_for_h1(table, validation)
            validated_tables.append(formatted_table)
            total_rows += len(formatted_table.get('rows', []))

            # 警告・エラーをログ
            for warn in validation.warnings:
                logger.warning(f"[G1] {anchor_id}: {warn}")
            for err in validation.errors:
                logger.error(f"[G1] {anchor_id}: {err}")

        # 統計
        valid_count = sum(1 for v in validation_results if v.is_valid)
        statistics = {
            'total_tables': len(tables),
            'valid_tables': valid_count,
            'fixed_tables': fixed_count,
            'polished_tables': polished_count,
            'total_rows': total_rows,
            'heavy_tables': sum(1 for t in validated_tables if t.get('is_heavy', False))
        }

        # トークン使用量をログ出力
        logger.info(f"[G1] トークン使用量: prompt={self._token_usage['prompt_tokens']}, "
                   f"completion={self._token_usage['completion_tokens']}, "
                   f"total={self._token_usage['total_tokens']} (model={self._token_usage['model']})")
        logger.info(f"[G1] AI研磨: {polished_count}表に適用")

        logger.info(f"[G1] 完了: {valid_count}/{len(tables)}表が有効, "
                   f"{fixed_count}表を自動修正, {polished_count}表をAI研磨, 計{total_rows}行")

        return {
            'tables': validated_tables,
            'validation_results': [self._validation_to_dict(v) for v in validation_results],
            'table_page_context': table_page_context,
            'statistics': statistics,
            'token_usage': self._token_usage.copy()
        }

    # ============================================
    # 【Ver 6.2】物理分離フロー専用処理
    # ============================================
    def _process_v62(self, g1_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        【Ver 6.4】表専用レーンの処理

        Fから届いた文字の最終研磨（改行・泣き別れ修正）のみ。
        セル内の文字を溶接（分断された単語の結合）してH1へ渡す。

        【Ver 6.4】異常セルの外科手術:
        anomaly_report にあるセルに対して、画像再読を実行。

        Args:
            g1_input: G-Refiner からの入力
                - tables: 表データリスト（tagged_texts 含む）
                - _v62_cell_only: True
                - anomaly_report: 異常セルリスト（Ver 6.4）
                - file_path: 画像ファイルパス（Ver 6.4）

        Returns:
            H1 用の整形済み JSON + audit_log
        """
        logger.info("[G1] 【Ver 6.4】表専用レーン処理開始（研磨 + 異常再読）")

        tables = g1_input.get('tables', [])
        cell_count = sum(t.get('_cell_count', 0) for t in tables)

        # 【Ver 6.4】異常セルリストと画像パスを取得
        anomaly_report = g1_input.get('anomaly_report', [])
        file_path = g1_input.get('file_path')

        logger.info(f"[G1] 入力: {len(tables)}表, {cell_count}セル（F-10洗い替え済み）")
        if anomaly_report:
            logger.info(f"[G1] 【Ver 6.4】異常セル: {len(anomaly_report)}件 → 外科手術対象")

        if not tables:
            logger.info("[G1] 【Ver 6.2】表なし → スキップ")
            return {
                'tables': [],
                'statistics': {'total_tables': 0, 'total_cells': 0},
                'token_usage': self._token_usage.copy()
            }

        # 各表をセル内溶接
        processed_tables = []
        total_welded = 0

        for table in tables:
            anchor_id = table.get('anchor_id', 'TBL_???')
            tagged_texts = table.get('tagged_texts', [])
            x_headers = table.get('x_headers', [])
            y_headers = table.get('y_headers', [])

            logger.debug(f"[G1] 【Ver 6.2】表処理: {anchor_id} ({len(tagged_texts)}セル)")

            # 【Ver 6.4】セル内文字の溶接: AI呼び出しを廃止
            # F-10 で洗い替え済みのため、ここでは整形のみ
            # 異常がある場合のみ後続の外科手術で処理
            welded_texts = tagged_texts

            # H1 用に整形
            processed_tables.append({
                'anchor_id': anchor_id,
                'tagged_texts': welded_texts,
                'x_headers': x_headers,
                'y_headers': y_headers,
                'row_count': len(set(t.get('y_header', '') for t in welded_texts)),
                'col_count': len(x_headers),
                '_v62_welded': True
            })

        # ============================================
        # 【Ver 6.4】異常セルの外科手術
        # ============================================
        anomaly_resolved = 0
        anomaly_failed = 0
        resolution_details = []

        if anomaly_report and self.llm and file_path:
            logger.info(f"[G1] 【Ver 6.4】異常セル外科手術開始: {len(anomaly_report)}件")

            for anomaly in anomaly_report:
                try:
                    result = self._reread_anomaly_cell(anomaly, file_path, processed_tables)
                    if result.get('success'):
                        anomaly_resolved += 1
                        resolution_details.append({
                            'id': anomaly.get('id', ''),
                            'reason': anomaly.get('reason', ''),
                            'original_text': anomaly.get('text', ''),
                            'corrected_text': result.get('corrected_text', ''),
                            'status': 'resolved'
                        })
                    else:
                        anomaly_failed += 1
                        resolution_details.append({
                            'id': anomaly.get('id', ''),
                            'reason': anomaly.get('reason', ''),
                            'original_text': anomaly.get('text', ''),
                            'error': result.get('error', 'unknown'),
                            'status': 'failed'
                        })
                except Exception as e:
                    anomaly_failed += 1
                    logger.warning(f"[G1] 異常セル再読失敗: {anomaly.get('id', '')} - {e}")
                    resolution_details.append({
                        'id': anomaly.get('id', ''),
                        'reason': anomaly.get('reason', ''),
                        'error': str(e),
                        'status': 'failed'
                    })

            logger.info(f"[G1] 【Ver 6.4】外科手術完了: 成功{anomaly_resolved}件, 失敗{anomaly_failed}件")

        # 監査ログ構築
        audit_log = {
            'scrubbed_count': cell_count,
            'anomaly_resolved': anomaly_resolved,
            'anomaly_failed': anomaly_failed,
            'resolution_details': resolution_details
        }

        logger.info(f"[G1] 【Ver 6.4】完了: {len(processed_tables)}表, 溶接{total_welded}件, 異常解決{anomaly_resolved}件")

        return {
            'tables': processed_tables,
            'statistics': {
                'total_tables': len(processed_tables),
                'total_cells': cell_count,
                'welded_cells': total_welded,
                'anomaly_resolved': anomaly_resolved,
                'anomaly_failed': anomaly_failed
            },
            'token_usage': self._token_usage.copy(),
            'audit_log': audit_log
        }

    # ============================================
    # 【Ver 6.4】異常セル外科手術
    # ============================================
    def _reread_anomaly_cell(
        self,
        anomaly: Dict[str, Any],
        file_path: Path,
        processed_tables: List[Dict]
    ) -> Dict[str, Any]:
        """
        【Ver 6.4】異常セルのピンポイント再読

        1. 対象座標の画像を切り抜き
        2. 画像が不鮮明ならCLAHE/シャープ化
        3. Gemini Vision に「この箱の中の文字だけを読め」と命令
        4. 結果で元データを上書き

        Args:
            anomaly: 異常セル情報
            file_path: 画像ファイルパス
            processed_tables: 処理済み表リスト（上書き対象）

        Returns:
            {'success': bool, 'corrected_text': str} or {'success': False, 'error': str}
        """
        bbox = anomaly.get('bbox')
        if not bbox or len(bbox) < 4:
            return {'success': False, 'error': 'invalid_bbox'}

        anomaly_id = anomaly.get('id', '')
        x_header = anomaly.get('x_header', '')
        y_header = anomaly.get('y_header', '')

        logger.debug(f"[G1] 異常セル再読: {anomaly_id} bbox={bbox}")

        try:
            # Step 1: 画像を切り抜き
            cropped_image = self._crop_image(file_path, bbox, padding=5)
            if cropped_image is None:
                return {'success': False, 'error': 'crop_failed'}

            # Step 2: 画像補正（CLAHE + シャープ化）
            enhanced_image = self._enhance_image(cropped_image)

            # Step 3: 一時ファイルに保存してVision呼び出し
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                tmp_path = tmp.name
                if PIL_AVAILABLE:
                    Image.fromarray(enhanced_image).save(tmp_path)
                elif CV2_AVAILABLE:
                    cv2.imwrite(tmp_path, enhanced_image)
                else:
                    return {'success': False, 'error': 'no_image_library'}

            # Step 4: Gemini Vision で一点集中読解
            prompt = f"""この画像に写っている文字だけを、正確に読み取ってください。

【絶対ルール】
1. 画像に写っている文字のみを出力
2. 推測や補完は一切しない
3. 読めない場合は「読取不能」と出力

【出力形式】
読み取った文字のみ（説明不要）"""

            response = self.llm.generate_with_vision(
                prompt=prompt,
                image_path=tmp_path,
                model=G1_REREAD_MODEL,
                max_tokens=500,
                temperature=0.0
            )

            # 一時ファイル削除
            import os
            try:
                os.unlink(tmp_path)
            except:
                pass

            # トークン使用量を記録
            if hasattr(self.llm, 'last_usage') and self.llm.last_usage:
                usage = self.llm.last_usage
                self._token_usage['prompt_tokens'] += usage.get('prompt_tokens', 0)
                self._token_usage['completion_tokens'] += usage.get('completion_tokens', 0)
                self._token_usage['total_tokens'] += usage.get('total_tokens', 0)

            corrected_text = response.strip() if isinstance(response, str) else response.get('content', '').strip()

            if not corrected_text or corrected_text == '読取不能':
                return {'success': False, 'error': 'unreadable'}

            # Step 5: processed_tables 内の該当セルを上書き
            self._update_cell_in_tables(processed_tables, anomaly_id, x_header, y_header, corrected_text)

            logger.info(f"[G1] 異常セル修正成功: {anomaly_id} '{anomaly.get('text', '')}' → '{corrected_text}'")
            return {'success': True, 'corrected_text': corrected_text}

        except Exception as e:
            logger.warning(f"[G1] 異常セル再読エラー: {anomaly_id} - {e}")
            return {'success': False, 'error': str(e)}

    def _crop_image(
        self,
        file_path: Path,
        bbox: List[float],
        padding: int = 5
    ) -> Optional[Any]:
        """
        画像から指定座標を切り抜き

        Args:
            file_path: 画像ファイルパス
            bbox: [x1, y1, x2, y2]
            padding: 余白（ピクセル）

        Returns:
            切り抜かれた画像（numpy配列）or None
        """
        if not PIL_AVAILABLE and not CV2_AVAILABLE:
            return None

        try:
            x1, y1, x2, y2 = [int(v) for v in bbox[:4]]

            if PIL_AVAILABLE:
                img = Image.open(file_path)
                # パディングを追加
                x1 = max(0, x1 - padding)
                y1 = max(0, y1 - padding)
                x2 = min(img.width, x2 + padding)
                y2 = min(img.height, y2 + padding)

                cropped = img.crop((x1, y1, x2, y2))
                return np.array(cropped)

            elif CV2_AVAILABLE:
                img = cv2.imread(str(file_path))
                if img is None:
                    return None
                h, w = img.shape[:2]
                x1 = max(0, x1 - padding)
                y1 = max(0, y1 - padding)
                x2 = min(w, x2 + padding)
                y2 = min(h, y2 + padding)
                return img[y1:y2, x1:x2]

        except Exception as e:
            logger.warning(f"[G1] 画像切り抜きエラー: {e}")
            return None

    def _enhance_image(self, image: Any) -> Any:
        """
        画像補正（CLAHE + シャープ化）

        Args:
            image: 入力画像（numpy配列）

        Returns:
            補正済み画像
        """
        if not CV2_AVAILABLE:
            return image

        try:
            # グレースケール変換
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            else:
                gray = image

            # CLAHE（コントラスト制限付き適応的ヒストグラム均等化）
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            enhanced = clahe.apply(gray)

            # シャープ化カーネル
            kernel = np.array([
                [-1, -1, -1],
                [-1,  9, -1],
                [-1, -1, -1]
            ])
            sharpened = cv2.filter2D(enhanced, -1, kernel)

            # 3チャンネルに戻す
            return cv2.cvtColor(sharpened, cv2.COLOR_GRAY2RGB)

        except Exception as e:
            logger.warning(f"[G1] 画像補正エラー: {e}")
            return image

    def _update_cell_in_tables(
        self,
        tables: List[Dict],
        anomaly_id: str,
        x_header: str,
        y_header: str,
        corrected_text: str
    ):
        """
        処理済みテーブル内の該当セルを上書き

        Args:
            tables: 処理済み表リスト
            anomaly_id: 異常セルのID
            x_header: X軸ヘッダー
            y_header: Y軸ヘッダー
            corrected_text: 修正後テキスト
        """
        for table in tables:
            tagged_texts = table.get('tagged_texts', [])
            for tt in tagged_texts:
                # IDマッチまたは座標マッチ
                if tt.get('id') == anomaly_id or (
                    tt.get('x_header') == x_header and
                    tt.get('y_header') == y_header
                ):
                    tt['text'] = corrected_text
                    tt['_v64_reread'] = True
                    logger.debug(f"[G1] セル上書き完了: {anomaly_id}")
                    return

    # ============================================
    # 【Ver 6.4】AI研磨機能は完全削除
    # 時限爆弾（名寄せ・穴埋め・重複判断）を物理的に排除
    # 異常処理のみ _reread_anomaly_cell で許可
    # ============================================

    # --- 以下 Ver 6.4 で削除されたメソッド ---
    # _needs_ai_polish: AI研磨判定（削除）
    # _polish_table_with_ai: AI研磨実行（削除）
    # _build_polish_prompt: AI研磨プロンプト（削除）
    # _parse_polish_response: AI研磨レスポンス解析（削除）
    # _weld_cell_texts_v62: セル内溶接（削除）
    # ----------------------------------------

    # ============================================
    # 検算・修正機能（既存）
    # ============================================
    def _validate_table(self, table: Dict[str, Any]) -> TableValidationResult:
        """
        表の検算を実行

        【Ver 5.0】ヘッダータグ形式の場合はグリッド検算をスキップ
        「住所（タグ）が正しければ正解」という新基準を適用

        チェック項目（従来形式のみ）:
        1. headers と rows のセル数一致
        2. 空行の検出
        3. 空列の検出
        4. 重複行の検出
        5. 行数・列数の整合性
        """
        anchor_id = table.get('anchor_id', 'unknown')

        # ============================================
        # 【Ver 5.7】タグ形式の場合はグリッド検算をスキップ
        # 「住所（タグ）が正しければ正解」という新基準を適用
        # ============================================
        source = table.get('source', '')
        has_tagged_data = (
            source == 'ver5_tagged' or
            table.get('x_headers') or
            table.get('y_headers') or
            table.get('tagged_texts')
        )
        if has_tagged_data:
            logger.info(f"[G1] Ver5.7形式検出: {anchor_id} - グリッド検算スキップ（直送）")
            return TableValidationResult(
                anchor_id=anchor_id,
                is_valid=True,
                warnings=[],
                errors=[]
            )

        headers = table.get('headers', [])
        rows = table.get('rows', [])
        declared_row_count = table.get('row_count', len(rows))
        declared_col_count = table.get('col_count', len(headers))

        warnings = []
        errors = []

        # 1. ヘッダーと行のセル数一致チェック
        if headers:
            header_count = len(headers)
            mismatch_rows = []
            for i, row in enumerate(rows):
                if isinstance(row, list) and len(row) != header_count:
                    mismatch_rows.append(i)
            if mismatch_rows:
                if len(mismatch_rows) <= 3:
                    warnings.append(f"Cell count mismatch in rows: {mismatch_rows}")
                else:
                    warnings.append(f"Cell count mismatch in {len(mismatch_rows)} rows")

        # 2. 空行の検出
        empty_rows = []
        for i, row in enumerate(rows):
            if isinstance(row, list) and all(not str(cell).strip() for cell in row):
                empty_rows.append(i)
        if empty_rows:
            warnings.append(f"Empty rows: {empty_rows[:5]}{'...' if len(empty_rows) > 5 else ''}")

        # 3. 空列の検出（全ての行でその列が空）
        if headers and rows:
            empty_cols = []
            for col_idx in range(len(headers)):
                all_empty = True
                for row in rows:
                    if isinstance(row, list) and col_idx < len(row):
                        if str(row[col_idx]).strip():
                            all_empty = False
                            break
                if all_empty:
                    empty_cols.append(headers[col_idx] if col_idx < len(headers) else col_idx)
            if empty_cols:
                warnings.append(f"Empty columns: {empty_cols}")

        # 4. 重複行の検出
        if rows:
            row_strings = [str(row) for row in rows if isinstance(row, list)]
            seen = {}
            duplicates = []
            for i, rs in enumerate(row_strings):
                if rs in seen:
                    duplicates.append((seen[rs], i))
                else:
                    seen[rs] = i
            if duplicates:
                dup_ratio = len(duplicates) / len(rows)
                if dup_ratio > self.MAX_DUPLICATE_ROW_RATIO:
                    warnings.append(f"High duplicate row ratio: {dup_ratio:.1%}")

        # 5. 行数・列数の整合性
        actual_row_count = len(rows)
        actual_col_count = len(headers) if headers else (max(len(r) for r in rows if isinstance(r, list)) if rows else 0)

        if declared_row_count != actual_row_count:
            errors.append(f"Row count: declared={declared_row_count}, actual={actual_row_count}")

        if declared_col_count != actual_col_count and declared_col_count > 0:
            errors.append(f"Column count: declared={declared_col_count}, actual={actual_col_count}")

        # 6. 空セル率チェック
        if rows and headers:
            total_cells = len(rows) * len(headers)
            empty_cells = 0
            for row in rows:
                if isinstance(row, list):
                    empty_cells += sum(1 for cell in row if not str(cell).strip())
            if total_cells > 0:
                empty_ratio = empty_cells / total_cells
                if empty_ratio > self.MAX_EMPTY_CELL_RATIO:
                    warnings.append(f"High empty cell ratio: {empty_ratio:.1%}")

        # 判定
        is_valid = len(errors) == 0

        return TableValidationResult(
            anchor_id=anchor_id,
            is_valid=is_valid,
            warnings=warnings,
            errors=errors
        )

    def _auto_fix_table(
        self,
        table: Dict[str, Any],
        validation: TableValidationResult
    ) -> Tuple[Dict[str, Any], TableValidationResult]:
        """
        表の自動修正を試みる（ルールベース）

        修正内容:
        1. 行のセル数を揃える（不足は空文字で埋める、過剰は切り詰める）
        2. row_count / col_count を実際の値に更新
        """
        fixed_table = table.copy()
        fix_descriptions = []

        headers = fixed_table.get('headers', [])
        rows = fixed_table.get('rows', [])
        target_col_count = len(headers) if headers else 0

        # 1. 行のセル数を揃える
        if target_col_count > 0:
            fixed_rows = []
            for row in rows:
                if isinstance(row, list):
                    if len(row) < target_col_count:
                        row = row + [''] * (target_col_count - len(row))
                    elif len(row) > target_col_count:
                        row = row[:target_col_count]
                    fixed_rows.append(row)
                elif isinstance(row, dict):
                    fixed_rows.append([str(row.get(h, '')) for h in headers])

            fixed_table['rows'] = fixed_rows
            if fixed_rows != rows:
                fix_descriptions.append("Normalized cell counts")

        # 2. row_count / col_count を更新
        fixed_table['row_count'] = len(fixed_table.get('rows', []))
        fixed_table['col_count'] = len(fixed_table.get('headers', []))

        # 再検証
        new_validation = self._validate_table(fixed_table)
        new_validation.auto_fixed = True
        new_validation.fix_description = '; '.join(fix_descriptions) if fix_descriptions else 'No changes'

        return fixed_table, new_validation

    def _format_table_for_h1(
        self,
        table: Dict[str, Any],
        validation: TableValidationResult
    ) -> Dict[str, Any]:
        """H1 用に表を整形（Ver 6.4: e_content排除）"""
        rows = table.get('rows', [])
        headers = table.get('headers', [])

        # is_heavy の再判定（20行以上 or 5列以上）
        is_heavy = len(rows) >= 20 or len(headers) >= 5 or table.get('is_heavy', False)

        return {
            'anchor_id': table.get('anchor_id', ''),
            'page': table.get('page', 0),
            'title': table.get('title', ''),
            'table_type': table.get('table_type', 'visual_table'),
            'headers': headers,
            'rows': rows,
            'row_count': len(rows),
            'col_count': len(headers),
            'source': table.get('source', 'unknown'),
            'is_heavy': is_heavy,
            'is_valid': validation.is_valid,
            'ai_polished': validation.ai_polished,
            'validation_warnings': validation.warnings,
            # 三位一体データ
            'x_headers': table.get('x_headers', []),
            'y_headers': table.get('y_headers', []),
            'tagged_texts': table.get('tagged_texts', []),
            'f8_anchors': table.get('f8_anchors', []),
        }

    def _validation_to_dict(self, validation: TableValidationResult) -> Dict[str, Any]:
        """検算結果を辞書に変換"""
        return {
            'anchor_id': validation.anchor_id,
            'is_valid': validation.is_valid,
            'warnings': validation.warnings,
            'errors': validation.errors,
            'auto_fixed': validation.auto_fixed,
            'ai_polished': validation.ai_polished,
            'fix_description': validation.fix_description
        }

    # ============================================
    # ヘルパー関数
    # ============================================
    def structure_from_e_text(
        self,
        raw_text: str,
        context: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """E の生テキストから表構造を推測"""
        if not raw_text:
            return None

        lines = [l for l in raw_text.split('\n') if l.strip()]
        if len(lines) < 2:
            return None

        delimiter = self._detect_delimiter(raw_text)
        headers = [c.strip() for c in lines[0].split(delimiter) if c.strip()]
        rows = []

        for line in lines[1:]:
            cells = [c.strip() for c in line.split(delimiter)]
            if any(c for c in cells):
                rows.append(cells)

        if not headers or not rows:
            return None

        title = self._guess_table_title(raw_text, context)
        table_type = self._guess_table_type(headers, rows)

        return {
            'headers': headers,
            'rows': rows,
            'title': title,
            'table_type': table_type,
            'row_count': len(rows),
            'col_count': len(headers)
        }

    def _detect_delimiter(self, text: str) -> str:
        """区切り文字を検出"""
        tab_count = text.count('\t')
        pipe_count = text.count('|')
        comma_count = text.count(',')

        if tab_count > pipe_count and tab_count > comma_count:
            return '\t'
        elif pipe_count > comma_count:
            return '|'
        else:
            return '\t'

    def _guess_table_title(self, raw_text: str, context: Optional[str]) -> str:
        """表のタイトルを推測"""
        if context:
            patterns = [
                r'【(.+?)】',
                r'■\s*(.+)',
                r'◆\s*(.+)',
                r'^(.+?[表一覧リスト])\s*$',
            ]
            for pattern in patterns:
                match = re.search(pattern, context, re.MULTILINE)
                if match:
                    return match.group(1).strip()
        return ""

    def _guess_table_type(self, headers: List[str], rows: List[List[str]]) -> str:
        """表のタイプを推測"""
        header_text = ' '.join(headers).lower()
        all_text = header_text + ' ' + ' '.join(str(cell) for row in rows for cell in row).lower()

        if any(kw in header_text for kw in ['順位', '位', 'rank', '№']):
            return 'ranking'
        if any(kw in header_text for kw in ['日付', '日時', '時間', '曜日']):
            return 'schedule'
        if any(kw in all_text for kw in ['円', '¥', '料金', '価格', '金額']):
            return 'pricing'
        if any(kw in header_text for kw in ['氏名', '名前', '生徒', '参加者']):
            return 'roster'
        if any(kw in header_text for kw in ['点数', '得点', '成績', 'スコア']):
            return 'score'

        return 'visual_table'
```

## shared/pipeline/stage_g2_text_refiner.py

```python
"""
Stage G2: Text Refiner（テキスト専用整理）- Ver 6.4 事実陳列専用

【設計 2026-01-31】Ver 6.4: JSON生成でのAI使用を完全禁止

============================================
【Ver 6.4】絶対禁止事項（時限爆弾の除去）:
  ┌─────────────────────────────────────────┐
  │ ✗ 名寄せ: 表記揺れの勝手な清書         │
  │ ✗ 穴埋め: 読み取れない値の推測補完     │
  │ ✗ OCR浄化: 「文脈から推測して自然に」  │
  │ ✗ スライディング・ウィンドウ研磨       │
  │                                         │
  │ これらは「善意の改ざん」であり          │
  │ 100%正確な抽出を破壊する時限爆弾である  │
  └─────────────────────────────────────────┘

【Ver 6.4】役割:
  - F-10 洗い替え済みデータを「そのまま」JSON配列に格納
  - テキストの順序整理とアンカー挿入のみ（プログラム的処理）
  - AIの「作文」ではなく「事実の陳列」
  - 文字の書き換え・推測による補完・項目の結合は一切禁止

【Ver 6.4】許可されるAI使用:
  - anomaly_report にある異常テキストのみのピンポイント画像再読
  - これは「読み直し」であり「推測」ではない

入力:
  - text_blocks: 地文データリスト（F-10 洗い替え済み）
  - table_anchors: 表のアンカー情報
  - anomaly_report: 異常テキストリスト（外科手術対象）
  - file_path: 画像ファイルパス（画像再読用）

出力:
  - unified_text: 一本化された原稿
  - segments: セグメントリスト
  - token_usage: トークン使用量
  - audit_log: 監査ログ
============================================
"""
from pathlib import Path
from typing import Dict, Any, List, Optional, Set
from dataclasses import dataclass
from loguru import logger
import re
from difflib import SequenceMatcher

# 画像処理ライブラリ（オプション）
try:
    import cv2
    import numpy as np
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# G2 で使用するモデル
G2_MODEL = "gemini-2.5-flash-lite"
G2_REREAD_MODEL = "gemini-2.5-flash"  # 異常再読には高精度モデル


@dataclass
class DedupStats:
    """重複排除統計"""
    total_input: int = 0
    total_output: int = 0
    duplicates_removed: int = 0
    merged_segments: int = 0


class StageG2TextRefiner:
    """G2: テキストの統合・ソート・整形"""

    # 類似度の閾値（これ以上なら重複とみなす）
    SIMILARITY_THRESHOLD = 0.85

    # ソースの優先順位（数字が小さいほど優先）
    SOURCE_PRIORITY = {
        'post_body': 1,
        'stage_f': 2,
        'stage_e': 3,
        'g_gate': 4,
    }

    def __init__(self, llm_client=None):
        """
        Args:
            llm_client: LLMクライアント（オプション、複雑なテキスト整形時に使用）
        """
        self.llm = llm_client
        self._token_usage: Dict[str, Any] = {
            'prompt_tokens': 0,
            'completion_tokens': 0,
            'total_tokens': 0,
            'model': G2_MODEL
        }

    def process(self, g2_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        テキストセグメントを整理・統合

        Args:
            g2_input: G-Refiner からの入力
                - text_blocks: 地文データリスト（Ver 6.2）
                - table_anchors: 表アンカー情報（Ver 6.2）
                - segments: テキストセグメントリスト（従来）
                - post_body: 投稿本文

        Returns:
            H2 用の整形済み JSON
        """
        # ============================================
        # 【Ver 6.2】物理分離フローの場合は専用処理
        # ============================================
        if g2_input.get('_v62_text_only'):
            return self._process_v62(g2_input)

        # 以下は従来処理
        logger.info("[G2] テキスト整理開始...")

        segments = g2_input.get('segments', [])
        post_body = g2_input.get('post_body', {})
        placeholder_count = g2_input.get('placeholder_count', 0)

        stats = DedupStats(total_input=len(segments))

        if not segments:
            logger.info("[G2] セグメントなし → post_body のみ")
            logger.info(f"[G2] トークン使用量: 0 (セグメントなし)")
            unified_text = post_body.get('text', '') if post_body else ''
            return {
                'segments': [],
                'unified_text': unified_text,
                'dedup_stats': self._stats_to_dict(stats),
                'post_body': post_body or {},
                'token_usage': self._token_usage.copy()
            }

        # ============================================
        # Step 1: ページ順・読み順でソート
        # ============================================
        sorted_segments = self._sort_segments(segments)
        logger.debug(f"[G2] ソート完了: {len(sorted_segments)}セグメント")

        # ============================================
        # Step 2: 重複排除
        # ============================================
        deduped_segments = self._deduplicate_segments(sorted_segments)
        stats.duplicates_removed = len(sorted_segments) - len(deduped_segments)
        logger.info(f"[G2] 重複排除: {stats.duplicates_removed}件削除")

        # ============================================
        # Step 3: REF_ID を再付与
        # ============================================
        renumbered_segments = self._renumber_ref_ids(deduped_segments)
        stats.total_output = len(renumbered_segments)

        # ============================================
        # Step 4: unified_text を構築
        # ============================================
        unified_text = self._build_unified_text(renumbered_segments)

        # 【Ver 6.4】AI研磨: 無条件実行を廃止
        # 異常がある場合のみ _process_v62 で anomaly_report ベースの再読を実行
        # ここでは統合のみ行い、無駄なAI呼び出しを排除

        # トークン使用量をログ出力
        logger.info(f"[G2] トークン使用量: prompt={self._token_usage['prompt_tokens']}, "
                   f"completion={self._token_usage['completion_tokens']}, "
                   f"total={self._token_usage['total_tokens']} (model={self._token_usage['model']})")

        logger.info(f"[G2] 完了: {stats.total_input}→{stats.total_output}セグメント, unified_text={len(unified_text)}文字")

        return {
            'segments': renumbered_segments,
            'unified_text': unified_text,
            'dedup_stats': self._stats_to_dict(stats),
            'post_body': post_body or {},
            'placeholder_count': placeholder_count,
            'token_usage': self._token_usage.copy()
        }

    # ============================================
    # 【Ver 6.2】物理分離フロー専用処理
    # ============================================
    def _process_v62(self, g2_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        【Ver 6.4】地文専用レーンの処理

        Fから届いたセグメントの論理的な接続（溶接）のみ。
        表アンカーを保持しながら一本の完璧な原稿に統合する。

        【Ver 6.4】異常テキストの外科手術:
        anomaly_report にあるテキストに対して、画像再読を実行。

        Args:
            g2_input: G-Refiner からの入力
                - text_blocks: 地文データリスト（F-10 洗い替え済み）
                - table_anchors: 表のアンカー情報
                - post_body: 投稿本文
                - anomaly_report: 異常テキストリスト（Ver 6.4）
                - file_path: 画像ファイルパス（Ver 6.4）

        Returns:
            H2 用の整形済み JSON + audit_log
        """
        import json

        logger.info("[G2] 【Ver 6.4】地文専用レーン処理開始（溶接 + 異常再読）")

        text_blocks = g2_input.get('text_blocks', [])
        table_anchors = g2_input.get('table_anchors', [])
        post_body = g2_input.get('post_body', {})

        # 【Ver 6.4】異常テキストリストと画像パスを取得
        anomaly_report = g2_input.get('anomaly_report', [])
        file_path = g2_input.get('file_path')

        logger.info(f"[G2] 入力: {len(text_blocks)}テキスト, {len(table_anchors)}アンカー")
        if anomaly_report:
            logger.info(f"[G2] 【Ver 6.4】異常テキスト: {len(anomaly_report)}件 → 外科手術対象")

        if not text_blocks:
            logger.info("[G2] 【Ver 6.2】テキストなし → スキップ")
            unified = post_body.get('text', '') if post_body else ''
            return {
                'segments': [],
                'unified_text': unified,
                'post_body': post_body or {},
                'token_usage': self._token_usage.copy()
            }

        # ============================================
        # Step 1: テキストを読み順でソート
        # ============================================
        sorted_texts = sorted(text_blocks, key=lambda x: (
            x.get('page', 0),
            x.get('reading_order', 0)
        ))

        # ============================================
        # Step 2: 表アンカーを適切な位置に挿入
        # ============================================
        # テキスト配列に表のプレースホルダーを挿入
        texts_with_anchors = []
        for tt in sorted_texts:
            texts_with_anchors.append(tt.get('text', ''))

        # 表アンカーを挿入（簡易版: 末尾に追加）
        for anchor in table_anchors:
            anchor_id = anchor.get('anchor_id', 'TBL_???')
            texts_with_anchors.append(f"\n[→ {anchor_id} 参照]\n")

        # ============================================
        # Step 3: テキストを結合（AI統合を廃止）
        # 【Ver 6.4】無条件のAI呼び出しを排除
        # 異常がある場合のみ後続の外科手術で処理
        # ============================================
        unified_text = "\n".join(texts_with_anchors)

        # ============================================
        # Step 4: セグメントを構築
        # ============================================
        segments = []
        for i, tt in enumerate(sorted_texts):
            segments.append({
                'ref_id': f'REF_{i+1:03d}',
                'text': tt.get('text', ''),
                'page': tt.get('page', 0),
                'segment_type': tt.get('type', 'paragraph'),
                '_v62_processed': True
            })

        # ============================================
        # 【Ver 6.4】異常テキストの外科手術
        # ============================================
        anomaly_resolved = 0
        anomaly_failed = 0
        resolution_details = []

        if anomaly_report and self.llm and file_path:
            logger.info(f"[G2] 【Ver 6.4】異常テキスト外科手術開始: {len(anomaly_report)}件")

            for anomaly in anomaly_report:
                try:
                    result = self._reread_anomaly_text(anomaly, file_path, segments)
                    if result.get('success'):
                        anomaly_resolved += 1
                        resolution_details.append({
                            'id': anomaly.get('id', ''),
                            'reason': anomaly.get('reason', ''),
                            'original_text': anomaly.get('text', ''),
                            'corrected_text': result.get('corrected_text', ''),
                            'status': 'resolved'
                        })
                    else:
                        anomaly_failed += 1
                        resolution_details.append({
                            'id': anomaly.get('id', ''),
                            'reason': anomaly.get('reason', ''),
                            'original_text': anomaly.get('text', ''),
                            'error': result.get('error', 'unknown'),
                            'status': 'failed'
                        })
                except Exception as e:
                    anomaly_failed += 1
                    logger.warning(f"[G2] 異常テキスト再読失敗: {anomaly.get('id', '')} - {e}")
                    resolution_details.append({
                        'id': anomaly.get('id', ''),
                        'reason': anomaly.get('reason', ''),
                        'error': str(e),
                        'status': 'failed'
                    })

            logger.info(f"[G2] 【Ver 6.4】外科手術完了: 成功{anomaly_resolved}件, 失敗{anomaly_failed}件")

            # 修正後のテキストで unified_text を再構築
            if anomaly_resolved > 0:
                unified_text = self._rebuild_unified_text(segments, table_anchors)

        # 監査ログ構築
        audit_log = {
            'scrubbed_count': len(text_blocks),
            'anomaly_resolved': anomaly_resolved,
            'anomaly_failed': anomaly_failed,
            'resolution_details': resolution_details
        }

        logger.info(f"[G2] 【Ver 6.4】完了: {len(segments)}セグメント, unified_text={len(unified_text)}文字, 異常解決{anomaly_resolved}件")

        return {
            'segments': segments,
            'unified_text': unified_text,
            'post_body': post_body or {},
            'table_anchors': table_anchors,
            'token_usage': self._token_usage.copy(),
            'audit_log': audit_log
        }

    # ============================================
    # 【Ver 6.4】AI統合機能は完全削除
    # 時限爆弾（OCR浄化・文脈推測）を物理的に排除
    # 異常処理のみ _reread_anomaly_text で許可
    # ============================================

    # --- 以下 Ver 6.4 で削除されたメソッド ---
    # _unify_text_v62: AI統合（削除）
    # _polish_text_with_ai: AI研磨（削除）
    # _polish_single_chunk: チャンク研磨（削除）
    # _polish_chained: 連鎖研磨（削除）
    # _split_into_chunks: チャンク分割（削除）
    # _merge_polished_chunks: チャンク結合（削除）
    # _build_text_polish_prompt: 研磨プロンプト（削除）
    # ----------------------------------------

    # ============================================
    # 【Ver 6.4】異常テキスト外科手術
    # ============================================
    def _reread_anomaly_text(
        self,
        anomaly: Dict[str, Any],
        file_path: Path,
        segments: List[Dict]
    ) -> Dict[str, Any]:
        """
        【Ver 6.4】異常テキストのピンポイント再読

        1. 対象座標の画像を切り抜き
        2. Gemini Vision で文脈考慮の再読
        3. アンカーを破壊しないよう segments を更新

        Args:
            anomaly: 異常テキスト情報
            file_path: 画像ファイルパス
            segments: セグメントリスト（上書き対象）

        Returns:
            {'success': bool, 'corrected_text': str} or {'success': False, 'error': str}
        """
        bbox = anomaly.get('bbox')
        if not bbox or len(bbox) < 4:
            return {'success': False, 'error': 'invalid_bbox'}

        anomaly_id = anomaly.get('id', '')
        page = anomaly.get('page', 0)

        logger.debug(f"[G2] 異常テキスト再読: {anomaly_id} bbox={bbox}")

        try:
            # Step 1: 画像を切り抜き
            cropped_image = self._crop_image(file_path, bbox, padding=10)
            if cropped_image is None:
                return {'success': False, 'error': 'crop_failed'}

            # Step 2: 一時ファイルに保存してVision呼び出し
            import tempfile
            import os

            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                tmp_path = tmp.name
                if PIL_AVAILABLE:
                    Image.fromarray(cropped_image).save(tmp_path)
                elif CV2_AVAILABLE:
                    cv2.imwrite(tmp_path, cropped_image)
                else:
                    return {'success': False, 'error': 'no_image_library'}

            # Step 3: Gemini Vision で文脈考慮の再読
            prompt = f"""この画像に写っているテキストを、正確に読み取ってください。

【絶対ルール】
1. 画像に写っている文字のみを出力
2. 推測や補完は一切しない
3. 改行は空白に置き換える
4. 読めない場合は「読取不能」と出力

【出力形式】
読み取ったテキストのみ（説明不要）"""

            response = self.llm.generate_with_vision(
                prompt=prompt,
                image_path=tmp_path,
                model=G2_REREAD_MODEL,
                max_tokens=1000,
                temperature=0.0
            )

            # 一時ファイル削除
            try:
                os.unlink(tmp_path)
            except:
                pass

            # トークン使用量を記録
            if hasattr(self.llm, 'last_usage') and self.llm.last_usage:
                usage = self.llm.last_usage
                self._token_usage['prompt_tokens'] += usage.get('prompt_tokens', 0)
                self._token_usage['completion_tokens'] += usage.get('completion_tokens', 0)
                self._token_usage['total_tokens'] += usage.get('total_tokens', 0)

            corrected_text = response.strip() if isinstance(response, str) else response.get('content', '').strip()

            if not corrected_text or corrected_text == '読取不能':
                return {'success': False, 'error': 'unreadable'}

            # Step 4: segments 内の該当テキストを上書き
            self._update_text_in_segments(segments, anomaly_id, page, corrected_text)

            logger.info(f"[G2] 異常テキスト修正成功: {anomaly_id} '{anomaly.get('text', '')[:30]}...' → '{corrected_text[:30]}...'")
            return {'success': True, 'corrected_text': corrected_text}

        except Exception as e:
            logger.warning(f"[G2] 異常テキスト再読エラー: {anomaly_id} - {e}")
            return {'success': False, 'error': str(e)}

    def _crop_image(
        self,
        file_path: Path,
        bbox: List[float],
        padding: int = 10
    ) -> Optional[any]:
        """
        画像から指定座標を切り抜き

        Args:
            file_path: 画像ファイルパス
            bbox: [x1, y1, x2, y2]
            padding: 余白（ピクセル）

        Returns:
            切り抜かれた画像（numpy配列）or None
        """
        if not PIL_AVAILABLE and not CV2_AVAILABLE:
            return None

        try:
            x1, y1, x2, y2 = [int(v) for v in bbox[:4]]

            if PIL_AVAILABLE:
                img = Image.open(file_path)
                # パディングを追加
                x1 = max(0, x1 - padding)
                y1 = max(0, y1 - padding)
                x2 = min(img.width, x2 + padding)
                y2 = min(img.height, y2 + padding)

                cropped = img.crop((x1, y1, x2, y2))
                import numpy as np
                return np.array(cropped)

            elif CV2_AVAILABLE:
                img = cv2.imread(str(file_path))
                if img is None:
                    return None
                h, w = img.shape[:2]
                x1 = max(0, x1 - padding)
                y1 = max(0, y1 - padding)
                x2 = min(w, x2 + padding)
                y2 = min(h, y2 + padding)
                return img[y1:y2, x1:x2]

        except Exception as e:
            logger.warning(f"[G2] 画像切り抜きエラー: {e}")
            return None

    def _update_text_in_segments(
        self,
        segments: List[Dict],
        anomaly_id: str,
        page: int,
        corrected_text: str
    ):
        """
        セグメントリスト内の該当テキストを上書き

        Args:
            segments: セグメントリスト
            anomaly_id: 異常テキストのID
            page: ページ番号
            corrected_text: 修正後テキスト
        """
        for seg in segments:
            # IDマッチまたはページ+位置マッチ
            if seg.get('id') == anomaly_id or (
                seg.get('page', 0) == page and
                seg.get('ref_id', '').endswith(anomaly_id.split('_')[-1] if '_' in anomaly_id else anomaly_id)
            ):
                seg['text'] = corrected_text
                seg['_v64_reread'] = True
                logger.debug(f"[G2] セグメント上書き完了: {anomaly_id}")
                return

    def _rebuild_unified_text(
        self,
        segments: List[Dict],
        table_anchors: List[Dict]
    ) -> str:
        """
        修正後のセグメントから unified_text を再構築

        アンカーを保持しながら溶接

        Args:
            segments: 修正済みセグメントリスト
            table_anchors: 表のアンカー情報

        Returns:
            再構築された unified_text
        """
        texts_with_anchors = []
        for seg in segments:
            texts_with_anchors.append(seg.get('text', ''))

        # 表アンカーを挿入
        for anchor in table_anchors:
            anchor_id = anchor.get('anchor_id', 'TBL_???')
            texts_with_anchors.append(f"\n[→ {anchor_id} 参照]\n")

        return "\n".join(texts_with_anchors)

    def _sort_segments(self, segments: List[Dict]) -> List[Dict]:
        """
        セグメントをページ順 → 読み順でソート

        ソート順:
        1. page (昇順)
        2. segment_type (post_body が最初)
        3. reading_order または ref_id (昇順)
        """
        def sort_key(seg):
            page = seg.get('page', 0)

            # segment_type の優先度
            seg_type = seg.get('segment_type', 'paragraph')
            type_order = {
                'post_body': 0,
                'heading': 1,
                'table_marker': 2,
                'paragraph': 3,
                'list_item': 4,
            }.get(seg_type, 5)

            # reading_order または ref_id から順序を取得
            reading_order = seg.get('reading_order', 0)
            if reading_order == 0:
                # ref_id から番号を抽出
                ref_id = seg.get('ref_id', '')
                match = re.search(r'(\d+)', ref_id)
                if match:
                    reading_order = int(match.group(1))

            return (page, type_order, reading_order)

        return sorted(segments, key=sort_key)

    def _deduplicate_segments(self, segments: List[Dict]) -> List[Dict]:
        """
        重複セグメントを排除

        ルール:
        1. 完全一致 → 優先度高いソースを採用
        2. 高類似度 → 長い方を採用
        3. 包含関係 → 長い方を採用
        """
        if not segments:
            return []

        result = []
        seen_texts: Set[str] = set()

        for seg in segments:
            text = seg.get('text', '').strip()

            # 空テキストは table_marker 以外スキップ
            if not text and seg.get('segment_type') != 'table_marker':
                continue

            # table_marker は常に採用
            if seg.get('segment_type') == 'table_marker':
                result.append(seg)
                continue

            # 正規化したテキスト
            normalized = self._normalize_text(text)

            # 完全一致チェック
            if normalized in seen_texts:
                logger.debug(f"[G2] 重複スキップ (完全一致): {text[:50]}...")
                continue

            # 類似度チェック（既存のセグメントと比較）
            is_duplicate = False
            for existing in result:
                existing_text = existing.get('text', '').strip()
                if not existing_text:
                    continue

                similarity = self._calculate_similarity(text, existing_text)
                if similarity >= self.SIMILARITY_THRESHOLD:
                    # 長い方を採用
                    if len(text) > len(existing_text):
                        result.remove(existing)
                        result.append(seg)
                        seen_texts.add(self._normalize_text(existing_text))
                        logger.debug(f"[G2] 類似テキスト置換: {existing_text[:30]}... → {text[:30]}...")
                    else:
                        logger.debug(f"[G2] 重複スキップ (類似): {text[:50]}...")
                    is_duplicate = True
                    break

                # 包含関係チェック
                if text in existing_text or existing_text in text:
                    # 長い方を採用
                    if len(text) > len(existing_text):
                        result.remove(existing)
                        result.append(seg)
                        seen_texts.add(self._normalize_text(existing_text))
                    is_duplicate = True
                    break

            if not is_duplicate:
                result.append(seg)
                seen_texts.add(normalized)

        return result

    def _normalize_text(self, text: str) -> str:
        """テキストを正規化（比較用）"""
        # 空白を統一
        normalized = re.sub(r'\s+', ' ', text.strip().lower())
        # 句読点を除去
        normalized = re.sub(r'[、。，．,\.]+', '', normalized)
        return normalized

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """2つのテキストの類似度を計算（0.0〜1.0）"""
        if not text1 or not text2:
            return 0.0

        # 短いテキストは厳しく判定
        if len(text1) < 20 or len(text2) < 20:
            return 1.0 if text1 == text2 else 0.0

        return SequenceMatcher(None, text1, text2).ratio()

    def _renumber_ref_ids(self, segments: List[Dict]) -> List[Dict]:
        """REF_ID を連番で再付与"""
        result = []
        ref_counter = 0

        for seg in segments:
            ref_counter += 1
            new_seg = seg.copy()
            new_seg['ref_id'] = f'REF_{ref_counter:03d}'
            result.append(new_seg)

        return result

    def _build_unified_text(self, segments: List[Dict]) -> str:
        """
        統合テキストを構築

        フォーマット:
        - post_body: そのまま
        - heading: 【見出し】形式
        - table_marker: プレースホルダーをそのまま挿入
        - paragraph/list_item: そのまま
        """
        parts = []
        current_page = -1

        for seg in segments:
            page = seg.get('page', 0)
            seg_type = seg.get('segment_type', 'paragraph')
            text = seg.get('text', '').strip()
            placeholder = seg.get('table_placeholder')

            # ページ区切り（オプション）
            if page != current_page and current_page >= 0:
                # parts.append(f"\n--- Page {page + 1} ---\n")
                pass
            current_page = page

            # セグメントタイプに応じたフォーマット
            if seg_type == 'table_marker' and placeholder:
                parts.append(f"\n{placeholder}\n")
            elif seg_type == 'heading':
                parts.append(f"\n【{text}】\n")
            elif seg_type == 'post_body':
                parts.append(text)
            elif seg_type == 'list_item':
                parts.append(f"・{text}")
            else:
                parts.append(text)

        # 結合して整形
        unified = '\n\n'.join(p for p in parts if p.strip())

        # 連続する改行を整理
        unified = re.sub(r'\n{3,}', '\n\n', unified)

        return unified.strip()

    def _stats_to_dict(self, stats: DedupStats) -> Dict[str, Any]:
        """統計を辞書に変換"""
        return {
            'total_input': stats.total_input,
            'total_output': stats.total_output,
            'duplicates_removed': stats.duplicates_removed,
            'merged_segments': stats.merged_segments,
            'dedup_rate': f"{(stats.duplicates_removed / max(stats.total_input, 1)) * 100:.1f}%"
        }

    # ============================================
    # 追加ヘルパー: E と F のテキストマージ
    # ============================================
    def merge_e_f_texts(
        self,
        e_text: str,
        f_text: str,
        post_body_text: str = ""
    ) -> str:
        """
        E と F のテキストをマージ（重複排除付き）

        Args:
            e_text: Stage E の抽出テキスト
            f_text: Stage F の抽出テキスト
            post_body_text: 投稿本文

        Returns:
            マージ済みテキスト
        """
        segments = []
        ref_counter = 0

        # post_body
        if post_body_text:
            ref_counter += 1
            segments.append({
                'ref_id': f'REF_{ref_counter:03d}',
                'page': 0,
                'text': post_body_text,
                'segment_type': 'post_body',
                'source': 'post_body'
            })

        # E テキストを段落分割
        e_paragraphs = self._split_paragraphs(e_text)
        for para in e_paragraphs:
            ref_counter += 1
            segments.append({
                'ref_id': f'REF_{ref_counter:03d}',
                'page': 0,
                'text': para,
                'segment_type': 'paragraph',
                'source': 'stage_e'
            })

        # F テキストを段落分割
        f_paragraphs = self._split_paragraphs(f_text)
        for para in f_paragraphs:
            ref_counter += 1
            segments.append({
                'ref_id': f'REF_{ref_counter:03d}',
                'page': 0,
                'text': para,
                'segment_type': 'paragraph',
                'source': 'stage_f'
            })

        # 重複排除
        deduped = self._deduplicate_segments(segments)

        # 統合テキスト構築
        return self._build_unified_text(deduped)

    def _split_paragraphs(self, text: str) -> List[str]:
        """テキストを段落に分割"""
        if not text:
            return []

        # 空行で分割
        paragraphs = re.split(r'\n\s*\n', text)
        return [p.strip() for p in paragraphs if p.strip()]

    # ============================================
    # クロスバリデーション: E vs F の一致度計算
    # ============================================
    def cross_validate(
        self,
        e_segments: List[Dict],
        f_segments: List[Dict]
    ) -> Dict[str, Any]:
        """
        E と F のセグメントをクロスバリデーション

        Returns:
            {
                'matched_count': int,  # 一致したセグメント数
                'e_only_count': int,   # E にのみあるセグメント数
                'f_only_count': int,   # F にのみあるセグメント数
                'match_rate': float,   # 一致率
                'confidence': str      # 'high', 'medium', 'low'
            }
        """
        e_texts = {self._normalize_text(s.get('text', '')) for s in e_segments if s.get('text')}
        f_texts = {self._normalize_text(s.get('text', '')) for s in f_segments if s.get('text')}

        matched = e_texts & f_texts
        e_only = e_texts - f_texts
        f_only = f_texts - e_texts

        total = len(e_texts | f_texts)
        match_rate = len(matched) / max(total, 1)

        # 信頼度判定
        if match_rate >= 0.7:
            confidence = 'high'
        elif match_rate >= 0.4:
            confidence = 'medium'
        else:
            confidence = 'low'

        return {
            'matched_count': len(matched),
            'e_only_count': len(e_only),
            'f_only_count': len(f_only),
            'match_rate': match_rate,
            'confidence': confidence
        }

    # ============================================
    # 【Ver 6.4】AI研磨機能は完全削除
    # 時限爆弾として物理的に排除（2026-01-31）
    # 異常処理のみ _reread_anomaly_text で許可
    # ============================================

    # Ver 6.4: 以下の全メソッドを削除
    # - _polish_text_with_ai, _polish_single_chunk, _polish_chained
    # - _split_into_chunks, _merge_polished_chunks, _build_text_polish_prompt
    # 時限爆弾（名寄せ・穴埋め・OCR浄化・スライディングウィンドウ）は
    # 「善意の改ざん」であり、100%正確な抽出を破壊するため完全削除

```

## shared/pipeline/stage_g_gate.py

```python
"""
Stage G Gate: 純粋物流センター（Ver 5.9）

【設計 2026-01-31】物理的振分に専念

役割: F7/F8/E を束ねて G1/G2 に運ぶだけ
      「知能」を一切持たない。解釈しない。判断しない。

============================================
禁止事項（物理的に削除済み）:
  - 洗い替え（Scrubbing）→ G1/G2の仕事
  - 表候補の検出・分割 → F7が既に決定済み
  - 勝手な廃棄・フィルタリング

許可事項:
  1. パッキング: F7 + F8 + E を一つの袋に詰める
  2. 振分: 表 → G1、テキスト → G2 へ直送

【Ver 5.9】G1/H1への座標データ強化
  - e_content: Stage Eの正確なテキスト（OCR置換用）
  - f8_anchors: Suryaの物理座標（全軸マッピング用）
============================================
"""
from typing import Dict, Any, List, Optional, Tuple
from loguru import logger


class StageGGate:
    """
    【Ver 5.9】純粋物流センター

    G-Gateは「運び屋」に徹する。
    知能が必要な仕事（洗い替え・座標マッピング）はG1/G2へ委譲。

    【Ver 5.9 追加】
    - e_content: Stage Eの正確なテキスト → G1がOCR誤読を物理座標で置換
    - f8_anchors: Suryaの物理座標 → G1/H1が全軸座標ロック + 欠落検知
    """

    def __init__(self):
        self._packet_counter = 0

    def route(
        self,
        stage_e_result: Dict[str, Any],
        stage_f_payload: Dict[str, Any],
        post_body: Optional[Dict[str, Any]] = None
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        【Ver 5.9】物理的振分のみ

        Args:
            stage_e_result: Stage E の出力（デジタル文字）
            stage_f_payload: Stage F の出力（F7論理 + F8物理）
            post_body: 投稿本文

        Returns:
            (g1_input, g2_input)
        """
        logger.info("[G-Gate] 【Ver 5.9】純粋物流センター起動")
        logger.info("  ├─ 許可: パッキング・振分・直送")
        logger.info("  ├─ 追加: E正確テキスト + F8座標 → G1/H1へ全量直送")
        logger.info("  └─ 禁止: 洗い替え・解釈・判断（G1/G2の仕事）")

        # ============================================
        # Step 1: 三位一体の素材を収集（解釈なし）
        # ============================================

        # E: デジタル文字（全量をそのまま渡す）
        e_content = stage_e_result.get('content', '')
        logger.info(f"[G-Gate] E素材: {len(e_content)}文字（G1/G2へ全量直送）")

        # F7: 論理データ（表構造 + tagged_texts）
        f7_tables = self._extract_f7_tables(stage_f_payload)
        logger.info(f"[G-Gate] F7素材: {len(f7_tables)}表")

        # F8: 物理座標（Surya BBox）
        f8_anchors = self._extract_f8_anchors(stage_f_payload)
        logger.info(f"[G-Gate] F8素材: {len(f8_anchors)}アンカー")

        # テキストブロック
        f_text_blocks = self._extract_text_blocks(stage_f_payload)
        logger.info(f"[G-Gate] テキスト素材: {len(f_text_blocks)}ブロック")

        # ============================================
        # Step 2: パッキング（詰めるだけ、解釈しない）
        # ============================================
        table_packets = self._pack_table_packets(f7_tables, f8_anchors)
        text_packets = self._pack_text_packets(f_text_blocks)

        logger.info(f"[G-Gate] パッキング完了: 表{len(table_packets)}個, テキスト{len(text_packets)}個")

        # ============================================
        # Step 3: 直送（振分のみ、判断しない）
        # ============================================
        g1_input = self._build_g1_package(table_packets, e_content, f8_anchors)
        g2_input = self._build_g2_package(text_packets, post_body, e_content)

        logger.info(f"[G-Gate] 振分完了:")
        logger.info(f"  ├─ → G1: {len(g1_input.get('tables', []))}表 + E全文 + F8座標{len(f8_anchors)}個")
        logger.info(f"  └─ → G2: {len(g2_input.get('segments', []))}セグメント + E全文")

        return g1_input, g2_input

    # ============================================
    # 素材収集（解釈なし）
    # ============================================

    def _extract_f7_tables(self, stage_f_payload: Dict[str, Any]) -> List[Dict]:
        """F7の論理データを抽出（そのまま）"""
        tables = []

        for tbl in stage_f_payload.get('tables', []):
            tables.append({
                'block_id': tbl.get('block_id', ''),
                'page': tbl.get('page', tbl.get('chunk_start_page', 0)),
                'title': tbl.get('table_title', ''),
                'table_type': tbl.get('table_type', 'visual_table'),
                'headers': tbl.get('headers', tbl.get('columns', [])),
                'rows': tbl.get('rows', []),
                'x_headers': tbl.get('x_headers', []),
                'y_headers': tbl.get('y_headers', []),
                'tagged_texts': tbl.get('tagged_texts', []),
                'row_count': tbl.get('row_count', 0),
                'col_count': tbl.get('col_count', 0),
            })

        for anchor in stage_f_payload.get('anchors', []):
            if anchor.get('type') == 'table':
                anchor_id = anchor.get('anchor_id', '')
                if not any(t.get('block_id') == anchor_id for t in tables):
                    tables.append({
                        'block_id': anchor_id,
                        'page': anchor.get('page', 0),
                        'title': anchor.get('title', ''),
                        'table_type': anchor.get('table_type', 'visual_table'),
                        'headers': anchor.get('columns', []),
                        'rows': anchor.get('rows', []),
                        'x_headers': anchor.get('x_headers', []),
                        'y_headers': anchor.get('y_headers', []),
                        'tagged_texts': anchor.get('tagged_texts', []),
                        'row_count': anchor.get('row_count', 0),
                        'col_count': anchor.get('col_count', 0),
                    })

        return tables

    def _extract_f8_anchors(self, stage_f_payload: Dict[str, Any]) -> List[Dict]:
        """F8の物理座標を抽出（そのまま）"""
        anchors = []

        text_source = stage_f_payload.get('text_source', {})
        for block in text_source.get('blocks', []):
            if block.get('bbox') or block.get('text'):
                anchors.append({
                    'block_id': block.get('block_id', ''),
                    'text': block.get('text', ''),
                    'bbox': block.get('bbox'),
                    'page': block.get('page', 0),
                })

        for anchor in stage_f_payload.get('anchors', []):
            anchors.append({
                'anchor_id': anchor.get('anchor_id', ''),
                'text': anchor.get('content', ''),
                'bbox': anchor.get('bbox'),
                'page': anchor.get('page', 0),
                'type': anchor.get('type', 'unknown')
            })

        return anchors

    def _extract_text_blocks(self, stage_f_payload: Dict[str, Any]) -> List[Dict]:
        """テキストブロックを抽出（そのまま）"""
        blocks = []

        text_source = stage_f_payload.get('text_source', {})
        for block in text_source.get('blocks', []):
            blocks.append({
                'block_id': block.get('block_id', ''),
                'text': block.get('text', ''),
                'page': block.get('page', 0),
                'bbox': block.get('bbox'),
                'type': block.get('block_type', 'paragraph')
            })

        for anchor in stage_f_payload.get('anchors', []):
            if anchor.get('type') == 'text':
                blocks.append({
                    'block_id': anchor.get('anchor_id', ''),
                    'text': anchor.get('content', ''),
                    'page': anchor.get('page', 0),
                    'bbox': anchor.get('bbox'),
                    'type': 'text'
                })

        return blocks

    # ============================================
    # パッキング（詰めるだけ）
    # ============================================

    def _pack_table_packets(
        self,
        f7_tables: List[Dict],
        f8_anchors: List[Dict]
    ) -> List[Dict]:
        """表パケットを詰める（解釈なし）"""
        packets = []

        for tbl in f7_tables:
            self._packet_counter += 1
            packet_id = f"TBL_{self._packet_counter:03d}"

            # 同一ページのF8座標を紐付け
            page = tbl.get('page', 0)
            related_anchors = [a for a in f8_anchors if a.get('page', -1) == page]

            packet = {
                'packet_id': packet_id,
                'page': page,
                # F7論理データ（そのまま）
                'title': tbl.get('title', ''),
                'table_type': tbl.get('table_type', ''),
                'headers': tbl.get('headers', []),
                'rows': tbl.get('rows', []),
                'x_headers': tbl.get('x_headers', []),
                'y_headers': tbl.get('y_headers', []),
                'tagged_texts': tbl.get('tagged_texts', []),
                'row_count': tbl.get('row_count', 0),
                'col_count': tbl.get('col_count', 0),
                # F8物理座標（そのまま）
                'f8_anchors': related_anchors,
            }
            packets.append(packet)

        return packets

    def _pack_text_packets(self, f_text_blocks: List[Dict]) -> List[Dict]:
        """テキストパケットを詰める（解釈なし）"""
        packets = []

        for block in f_text_blocks:
            self._packet_counter += 1
            packet_id = f"TXT_{self._packet_counter:03d}"

            packet = {
                'packet_id': packet_id,
                'page': block.get('page', 0),
                'text': block.get('text', ''),
                'bbox': block.get('bbox'),
                'block_type': block.get('type', 'paragraph'),
            }
            packets.append(packet)

        return packets

    # ============================================
    # 直送（振分のみ）
    # ============================================

    def _build_g1_package(
        self,
        table_packets: List[Dict],
        e_content: str,
        f8_anchors: List[Dict]
    ) -> Dict[str, Any]:
        """G1向けパッケージ（表 + E全文 + F8座標）"""
        tables = []

        for packet in table_packets:
            table = {
                'anchor_id': packet.get('packet_id', ''),
                'page': packet.get('page', 0),
                'title': packet.get('title', ''),
                'table_type': packet.get('table_type', 'visual_table'),
                'headers': packet.get('headers', []),
                'rows': packet.get('rows', []),
                'row_count': packet.get('row_count', 0),
                'col_count': packet.get('col_count', 0),
                'x_headers': packet.get('x_headers', []),
                'y_headers': packet.get('y_headers', []),
                'tagged_texts': packet.get('tagged_texts', []),
                'f8_anchors': packet.get('f8_anchors', []),
                'is_heavy': packet.get('row_count', 0) >= 20,
            }
            tables.append(table)

        tables.sort(key=lambda x: (x.get('page', 0), x.get('anchor_id', '')))

        return {
            'tables': tables,
            'e_content': e_content,  # 【Ver 5.9】G1が座標ベースE置換に使う
            'f8_anchors': f8_anchors,  # 【Ver 5.9】G1/H1が全軸座標ロックに使う
            'table_count': len(tables),
            'version': '5.9_full_coord_lock'
        }

    def _build_g2_package(
        self,
        text_packets: List[Dict],
        post_body: Optional[Dict[str, Any]],
        e_content: str
    ) -> Dict[str, Any]:
        """G2向けパッケージ（テキスト + E全文）"""
        segments = []
        ref_counter = 0

        if post_body and post_body.get('text'):
            ref_counter += 1
            segments.append({
                'ref_id': f'REF_{ref_counter:03d}',
                'page': 0,
                'text': post_body['text'],
                'segment_type': 'post_body',
            })

        for packet in text_packets:
            ref_counter += 1
            segments.append({
                'ref_id': f'REF_{ref_counter:03d}',
                'page': packet.get('page', 0),
                'text': packet.get('text', ''),
                'segment_type': packet.get('block_type', 'paragraph'),
                'bbox': packet.get('bbox'),
            })

        segments.sort(key=lambda x: (x.get('page', 0), x.get('ref_id', '')))

        return {
            'segments': segments,
            'e_content': e_content,  # 【Ver 5.9】G2がAI洗い替えに使う
            'segment_count': len(segments),
            'version': '5.9_full_coord_lock'
        }
```

## shared/pipeline/stage_g_refiner.py

```python
"""
Stage G: Integration Refiner (統合精錬) - v2.0 + Ver 6.4 異常検知対応

【設計 2026-01-28】G-Gate + G1 + G2 による物理的分離
【設計 2026-01-31】Ver 6.4: F-10 anomaly_report による外科手術

役割: Stage E（物理抽出）と Stage F（独立読解）の結果を統合し、
      G1（表専用）と G2（テキスト専用）で整理してから H1/H2 へ渡す

============================================
新アーキテクチャ (Ver 6.4):

[Stage E] + [Stage F (F-10)]
         ↓
    [G-Gate] ←─── 仕分けゲート + anomaly_report 振り分け
         ↓
   ┌─────┴─────┐
   ↓           ↓
 [G1]        [G2]
 表整理      テキスト整理
 +異常再読   +異常再読
   ↓           ↓
 [H1]        [H2]

【Ver 6.4】F-10 からの入力:
  - scrubbed_data: 洗い替え成功データ（信頼度100%）
  - anomaly_report: 異常箇所リスト（外科手術対象）
    - no_bbox: 座標情報がない
    - invalid_bbox: bbox形式が不正
    - no_evidence_in_bbox: 枠内にE文字がない
    - no_physical_evidence: 物理証拠自体がない（画像PDF等）

出力:
  - g1_result: 表データ（検証済み）→ H1 へ
  - g2_result: テキストセグメント（重複排除済み）→ H2 へ
  - unified_text: 統合テキスト（後方互換）
  - source_inventory: REF_ID付きセグメント（後方互換）
  - table_inventory: TBL_ID付き表（後方互換）
  - audit_log: 監査ログ（洗い替え・異常解決の履歴）
============================================
"""
import json
from typing import Dict, Any, List, Optional, Tuple
from loguru import logger

from shared.ai.llm_client.llm_client import LLMClient
from .constants import STAGE_H_INPUT_SCHEMA_VERSION, G_MODEL
from .stage_g_gate import StageGGate
from .stage_g1_table_refiner import StageG1TableRefiner
from .stage_g2_text_refiner import StageG2TextRefiner


class StageGRefiner:
    """Stage G: 統合精錬（G-Gate + G1 + G2）"""

    def __init__(self, llm_client: LLMClient):
        """
        Args:
            llm_client: LLMクライアント
        """
        self.llm = llm_client
        self._g_usage: Dict[str, Any] = {}

        # 新しいサブモジュールを初期化（LLMクライアントを渡す）
        self.gate = StageGGate()
        self.g1 = StageG1TableRefiner(llm_client=llm_client)
        self.g2 = StageG2TextRefiner(llm_client=llm_client)

    def process(
        self,
        stage_e_result: Optional[Dict[str, Any]] = None,
        stage_f_payload: Optional[Dict[str, Any]] = None,
        post_body: Optional[Dict[str, Any]] = None,
        model: str = "gemini-2.0-flash-lite",
        workspace: str = "default"
    ) -> Dict[str, Any]:
        """
        Stage E と Stage F の結果を統合（v2.0: G-Gate + G1 + G2）

        Args:
            stage_e_result: Stage E の出力
            stage_f_payload: Stage F の出力
            post_body: 投稿本文
            model: 使用するモデル（未使用、後方互換のため残す）
            workspace: ワークスペース

        Returns:
            {
                'unified_text': str,
                'source_inventory': List[Dict],
                'table_inventory': List[Dict],
                'cross_validation': Dict,
                'ref_count': int,
                'warnings': List[str],
                'g1_result': Dict,  # H1 用
                'g2_result': Dict,  # H2 用
                'processing_mode': str
            }
        """
        logger.info(f"[Stage G] 統合精錬開始（v2.0: G-Gate + G1 + G2）")

        # 入力のデフォルト値
        stage_e_result = stage_e_result or {}
        stage_f_payload = stage_f_payload or {}

        # 入力データ取得
        e_content = stage_e_result.get('content', '')
        e_method = stage_e_result.get('method', 'unknown')
        f_processing_mode = stage_f_payload.get('processing_mode', 'unknown')

        logger.info(f"[Stage G] 入力:")
        logger.info(f"  ├─ Stage E: {len(e_content)}文字 (method={e_method})")
        logger.info(f"  ├─ Stage F: tables={len(stage_f_payload.get('tables', []))}, anchors={len(stage_f_payload.get('anchors', []))}")
        logger.info(f"  └─ F processing_mode: {f_processing_mode}")

        # ============================================
        # 特殊ケースの処理（従来ロジックを維持）
        # ============================================

        # 添付なし（E, F 両方スキップ）の場合
        if not e_content and f_processing_mode == 'skipped':
            logger.info("[Stage G] 添付なし → post_body のみで処理")
            return self._process_post_body_only(post_body)

        # Stage F がスキップされた場合（ドキュメントのみ）
        if f_processing_mode == 'skipped':
            logger.info("[Stage G] Stage F スキップ → Stage E のみで処理")
            return self._process_e_only(stage_e_result, post_body)

        # 音声/動画の場合（Transcription のみ）
        if f_processing_mode == 'transcription_only':
            logger.info("[Stage G] Transcription モード → F-7 結果を使用")
            return self._process_transcription(stage_f_payload, post_body)

        # ============================================
        # 【Ver 6.2】G-Gate 廃止 → F から G1/G2 へ直結
        # ============================================
        logger.info("[Stage G] 【Ver 6.2】G-Gate廃止 → F→G1/G2 直結モード")

        try:
            # ============================================
            # Step 1: F の出力から直接 G1/G2 入力を構築
            # ============================================
            g1_input, g2_input = self._build_direct_inputs(
                stage_e_result=stage_e_result,
                stage_f_payload=stage_f_payload,
                post_body=post_body
            )

            # Step 2: G1 で表を整理
            logger.info(f"[Stage G] G1処理開始: {len(g1_input.get('tables', []))}表")
            g1_result = self.g1.process(g1_input)

            # Step 3: G2 でテキストを整理
            logger.info(f"[Stage G] G2処理開始: {len(g2_input.get('text_blocks', []))}ブロック")
            g2_result = self.g2.process(g2_input)

            # Step 4: 後方互換のための出力を構築
            return self._build_unified_result(g1_result, g2_result, post_body)

        except Exception as e:
            logger.warning(f"[Stage G] 【Ver 6.2】G1/G2 処理失敗、フォールバック: {e}")
            # フォールバック: 従来のルールベース処理
            return self._legacy_rule_based_merge(stage_e_result, stage_f_payload, post_body)

    def _build_direct_inputs(
        self,
        stage_e_result: Dict[str, Any],
        stage_f_payload: Dict[str, Any],
        post_body: Optional[Dict[str, Any]]
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        【Ver 6.4】F-10 の出力を type で物理分離して G1/G2 に直結

        物理分離ルール:
        - type: "cell" → G1（表専用レーン）
        - type: "text", "untagged", "title", "note" → G2（地文専用レーン）

        【Ver 6.4】anomaly_report の振り分け:
        - セル異常（type=cell）→ G1
        - テキスト異常（type=text等）→ G2

        Args:
            stage_e_result: Stage E の出力
            stage_f_payload: Stage F の出力（F-10 洗い替え済み）
            post_body: 投稿本文

        Returns:
            (g1_input, g2_input)
        """
        # E: デジタル文字（洗い替え済み）
        e_content = stage_e_result.get('content', '')

        # F-10 の出力を取得
        path_a = stage_f_payload.get('path_a_result', {})
        tagged_texts = path_a.get('tagged_texts', [])
        x_headers = path_a.get('x_headers', [])
        y_headers = path_a.get('y_headers', [])

        # 【Ver 6.4】anomaly_report を取得（F-10 の外科手術指示書）
        anomaly_report = stage_f_payload.get('anomaly_report', [])

        # ============================================
        # 【Ver 6.2 核心】type による物理分離
        # ============================================
        cell_items = []  # → G1（表専用）
        text_items = []  # → G2（地文専用）

        for item in tagged_texts:
            item_type = item.get('type', 'untagged').lower()

            if item_type == 'cell':
                # 表データ → G1
                cell_items.append(item)
            else:
                # 地文（text, untagged, title, note）→ G2
                text_items.append(item)

        # ============================================
        # 【Ver 6.4】anomaly_report を G1/G2 に振り分け
        # ============================================
        cell_anomalies = []   # → G1（表専用）
        text_anomalies = []   # → G2（地文専用）

        for anomaly in anomaly_report:
            # x_header/y_header があれば表セル異常
            if anomaly.get('x_header') or anomaly.get('y_header'):
                cell_anomalies.append(anomaly)
            else:
                text_anomalies.append(anomaly)

        logger.info(f"[Stage G] 【Ver 6.4】物理分離完了:")
        logger.info(f"  ├─ cell(表) → G1: {len(cell_items)}件 + 異常{len(cell_anomalies)}件")
        logger.info(f"  └─ text(地文) → G2: {len(text_items)}件 + 異常{len(text_anomalies)}件")

        # ============================================
        # G1 入力: 表専用レーン
        # ============================================
        tables = []
        if cell_items:
            tables.append({
                'anchor_id': 'TBL_001',
                'tagged_texts': cell_items,
                'x_headers': x_headers,
                'y_headers': y_headers,
                'e_content': e_content,
                '_v62_format': True,
                '_cell_count': len(cell_items)
            })

        # F8 の表データがあれば追加
        for tbl in path_a.get('tables', []):
            if tbl.get('anchor_id') not in [t.get('anchor_id') for t in tables]:
                tbl['e_content'] = e_content
                tables.append(tbl)

        g1_input = {
            'tables': tables,
            'e_content': e_content,
            'workspace': stage_f_payload.get('workspace', 'unknown'),
            '_v62_cell_only': True,
            # 【Ver 6.4】異常セルリスト（外科手術対象）
            'anomaly_report': cell_anomalies,
            'file_path': stage_f_payload.get('file_path'),  # 画像再読用
        }

        # ============================================
        # G2 入力: 地文専用レーン
        # ============================================
        # text_items を読み順でソート
        text_items_sorted = sorted(text_items, key=lambda x: (
            x.get('page', 0),
            x.get('reading_order', 0)
        ))

        # テーブルのアンカー位置を計算（地文に埋め込む参照用）
        table_anchors = []
        for i, tbl in enumerate(tables):
            table_anchors.append({
                'anchor_id': tbl.get('anchor_id', f'TBL_{i+1:03d}'),
                'position': i,  # 後で物理位置に置き換え
                'x_headers': tbl.get('x_headers', []),
                'y_headers': tbl.get('y_headers', [])
            })

        g2_input = {
            'text_blocks': text_items_sorted,
            'table_anchors': table_anchors,
            'post_body': post_body,
            'e_content': e_content,
            '_v62_text_only': True,
            # 【Ver 6.4】異常テキストリスト（外科手術対象）
            'anomaly_report': text_anomalies,
            'file_path': stage_f_payload.get('file_path'),  # 画像再読用
        }

        logger.info(f"[Stage G] 【Ver 6.4】直結入力構築完了:")
        logger.info(f"  ├─ G1: {len(tables)}表 ({len(cell_items)}セル) + 異常{len(cell_anomalies)}件")
        logger.info(f"  └─ G2: {len(text_items_sorted)}テキスト + {len(table_anchors)}アンカー + 異常{len(text_anomalies)}件")

        return g1_input, g2_input

    def _build_unified_result(
        self,
        g1_result: Dict[str, Any],
        g2_result: Dict[str, Any],
        post_body: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        G1 と G2 の結果を統合し、後方互換の出力形式を構築

        Args:
            g1_result: G1（表整理）の出力
            g2_result: G2（テキスト整理）の出力
            post_body: 投稿本文

        Returns:
            後方互換の Stage G 出力
        """
        # source_inventory: G2 のセグメントから構築
        source_inventory = []
        for seg in g2_result.get('segments', []):
            # table_marker はスキップ（table_inventory に含まれる）
            if seg.get('segment_type') == 'table_marker':
                continue

            source_inventory.append({
                'ref_id': seg.get('ref_id', ''),
                'text': seg.get('text', ''),
                'type': seg.get('segment_type', 'paragraph'),
                'source': seg.get('source', 'unknown'),
                'page': seg.get('page', 0),
                'confidence': 'high' if seg.get('source') == 'post_body' else 'medium'
            })

        # table_inventory: G1 の表から構築
        table_inventory = []
        for tbl in g1_result.get('tables', []):
            table_inventory.append({
                'ref_id': tbl.get('anchor_id', ''),
                'table_title': tbl.get('title', ''),
                'table_type': tbl.get('table_type', 'visual_table'),
                'headers': tbl.get('headers', []),
                'rows': tbl.get('rows', []),
                'row_count': tbl.get('row_count', 0),
                'col_count': tbl.get('col_count', 0),
                'page': tbl.get('page', 0),
                'source': tbl.get('source', 'unknown'),
                'is_heavy': tbl.get('is_heavy', False),
                'is_valid': tbl.get('is_valid', True)
            })

        # unified_text: G2 の出力を使用
        unified_text = g2_result.get('unified_text', '')

        # cross_validation: 統計情報を構築
        g1_stats = g1_result.get('statistics', {})
        g2_stats = g2_result.get('dedup_stats', {})

        cross_validation = {
            'mode': 'g_gate_v2',
            'table_count': g1_stats.get('total_tables', 0),
            'valid_tables': g1_stats.get('valid_tables', 0),
            'total_table_rows': g1_stats.get('total_rows', 0),
            'text_segments': g2_stats.get('total_output', 0),
            'duplicates_removed': g2_stats.get('duplicates_removed', 0),
            'dedup_rate': g2_stats.get('dedup_rate', '0%')
        }

        # 警告を収集
        warnings = []
        for val in g1_result.get('validation_results', []):
            for warn in val.get('warnings', []):
                warnings.append(f"G1_{val.get('anchor_id', '')}: {warn}")
            for err in val.get('errors', []):
                warnings.append(f"G1_ERROR_{val.get('anchor_id', '')}: {err}")

        # トークン使用量を集計
        g1_tokens = g1_result.get('token_usage', {})
        g2_tokens = g2_result.get('token_usage', {})
        total_tokens = {
            'G1': g1_tokens,
            'G2': g2_tokens,
            'total_prompt': g1_tokens.get('prompt_tokens', 0) + g2_tokens.get('prompt_tokens', 0),
            'total_completion': g1_tokens.get('completion_tokens', 0) + g2_tokens.get('completion_tokens', 0),
            'total': g1_tokens.get('total_tokens', 0) + g2_tokens.get('total_tokens', 0)
        }

        # ============================================
        # 【Ver 6.4】監査ログの構築
        # ============================================
        g1_audit = g1_result.get('audit_log', {})
        g2_audit = g2_result.get('audit_log', {})

        audit_log = {
            'version': '6.4',
            'g1_scrubbed_count': g1_audit.get('scrubbed_count', 0),
            'g1_anomaly_resolved': g1_audit.get('anomaly_resolved', 0),
            'g1_anomaly_failed': g1_audit.get('anomaly_failed', 0),
            'g2_scrubbed_count': g2_audit.get('scrubbed_count', 0),
            'g2_anomaly_resolved': g2_audit.get('anomaly_resolved', 0),
            'g2_anomaly_failed': g2_audit.get('anomaly_failed', 0),
            'total_scrubbed': g1_audit.get('scrubbed_count', 0) + g2_audit.get('scrubbed_count', 0),
            'total_anomalies_resolved': g1_audit.get('anomaly_resolved', 0) + g2_audit.get('anomaly_resolved', 0),
            'resolution_details': g1_audit.get('resolution_details', []) + g2_audit.get('resolution_details', [])
        }

        logger.info(f"[Stage G] 統合完了:")
        logger.info(f"  ├─ source_inventory: {len(source_inventory)}件")
        logger.info(f"  ├─ table_inventory: {len(table_inventory)}件")
        logger.info(f"  ├─ unified_text: {len(unified_text)}文字")
        logger.info(f"  ├─ warnings: {len(warnings)}件")
        logger.info(f"  ├─ 【Ver 6.4】洗替: {audit_log['total_scrubbed']}件, 異常解決: {audit_log['total_anomalies_resolved']}件")
        logger.info(f"  └─ トークン合計: {total_tokens['total']} (G1={g1_tokens.get('total_tokens', 0)}, G2={g2_tokens.get('total_tokens', 0)})")

        return {
            'unified_text': unified_text,
            'source_inventory': source_inventory,
            'table_inventory': table_inventory,
            'cross_validation': cross_validation,
            'ref_count': len(source_inventory) + len(table_inventory),
            'warnings': warnings,
            'processing_mode': 'g_gate_v2',
            'post_body': post_body or {},
            # 新しい出力（H1/H2 で直接使用）
            'g1_result': g1_result,
            'g2_result': g2_result,
            'token_usage': total_tokens,
            # 【Ver 6.4】監査ログ
            'audit_log': audit_log
        }

    # ============================================
    # H1/H2 ルーティング（新設計対応版）
    # ============================================
    def route_anchors_to_stages(
        self,
        stage_g_result: Dict[str, Any],
        anchors: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        アンカー単位でH1/H2への振り分けを行う（v2.0対応）

        新設計では G1/G2 の結果を直接使用可能

        Args:
            stage_g_result: Stage G の処理結果
            anchors: Stage F からのアンカー配列（レガシー用）

        Returns:
            {
                'h1_payload': {...},
                'h2_payload': {...},
                'anchor_map': {...}
            }
        """
        logger.info("[Stage G] H1/H2 ルーティング開始")

        # 新設計: g1_result と g2_result が存在する場合
        g1_result = stage_g_result.get('g1_result')
        g2_result = stage_g_result.get('g2_result')

        if g1_result and g2_result:
            return self._route_from_g1_g2(g1_result, g2_result, stage_g_result)

        # レガシー: 従来のルーティングロジック
        return self._legacy_route_anchors(stage_g_result, anchors)

    def _route_from_g1_g2(
        self,
        g1_result: Dict[str, Any],
        g2_result: Dict[str, Any],
        stage_g_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        G1/G2 の結果から H1/H2 ペイロードを構築

        Args:
            g1_result: G1（表整理）の出力
            g2_result: G2（テキスト整理）の出力
            stage_g_result: Stage G 全体の出力

        Returns:
            H1/H2 ルーティング結果
        """
        tables = g1_result.get('tables', [])
        segments = g2_result.get('segments', [])

        # H1 ペイロード: 全ての表（重い表を優先処理）
        heavy_tables = [t for t in tables if t.get('is_heavy', False)]
        light_tables = [t for t in tables if not t.get('is_heavy', False)]

        h1_payload = {
            'heavy_tables': heavy_tables,
            'light_tables': light_tables,
            'table_anchors': [t.get('anchor_id', '') for t in heavy_tables],
            'table_page_context': g1_result.get('table_page_context', {}),
            'validation_results': g1_result.get('validation_results', [])
        }

        # H2 ペイロード: テキストセグメント + 軽い表
        text_anchors = [s for s in segments if s.get('segment_type') != 'table_marker']

        h2_payload = {
            'text_anchors': text_anchors,
            'light_tables': light_tables,
            'reduced_text': g2_result.get('unified_text', ''),
            'dedup_stats': g2_result.get('dedup_stats', {}),
            'post_body': g2_result.get('post_body', {})
        }

        # アンカーマップ
        anchor_map = {}
        for t in heavy_tables:
            anchor_map[t.get('anchor_id', '')] = 'h1'
        for t in light_tables:
            anchor_map[t.get('anchor_id', '')] = 'h2'
        for s in text_anchors:
            anchor_map[s.get('ref_id', '')] = 'h2'

        logger.info(f"[Stage G] ルーティング完了（v2.0）:")
        logger.info(f"  ├─ H1: {len(heavy_tables)}重い表 + {len(light_tables)}軽い表")
        logger.info(f"  ├─ H2: {len(text_anchors)}テキスト")
        logger.info(f"  └─ reduced_text: {len(h2_payload['reduced_text'])}文字")

        return {
            'h1_payload': h1_payload,
            'h2_payload': h2_payload,
            'anchor_map': anchor_map
        }

    # ============================================
    # 特殊ケース処理（従来ロジック）
    # ============================================
    def _process_post_body_only(self, post_body: Optional[Dict]) -> Dict[str, Any]:
        """投稿本文のみの処理（添付なし）"""
        text = post_body.get('text', '') if post_body else ''

        source_inventory = []
        if text:
            source_inventory.append({
                'ref_id': 'REF_001',
                'text': text,
                'type': 'post_body',
                'source': 'post_body',
                'confidence': 'high'
            })

        # 空の G1/G2 結果を作成
        g1_result = {
            'tables': [],
            'validation_results': [],
            'table_page_context': {},
            'statistics': {'total_tables': 0, 'valid_tables': 0, 'total_rows': 0}
        }
        g2_result = {
            'segments': source_inventory,
            'unified_text': text,
            'dedup_stats': {'total_input': 1, 'total_output': 1, 'duplicates_removed': 0},
            'post_body': post_body or {}
        }

        return {
            'unified_text': text,
            'source_inventory': source_inventory,
            'table_inventory': [],
            'cross_validation': {'mode': 'post_body_only'},
            'ref_count': len(source_inventory),
            'warnings': [],
            'processing_mode': 'post_body_only',
            'post_body': post_body or {},
            'g1_result': g1_result,
            'g2_result': g2_result
        }

    def _process_e_only(
        self,
        stage_e_result: Dict[str, Any],
        post_body: Optional[Dict]
    ) -> Dict[str, Any]:
        """Stage E のみの処理（ドキュメント、Stage F スキップ）"""
        e_content = stage_e_result.get('content', '')

        source_inventory = []
        ref_index = 1

        # post_body を先頭に
        if post_body and post_body.get('text'):
            source_inventory.append({
                'ref_id': f'REF_{ref_index:03d}',
                'text': post_body['text'],
                'type': 'post_body',
                'source': 'post_body',
                'confidence': 'high'
            })
            ref_index += 1

        # Stage E テキストを追加
        if e_content:
            paragraphs = self._split_paragraphs(e_content)
            for para in paragraphs:
                if para.strip():
                    source_inventory.append({
                        'ref_id': f'REF_{ref_index:03d}',
                        'text': para.strip(),
                        'type': 'paragraph',
                        'source': 'stage_e',
                        'confidence': 'high'
                    })
                    ref_index += 1

        # unified_text 構築
        unified_parts = []
        if post_body and post_body.get('text'):
            unified_parts.append(post_body['text'])
        if e_content:
            unified_parts.append(e_content)
        unified_text = '\n\n'.join(unified_parts)

        # G1/G2 結果
        g1_result = {
            'tables': [],
            'validation_results': [],
            'table_page_context': {},
            'statistics': {'total_tables': 0, 'valid_tables': 0, 'total_rows': 0}
        }
        g2_result = {
            'segments': source_inventory,
            'unified_text': unified_text,
            'dedup_stats': {'total_input': len(source_inventory), 'total_output': len(source_inventory), 'duplicates_removed': 0},
            'post_body': post_body or {}
        }

        return {
            'unified_text': unified_text,
            'source_inventory': source_inventory,
            'table_inventory': [],
            'cross_validation': {'mode': 'e_only'},
            'ref_count': len(source_inventory),
            'warnings': [],
            'processing_mode': 'e_only',
            'post_body': post_body or {},
            'g1_result': g1_result,
            'g2_result': g2_result
        }

    def _process_transcription(
        self,
        stage_f_payload: Dict[str, Any],
        post_body: Optional[Dict]
    ) -> Dict[str, Any]:
        """音声/動画の Transcription 処理"""
        f_path_a = stage_f_payload.get('path_a_result', {})
        transcript = f_path_a.get('transcript', '')
        visual_log = f_path_a.get('visual_log', '')

        source_inventory = []
        ref_index = 1

        # post_body を先頭に
        if post_body and post_body.get('text'):
            source_inventory.append({
                'ref_id': f'REF_{ref_index:03d}',
                'text': post_body['text'],
                'type': 'post_body',
                'source': 'post_body',
                'confidence': 'high'
            })
            ref_index += 1

        # Transcript を追加
        if transcript:
            source_inventory.append({
                'ref_id': f'REF_{ref_index:03d}',
                'text': transcript,
                'type': 'transcript',
                'source': 'stage_f.path_a',
                'confidence': 'high'
            })
            ref_index += 1

        # Visual log を追加（動画の場合）
        if visual_log:
            source_inventory.append({
                'ref_id': f'REF_{ref_index:03d}',
                'text': visual_log,
                'type': 'visual_log',
                'source': 'stage_f.path_a',
                'confidence': 'high'
            })
            ref_index += 1

        # unified_text 構築
        unified_parts = []
        if post_body and post_body.get('text'):
            unified_parts.append(f"【投稿本文】\n{post_body['text']}")
        if transcript:
            unified_parts.append(f"【書き起こし】\n{transcript}")
        if visual_log:
            unified_parts.append(f"【映像ログ】\n{visual_log}")
        unified_text = '\n\n---\n\n'.join(unified_parts)

        # G1/G2 結果
        g1_result = {
            'tables': [],
            'validation_results': [],
            'table_page_context': {},
            'statistics': {'total_tables': 0, 'valid_tables': 0, 'total_rows': 0}
        }
        g2_result = {
            'segments': source_inventory,
            'unified_text': unified_text,
            'dedup_stats': {'total_input': len(source_inventory), 'total_output': len(source_inventory), 'duplicates_removed': 0},
            'post_body': post_body or {}
        }

        return {
            'unified_text': unified_text,
            'source_inventory': source_inventory,
            'table_inventory': [],
            'cross_validation': {'mode': 'transcription'},
            'ref_count': len(source_inventory),
            'warnings': [],
            'processing_mode': 'transcription',
            'post_body': post_body or {},
            'g1_result': g1_result,
            'g2_result': g2_result
        }

    # ============================================
    # レガシー処理（フォールバック用）
    # ============================================
    def _legacy_rule_based_merge(
        self,
        stage_e_result: Dict[str, Any],
        stage_f_payload: Dict[str, Any],
        post_body: Optional[Dict]
    ) -> Dict[str, Any]:
        """レガシーのルールベース統合（フォールバック）"""
        logger.warning("[Stage G] レガシーモードで処理")

        e_content = stage_e_result.get('content', '')
        f_full_text = stage_f_payload.get('full_text', '')
        f_tables = stage_f_payload.get('tables', [])

        source_inventory = []
        table_inventory = []
        ref_index = 1
        tbl_index = 1

        # post_body
        if post_body and post_body.get('text'):
            source_inventory.append({
                'ref_id': f'REF_{ref_index:03d}',
                'text': post_body['text'],
                'type': 'post_body',
                'source': 'post_body',
                'confidence': 'high'
            })
            ref_index += 1

        # E テキスト
        if e_content:
            paragraphs = self._split_paragraphs(e_content)
            for para in paragraphs:
                if para.strip():
                    source_inventory.append({
                        'ref_id': f'REF_{ref_index:03d}',
                        'text': para.strip(),
                        'type': 'paragraph',
                        'source': 'stage_e',
                        'confidence': 'medium'
                    })
                    ref_index += 1

        # F 表
        for tbl in f_tables:
            table_inventory.append({
                'ref_id': f'TBL_{tbl_index:03d}',
                'table_title': tbl.get('table_title', ''),
                'table_type': tbl.get('table_type', 'visual_table'),
                'headers': tbl.get('headers', tbl.get('columns', [])),
                'rows': tbl.get('rows', []),
                'row_count': len(tbl.get('rows', [])),
                'col_count': len(tbl.get('headers', tbl.get('columns', []))),
                'source': 'stage_f'
            })
            tbl_index += 1

        # unified_text
        unified_parts = [s['text'] for s in source_inventory]
        unified_text = '\n\n'.join(unified_parts)

        # 空の G1/G2 結果
        g1_result = {
            'tables': table_inventory,
            'validation_results': [],
            'table_page_context': {},
            'statistics': {'total_tables': len(table_inventory), 'valid_tables': len(table_inventory), 'total_rows': sum(t.get('row_count', 0) for t in table_inventory)}
        }
        g2_result = {
            'segments': source_inventory,
            'unified_text': unified_text,
            'dedup_stats': {'total_input': len(source_inventory), 'total_output': len(source_inventory), 'duplicates_removed': 0},
            'post_body': post_body or {}
        }

        return {
            'unified_text': unified_text,
            'source_inventory': source_inventory,
            'table_inventory': table_inventory,
            'cross_validation': {'mode': 'legacy_rule_based'},
            'ref_count': len(source_inventory) + len(table_inventory),
            'warnings': ['Used legacy rule-based merge as fallback'],
            'processing_mode': 'legacy_rule_based',
            'post_body': post_body or {},
            'g1_result': g1_result,
            'g2_result': g2_result
        }

    def _legacy_route_anchors(
        self,
        stage_g_result: Dict[str, Any],
        anchors: Optional[List[Dict]]
    ) -> Dict[str, Any]:
        """レガシーのアンカールーティング"""
        logger.info("[Stage G] レガシールーティング使用")

        h1_payload = {'heavy_tables': [], 'table_anchors': []}
        h2_payload = {'text_anchors': [], 'light_tables': [], 'reduced_text': ''}
        anchor_map = {}

        table_inventory = stage_g_result.get('table_inventory', [])
        source_inventory = stage_g_result.get('source_inventory', [])

        # 表をH1/H2に振り分け
        for tbl in table_inventory:
            ref_id = tbl.get('ref_id', '')
            rows = tbl.get('rows', [])
            headers = tbl.get('headers', [])
            is_heavy = len(rows) >= 20 or len(headers) >= 5

            if is_heavy:
                h1_payload['heavy_tables'].append(tbl)
                h1_payload['table_anchors'].append(ref_id)
                anchor_map[ref_id] = 'h1'
            else:
                h2_payload['light_tables'].append(tbl)
                anchor_map[ref_id] = 'h2'

        # テキストはH2
        for src in source_inventory:
            ref_id = src.get('ref_id', '')
            h2_payload['text_anchors'].append(src)
            anchor_map[ref_id] = 'h2'

        h2_payload['reduced_text'] = stage_g_result.get('unified_text', '')

        return {
            'h1_payload': h1_payload,
            'h2_payload': h2_payload,
            'anchor_map': anchor_map
        }

    def _split_paragraphs(self, text: str) -> List[str]:
        """テキストを段落に分割"""
        import re
        paragraphs = re.split(r'\n\s*\n|\r\n\s*\r\n', text)
        return [p.strip() for p in paragraphs if p.strip()]
```

## shared/pipeline/stage_h1_table.py

```python
"""
Stage H1: Table Specialist (表処理専門)

【設計 2026-01-27】Stage HI分割: H1 + H2

役割: Stage G の table_inventory から定型表・構造化表を処理
      スキーマやテンプレートに基づいて表データを構造化

============================================
入力:
  - table_inventory: REF_ID付き表リスト（Stage G出力）
  - doc_type: ドキュメントタイプ
  - workspace: ワークスペース

出力:
  - processed_tables: 処理済み表データ
  - extracted_metadata: 表から抽出したメタデータ
  - table_text_fragments: H2から削除すべきテキスト断片

特徴:
  - 軽量モデル使用（Flash-Lite）または LLMなしのルールベース処理
  - カラムナ形式を辞書リストに復元
  - H2への入力量削減のため、処理済み表のテキストを返す
============================================
"""
import json
from typing import Dict, Any, List, Optional, Set
from loguru import logger

from .utils.table_parser import recompose_columnar_data, is_columnar_format, extract_table_text_for_removal


class StageH1Table:
    """Stage H1: 表処理専門"""

    # 定型表のスキーマ定義（doc_type別）
    TABLE_SCHEMAS = {
        "school_letter": {
            "weekly_schedule": {
                "required_columns": ["曜日", "時間", "科目"],
                "alt_columns": [["日", "時限", "教科"], ["曜", "時間割", "授業"]],
                "table_type": "schedule"
            },
            "event_list": {
                "required_columns": ["日付", "行事"],
                "alt_columns": [["日", "イベント"], ["月日", "予定"]],
                "table_type": "event"
            },
            "持ち物リスト": {
                "required_columns": ["品目", "数量"],
                "alt_columns": [["持ち物", "個数"], ["もちもの", "かず"]],
                "table_type": "item_list"
            }
        },
        "flyer": {
            "price_list": {
                "required_columns": ["商品", "価格"],
                "alt_columns": [["品名", "金額"], ["メニュー", "値段"]],
                "table_type": "price"
            },
            "schedule": {
                "required_columns": ["日時", "内容"],
                "alt_columns": [["時間", "プログラム"]],
                "table_type": "schedule"
            }
        },
        "default": {
            "generic_table": {
                "required_columns": [],
                "table_type": "generic"
            }
        }
    }

    def __init__(self, llm_client=None):
        """
        Args:
            llm_client: LLMクライアント（オプション、複雑な表処理時に使用）
        """
        self.llm = llm_client

    def process(
        self,
        table_inventory: List[Dict[str, Any]],
        doc_type: str = "default",
        workspace: str = "default",
        unified_text: str = ""
    ) -> Dict[str, Any]:
        """
        表データを処理

        Args:
            table_inventory: Stage G の table_inventory
            doc_type: ドキュメントタイプ
            workspace: ワークスペース
            unified_text: Stage G の unified_text（テキスト断片抽出用）

        Returns:
            {
                'processed_tables': List[Dict],  # 処理済み表
                'extracted_metadata': Dict,       # 表から抽出したメタデータ
                'table_text_fragments': List[str], # H2で削除すべきテキスト
                'removed_table_ids': Set[str],    # H2から除外する表のID
                'statistics': Dict                 # 処理統計
            }
        """
        logger.info(f"[Stage H1] 表処理開始: {len(table_inventory)}表 (doc_type={doc_type})")

        if not table_inventory:
            logger.info("[Stage H1] 表なし、スキップ")
            return {
                'processed_tables': [],
                'extracted_metadata': {},
                'table_text_fragments': [],
                'removed_table_ids': set(),
                'statistics': {'total': 0, 'processed': 0, 'skipped': 0}
            }

        processed_tables = []
        extracted_metadata = {}
        table_text_fragments = []
        removed_table_ids = set()
        unrepairable_tables = []  # G1で修復不能と判定された表
        stats = {'total': len(table_inventory), 'processed': 0, 'skipped': 0, 'unrepairable': 0}

        # doc_type に対応するスキーマを取得
        schemas = self.TABLE_SCHEMAS.get(doc_type, self.TABLE_SCHEMAS['default'])

        for table in table_inventory:
            ref_id = table.get('ref_id', 'UNKNOWN')
            table_title = table.get('table_title', '')
            table_type = table.get('table_type', 'unknown')

            # ============================================
            # G1の「白旗」検知: unrepairable フラグ
            # ============================================
            if table.get('status') == 'unrepairable':
                reason = table.get('unrepairable_reason', '理由不明')
                logger.warning(f"[Stage H1] 修復不能表をスキップ: {ref_id} - {reason}")

                # 修復不能表の情報を記録（レポート用）
                unrepairable_tables.append({
                    'ref_id': ref_id,
                    'table_title': table_title,
                    'reason': reason,
                    'page': table.get('page', 0)
                })
                stats['unrepairable'] += 1
                stats['skipped'] += 1

                # H2への通知用：この表は処理できなかったことを伝える
                removed_table_ids.add(ref_id)
                continue  # 次の表へ（ハルシネーション防止）

            logger.debug(f"[Stage H1] 表処理: {ref_id} - {table_title} ({table_type})")

            # ============================================
            # 【Ver 5.8】物理座標による強制マッピング
            # ============================================
            tagged_texts = table.get('tagged_texts', [])
            f8_anchors = table.get('f8_anchors', [])  # Suryaの物理座標

            if tagged_texts:
                logger.info(f"[Stage H1] Ver5.9形式検出: {ref_id} - tagged_texts={len(tagged_texts)}, f8_anchors={len(f8_anchors)}")
                built_table = self.build_table_from_tagged_texts(
                    tagged_texts=tagged_texts,
                    x_headers=table.get('x_headers'),
                    y_headers=table.get('y_headers'),
                    f8_anchors=f8_anchors,  # 【Ver 5.9】物理座標を渡す
                    e_content=table.get('e_content', '')  # 【Ver 5.9】E正確テキスト
                )
                if built_table and built_table.get('rows'):
                    processed_table = {
                        'ref_id': ref_id,
                        'table_title': table_title,
                        'table_type': 'ver5.9_full_coord_lock',
                        'schema_matched': False,
                        'columns': built_table.get('columns', []),
                        'rows': built_table.get('rows', []),
                        'row_count': len(built_table.get('rows', [])),
                        'source': 'ver5.9_full_coord_lock',
                        'x_headers': built_table.get('x_headers', []),
                        'y_headers': built_table.get('y_headers', []),
                        'y_override_count': built_table.get('y_override_count', 0),
                        'x_override_count': built_table.get('x_override_count', 0),
                        'forced_insert_count': built_table.get('forced_insert_count', 0),
                        'total_coord_overrides': built_table.get('total_coord_overrides', 0)
                    }
                    processed_tables.append(processed_table)
                    removed_table_ids.add(ref_id)
                    stats['processed'] += 1
                    logger.info(f"[Stage H1] Ver5.8テーブル構築完了: {ref_id} ({len(processed_table['rows'])}行)")
                    continue  # Ver 5.8形式で処理完了、次の表へ

            # カラムナ形式を辞書リストに変換
            rows_data = self._normalize_table_rows(table)

            # スキーママッチング
            matched_schema = self._match_schema(table, schemas)

            if matched_schema:
                # 定型表として処理
                logger.info(f"[Stage H1] 定型表検出: {ref_id} → {matched_schema['table_type']}")

                processed_table = {
                    'ref_id': ref_id,
                    'table_title': table_title,
                    'table_type': matched_schema['table_type'],
                    'schema_matched': True,
                    'columns': table.get('columns', []) or table.get('headers', []),
                    'rows': rows_data,
                    'row_count': len(rows_data),
                    'source': table.get('source', 'stage_g')
                }
                processed_tables.append(processed_table)

                # メタデータ抽出（特定のtable_typeに対して）
                meta = self._extract_metadata_from_table(processed_table, matched_schema['table_type'])
                if meta:
                    extracted_metadata.update(meta)

                # H2から削除すべきテキスト断片を収集
                fragments = extract_table_text_for_removal(table)
                table_text_fragments.extend(fragments)

                # この表はH1で処理済みとしてマーク
                removed_table_ids.add(ref_id)
                stats['processed'] += 1

            else:
                # 定型外の表は軽量処理のみ
                logger.debug(f"[Stage H1] 汎用表: {ref_id}")

                processed_table = {
                    'ref_id': ref_id,
                    'table_title': table_title,
                    'table_type': table_type or 'generic',
                    'schema_matched': False,
                    'columns': table.get('columns', []) or table.get('headers', []),
                    'rows': rows_data,
                    'row_count': len(rows_data),
                    'source': table.get('source', 'stage_g')
                }
                processed_tables.append(processed_table)

                # 汎用表も大きければH2から削除対象に
                if len(rows_data) >= 3:
                    fragments = extract_table_text_for_removal(table)
                    table_text_fragments.extend(fragments)
                    removed_table_ids.add(ref_id)
                    stats['processed'] += 1
                else:
                    stats['skipped'] += 1

        logger.info(f"[Stage H1] 完了: processed={stats['processed']}, skipped={stats['skipped']}, unrepairable={stats['unrepairable']}")

        # 修復不能表があれば警告
        if unrepairable_tables:
            logger.warning(f"[Stage H1] 修復不能表: {len(unrepairable_tables)}件 → H2へ通知済み")

        return {
            'processed_tables': processed_tables,
            'extracted_metadata': extracted_metadata,
            'table_text_fragments': table_text_fragments,
            'removed_table_ids': removed_table_ids,
            'unrepairable_tables': unrepairable_tables,  # G1で修復不能と判定された表の一覧
            'statistics': stats
        }

    def _normalize_table_rows(self, table: Dict) -> List[Dict]:
        """
        表の行データを正規化（カラムナ形式→辞書リスト）
        【データ救済版】ヘッダーがなくても全データを保持

        Args:
            table: 表データ

        Returns:
            辞書リスト形式の行データ
        """
        # columns + rows 形式（カラムナ）
        if 'columns' in table and 'rows' in table:
            if is_columnar_format(table):
                return recompose_columnar_data(table)

        # headers + rows 形式
        if 'headers' in table and 'rows' in table:
            headers = table.get('headers', []) or []
            rows = table.get('rows', []) or []
            result = []

            for row_idx, row in enumerate(rows):
                if isinstance(row, list):
                    # 【データ救済】ヘッダーが不足している場合、仮ヘッダーを生成
                    effective_headers = list(headers)  # コピー
                    while len(effective_headers) < len(row):
                        effective_headers.append(f'column_{len(effective_headers) + 1}')

                    # 全データを辞書に格納（ヘッダーがなくても消さない）
                    row_dict = {}
                    for i, value in enumerate(row):
                        if i < len(effective_headers):
                            key = effective_headers[i] or f'column_{i + 1}'
                        else:
                            key = f'column_{i + 1}'
                        # 値が存在すれば必ず保持
                        if value is not None and str(value).strip():
                            row_dict[key] = value

                    # 空でない行のみ追加（ただし1つでも値があれば追加）
                    if row_dict:
                        result.append(row_dict)
                    elif any(v for v in row if v):  # row自体に値があるが辞書化で消えた場合
                        # フォールバック：インデックスベースで保持
                        fallback_dict = {f'value_{i}': v for i, v in enumerate(row) if v}
                        if fallback_dict:
                            result.append(fallback_dict)

                elif isinstance(row, dict):
                    if row:  # 空でない辞書のみ
                        result.append(row)

            return result

        # rows のみ（ヘッダーなし）→ 仮ヘッダーで辞書化
        if 'rows' in table:
            rows = table['rows']
            if not isinstance(rows, list):
                return []

            result = []
            for row in rows:
                if isinstance(row, list):
                    # 仮ヘッダーで辞書化
                    row_dict = {f'column_{i + 1}': v for i, v in enumerate(row) if v is not None and str(v).strip()}
                    if row_dict:
                        result.append(row_dict)
                elif isinstance(row, dict) and row:
                    result.append(row)
            return result

        return []

    def _match_schema(self, table: Dict, schemas: Dict) -> Optional[Dict]:
        """
        表がスキーマにマッチするかチェック

        Args:
            table: 表データ
            schemas: スキーマ定義

        Returns:
            マッチしたスキーマ、またはNone
        """
        columns = table.get('columns', []) or table.get('headers', [])
        if not columns:
            return None

        columns_lower = [str(c).lower() for c in columns]

        for schema_name, schema in schemas.items():
            required = schema.get('required_columns', [])
            alt_sets = schema.get('alt_columns', [])

            # 必須カラムのチェック
            if required:
                required_lower = [c.lower() for c in required]
                if all(any(req in col for col in columns_lower) for req in required_lower):
                    return schema

            # 代替カラムセットのチェック
            for alt_set in alt_sets:
                alt_lower = [c.lower() for c in alt_set]
                if all(any(alt in col for col in columns_lower) for alt in alt_lower):
                    return schema

        return None

    def _extract_metadata_from_table(self, table: Dict, table_type: str) -> Dict[str, Any]:
        """
        表からメタデータを抽出

        Args:
            table: 処理済み表データ
            table_type: 表タイプ

        Returns:
            抽出したメタデータ
        """
        metadata = {}
        rows = table.get('rows', [])

        if table_type == 'schedule':
            # 時間割/スケジュール表
            schedule_items = []
            for row in rows:
                if isinstance(row, dict):
                    schedule_items.append(row)
            if schedule_items:
                metadata['weekly_schedule'] = schedule_items

        elif table_type == 'event':
            # イベントリスト
            events = []
            for row in rows:
                if isinstance(row, dict):
                    events.append(row)
            if events:
                metadata['event_list'] = events

        elif table_type == 'item_list':
            # 持ち物リスト
            items = []
            for row in rows:
                if isinstance(row, dict):
                    items.append(row)
            if items:
                metadata['required_items'] = items

        elif table_type == 'price':
            # 価格表
            prices = []
            for row in rows:
                if isinstance(row, dict):
                    prices.append(row)
            if prices:
                metadata['price_list'] = prices

        return metadata

    def remove_table_text_from_unified(
        self,
        unified_text: str,
        table_text_fragments: List[str]
    ) -> str:
        """
        unified_text から表関連テキストを削除

        H2への入力量を削減するため、H1で処理済みの表の
        テキスト表現を削除

        Args:
            unified_text: Stage G の unified_text
            table_text_fragments: 削除すべきテキスト断片

        Returns:
            軽量化された unified_text
        """
        if not table_text_fragments:
            return unified_text

        result = unified_text

        # 断片を長い順にソート（部分一致を避けるため）
        sorted_fragments = sorted(table_text_fragments, key=len, reverse=True)

        for fragment in sorted_fragments:
            if len(fragment) < 10:
                continue  # 短すぎる断片はスキップ

            # 断片を削除（複数回出現する可能性あり）
            if fragment in result:
                result = result.replace(fragment, '')

        # 連続する空行を整理
        import re
        result = re.sub(r'\n{3,}', '\n\n', result)

        original_len = len(unified_text)
        reduced_len = len(result)
        reduction = original_len - reduced_len

        logger.info(f"[Stage H1] テキスト軽量化: {original_len}→{reduced_len}文字 (-{reduction}文字, -{reduction*100//original_len if original_len > 0 else 0}%)")

        return result.strip()

    # ============================================
    # 【Ver 5.9】全軸座標ロック + 物理逆引き
    # ============================================
    def build_table_from_tagged_texts(
        self,
        tagged_texts: List[Dict[str, Any]],
        x_headers: List[str] = None,
        y_headers: List[str] = None,
        f8_anchors: List[Dict[str, Any]] = None,
        e_content: str = ""
    ) -> Dict[str, Any]:
        """
        【Ver 5.9】全軸座標ロック + 物理逆引き

        AIのタグ（x_header, y_header）を一切信用しない。
        X軸・Y軸ともにSuryaの物理座標（bbox）に基づき、
        数学的に最も近いヘッダーに強制配置。

        さらに、AIが読み飛ばしたテキストを座標から逆引きして強制挿入。

        Args:
            tagged_texts: F-7からのタグ付きテキストリスト
            x_headers: X軸ヘッダーリスト（試験日: 2/3, 2/4...）
            y_headers: Y軸ヘッダーリスト（偏差値: 70, 69...）
            f8_anchors: Suryaの物理座標リスト（Ver 5.9で必須）
            e_content: Stage Eのテキスト（洗い替え用）

        Returns:
            物理座標で正しく配置されたテーブル
        """
        logger.info(f"[Stage H1] 【Ver 5.9】全軸座標ロック開始: {len(tagged_texts)}テキスト")

        # ============================================
        # Step 1: 物理座標マップを構築
        # ============================================
        coord_map = {}  # {text: {x, y, bbox}}
        all_f8_texts = set()  # Suryaが検出した全テキスト

        if f8_anchors:
            for anchor in f8_anchors:
                text = anchor.get('text', '').strip()
                bbox = anchor.get('bbox')
                if text and bbox:
                    # bboxの中心座標を計算
                    if isinstance(bbox, dict):
                        y_center = (bbox.get('y1', 0) + bbox.get('y2', 0)) / 2
                        x_center = (bbox.get('x1', 0) + bbox.get('x2', 0)) / 2
                    elif isinstance(bbox, (list, tuple)) and len(bbox) >= 4:
                        y_center = (bbox[1] + bbox[3]) / 2
                        x_center = (bbox[0] + bbox[2]) / 2
                    else:
                        continue
                    coord_map[text] = {'x': x_center, 'y': y_center, 'bbox': bbox}
                    all_f8_texts.add(text)
            logger.info(f"[Stage H1] 物理座標マップ: {len(coord_map)}テキスト")

        # ============================================
        # Step 2: X軸・Y軸ヘッダーの物理座標を抽出
        # ============================================
        y_header_coords = {}  # {y_header: y_coordinate}
        x_header_coords = {}  # {x_header: x_coordinate}

        if y_headers and coord_map:
            for y_h in y_headers:
                y_h_str = str(y_h).strip()
                if y_h_str in coord_map:
                    y_header_coords[y_h_str] = coord_map[y_h_str]['y']
                    logger.debug(f"[H1座標] Y軸ヘッダー '{y_h_str}' → Y={coord_map[y_h_str]['y']:.1f}")

        if x_headers and coord_map:
            for x_h in x_headers:
                x_h_str = str(x_h).strip()
                if x_h_str in coord_map:
                    x_header_coords[x_h_str] = coord_map[x_h_str]['x']
                    logger.debug(f"[H1座標] X軸ヘッダー '{x_h_str}' → X={coord_map[x_h_str]['x']:.1f}")

        # タグ付きテキストを抽出
        valid_texts = [t for t in tagged_texts if t.get("text")]
        tagged_text_set = set(t.get("text", "").strip() for t in valid_texts)

        if not valid_texts:
            logger.warning("[Stage H1] Ver5.9: テキストなし")
            return {
                'columns': [], 'rows': [], 'row_count': 0, 'col_count': 0,
                'source': 'ver5.9_coord', 'error': 'no_texts'
            }

        # ヘッダーを自動抽出（指定がない場合）
        if x_headers is None:
            x_headers = sorted(set(t.get("x_header", "") for t in valid_texts if t.get("x_header")))
        if y_headers is None:
            y_headers = sorted(set(t.get("y_header", "") for t in valid_texts if t.get("y_header")))

        logger.info(f"[Stage H1] Ver5.9: X軸{len(x_headers)}個（座標付き{len(x_header_coords)}個）, Y軸{len(y_headers)}個（座標付き{len(y_header_coords)}個）")

        # ============================================
        # Step 3: 【核心】全軸物理座標による強制マッピング
        # ============================================
        intersection_map = {}  # {(y_header, x_header): [texts]}
        y_override_count = 0
        x_override_count = 0

        for text_item in valid_texts:
            text = text_item.get("text", "").strip()
            ai_x_header = str(text_item.get("x_header", "")).strip()
            ai_y_header = str(text_item.get("y_header", "")).strip()

            # 物理座標による強制上書き（初期値はAIのタグ）
            final_y_header = ai_y_header
            final_x_header = ai_x_header

            if text in coord_map:
                text_x = coord_map[text]['x']
                text_y = coord_map[text]['y']

                # ============================================
                # Y軸: 最も近いY軸ヘッダーを数学的に算出
                # ============================================
                if y_header_coords:
                    nearest_y_header = None
                    min_y_distance = float('inf')

                    for y_h, y_coord in y_header_coords.items():
                        distance = abs(text_y - y_coord)
                        if distance < min_y_distance:
                            min_y_distance = distance
                            nearest_y_header = y_h

                    if nearest_y_header and nearest_y_header != ai_y_header:
                        logger.debug(f"[H1-Y上書] '{text}': AI={ai_y_header} → 物理={nearest_y_header} (距離={min_y_distance:.1f})")
                        final_y_header = nearest_y_header
                        y_override_count += 1

                # ============================================
                # X軸: 最も近いX軸ヘッダーを数学的に算出
                # ============================================
                if x_header_coords:
                    nearest_x_header = None
                    min_x_distance = float('inf')

                    for x_h, x_coord in x_header_coords.items():
                        distance = abs(text_x - x_coord)
                        if distance < min_x_distance:
                            min_x_distance = distance
                            nearest_x_header = x_h

                    if nearest_x_header and nearest_x_header != ai_x_header:
                        logger.debug(f"[H1-X上書] '{text}': AI={ai_x_header} → 物理={nearest_x_header} (距離={min_x_distance:.1f})")
                        final_x_header = nearest_x_header
                        x_override_count += 1

            # 交差点マップに追加
            key = (final_y_header, final_x_header)
            if key not in intersection_map:
                intersection_map[key] = []
            intersection_map[key].append(text)

        logger.info(f"[Stage H1] Y軸上書き: {y_override_count}件, X軸上書き: {x_override_count}件")

        # ============================================
        # Step 4: 【欠落検知】Suryaが検出したがAIがタグ付けしなかったテキスト
        # ============================================
        missing_texts = all_f8_texts - tagged_text_set
        # ヘッダー自体は除外
        header_set = set(str(h).strip() for h in (x_headers or [])) | set(str(h).strip() for h in (y_headers or []))
        missing_texts = missing_texts - header_set
        forced_insert_count = 0

        if missing_texts:
            logger.warning(f"[Stage H1] 【欠落検知】AIが読み飛ばしたテキスト: {len(missing_texts)}件")

            for missing_text in missing_texts:
                if missing_text not in coord_map:
                    continue  # 座標がなければスキップ

                text_x = coord_map[missing_text]['x']
                text_y = coord_map[missing_text]['y']

                # 最近傍ヘッダーを計算
                nearest_y = None
                nearest_x = None
                min_y_dist = float('inf')
                min_x_dist = float('inf')

                for y_h, y_coord in y_header_coords.items():
                    dist = abs(text_y - y_coord)
                    if dist < min_y_dist:
                        min_y_dist = dist
                        nearest_y = y_h

                for x_h, x_coord in x_header_coords.items():
                    dist = abs(text_x - x_coord)
                    if dist < min_x_dist:
                        min_x_dist = dist
                        nearest_x = x_h

                if nearest_y and nearest_x:
                    key = (nearest_y, nearest_x)
                    if key not in intersection_map:
                        intersection_map[key] = []
                    intersection_map[key].append(f"[復元]{missing_text}")
                    forced_insert_count += 1
                    logger.info(f"[H1強制挿入] '{missing_text}' → ({nearest_y}, {nearest_x})")

        if forced_insert_count > 0:
            logger.info(f"[Stage H1] 欠落テキスト強制挿入: {forced_insert_count}件")

        # ============================================
        # Step 5: テーブル構築
        # ============================================
        columns = ["基準"] + list(x_headers)
        rows = []

        for y_h in y_headers:
            row = [y_h]  # 最初の列はY軸ヘッダー自体
            for x_h in x_headers:
                key = (str(y_h).strip(), str(x_h).strip())
                if key in intersection_map:
                    # 同一交差点に複数テキストがある場合は結合
                    cell_value = " / ".join(intersection_map[key])
                else:
                    cell_value = ""  # 空セル（埋めない）
                row.append(cell_value)
            rows.append(row)

        total_overrides = y_override_count + x_override_count
        logger.info(f"[Stage H1] 【Ver5.9】テーブル構築完了: {len(rows)}行 x {len(columns)}列")
        logger.info(f"  ├─ Y軸上書き: {y_override_count}件")
        logger.info(f"  ├─ X軸上書き: {x_override_count}件")
        logger.info(f"  └─ 欠落復元: {forced_insert_count}件")

        return {
            'columns': columns,
            'rows': rows,
            'row_count': len(rows),
            'col_count': len(columns),
            'x_headers': list(x_headers) if x_headers else [],
            'y_headers': list(y_headers) if y_headers else [],
            'intersection_count': len(intersection_map),
            'y_override_count': y_override_count,
            'x_override_count': x_override_count,
            'forced_insert_count': forced_insert_count,
            'total_coord_overrides': total_overrides + forced_insert_count,
            'source': 'ver5.9_full_coord_lock'
        }

    # ============================================
    # 【Ver 6.2】ID:テキスト溶接形式からテーブル構築
    # ============================================
    def build_table_from_v62_output(
        self,
        f7_output: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        【Ver 6.2】F7の「ID:テキスト」溶接形式からテーブルを構築

        F7 出力形式:
        {
          "cols": ["id", "text", "xh", "yh", "type"],
          "rows": [
            ["1", "2/1", "none", "none", "x_header"],
            ["10", "74", "none", "none", "y_header"],
            ["20", "筑波大駒場", "1:2/1", "10:74", "cell"],
            ...
          ]
        }

        xh, yh は "ID:テキスト" 形式（例: "1:2/1", "10:74"）
        これをパースしてテキスト部分でヘッダーを特定し、テーブルを構築

        Args:
            f7_output: F7のCSV型JSON出力

        Returns:
            構築済みテーブル
        """
        logger.info("[Stage H1] 【Ver 6.2】ID:テキスト溶接形式からテーブル構築開始")

        cols = f7_output.get("cols", [])
        rows = f7_output.get("rows", [])

        if not rows:
            logger.warning("[Stage H1] Ver 6.2: rows が空")
            return {
                'columns': [], 'rows': [], 'row_count': 0, 'col_count': 0,
                'source': 'ver6.2_empty'
            }

        # 列インデックスをマッピング
        try:
            idx_id = cols.index("id")
            idx_text = cols.index("text")
            idx_xh = cols.index("xh")
            idx_yh = cols.index("yh")
            idx_type = cols.index("type")
        except ValueError as e:
            logger.error(f"[Stage H1] Ver 6.2: 必須列が見つからない: {e}")
            return {
                'columns': [], 'rows': [], 'row_count': 0, 'col_count': 0,
                'source': 'ver6.2_missing_cols', 'error': str(e)
            }

        # ============================================
        # Step 1: ヘッダーを抽出
        # ============================================
        x_headers = []  # [(id, text), ...]
        y_headers = []  # [(id, text), ...]
        cells = []  # [(id, text, xh, yh), ...]

        for row in rows:
            if len(row) < 5:
                continue

            row_id = str(row[idx_id]).strip()
            row_text = str(row[idx_text]).strip()
            row_xh = str(row[idx_xh]).strip()
            row_yh = str(row[idx_yh]).strip()
            row_type = str(row[idx_type]).strip().lower()

            if row_type == "x_header":
                x_headers.append((row_id, row_text))
                logger.debug(f"[H1-6.2] X軸ヘッダー検出: #{row_id}='{row_text}'")
            elif row_type == "y_header":
                y_headers.append((row_id, row_text))
                logger.debug(f"[H1-6.2] Y軸ヘッダー検出: #{row_id}='{row_text}'")
            elif row_type == "cell":
                cells.append((row_id, row_text, row_xh, row_yh))
            # title, note は無視（表データではない）

        logger.info(f"[Stage H1] Ver 6.2: X軸ヘッダー{len(x_headers)}個, Y軸ヘッダー{len(y_headers)}個, セル{len(cells)}個")

        if not x_headers or not y_headers:
            logger.warning("[Stage H1] Ver 6.2: ヘッダーが検出されなかった")
            # フォールバック: 従来形式として処理を試みる
            return self._fallback_to_legacy_format(f7_output)

        # ============================================
        # Step 2: ID:テキスト をパースしてマッピング構築
        # ============================================
        # ヘッダーID→テキストのマップ
        x_id_to_text = {h[0]: h[1] for h in x_headers}
        y_id_to_text = {h[0]: h[1] for h in y_headers}

        # 交差点マップ: {(y_text, x_text): [cell_texts]}
        intersection_map = {}
        parse_success = 0
        parse_fail = 0

        for cell_id, cell_text, xh, yh in cells:
            # "ID:テキスト" 形式をパース
            x_text = self._parse_id_text(xh, x_id_to_text)
            y_text = self._parse_id_text(yh, y_id_to_text)

            if x_text is None or y_text is None:
                logger.warning(f"[H1-6.2] パース失敗: #{cell_id}='{cell_text}' (xh={xh}, yh={yh})")
                parse_fail += 1
                continue

            parse_success += 1
            key = (y_text, x_text)
            if key not in intersection_map:
                intersection_map[key] = []
            intersection_map[key].append(cell_text)
            logger.debug(f"[H1-6.2] セル配置: '{cell_text}' → ({y_text}, {x_text})")

        logger.info(f"[Stage H1] Ver 6.2: パース成功{parse_success}件, 失敗{parse_fail}件")

        # ============================================
        # Step 3: テーブル構築
        # ============================================
        x_header_texts = [h[1] for h in x_headers]
        y_header_texts = [h[1] for h in y_headers]

        columns = ["基準"] + x_header_texts
        table_rows = []

        for y_text in y_header_texts:
            row = [y_text]
            for x_text in x_header_texts:
                key = (y_text, x_text)
                if key in intersection_map:
                    cell_value = " / ".join(intersection_map[key])
                else:
                    cell_value = ""
                row.append(cell_value)
            table_rows.append(row)

        logger.info(f"[Stage H1] 【Ver 6.2】テーブル構築完了: {len(table_rows)}行 x {len(columns)}列")

        return {
            'columns': columns,
            'rows': table_rows,
            'row_count': len(table_rows),
            'col_count': len(columns),
            'x_headers': x_header_texts,
            'y_headers': y_header_texts,
            'intersection_count': len(intersection_map),
            'parse_success': parse_success,
            'parse_fail': parse_fail,
            'source': 'ver6.2_id_text_weld'
        }

    def _parse_id_text(self, id_text_str: str, id_to_text_map: Dict[str, str]) -> Optional[str]:
        """
        "ID:テキスト" 形式をパースしてテキストを返す

        Args:
            id_text_str: "1:2/1" 形式の文字列、または "none"
            id_to_text_map: {ID: テキスト} のマップ

        Returns:
            テキスト部分、または None（パース失敗時）
        """
        if not id_text_str or id_text_str.lower() == "none":
            return None

        if ":" in id_text_str:
            parts = id_text_str.split(":", 1)
            id_part = parts[0].strip()
            text_part = parts[1].strip() if len(parts) > 1 else ""

            # IDからテキストを逆引きして検証
            expected_text = id_to_text_map.get(id_part)
            if expected_text:
                # ID が正しければ、マップのテキストを優先（OCRミス防止）
                return expected_text
            else:
                # IDがマップにない場合は、テキスト部分をそのまま使用
                return text_part if text_part else None
        else:
            # ":" がない場合はテキストとして扱う
            return id_text_str

    def _fallback_to_legacy_format(self, f7_output: Dict[str, Any]) -> Dict[str, Any]:
        """
        Ver 6.2 形式でない場合のフォールバック処理

        従来の x_headers, y_headers, extracted_texts 形式を試す
        """
        logger.info("[Stage H1] Ver 6.2 フォールバック: 従来形式を試行")

        # 従来形式の処理
        x_headers = f7_output.get("x_headers", [])
        y_headers = f7_output.get("y_headers", [])

        if x_headers and y_headers:
            # cols/rows 形式から tagged_texts を再構築
            tagged_texts = self._convert_csv_to_tagged_texts(f7_output)
            if tagged_texts:
                return self.build_table_from_tagged_texts(
                    tagged_texts=tagged_texts,
                    x_headers=x_headers,
                    y_headers=y_headers
                )

        return {
            'columns': [], 'rows': [], 'row_count': 0, 'col_count': 0,
            'source': 'ver6.2_fallback_failed'
        }

    def _convert_csv_to_tagged_texts(self, f7_output: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        CSV型JSON（cols/rows）を tagged_texts 形式に変換
        """
        cols = f7_output.get("cols", [])
        rows = f7_output.get("rows", [])

        if not cols or not rows:
            return []

        tagged_texts = []
        for row in rows:
            if len(row) < len(cols):
                continue

            item = {}
            for i, col in enumerate(cols):
                # 列名を正規化
                normalized_col = col.lower().replace("_", "")
                if normalized_col in ("text", "txt"):
                    item["text"] = row[i]
                elif normalized_col in ("xh", "xheader", "x_header", "x_h"):
                    item["x_header"] = row[i]
                elif normalized_col in ("yh", "yheader", "y_header", "y_h"):
                    item["y_header"] = row[i]
                elif normalized_col == "type":
                    item["type"] = row[i]
                else:
                    item[col] = row[i]

            if item.get("text"):
                tagged_texts.append(item)

        return tagged_texts
```

## shared/pipeline/stage_h2_text.py

```python
"""
Stage H2: Text Specialist (テキスト処理専門)

【設計 2026-01-27】Stage HI分割: H1 + H2

役割: H1で軽量化されたテキストを処理し、構造化 + 統合・要約を実行
      従来のStage HI統合版と同等の機能（ただし入力量が削減済み）

============================================
入力:
  - reduced_text: H1で表テキストを削除した軽量テキスト
  - h1_result: Stage H1の処理結果（processed_tables等）
  - source_inventory: REF_ID付きセグメントリスト

出力:
  - document_date: 基準日付
  - tags: 検索用タグ
  - metadata: 構造化データ（H1の結果も含む）
  - title: ドキュメントタイトル
  - summary: 要約
  - calendar_events: カレンダーイベント
  - tasks: タスクリスト
  - audit_canonical_text: 監査用正本テキスト

特徴:
  - 従来のStage HIと同じ処理（互換性維持）
  - 入力テキスト量が削減されているため、トークン消費が減少
  - H1で抽出した表メタデータをマージ
============================================
"""
import re
import json
import json_repair
from typing import Dict, Any, Optional, List
from pathlib import Path
from string import Template
from loguru import logger
from datetime import datetime

from shared.ai.llm_client.llm_client import LLMClient
from .constants import STAGE_H_INPUT_SCHEMA_VERSION


class StageH2Text:
    """Stage H2: テキスト処理専門（従来のStage HI互換）"""

    def __init__(self, llm_client: LLMClient):
        """
        Args:
            llm_client: LLMクライアント
        """
        self.llm = llm_client

    def process(
        self,
        file_name: str,
        doc_type: str,
        workspace: str,
        reduced_text: str,
        prompt: str,
        model: str,
        h1_result: Optional[Dict[str, Any]] = None,
        stage_f_structure: Optional[Dict[str, Any]] = None,
        stage_g_result: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        テキスト処理 + 構造化 + 要約

        Args:
            file_name: ファイル名
            doc_type: ドキュメントタイプ
            workspace: ワークスペース
            reduced_text: H1で軽量化されたテキスト
            prompt: プロンプト
            model: モデル名
            h1_result: Stage H1の結果
            stage_f_structure: Stage F の構造化情報
            stage_g_result: Stage G の結果

        Returns:
            Stage HI 互換の出力形式
        """
        logger.info(f"[Stage H2] テキスト処理開始... (doc_type={doc_type}, model={model})")

        # 入力サイズをログ
        original_size = len(stage_g_result.get('unified_text', '')) if stage_g_result else 0
        reduced_size = len(reduced_text)
        reduction_pct = (original_size - reduced_size) * 100 // original_size if original_size > 0 else 0
        logger.info(f"[Stage H2] 入力テキスト: {reduced_size}文字 (元: {original_size}文字, -{reduction_pct}%削減)")

        # Stage G の結果から source_inventory を取得
        source_inventory = []
        table_inventory = []
        if stage_g_result:
            source_inventory = stage_g_result.get('source_inventory', [])
            # H1で処理済みの表を除外した table_inventory
            removed_ids = h1_result.get('removed_table_ids', set()) if h1_result else set()
            table_inventory = [
                t for t in stage_g_result.get('table_inventory', [])
                if t.get('ref_id') not in removed_ids
            ]
            logger.info(f"[Stage H2] source_inventory={len(source_inventory)}, 残存table_inventory={len(table_inventory)}")

        if not reduced_text or not reduced_text.strip():
            logger.warning("[Stage H2] 入力テキストが空です")
            return self._get_fallback_result(doc_type, h1_result)

        try:
            # プロンプト構築
            logger.info("[Stage H2] プロンプト構築中...")

            # H1の結果から情報を取得
            h1_tables = h1_result.get('processed_tables', []) if h1_result else []
            unrepairable_tables = h1_result.get('unrepairable_tables', []) if h1_result else []

            if unrepairable_tables:
                logger.info(f"[Stage H2] 修復不能表: {len(unrepairable_tables)}件 → プロンプトに通知")

            full_prompt = self._build_prompt(
                prompt_template=prompt,
                file_name=file_name,
                doc_type=doc_type,
                workspace=workspace,
                combined_text=reduced_text,
                source_inventory=source_inventory,
                table_inventory=table_inventory,
                h1_tables=h1_tables,
                unrepairable_tables=unrepairable_tables
            )
            logger.info(f"[Stage H2] プロンプト構築完了 ({len(full_prompt)}文字)")

            # LLM呼び出し
            logger.info(f"[Stage H2] LLM呼び出し中... (model={model})")
            response = self.llm.call_model(
                tier="default",
                prompt=full_prompt,
                model_name=model
            )
            logger.info(f"[Stage H2] LLM応答受信: success={response.get('success')}")

            if not response.get("success"):
                logger.error(f"[Stage H2 エラー] LLM呼び出し失敗: {response.get('error')}")
                return self._get_fallback_result(doc_type, h1_result)

            # JSON抽出
            content = response.get("content", "")
            logger.info(f"[Stage H2] ===== LLMレスポンス（最初の1000文字）=====\n{content[:1000]}")
            # リトライ禁止（2026-01-28）: エラー時は即座にフォールバック
            result = self._extract_json_with_retry(content, model=model, max_retries=0)

            # Stage F の構造化情報をマージ
            if stage_f_structure:
                result = self._merge_stage_f_structure(result, stage_f_structure)

            # H1 の結果をマージ
            if h1_result:
                result = self._merge_h1_result(result, h1_result)

            # audit_canonical_text の生成
            audit_canonical_text = self._generate_audit_canonical_text(
                result, reduced_text, source_inventory
            )

            # 結果の整形
            final_result = {
                'document_date': result.get('document_date'),
                'tags': result.get('tags', []),
                'metadata': result.get('metadata', {}),
                'title': result.get('title', ''),
                'summary': result.get('summary', ''),
                'calendar_events': result.get('calendar_events', []),
                'tasks': result.get('tasks', []),
                'audit_canonical_text': audit_canonical_text
            }

            logger.info(f"[Stage H2完了] title={final_result['title'][:50] if final_result['title'] else 'N/A'}...")
            return final_result

        except Exception as e:
            logger.error(f"[Stage H2 エラー] 処理失敗: {e}", exc_info=True)
            return self._get_fallback_result(doc_type, h1_result)

    def _build_prompt(
        self,
        prompt_template: str,
        file_name: str,
        doc_type: str,
        workspace: str,
        combined_text: str,
        source_inventory: List[Dict],
        table_inventory: List[Dict],
        h1_tables: List[Dict],
        unrepairable_tables: List[Dict] = None
    ) -> str:
        """プロンプトを構築（アンカー活用強化版）"""
        # source_inventory を簡略化
        inventory_summary = []
        for item in source_inventory[:30]:
            inventory_summary.append({
                'ref_id': item.get('ref_id'),
                'type': item.get('type'),
                'text': item.get('text', '')[:200]
            })

        # H1で処理済み表の概要（詳細データはメタデータとして既に含まれている）
        h1_tables_summary = []
        for tbl in h1_tables[:10]:
            h1_tables_summary.append({
                'ref_id': tbl.get('ref_id'),
                'table_title': tbl.get('table_title'),
                'table_type': tbl.get('table_type'),
                'row_count': tbl.get('row_count', 0)
            })

        # テンプレート変数を置換
        template = Template(prompt_template)
        prompt = template.substitute(
            file_name=file_name,
            doc_type=doc_type,
            workspace=workspace,
            combined_text=combined_text,
            current_date=datetime.now().strftime("%Y-%m-%d"),
            source_inventory_json=json.dumps(inventory_summary, ensure_ascii=False, indent=2),
            table_inventory_json=json.dumps(h1_tables_summary, ensure_ascii=False, indent=2),
            source_count=len(source_inventory),
            table_count=len(h1_tables)
        )

        # ============================================
        # アンカー活用強化: H1で処理済みの表を参照させる
        # ============================================
        anchor_instructions = self._build_anchor_instructions(h1_tables, unrepairable_tables or [])
        if anchor_instructions:
            prompt = prompt + "\n\n" + anchor_instructions

        return prompt

    def _build_anchor_instructions(
        self,
        h1_tables: List[Dict],
        unrepairable_tables: List[Dict]
    ) -> str:
        """
        アンカー活用のための追加指示を構築

        G2が埋め込んだアンカー（[→ TBL_xxx 参照]）を
        AIが最大限に活用するための指示
        """
        if not h1_tables and not unrepairable_tables:
            return ""

        instructions = []
        instructions.append("=" * 50)
        instructions.append("【重要: 表データの活用指示】")
        instructions.append("=" * 50)

        instructions.append("""
テキスト中に `[→ TBL_xxx 参照]` というアンカー（しおり）が出現します。
これは「ここに表データがある」という座標です。以下のルールで活用してください。

【ルール1: アンカーは聖域】
アンカーは情報の座標です。要約や構造化において、
このアンカーの存在を認識し、表の内容を考慮して文脈を補完してください。

【ルール2: 表の内容を参照】
以下に、各アンカーに対応する表の概要を示します。
要約やカレンダーイベント、タスク抽出の際は、これらの表データを活用してください。
""")

        # H1で処理済みの表
        if h1_tables:
            instructions.append("\n■ 処理済み表データ（H1で構造化済み）:")
            for tbl in h1_tables[:10]:
                ref_id = tbl.get('ref_id', '')
                title = tbl.get('table_title', '(無題)')
                ttype = tbl.get('table_type', 'generic')
                rows = tbl.get('rows', [])
                row_count = tbl.get('row_count', len(rows))

                instructions.append(f"  - {ref_id}: {title} ({ttype}, {row_count}行)")

                # 表の内容サンプル（最初の3行）
                if rows and len(rows) > 0:
                    sample_rows = rows[:3]
                    for i, row in enumerate(sample_rows):
                        if isinstance(row, dict):
                            row_text = ', '.join(f"{k}:{v}" for k, v in list(row.items())[:4])
                            instructions.append(f"      行{i+1}: {row_text[:80]}")

        # 修復不能だった表
        if unrepairable_tables:
            instructions.append("\n■ 修復不能だった表（参照のみ）:")
            for tbl in unrepairable_tables:
                ref_id = tbl.get('ref_id', '')
                title = tbl.get('table_title', '(無題)')
                reason = tbl.get('reason', '')
                instructions.append(f"  - {ref_id}: {title} (※構造化できず: {reason})")
            instructions.append("  ※これらの表は構造化に失敗しましたが、テキスト中の情報として参照可能です。")

        instructions.append("""
【ルール3: カレンダー・タスクへの反映】
表に日付や予定が含まれている場合は、calendar_events や tasks として抽出してください。
表のデータは信頼できる情報源です。積極的に活用してください。

【ルール4: 要約への反映】
表の存在と概要を要約に含めてください。
例: 「週間予定表によると、月曜は国語、火曜は算数...」
""")

        return "\n".join(instructions)

    def _extract_json_with_retry(
        self,
        content: str,
        model: str,
        max_retries: int = 2
    ) -> Dict[str, Any]:
        """JSON抽出（リトライ機能付き）"""
        for attempt in range(max_retries + 1):
            try:
                result = self._extract_json(content)
                logger.debug(f"[Stage H2] JSON抽出成功 (試行{attempt + 1}/{max_retries + 1})")
                return result

            except Exception as e:
                if attempt < max_retries:
                    logger.warning(f"[Stage H2] JSON抽出失敗 (試行{attempt + 1}/{max_retries + 1}): {e}")
                    content = self._retry_json_extraction(content, str(e), model)
                else:
                    logger.error(f"[Stage H2] JSON抽出失敗（最終試行）: {e}")
                    raise

        return {}

    def _extract_json(self, content: str) -> Dict[str, Any]:
        """コンテンツからJSONを抽出"""
        patterns = [
            r'```json\s*(.*?)```',
            r'```\s*(.*?)```',
            r'\{[\s\S]*?\}',
        ]

        json_str = None
        for pattern in patterns:
            match = re.search(pattern, content, re.DOTALL)
            if match:
                json_str = match.group(1) if match.lastindex else match.group(0)
                json_str = json_str.strip()
                if json_str.startswith('{'):
                    break
                else:
                    json_str = None

        if not json_str:
            match = re.search(r'\{[\s\S]*\}', content, re.DOTALL)
            if match:
                json_str = match.group(0).strip()
            else:
                json_str = content.strip()

        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.error(f"[Stage H2] JSON解析失敗: {e}")
            try:
                return json_repair.loads(json_str)
            except Exception as repair_error:
                logger.error(f"[Stage H2] JSON修復も失敗: {repair_error}")
                raise

    def _retry_json_extraction(
        self,
        failed_content: str,
        error_message: str,
        model: str
    ) -> str:
        """JSON抽出失敗時、LLMにJSON修正を依頼"""
        prompt = f"""以下のJSONにエラーがあります。修正してください。

エラー: {error_message}

元のJSON:
```
{failed_content[:3000]}
```

修正されたJSONを ```json ブロックで出力してください。
"""

        try:
            response = self.llm.call_model(
                tier="default",
                prompt=prompt,
                model_name=model
            )

            if response.get("success"):
                return response.get("content", "")
            else:
                return failed_content

        except Exception as e:
            logger.error(f"[Stage H2] JSON修正エラー: {e}")
            return failed_content

    def _merge_stage_f_structure(
        self,
        result: Dict[str, Any],
        stage_f_structure: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Stage F の構造化情報をマージ"""
        metadata = result.get('metadata', {})

        schema_ver = stage_f_structure.get('schema_version', '')
        is_v1_1 = (schema_ver == STAGE_H_INPUT_SCHEMA_VERSION)

        if is_v1_1:
            stage_f_text_blocks = stage_f_structure.get('text_blocks', [])
            if stage_f_text_blocks:
                metadata['_raw_text_blocks'] = stage_f_text_blocks

        visual_elements = stage_f_structure.get('visual_elements', {})
        if visual_elements:
            deadline_info = visual_elements.get('deadline_info')
            if deadline_info and not result.get('document_date'):
                result['document_date'] = deadline_info

        result['metadata'] = metadata
        return result

    def _merge_h1_result(
        self,
        result: Dict[str, Any],
        h1_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """H1の結果をマージ"""
        metadata = result.get('metadata', {})

        # H1で抽出したメタデータをマージ
        h1_metadata = h1_result.get('extracted_metadata', {})
        if h1_metadata:
            for key, value in h1_metadata.items():
                if key not in metadata:
                    metadata[key] = value
                    logger.debug(f"[Stage H2] H1メタデータをマージ: {key}")

        # H1で処理した表を extracted_tables に追加
        processed_tables = h1_result.get('processed_tables', [])
        if processed_tables:
            # 既存の extracted_tables と統合
            existing_tables = metadata.get('extracted_tables', [])
            metadata['extracted_tables'] = existing_tables + processed_tables
            logger.info(f"[Stage H2] H1処理済み表をマージ: {len(processed_tables)}表")

        # 処理統計
        stats = h1_result.get('statistics', {})
        if stats:
            metadata['_h1_statistics'] = stats

        result['metadata'] = metadata
        return result

    def _generate_audit_canonical_text(
        self,
        result: Dict[str, Any],
        combined_text: str,
        source_inventory: List[Dict]
    ) -> str:
        """監査用正本テキストを生成"""
        parts = []

        title = result.get('title', '')
        if title:
            parts.append(f"# {title}\n")

        metadata = result.get('metadata', {})
        basic_info = metadata.get('basic_info', {})
        if basic_info:
            parts.append("## 基本情報")
            for key, value in basic_info.items():
                if value:
                    parts.append(f"- {key}: {value}")
            parts.append("")

        summary = result.get('summary', '')
        if summary:
            parts.append("## 要約")
            parts.append(summary)
            parts.append("")

        articles = metadata.get('articles', [])
        if articles:
            parts.append("## 記事・お知らせ")
            for article in articles:
                title_a = article.get('title', '')
                body = article.get('body', '')
                if title_a:
                    parts.append(f"### {title_a}")
                if body:
                    parts.append(body)
                parts.append("")

        calendar_events = result.get('calendar_events', [])
        if calendar_events:
            parts.append("## カレンダーイベント")
            for event in calendar_events:
                date = event.get('event_date', '')
                name = event.get('event_name', '')
                parts.append(f"- {date}: {name}")
            parts.append("")

        tasks = result.get('tasks', [])
        if tasks:
            parts.append("## タスク")
            for task in tasks:
                name = task.get('task_name', '')
                deadline = task.get('deadline', '')
                parts.append(f"- {name} (期限: {deadline})")
            parts.append("")

        # 表データ（H1で処理したもの）
        extracted_tables = metadata.get('extracted_tables', [])
        if extracted_tables:
            parts.append("## 抽出された表")
            for tbl in extracted_tables[:5]:
                ref_id = tbl.get('ref_id', '')
                title = tbl.get('table_title', '表')
                row_count = tbl.get('row_count', 0)
                parts.append(f"- {ref_id}: {title} ({row_count}行)")
            if len(extracted_tables) > 5:
                parts.append(f"  ... 他 {len(extracted_tables) - 5} 表")

        if source_inventory:
            parts.append("\n## 参照元（REF_ID）")
            for item in source_inventory[:10]:
                ref_id = item.get('ref_id', '')
                text = item.get('text', '')[:100]
                parts.append(f"- {ref_id}: {text}...")
            if len(source_inventory) > 10:
                parts.append(f"  ... 他 {len(source_inventory) - 10} 件")

        return '\n'.join(parts)

    def _get_fallback_result(
        self,
        doc_type: str,
        h1_result: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """フォールバック結果を返す"""
        metadata = {
            'doc_type': doc_type,
            'extraction_failed': True
        }

        # H1の結果があればマージ
        if h1_result:
            h1_metadata = h1_result.get('extracted_metadata', {})
            metadata.update(h1_metadata)

            processed_tables = h1_result.get('processed_tables', [])
            if processed_tables:
                metadata['extracted_tables'] = processed_tables

        return {
            'document_date': None,
            'tags': [],
            'metadata': metadata,
            'title': '',
            'summary': '',
            'calendar_events': [],
            'tasks': [],
            'audit_canonical_text': ''
        }
```

## shared/pipeline/stage_h_kakeibo.py

```python
"""
Stage H: Kakeibo Structuring (家計簿構造化)

家計簿レシート専用のStage H処理
- 税額按分計算
- 商品分類
- マスタデータとの紐付け
"""

from typing import Dict, Any, List
from loguru import logger
from datetime import date

from shared.common.database.client import DatabaseClient


class StageHKakeibo:
    """家計簿専用のStage H（税額按分・分類）"""

    def __init__(self, db_client: DatabaseClient):
        """
        Args:
            db_client: データベースクライアント
        """
        self.db = db_client

        # マスタデータをロード
        self.aliases = self._load_aliases()
        self.product_dict = self._load_product_dictionary()
        self.situations = self._load_situations()
        self.categories = self._load_categories()

    def process(
        self,
        stage_g_output: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Stage Gの出力から最終データを生成

        Args:
            stage_g_output: Stage Gの構造化データ

        Returns:
            Dict: 最終データ（DB保存可能な形式）
        """
        try:
            # 【重要】レシート記載の金額は改ざんしない
            # 割引行は別行としてそのまま保持（マイナス金額）
            items = stage_g_output.get("items", [])

            # 割引を商品にリンク（税込価計算用）
            items = self._link_discounts_to_items(items)

            # 1. 商品を正規化（マスタとの紐付け）
            normalized_items = []
            for item in items:
                # 値引き行も含めて処理（金額はそのまま）
                if item.get("line_type") == "DISCOUNT":
                    normalized_items.append({
                        "raw_item": item,
                        "normalized": {
                            "product_name": item.get("product_name", "値引"),
                            "category_id": None,
                            "tax_rate": self._get_discount_tax_rate(item, items),
                            "tax_rate_source": "discount",
                            "is_discount": True
                        }
                    })
                    continue

                normalized = self._normalize_item(
                    item,
                    stage_g_output["shop_info"]["name"],
                    stage_g_output.get("amounts", {})
                )
                normalized_items.append({
                    "raw_item": item,
                    "normalized": normalized
                })

            # 2. シチュエーション判定
            trans_date = date.fromisoformat(stage_g_output["transaction_info"]["date"])
            situation_id = self._determine_situation(trans_date)

            # 3. 税額を按分計算
            items_with_tax, tax_subtotals = self._calculate_and_distribute_tax(
                normalized_items,
                stage_g_output.get("amounts", {})
            )

            # 4. 最終データを構築
            result = {
                "receipt": {
                    **stage_g_output["shop_info"],
                    **stage_g_output["transaction_info"],
                    **stage_g_output.get("amounts", {}),
                    **tax_subtotals,  # 税対象額を追加
                    "situation_id": situation_id
                },
                "items": items_with_tax,
                "payment": stage_g_output.get("payment", {}),
                "other_info": stage_g_output.get("other_info", {})
            }

            logger.info(f"Stage H completed: {len(items_with_tax)} items processed")
            return result

        except Exception as e:
            logger.error(f"Stage H failed: {e}")
            raise

    def _get_discount_tax_rate(self, discount_item: Dict, all_items: List[Dict]) -> int:
        """
        割引行の税率を判定（適用先商品から推定）

        Args:
            discount_item: 割引行データ
            all_items: 全明細行リスト

        Returns:
            int: 税率（8 or 10）
        """
        # 行番号でインデックスを作成
        items_by_line = {item.get("line_number"): item for item in all_items}

        # 明示的に値引き適用先が指定されている場合
        applied_to_line = discount_item.get("discount_applied_to")
        if applied_to_line and applied_to_line in items_by_line:
            target = items_by_line[applied_to_line]
            tax_mark = target.get("tax_mark")
            if tax_mark and (
                tax_mark in ["*", "※", "◆"] or
                "8%" in str(tax_mark) or
                "8" in str(tax_mark)
            ):
                return 8
            return 10

        # 直前の商品から推定
        discount_line_num = discount_item.get("line_number")
        if discount_line_num:
            for i in range(discount_line_num - 1, 0, -1):
                if i in items_by_line and items_by_line[i].get("line_type") != "DISCOUNT":
                    target = items_by_line[i]
                    tax_mark = target.get("tax_mark")
                    if tax_mark and (
                        tax_mark in ["*", "※", "◆"] or
                        "8%" in str(tax_mark) or
                        "8" in str(tax_mark)
                    ):
                        return 8
                    return 10

        # デフォルト10%
        return 10

    def _link_discounts_to_items(self, items: List[Dict]) -> List[Dict]:
        """
        割引行を商品にリンク（税込価計算用）

        各商品に linked_discount フィールドを追加
        割引の適用先が明示されていない場合は直前の商品に適用

        Args:
            items: Stage Gで抽出された全明細行

        Returns:
            List[Dict]: リンク情報が追加された明細行リスト
        """
        # 行番号でインデックスを作成
        items_by_line = {item.get("line_number"): item for item in items}

        # 各商品のlinked_discountを初期化
        for item in items:
            if item.get("line_type") != "DISCOUNT":
                item["linked_discount"] = 0

        # 割引を適用先にリンク
        for item in items:
            if item.get("line_type") != "DISCOUNT":
                continue

            discount_amount = item.get("amount", 0)  # 負の値
            applied_to_line = item.get("discount_applied_to")

            target = None
            if applied_to_line and applied_to_line in items_by_line:
                # 明示的に適用先が指定されている場合
                target = items_by_line[applied_to_line]
            else:
                # 直前の商品を探す
                discount_line_num = item.get("line_number")
                if discount_line_num:
                    for i in range(discount_line_num - 1, 0, -1):
                        if i in items_by_line and items_by_line[i].get("line_type") != "DISCOUNT":
                            target = items_by_line[i]
                            break

            if target and target.get("line_type") != "DISCOUNT":
                target["linked_discount"] = target.get("linked_discount", 0) + discount_amount
                logger.info(f"Linked discount {discount_amount}円 to {target.get('product_name')}")

        return items

    def _normalize_item(self, item: Dict, shop_name: str, amounts: Dict = None) -> Dict:
        """
        商品名を正規化し、カテゴリ・税率を判定

        Args:
            item: 商品データ（Stage Gの出力）
            shop_name: 店舗名
            amounts: レシート全体の金額情報（税率判定に使用）

        Returns:
            Dict: {"product_name": "正規化後", "category_id": "...", "tax_rate": 10}
        """
        # 商品名を取得（空文字列もNoneとして扱う）
        product_name = item.get("product_name") or item.get("line_text") or item.get("ocr_raw_text") or "不明"
        # 空文字列の場合は「不明」に
        if not product_name or not product_name.strip():
            product_name = "不明"

        receipt_tax_mark = item.get("tax_mark")  # レシートの税率マーク

        # レシート全体の税率情報を確認（最優先）
        receipt_level_tax_rate = None
        if amounts:
            tax_8_amount = amounts.get("tax_8_amount") or 0
            tax_10_amount = amounts.get("tax_10_amount") or 0

            # 8%のみの場合
            if tax_8_amount > 0 and tax_10_amount == 0:
                receipt_level_tax_rate = 8
                logger.debug(f"Receipt has only 8% tax, setting all items to 8%")
            # 10%のみの場合
            elif tax_10_amount > 0 and tax_8_amount == 0:
                receipt_level_tax_rate = 10
                logger.debug(f"Receipt has only 10% tax, setting all items to 10%")
            # 混在の場合は個別判定に進む

        # レシート全体の税率が判定できた場合はそれを使用（最優先）
        if receipt_level_tax_rate is not None:
            return {
                "product_name": product_name,
                "category_id": None,
                "tax_rate": receipt_level_tax_rate,
                "tax_rate_source": "receipt_level",
                "tax_amount": None
            }

        # 1. エイリアス変換
        product_name = self.aliases.get(product_name.lower(), product_name)

        # 2. 商品辞書マッチング
        for entry in self.product_dict:
            if entry["raw_keyword"].lower() in product_name.lower():
                return {
                    "product_name": entry["official_name"],
                    "category_id": entry["category_id"],
                    "tax_rate": entry["tax_rate"],
                    "tax_rate_source": "master",  # マスタから取得
                    "tax_amount": None  # 後で計算
                }

        # 3. 商品名から税率パターンを検出（「外8」「内8」などのレシート記載パターン）
        if "外8" in product_name or "内8" in product_name or "外 8" in product_name or "内 8" in product_name:
            tax_rate = 8
            tax_rate_source = "product_name_pattern"
            # 商品名から税率パターンを削除
            product_name = product_name.replace("外8", "").replace("内8", "").replace("外 8", "").replace("内 8", "").strip()
            # 空文字列になった場合は「不明」に
            if not product_name:
                product_name = "不明"
        elif "外10" in product_name or "内10" in product_name or "外 10" in product_name or "内 10" in product_name:
            tax_rate = 10
            tax_rate_source = "product_name_pattern"
            # 商品名から税率パターンを削除
            product_name = product_name.replace("外10", "").replace("内10", "").replace("外 10", "").replace("内 10", "").strip()
            # 空文字列になった場合は「不明」に
            if not product_name:
                product_name = "不明"
        # 4. レシートのマークから税率を判定
        # 8%マークの判定（複数パターン対応）
        elif receipt_tax_mark and (
            receipt_tax_mark in ["*", "※", "◆"] or  # よくある軽減税率マーク
            "8%" in str(receipt_tax_mark) or
            "8" in str(receipt_tax_mark) or
            "(軽)" in str(receipt_tax_mark) or
            "外8" in str(receipt_tax_mark) or  # 外税8%のパターン
            "内8" in str(receipt_tax_mark)  # 内税8%のパターン
        ):
            tax_rate = 8
            tax_rate_source = "receipt_mark"
        # 10%マークの判定
        elif receipt_tax_mark and (
            receipt_tax_mark in ["★", "☆"] or  # よくある標準税率マーク
            "10%" in str(receipt_tax_mark) or
            "10" in str(receipt_tax_mark) or
            "外10" in str(receipt_tax_mark) or  # 外税10%のパターン
            "内10" in str(receipt_tax_mark)  # 内税10%のパターン
        ):
            tax_rate = 10
            tax_rate_source = "receipt_mark"
        else:
            # デフォルト10%（あとで要レビュー）
            tax_rate = 10
            tax_rate_source = "default"

        return {
            "product_name": product_name,
            "category_id": None,
            "tax_rate": tax_rate,
            "tax_rate_source": tax_rate_source,
            "tax_amount": None
        }

    def _determine_situation(self, trans_date: date) -> str:
        """
        取引日からシチュエーションを判定

        Args:
            trans_date: 取引日

        Returns:
            str: シチュエーションID
        """
        weekday = trans_date.weekday()  # 0=月曜, 6=日曜

        # 土日
        if weekday >= 5:
            for s in self.situations:
                if s["name"] == "週末":
                    return s["id"]

        # 平日
        for s in self.situations:
            if s["name"] == "平日":
                return s["id"]

        # デフォルト（最初のシチュエーション）
        return self.situations[0]["id"] if self.situations else None

    def _calculate_and_distribute_tax(
        self,
        normalized_items: List[Dict],
        amounts: Dict
    ) -> List[Dict]:
        """
        税額を按分計算（内税・外税対応）

        Args:
            normalized_items: 正規化済み商品リスト
            amounts: Stage Gで抽出した金額情報

        Returns:
            List[Dict]: 税額が計算された商品リスト
        """
        # 【重要】内税・外税の判定
        # Stage Gで判定済みの場合はそれを優先
        tax_type = amounts.get("tax_display_type")

        if tax_type:
            logger.info(f"Stage Gで判定済み: tax_display_type={tax_type}")
        else:
            # フォールバック: 小計と合計の比較で判定
            subtotal = amounts.get("subtotal")
            total = amounts.get("total")

            if subtotal is not None and total is not None and subtotal < total:
                tax_type = "excluded"  # 外税
                logger.info(f"外税レシート検出: 小計={subtotal}円 < 合計={total}円")
            else:
                tax_type = "included"  # 内税
                logger.info(f"内税レシート検出: 小計={subtotal}円 = 合計={total}円")

        # 商品を8%と10%にグループ化
        items_8 = []
        items_10 = []

        for item_data in normalized_items:
            if item_data["normalized"]["tax_rate"] == 8:
                items_8.append(item_data)
            else:
                items_10.append(item_data)

        # レシート記載の税額を使用（優先）
        tax_8_amount = amounts.get("tax_8_amount") or 0
        tax_10_amount = amounts.get("tax_10_amount") or 0

        # レシート記載がない場合のみ逆算（通常は記載されている）
        if tax_8_amount == 0:
            total_8 = sum(item["raw_item"]["amount"] or 0 for item in items_8)
            if total_8 > 0:
                if tax_type == "excluded":
                    tax_8_amount = round(total_8 * 8 / 100)  # 外税：本体価格×税率
                else:
                    tax_8_amount = round(total_8 * 8 / 108)  # 内税：税込額から逆算
                logger.warning(f"8% tax not in receipt, calculated: {tax_8_amount}円 (type={tax_type})")

        if tax_10_amount == 0:
            total_10 = sum(item["raw_item"]["amount"] or 0 for item in items_10)
            if total_10 > 0:
                if tax_type == "excluded":
                    tax_10_amount = round(total_10 * 10 / 100)  # 外税：本体価格×税率
                else:
                    tax_10_amount = round(total_10 * 10 / 110)  # 内税：税込額から逆算
                logger.warning(f"10% tax not in receipt, calculated: {tax_10_amount}円 (type={tax_type})")

        # 各商品に税額を按分（内外タイプを渡す）
        self._distribute_tax_to_items(items_8, tax_8_amount, tax_type)
        self._distribute_tax_to_items(items_10, tax_10_amount, tax_type)

        # 税対象額を計算（税抜額）
        total_8 = sum(item["raw_item"]["amount"] or 0 for item in items_8)
        total_10 = sum(item["raw_item"]["amount"] or 0 for item in items_10)

        # 税対象額を返す（内税の場合は税抜額、外税の場合も税抜額）
        if tax_type == "included":
            # 内税：税込額から税額を引いて税抜額を計算
            tax_8_subtotal = total_8 - tax_8_amount if total_8 > 0 else 0
            tax_10_subtotal = total_10 - tax_10_amount if total_10 > 0 else 0
        else:
            # 外税：表示額がそのまま税抜額
            tax_8_subtotal = total_8
            tax_10_subtotal = total_10

        tax_subtotals = {
            "tax_8_subtotal": tax_8_subtotal,
            "tax_10_subtotal": tax_10_subtotal
        }

        return normalized_items, tax_subtotals

    def _distribute_tax_to_items(self, items: List[Dict], total_tax: int, tax_type: str):
        """
        商品データの7要素を設定
        1. 数量
        2. 表示額
        3. 外or内
        4. 税率
        5. 本体価
        6. 税額
        7. 税込価

        Args:
            items: 商品リスト
            total_tax: グループ全体の税額
            tax_type: "included"（内税）or "excluded"（外税）
        """
        if not items:
            return

        # 金額0円の商品（セット内訳行など）を除外
        items_with_amount = [item for item in items if (item["raw_item"].get("amount") or 0) != 0]
        items_zero_amount = [item for item in items if (item["raw_item"].get("amount") or 0) == 0]

        # 金額0円の商品には税額0を設定
        for item in items_zero_amount:
            quantity = item["raw_item"].get("quantity", 1)
            displayed_amount = 0
            item["normalized"]["quantity"] = quantity
            item["normalized"]["displayed_amount"] = displayed_amount
            item["normalized"]["tax_display_type"] = tax_type
            item["normalized"]["base_price"] = 0
            item["normalized"]["tax_amount"] = 0
            item["normalized"]["tax_included_amount"] = 0
            logger.debug(f"Zero-amount item excluded from tax distribution: {item['raw_item'].get('product_name')}")

        # 金額がある商品のみで税額按分を行う
        if not items_with_amount:
            return

        if total_tax == 0:
            # 税額が0の場合も7要素を設定（割引は考慮）
            for item in items_with_amount:
                quantity = item["raw_item"].get("quantity", 1)
                displayed_amount = item["raw_item"].get("amount") or 0
                linked_discount = item["raw_item"].get("linked_discount", 0)

                # 税込価を計算（表示額 + 割引）
                tax_included_amount = displayed_amount + linked_discount

                # 7要素を設定
                item["normalized"]["quantity"] = quantity  # 1. 数量
                item["normalized"]["displayed_amount"] = displayed_amount  # 2. 表示額
                item["normalized"]["tax_display_type"] = tax_type  # 3. 外or内
                # 4. 税率 は _normalize_item で既に設定済み
                item["normalized"]["tax_amount"] = 0  # 6. 税額

                if tax_type == "excluded":
                    # 外税：表示額 = 本体価
                    item["normalized"]["base_price"] = displayed_amount + linked_discount  # 5. 本体価
                    item["normalized"]["tax_included_amount"] = displayed_amount + linked_discount  # 7. 税込価
                else:
                    # 内税：表示額 = 税込額
                    item["normalized"]["tax_included_amount"] = tax_included_amount  # 7. 税込価
                    item["normalized"]["base_price"] = tax_included_amount  # 5. 本体価
            return

        # Step 1: 各商品の税込価を計算（金額がある商品のみ）
        tax_included_amounts = []
        for item in items_with_amount:
            displayed_amount = item["raw_item"].get("amount") or 0
            linked_discount = item["raw_item"].get("linked_discount", 0)
            tax_included_amount = displayed_amount + linked_discount
            tax_included_amounts.append(tax_included_amount)

        # Step 2: 各商品の理論税額を計算（小数のまま保持）
        theoretical_taxes_float = []
        for i, item in enumerate(items_with_amount):
            tax_included_amount = tax_included_amounts[i]
            tax_rate = item["normalized"].get("tax_rate", 10)
            line_type = item["raw_item"].get("line_type", "ITEM")

            # 割引行は税額0（商品行にすでに割引後の税額が含まれているため）
            if line_type == "DISCOUNT":
                theoretical_tax = 0.0
            elif tax_type == "excluded":
                # 外税：理論税額 = 税抜額 × 税率 / 100
                theoretical_tax = tax_included_amount * tax_rate / 100
            else:
                # 内税：理論税額 = 税込価 - (税込価 / (1 + 税率/100))
                theoretical_tax = tax_included_amount - (tax_included_amount / (1 + tax_rate / 100))

            theoretical_taxes_float.append(theoretical_tax)

        # Step 3: 理論税額の合計（小数）とレシート記載税額の差分
        total_theoretical_tax = sum(theoretical_taxes_float)
        remainder = total_tax - total_theoretical_tax

        # Step 4: 各商品の理論税額を四捨五入し、端数を按分
        theoretical_taxes_rounded = [round(tax) for tax in theoretical_taxes_float]
        total_rounded = sum(theoretical_taxes_rounded)
        final_remainder = total_tax - total_rounded

        # Step 5: 最終端数を税込価の大きい順に1円ずつ配分
        distributed_tax = theoretical_taxes_rounded.copy()

        if final_remainder != 0:
            # 税込価の絶対値でソート（インデックスを保持）
            indexed_amounts = [(i, abs(tax_included_amounts[i])) for i in range(len(items_with_amount))]
            indexed_amounts.sort(key=lambda x: x[1], reverse=True)

            # 端数を1円ずつ配分
            for j in range(abs(final_remainder)):
                idx = indexed_amounts[j % len(items_with_amount)][0]
                if final_remainder > 0:
                    distributed_tax[idx] += 1
                else:
                    distributed_tax[idx] -= 1

        # 各商品に7要素を設定（金額がある商品のみ）
        for i, item in enumerate(items_with_amount):
            quantity = item["raw_item"].get("quantity", 1)
            displayed_amount = item["raw_item"].get("amount") or 0
            linked_discount = item["raw_item"].get("linked_discount", 0)  # リンクされた割引（負の値）

            # 税込価を計算（表示額 + 割引）
            # 割引は負の値なので加算すると減算になる
            tax_included_amount = displayed_amount + linked_discount

            # 按分された税額を使用
            tax_amount = distributed_tax[i]

            # 税率から本体価を計算
            tax_rate = item["normalized"].get("tax_rate", 10)
            if tax_type == "excluded":
                # 外税：表示額 = 本体価、税込価 = 本体価 + 税額
                base_price = displayed_amount + linked_discount
                tax_included_amount = base_price + tax_amount
            else:
                # 内税：税込価 - 按分税額 = 本体価
                base_price = tax_included_amount - tax_amount

            # 7要素を設定
            item["normalized"]["quantity"] = quantity  # 1. 数量
            item["normalized"]["displayed_amount"] = displayed_amount  # 2. 表示額
            item["normalized"]["tax_display_type"] = tax_type  # 3. 外or内
            # 4. 税率 は _normalize_item で既に設定済み
            item["normalized"]["base_price"] = base_price  # 5. 本体価
            item["normalized"]["tax_amount"] = tax_amount  # 6. 税額
            item["normalized"]["tax_included_amount"] = tax_included_amount  # 7. 税込価

            if linked_discount != 0:
                logger.info(f"{item['raw_item'].get('product_name')}: 表示額={displayed_amount}, 割引={linked_discount}, 税込価={tax_included_amount}, 本体価={base_price}, 税額={tax_amount}")

        logger.debug(f"Distributed tax ({tax_type})")

    # ========================================
    # マスタデータ読み込み
    # ========================================

    def _load_aliases(self) -> Dict[str, str]:
        """エイリアステーブルを読み込み"""
        result = self.db.client.table("MASTER_Rules_transaction_dict").select("*").execute()
        # product_name → official_name のマッピング
        aliases = {}
        for row in result.data:
            if row.get("product_name") and row.get("official_name"):
                aliases[row["product_name"].lower()] = row["official_name"]
        return aliases

    def _load_product_dictionary(self) -> List[Dict]:
        """商品辞書を読み込み"""
        result = self.db.client.table("MASTER_Product_classify").select("*").execute()
        return result.data

    def _load_situations(self) -> List[Dict]:
        """シチュエーションマスタを読み込み（名目）"""
        result = self.db.client.table("MASTER_Categories_purpose").select("*").execute()
        return result.data

    def _load_categories(self) -> List[Dict]:
        """カテゴリマスタを読み込み（商品カテゴリ）"""
        result = self.db.client.table("MASTER_Categories_product").select("*").execute()
        return result.data
```

## shared/pipeline/stage_hi_combined.py

```python
"""
Stage H+I: Combined Structuring & Synthesis (構造化 + 統合・要約)

【設計 2026-01-24】Stage F → G → H+I の情報進化パイプライン
============================================
役割: Stage G の整理済み出力を受け取り、1回のLLM呼び出しで
      構造化（旧Stage H）と統合・要約（旧Stage I）を同時に実行

入力:
  - unified_text: Stage G で整理されたMarkdown全文
  - source_inventory: REF_ID付きセグメントリスト
  - table_inventory: REF_ID付き表リスト
  - stage_f_structure: Stage F の構造化情報（フォールバック用）

出力:
  - document_date: 基準日付
  - tags: 検索用タグ
  - metadata: 構造化データ（basic_info, articles, weekly_schedule, etc.）
  - title: ドキュメントタイトル
  - summary: 要約
  - calendar_events: カレンダーイベント
  - tasks: タスクリスト
  - audit_canonical_text: 監査用正本テキスト

特徴:
  - 1回のLLM呼び出しでH+Iを実行（コスト削減）
  - REF_IDによる参照追跡
  - 情報の完全維持（1文字も削除しない）
============================================
"""
import re
import json
import json_repair
from typing import Dict, Any, Optional, List
from pathlib import Path
from string import Template
from loguru import logger
from datetime import datetime

from shared.ai.llm_client.llm_client import LLMClient
from .constants import STAGE_H_INPUT_SCHEMA_VERSION


class StageHICombined:
    """Stage H+I: 構造化 + 統合・要約（統合版）"""

    def __init__(self, llm_client: LLMClient):
        """
        Args:
            llm_client: LLMクライアント
        """
        self.llm = llm_client

    def process(
        self,
        file_name: str,
        doc_type: str,
        workspace: str,
        combined_text: str,
        prompt: str,
        model: str,
        stage_f_structure: Optional[Dict[str, Any]] = None,
        stage_g_result: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        構造化 + 統合・要約（統合版）

        Args:
            file_name: ファイル名
            doc_type: ドキュメントタイプ
            workspace: ワークスペース
            combined_text: 統合テキスト（Stage G の unified_text または Stage F の full_text）
            prompt: プロンプト
            model: モデル名
            stage_f_structure: Stage F の構造化情報
            stage_g_result: Stage G の結果（REF_ID付き目録）

        Returns:
            {
                'document_date': str,
                'tags': List[str],
                'metadata': Dict[str, Any],
                'title': str,
                'summary': str,
                'calendar_events': List[Dict],
                'tasks': List[Dict],
                'audit_canonical_text': str
            }
        """
        logger.info(f"[Stage H+I] 構造化+統合開始... (doc_type={doc_type}, model={model})")

        # Stage G の結果があれば使用
        source_inventory = []
        table_inventory = []
        if stage_g_result:
            source_inventory = stage_g_result.get('source_inventory', [])
            table_inventory = stage_g_result.get('table_inventory', [])
            logger.info(f"[Stage H+I] Stage G 結果: source_inventory={len(source_inventory)}, table_inventory={len(table_inventory)}")

        if not combined_text or not combined_text.strip():
            logger.warning("[Stage H+I] 入力テキストが空です")
            return self._get_fallback_result(doc_type)

        try:
            # プロンプト構築
            logger.info("[Stage H+I] プロンプト構築中...")
            full_prompt = self._build_prompt(
                prompt_template=prompt,
                file_name=file_name,
                doc_type=doc_type,
                workspace=workspace,
                combined_text=combined_text,
                source_inventory=source_inventory,
                table_inventory=table_inventory
            )
            logger.info(f"[Stage H+I] プロンプト構築完了 ({len(full_prompt)}文字)")

            # LLM呼び出し
            logger.info(f"[Stage H+I] LLM呼び出し中... (model={model})")
            response = self.llm.call_model(
                tier="default",
                prompt=full_prompt,
                model_name=model
            )
            logger.info(f"[Stage H+I] LLM応答受信: success={response.get('success')}")

            if not response.get("success"):
                logger.error(f"[Stage H+I エラー] LLM呼び出し失敗: {response.get('error')}")
                return self._get_fallback_result(doc_type)

            # JSON抽出
            content = response.get("content", "")
            logger.info(f"[Stage H+I] ===== LLMレスポンス（最初の1000文字）=====\n{content[:1000]}")
            # リトライ禁止（2026-01-28）: エラー時は即座にフォールバック
            result = self._extract_json_with_retry(content, model=model, max_retries=0)

            # Stage F の構造化情報をマージ
            if stage_f_structure:
                result = self._merge_stage_f_structure(result, stage_f_structure)

            # audit_canonical_text の生成（監査用正本）
            audit_canonical_text = self._generate_audit_canonical_text(
                result, combined_text, source_inventory
            )

            # 結果の整形
            final_result = {
                'document_date': result.get('document_date'),
                'tags': result.get('tags', []),
                'metadata': result.get('metadata', {}),
                'title': result.get('title', ''),
                'summary': result.get('summary', ''),
                'calendar_events': result.get('calendar_events', []),
                'tasks': result.get('tasks', []),
                'audit_canonical_text': audit_canonical_text
            }

            logger.info(f"[Stage H+I完了] title={final_result['title'][:50] if final_result['title'] else 'N/A'}...")
            return final_result

        except Exception as e:
            logger.error(f"[Stage H+I エラー] 処理失敗: {e}", exc_info=True)
            return self._get_fallback_result(doc_type)

    def _build_prompt(
        self,
        prompt_template: str,
        file_name: str,
        doc_type: str,
        workspace: str,
        combined_text: str,
        source_inventory: List[Dict],
        table_inventory: List[Dict]
    ) -> str:
        """プロンプトを構築"""
        # source_inventory を簡略化
        inventory_summary = []
        for item in source_inventory[:30]:  # 最大30件
            inventory_summary.append({
                'ref_id': item.get('ref_id'),
                'type': item.get('type'),
                'text': item.get('text', '')[:200]  # 最大200文字
            })

        # table_inventory を簡略化
        tables_summary = []
        for item in table_inventory[:10]:  # 最大10件
            tables_summary.append({
                'ref_id': item.get('ref_id'),
                'table_title': item.get('table_title'),
                'table_type': item.get('table_type')
            })

        # テンプレート変数を置換
        template = Template(prompt_template)
        prompt = template.substitute(
            file_name=file_name,
            doc_type=doc_type,
            workspace=workspace,
            combined_text=combined_text,
            current_date=datetime.now().strftime("%Y-%m-%d"),
            source_inventory_json=json.dumps(inventory_summary, ensure_ascii=False, indent=2),
            table_inventory_json=json.dumps(tables_summary, ensure_ascii=False, indent=2),
            source_count=len(source_inventory),
            table_count=len(table_inventory)
        )

        return prompt

    def _extract_json_with_retry(
        self,
        content: str,
        model: str,
        max_retries: int = 2
    ) -> Dict[str, Any]:
        """JSON抽出（リトライ機能付き）"""
        for attempt in range(max_retries + 1):
            try:
                result = self._extract_json(content)
                logger.debug(f"[Stage H+I] JSON抽出成功 (試行{attempt + 1}/{max_retries + 1})")
                return result

            except Exception as e:
                if attempt < max_retries:
                    logger.warning(f"[Stage H+I] JSON抽出失敗 (試行{attempt + 1}/{max_retries + 1}): {e}")
                    content = self._retry_json_extraction(content, str(e), model)
                else:
                    logger.error(f"[Stage H+I] JSON抽出失敗（最終試行）: {e}")
                    raise

        return {}

    def _extract_json(self, content: str) -> Dict[str, Any]:
        """コンテンツからJSONを抽出"""
        # 複数のパターンでJSONブロックを探す
        patterns = [
            r'```json\s*(.*?)```',
            r'```\s*(.*?)```',
            r'\{[\s\S]*?\}',
        ]

        json_str = None
        for pattern in patterns:
            match = re.search(pattern, content, re.DOTALL)
            if match:
                json_str = match.group(1) if match.lastindex else match.group(0)
                json_str = json_str.strip()
                if json_str.startswith('{'):
                    break
                else:
                    json_str = None

        if not json_str:
            match = re.search(r'\{[\s\S]*\}', content, re.DOTALL)
            if match:
                json_str = match.group(0).strip()
            else:
                json_str = content.strip()

        # JSONパース
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.error(f"[Stage H+I] JSON解析失敗: {e}")
            try:
                return json_repair.loads(json_str)
            except Exception as repair_error:
                logger.error(f"[Stage H+I] JSON修復も失敗: {repair_error}")
                raise

    def _retry_json_extraction(
        self,
        failed_content: str,
        error_message: str,
        model: str
    ) -> str:
        """JSON抽出失敗時、LLMにJSON修正を依頼"""
        prompt = f"""以下のJSONにエラーがあります。修正してください。

エラー: {error_message}

元のJSON:
```
{failed_content[:3000]}
```

修正されたJSONを ```json ブロックで出力してください。
"""

        try:
            response = self.llm.call_model(
                tier="default",
                prompt=prompt,
                model_name=model
            )

            if response.get("success"):
                return response.get("content", "")
            else:
                return failed_content

        except Exception as e:
            logger.error(f"[Stage H+I] JSON修正エラー: {e}")
            return failed_content

    def _merge_stage_f_structure(
        self,
        result: Dict[str, Any],
        stage_f_structure: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Stage F の構造化情報をマージ"""
        metadata = result.get('metadata', {})

        # v1.1契約の判定
        schema_ver = stage_f_structure.get('schema_version', '')
        is_v1_1 = (schema_ver == STAGE_H_INPUT_SCHEMA_VERSION)

        if is_v1_1:
            stage_f_tables = stage_f_structure.get('tables', [])
            stage_f_text_blocks = stage_f_structure.get('text_blocks', [])

            if stage_f_text_blocks:
                metadata['_raw_text_blocks'] = stage_f_text_blocks
                logger.info(f"[Stage H+I] Stage F text_blocks を _raw_text_blocks に保存")

        # visual_elements からデッドライン情報を抽出
        visual_elements = stage_f_structure.get('visual_elements', {})
        if visual_elements:
            deadline_info = visual_elements.get('deadline_info')
            if deadline_info and not result.get('document_date'):
                result['document_date'] = deadline_info

        result['metadata'] = metadata
        return result

    def _generate_audit_canonical_text(
        self,
        result: Dict[str, Any],
        combined_text: str,
        source_inventory: List[Dict]
    ) -> str:
        """
        監査用正本テキストを生成

        Stage H+I の出力から、全情報を含む監査用正本を生成。
        後で人間が検証可能な形式。
        """
        parts = []

        # タイトル
        title = result.get('title', '')
        if title:
            parts.append(f"# {title}\n")

        # 基本情報
        metadata = result.get('metadata', {})
        basic_info = metadata.get('basic_info', {})
        if basic_info:
            parts.append("## 基本情報")
            for key, value in basic_info.items():
                if value:
                    parts.append(f"- {key}: {value}")
            parts.append("")

        # 要約
        summary = result.get('summary', '')
        if summary:
            parts.append("## 要約")
            parts.append(summary)
            parts.append("")

        # 記事・お知らせ
        articles = metadata.get('articles', [])
        if articles:
            parts.append("## 記事・お知らせ")
            for article in articles:
                title_a = article.get('title', '')
                body = article.get('body', '')
                if title_a:
                    parts.append(f"### {title_a}")
                if body:
                    parts.append(body)
                parts.append("")

        # カレンダーイベント
        calendar_events = result.get('calendar_events', [])
        if calendar_events:
            parts.append("## カレンダーイベント")
            for event in calendar_events:
                date = event.get('event_date', '')
                name = event.get('event_name', '')
                parts.append(f"- {date}: {name}")
            parts.append("")

        # タスク
        tasks = result.get('tasks', [])
        if tasks:
            parts.append("## タスク")
            for task in tasks:
                name = task.get('task_name', '')
                deadline = task.get('deadline', '')
                parts.append(f"- {name} (期限: {deadline})")
            parts.append("")

        # 元テキスト参照
        if source_inventory:
            parts.append("## 参照元（REF_ID）")
            for item in source_inventory[:10]:  # 最大10件表示
                ref_id = item.get('ref_id', '')
                text = item.get('text', '')[:100]
                parts.append(f"- {ref_id}: {text}...")
            if len(source_inventory) > 10:
                parts.append(f"  ... 他 {len(source_inventory) - 10} 件")

        return '\n'.join(parts)

    def _get_fallback_result(self, doc_type: str) -> Dict[str, Any]:
        """フォールバック結果を返す"""
        return {
            'document_date': None,
            'tags': [],
            'metadata': {
                'doc_type': doc_type,
                'extraction_failed': True
            },
            'title': '',
            'summary': '',
            'calendar_events': [],
            'tasks': [],
            'audit_canonical_text': ''
        }
```

## shared/pipeline/stage_j_chunking.py

```python
"""
Stage J: Chunking (チャンク化)

メタデータからチャンクを生成
- 役割: 検索用チャンクの作成
- 処理: MetadataChunker でメタデータチャンク生成
"""
from typing import Dict, Any, List
from loguru import logger

from shared.common.processing.metadata_chunker import MetadataChunker


class StageJChunking:
    """Stage J: チャンク化"""

    def __init__(self):
        """初期化"""
        self.chunker = MetadataChunker()

    def create_chunks(
        self,
        display_subject: str,
        summary: str,
        tags: List[str],
        document_date: str,
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        メタデータからチャンクを生成

        Args:
            display_subject: 件名/ファイル名
            summary: 要約
            tags: タグリスト
            document_date: ドキュメント日付
            metadata: 構造化メタデータ

        Returns:
            チャンクリスト [
                {
                    'chunk_text': str,
                    'chunk_type': str,
                    'search_weight': float
                },
                ...
            ]
        """
        logger.info("[Stage J] チャンク化開始...")

        try:
            # metadata から構造化データを展開
            document_data = {
                'file_name': display_subject,
                'summary': summary,
                'tags': tags,
                'document_date': document_date,
                # metadata の中身を直接展開
                'persons': metadata.get('persons', []) if isinstance(metadata, dict) else [],
                'organizations': metadata.get('organizations', []) if isinstance(metadata, dict) else [],
                'people': metadata.get('people', []) if isinstance(metadata, dict) else [],
                # Stage H の構造化データを追加
                'text_blocks': metadata.get('text_blocks', []) if isinstance(metadata, dict) else [],
                'structured_tables': metadata.get('structured_tables', []) if isinstance(metadata, dict) else [],
                'weekly_schedule': metadata.get('weekly_schedule', []) if isinstance(metadata, dict) else [],
                'other_text': metadata.get('other_text', []) if isinstance(metadata, dict) else [],
                # Stage I の抽出データを追加
                'calendar_events': metadata.get('calendar_events', []) if isinstance(metadata, dict) else [],
                'tasks': metadata.get('tasks', []) if isinstance(metadata, dict) else [],
                # basic_info も展開（あれば）
                'doc_type': metadata.get('basic_info', {}).get('related_class', '') if isinstance(metadata, dict) else ''
            }

            chunks = self.chunker.create_metadata_chunks(document_data)

            logger.info(f"[Stage J完了] チャンク数: {len(chunks)}")

            return chunks

        except Exception as e:
            logger.error(f"[Stage J エラー] チャンク化失敗: {e}", exc_info=True)
            return []

    def process(
        self,
        display_subject: str,
        summary: str,
        tags: List[str],
        document_date: Any,
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        チャンク生成（process() エイリアス）

        Args:
            display_subject: 表示件名
            summary: 要約
            tags: タグ
            document_date: ドキュメント日付
            metadata: メタデータ

        Returns:
            chunks: チャンクリスト
        """
        return self.create_chunks(
            display_subject=display_subject,
            summary=summary,
            tags=tags,
            document_date=document_date,
            metadata=metadata
        )
```

## shared/pipeline/stage_k_embedding.py

```python
"""
Stage K: Embedding (ベクトル化)

チャンクをベクトル化して search_index に保存
- 役割: チャンクをベクトル化
- モデル: OpenAI text-embedding-3-small (1536次元)
"""
from typing import Dict, Any, List
from loguru import logger

from shared.ai.llm_client.llm_client import LLMClient
from shared.common.database.client import DatabaseClient


class StageKEmbedding:
    """Stage K: ベクトル化"""

    def __init__(self, llm_client: LLMClient, db_client: DatabaseClient):
        """
        Args:
            llm_client: LLMクライアント
            db_client: データベースクライアント
        """
        self.llm_client = llm_client
        self.db = db_client

    def embed_and_save(
        self,
        document_id: str,
        chunks: List[Dict[str, Any]],
        delete_existing: bool = False,
        owner_id: str = None
    ) -> Dict[str, Any]:
        """
        チャンクをベクトル化して search_index に保存

        Args:
            document_id: ドキュメントID
            chunks: チャンクリスト
            delete_existing: 既存のチャンクを削除するか
            owner_id: オーナーID（省略時は親ドキュメントから継承）

        Returns:
            {
                'success': bool,
                'saved_count': int,
                'failed_count': int
            }
        """
        logger.info("[Stage K] ベクトル化 + search_index保存開始...")

        # Phase 3: owner_id を取得（指定がない場合は親ドキュメントから継承）
        if owner_id is None:
            try:
                parent_doc = self.db.client.table('Rawdata_FILE_AND_MAIL')\
                    .select('owner_id')\
                    .eq('id', document_id)\
                    .execute()
                if parent_doc.data:
                    owner_id = parent_doc.data[0].get('owner_id')
                    logger.debug(f"[Stage K] 親ドキュメントから owner_id 継承: {owner_id}")
            except Exception as e:
                logger.warning(f"[Stage K 警告] 親ドキュメントの owner_id 取得エラー: {e}")

        if owner_id is None:
            logger.error(f"[Stage K エラー] owner_id が取得できません: document_id={document_id}")
            return {
                'success': False,
                'saved_count': 0,
                'failed_count': len(chunks),
                'error': 'owner_id is required but not available'
            }

        # 既存ドキュメントの場合は、古いチャンクを削除
        if delete_existing:
            try:
                logger.info(f"[Stage K] 既存チャンク削除: document_id={document_id}")
                self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
            except Exception as e:
                logger.warning(f"[Stage K 警告] 既存チャンク削除エラー（継続）: {e}")

        saved_count = 0
        failed_count = 0

        for chunk in chunks:
            try:
                # null文字を除去
                chunk_text = chunk['chunk_text'].replace('\u0000', '') if chunk['chunk_text'] else ''

                # Embedding生成
                embedding = self.llm_client.generate_embedding(chunk_text)

                # search_indexに保存
                chunk_data = {
                    'document_id': document_id,
                    'owner_id': owner_id,  # Phase 3: 親ドキュメントから継承
                    'chunk_content': chunk_text,
                    'chunk_size': len(chunk_text),
                    'chunk_type': chunk['chunk_type'],
                    'embedding': embedding,
                    'search_weight': chunk.get('search_weight', 1.0),
                    'chunk_index': chunk.get('chunk_index', 0),
                    'chunk_metadata': chunk.get('metadata')  # 構造化データを保存
                }

                self.db.client.table('10_ix_search_index').insert(chunk_data).execute()
                saved_count += 1

            except Exception as e:
                logger.error(f"[Stage K エラー] チャンク保存失敗: {e}")
                failed_count += 1

        logger.info(f"[Stage K完了] {saved_count}/{len(chunks)}チャンクを保存 (失敗: {failed_count})")

        # chunk_countを更新
        if saved_count > 0:
            try:
                self.db.client.table('Rawdata_FILE_AND_MAIL')\
                    .update({'chunk_count': saved_count})\
                    .eq('id', document_id)\
                    .execute()
                logger.debug(f"[Stage K] chunk_count更新: {saved_count}個")
            except Exception as e:
                logger.warning(f"[Stage K 警告] chunk_count更新エラー（継続）: {e}")

        # 成功条件: 最低1チャンク以上保存 & 失敗なし
        is_success = saved_count > 0 and failed_count == 0

        return {
            'success': is_success,
            'saved_count': saved_count,
            'failed_count': failed_count
        }

    def process(self, chunks: List[Dict[str, Any]], document_id: str) -> None:
        """
        チャンクをベクトル化して保存（process() エイリアス）

        Args:
            chunks: チャンクリスト
            document_id: ドキュメントID
        """
        self.embed_and_save(document_id, chunks)
```

## shared/pipeline/utils/__init__.py

```python
"""
Pipeline Utilities
"""
from .table_parser import recompose_columnar_data

__all__ = ['recompose_columnar_data']
```

## shared/pipeline/utils/table_parser.py

```python
"""
Table Parser Utilities

カラムナ形式（columns + rows）を辞書リスト形式に復元するユーティリティ
Stage F → Stage H1 間のデータ変換に使用
"""
from typing import Any, Dict, List, Union


def recompose_columnar_data(columnar_json: Union[Dict, List, Any]) -> List[Dict]:
    """
    軽量なカラムナ形式を通常の辞書リストに復元する

    Args:
        columnar_json: カラムナ形式のJSON
            {"columns": ["A", "B"], "rows": [["v1", "v2"], ["v3", "v4"]]}

    Returns:
        辞書リスト形式
            [{"A": "v1", "B": "v2"}, {"A": "v3", "B": "v4"}]

    Examples:
        >>> recompose_columnar_data({"columns": ["順位", "氏名"], "rows": [[1, "山田"], [2, "田中"]]})
        [{"順位": 1, "氏名": "山田"}, {"順位": 2, "氏名": "田中"}]

        >>> recompose_columnar_data([{"A": 1}])  # 既に辞書リスト形式
        [{"A": 1}]
    """
    # None や空の場合
    if not columnar_json:
        return []

    # 既に辞書リスト形式の場合はそのまま返す
    if isinstance(columnar_json, list):
        return columnar_json

    # 辞書でない場合
    if not isinstance(columnar_json, dict):
        return []

    # columns と rows を取得
    cols = columnar_json.get("columns", [])
    rows = columnar_json.get("rows", [])

    # columns がない場合（カラムナ形式ではない）
    if not cols:
        # headers + rows 形式の可能性をチェック
        headers = columnar_json.get("headers", [])
        if headers and rows:
            cols = headers
        else:
            return []

    # rows がない場合
    if not rows:
        return []

    # 辞書リストに変換
    result = []
    for row in rows:
        if isinstance(row, list):
            # 行の要素数が columns より少ない場合は空文字で補完
            row_dict = {}
            for i, col in enumerate(cols):
                row_dict[col] = row[i] if i < len(row) else ""
            result.append(row_dict)
        elif isinstance(row, dict):
            # 既に辞書形式の行はそのまま
            result.append(row)

    return result


def is_columnar_format(data: Any) -> bool:
    """
    データがカラムナ形式かどうかを判定

    Args:
        data: 判定対象のデータ

    Returns:
        カラムナ形式なら True
    """
    if not isinstance(data, dict):
        return False

    has_columns = "columns" in data and isinstance(data["columns"], list)
    has_rows = "rows" in data and isinstance(data["rows"], list)

    # rows の最初の要素が list であることを確認（辞書リストではない）
    if has_columns and has_rows and data["rows"]:
        first_row = data["rows"][0]
        return isinstance(first_row, list)

    return has_columns and has_rows


def extract_table_text_for_removal(table: Dict) -> List[str]:
    """
    表データからH2で削除すべきテキスト断片を抽出

    H1で処理した表の内容がH2のテキストに重複して含まれている場合、
    そのテキストを削除するための断片リストを生成

    Args:
        table: 表データ（columns/rows または headers/rows 形式）

    Returns:
        削除対象のテキスト断片リスト
    """
    fragments = []

    # テーブルタイトル
    title = table.get("table_title", "")
    if title:
        fragments.append(title)

    # columns/headers
    cols = table.get("columns", []) or table.get("headers", [])
    if cols:
        # ヘッダー行全体
        fragments.append(" ".join(str(c) for c in cols))

    # rows
    rows = table.get("rows", [])
    for row in rows:
        if isinstance(row, list):
            # 行全体のテキスト
            row_text = " ".join(str(cell) for cell in row)
            if len(row_text) > 5:  # 短すぎる断片は除外
                fragments.append(row_text)
            # 各セルの値（長いもののみ）
            for cell in row:
                cell_str = str(cell)
                if len(cell_str) > 10:
                    fragments.append(cell_str)
        elif isinstance(row, dict):
            # 辞書形式の行
            row_text = " ".join(str(v) for v in row.values())
            if len(row_text) > 5:
                fragments.append(row_text)

    return fragments
```

## shared/ai/__init__.py

```python

```

## shared/ai/embeddings/__init__.py

```python

```

## shared/ai/embeddings/embeddings.py

```python
"""
Embedding Client (DEPRECATED - OpenAI text-embedding-3-small を使用してください)
このクラスは後方互換性のために残されていますが、使用は推奨されません。
代わりに LLMClient.generate_embedding() を使用してください。
"""
from typing import List, Optional
from openai import OpenAI
from shared.common.config.settings import settings


class EmbeddingClient:
    """
    OpenAI text-embedding-3-small を使用したEmbedding生成クライアント (1536次元)

    注意: このクラスは非推奨です。LLMClient.generate_embedding() を使用してください。
    """

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = ***REDACTED*** or settings.OPENAI_API_KEY
        if not self.api_key:
            raise ValueError("OpenAI API Key が設定されていません")

        self.client = OpenAI(api_key=***REDACTED***
        self.model_name = "text-embedding-3-small"
        self.dimensions = 1536

    def generate_embedding(self, text: str, task_type: str = "RETRIEVAL_DOCUMENT") -> List[float]:
        """
        Embeddingを生成 (1536次元)

        注意: task_type パラメータは互換性のために残されていますが、使用されません
        """
        if not text or not text.strip():
            raise ValueError("空のテキストはembedding化できません")

        response = self.client.embeddings.create(
            model=self.model_name,
            input=text,
            dimensions=self.dimensions
        )

        return response.data[0].embedding

    def generate_embeddings_batch(self, texts: List[str], task_type: str = "RETRIEVAL_DOCUMENT") -> List[List[float]]:
        """バッチでEmbeddingを生成 (1536次元)"""
        if not texts:
            return []

        embeddings = []
        for text in texts:
            if text and text.strip():
                embedding = self.generate_embedding(text, task_type)
                embeddings.append(embedding)
            else:
                # OpenAI text-embedding-3-smallは1536次元
                embeddings.append([0.0] * 1536)

        return embeddings

    def generate_query_embedding(self, query: str) -> List[float]:
        """クエリ用のEmbeddingを生成 (1536次元)"""
        return self.generate_embedding(query, task_type="RETRIEVAL_QUERY")
```

## shared/ai/llm_client/__init__.py

```python

```

## shared/ai/llm_client/exceptions.py

```python
"""
LLMクライアントのカスタム例外
"""


class MaxTokensExceededError(Exception):
    """max_tokens上限に達して出力が途中で切れた場合のエラー"""

    def __init__(self, message: str, partial_output: str, finish_reason_name: str):
        """
        Args:
            message: エラーメッセージ
            partial_output: 途中で切れた出力テキスト
            finish_reason_name: finish_reasonの名前
        """
        super().__init__(message)
        self.partial_output = partial_output
        self.finish_reason_name = finish_reason_name
```

## shared/ai/llm_client/llm_client.py

```python
"""
LLMクライアント（v3.0: マルチプロバイダ対応）
Gemini / Anthropic / OpenAI を統一インターフェースで利用
"""

import os
import base64
from typing import Dict, List, Any, Optional, Union
from pathlib import Path
import mimetypes

import google.generativeai as genai
from anthropic import Anthropic, RateLimitError
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, RetryError
from loguru import logger

from shared.common.config.model_tiers import AIProvider, get_model_config
from shared.common.config.settings import settings
from .exceptions import MaxTokensExceededError

class LLMClient:
    """統合LLMクライアント"""
    
    def __init__(self):
        """設定からAPIキーを取得し、各プロバイダーを初期化"""

        # Settings経由でAPIキーを取得（環境変数管理の統一）
        self.gemini_api_key = ***REDACTED*** or os.getenv("GOOGLE_API_KEY")  # 後方互換性のためGOOGLE_API_KEYもサポート
        self.anthropic_api_key = ***REDACTED***
        self.openai_api_key = ***REDACTED***

        # Gemini設定 (トップレベル関数のみ使用)
        if self.gemini_api_key:
            genai.configure(api_key=***REDACTED***
        else:
            pass

        # Anthropic設定
        if self.anthropic_api_key:
            self.anthropic_client = Anthropic(api_key=***REDACTED***
        else:
            self.anthropic_client = None

        # OpenAI設定
        if self.openai_api_key:
            self.openai_client = OpenAI(api_key=***REDACTED***
        else:
            self.openai_client = None
    
    def generate_with_images(
        self,
        prompt: str,
        image_data: Union[str, List[str]],
        model: str = "gemini-2.5-flash-lite",
        temperature: float = 0.0,
        max_tokens: int = 8192
    ) -> str:
        """
        画像データを使ってGemini Vision APIを呼び出し

        Args:
            prompt: プロンプト
            image_data: Base64エンコードされた画像データ（単一または複数）
            model: モデル名
            temperature: 温度パラメータ
            max_tokens: 最大トークン数

        Returns:
            生成されたテキスト
        """
        if not self.gemini_api_key:
            raise ValueError("Gemini API key is missing")

        try:
            model_obj = genai.GenerativeModel(model)

            # 画像データをリスト化
            if isinstance(image_data, str):
                image_data_list = [image_data]
            else:
                image_data_list = image_data

            # コンテンツパーツを構築
            content_parts = [prompt]

            # 画像を追加
            for img_base64 in image_data_list:
                # Base64をバイトにデコード
                img_bytes = base64.b64decode(img_base64)

                # Geminiの画像形式に変換
                image_part = {
                    'mime_type': 'image/png',
                    'data': img_bytes
                }
                content_parts.append(image_part)

            # 安全フィルター設定
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]

            # APIを呼び出し
            response = model_obj.generate_content(
                content_parts,
                generation_config=genai.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=temperature
                ),
                safety_settings=safety_settings
            )

            # レスポンスの検証
            if not response.candidates:
                raise ValueError("Gemini returned no candidates")

            candidate = response.candidates[0]

            # finish_reason をチェック
            if candidate.finish_reason != 1:  # 1 = STOP (正常終了)
                finish_reason_name = candidate.finish_reason.name if hasattr(candidate.finish_reason, 'name') else str(candidate.finish_reason)
                error_details = {
                    "finish_reason": candidate.finish_reason,
                    "finish_reason_name": finish_reason_name
                }
                if hasattr(candidate, 'safety_ratings') and candidate.safety_ratings:
                    error_details["safety_ratings"] = [
                        {
                            "category": rating.category.name if hasattr(rating.category, 'name') else str(rating.category),
                            "probability": rating.probability.name if hasattr(rating.probability, 'name') else str(rating.probability)
                        }
                        for rating in candidate.safety_ratings
                    ]
                logger.error(f"Gemini Vision失敗: {error_details}")
                raise ValueError(f"Gemini finish_reason: {finish_reason_name} ({candidate.finish_reason})")

            # テキストを取得
            text_content = candidate.content.parts[0].text if candidate.content.parts else ""
            return text_content

        except Exception as e:
            logger.error(f"Gemini Vision API エラー: {e}")
            raise

    def call_model(
        self,
        tier: str,
        prompt: str,
        file_path: Optional[Path] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        指定されたタスクに最適なモデルを呼び出し

        Args:
            tier: モデル階層("stagea_classification", "stageh_extraction", "ui_response")
            prompt: プロンプト
            file_path: ファイルパス (GeminiのStage A分類用)
            **kwargs: 追加パラメータ

        Returns:
            モデルレスポンス
        """
        config = get_model_config(tier)
        provider = config["provider"]
        # kwargsからmodel_nameが渡されていればそれを優先、なければconfigから取得
        model_name = kwargs.pop('model_name', None) or config["model"]

        # モデル名からプロバイダーを自動判定（明示的なmodel_name指定時）
        if model_name:
            if 'claude' in model_name.lower():
                provider = AIProvider.CLAUDE
            elif 'gemini' in model_name.lower():
                provider = AIProvider.GEMINI
            elif 'gpt' in model_name.lower() or 'text-embedding' in model_name.lower():
                provider = AIProvider.OPENAI

        if provider == AIProvider.GEMINI:
            if not self.gemini_api_key:
                return {"success": False, "error": "Gemini API key is missing", "model": model_name}
            return self._call_gemini(model_name, prompt, file_path, config, **kwargs)

        elif provider == AIProvider.CLAUDE:
            if not self.anthropic_client:
                return {"success": False, "error": "Anthropic API key is missing", "model": model_name}
            try:
                return self._call_claude(model_name, prompt, config, **kwargs)
            except RetryError as e:
                # リトライが全て失敗した場合
                original_error = e.last_attempt.exception()
                return {"success": False, "error": str(original_error), "model": model_name, "provider": "claude"}

        elif provider == AIProvider.OPENAI:
            if not self.openai_client:
                return {"success": False, "error": "OpenAI API key is missing", "model": model_name}
            return self._call_openai(model_name, prompt, config, **kwargs)

        else:
            raise ValueError(f"未対応のプロバイダー: {provider}")

    def _call_gemini(
        self,
        model_name: str,
        prompt: str,
        file_path: Optional[Path],
        config: Dict,
        **kwargs
    ) -> Dict[str, Any]:
        """Gemini API呼び出し（トップレベル関数のみ使用）"""
        uploaded_file = None
        try:
            model = genai.GenerativeModel(model_name)

            content_parts = [prompt]

            if file_path and file_path.exists():
                # MIMEタイプを自動判定
                mime_type, _ = mimetypes.guess_type(str(file_path))
                if not mime_type:
                    mime_type = "application/pdf"  # デフォルト

                # ファイルをアップロード（トップレベル関数のみ使用）
                uploaded_file = genai.upload_file(path=str(file_path), mime_type=mime_type)
                content_parts.append(uploaded_file)

            # 安全フィルター設定（finish_reason: 2 対策）
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]

            # 生成設定
            generation_config = genai.GenerationConfig(
                max_output_tokens=config.get("max_tokens", 65536),
                temperature=config.get("temperature", 0.1)
            )

            # response_format が kwargs に含まれている場合
            response_format = kwargs.get('response_format')
            if response_format in ["json", "json_object"]:
                generation_config.response_mime_type = "application/json"

            response = model.generate_content(
                content_parts,
                generation_config=generation_config,
                safety_settings=safety_settings
            )

            # レスポンスの検証
            if not response.candidates:
                self._cleanup_uploaded_file(uploaded_file)
                return {"success": False, "error": "Gemini returned no candidates", "model": model_name, "provider": "gemini"}

            candidate = response.candidates[0]

            # finish_reason をチェック
            if candidate.finish_reason != 1:  # 1 = STOP (正常終了)
                self._cleanup_uploaded_file(uploaded_file)
                # 詳細なエラー情報を取得
                error_details = {
                    "finish_reason": candidate.finish_reason,
                    "finish_reason_name": candidate.finish_reason.name if hasattr(candidate.finish_reason, 'name') else str(candidate.finish_reason)
                }
                # safety_ratingsがあれば追加
                if hasattr(candidate, 'safety_ratings') and candidate.safety_ratings:
                    error_details["safety_ratings"] = [
                        {
                            "category": rating.category.name if hasattr(rating.category, 'name') else str(rating.category),
                            "probability": rating.probability.name if hasattr(rating.probability, 'name') else str(rating.probability)
                        }
                        for rating in candidate.safety_ratings
                    ]
                logger.error(f"Gemini Vision失敗: {error_details}")
                return {
                    "success": False,
                    "error": f"Gemini finish_reason: {candidate.finish_reason}",
                    "error_details": error_details,
                    "model": model_name,
                    "provider": "gemini"
                }

            # テキストを取得
            text_content = candidate.content.parts[0].text if candidate.content.parts else ""

            # トークン使用量を取得
            usage = {}
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                usage = {
                    "prompt_tokens": getattr(response.usage_metadata, 'prompt_token_count', 0),
                    "completion_tokens": getattr(response.usage_metadata, 'candidates_token_count', 0),
                    "total_tokens": getattr(response.usage_metadata, 'total_token_count', 0)
                }
                logger.info(f"[Gemini] トークン使用量: prompt={usage['prompt_tokens']}, completion={usage['completion_tokens']}, total={usage['total_tokens']}")

            # ファイルを削除
            self._cleanup_uploaded_file(uploaded_file)

            return {
                "success": True,
                "content": text_content,
                "model": model_name,
                "provider": "gemini",
                "usage": usage
            }

        except Exception as e:
            self._cleanup_uploaded_file(uploaded_file)
            return {"success": False, "error": str(e), "model": model_name, "provider": "gemini"}

    def _cleanup_uploaded_file(self, uploaded_file) -> None:
        """
        アップロードされたファイルを削除（トップレベル関数のみ使用）

        Args:
            uploaded_file: アップロードされたファイルオブジェクト
        """
        if not uploaded_file:
            return

        try:
            genai.delete_file(name=uploaded_file.name)
        except Exception:
            # 削除に失敗しても処理は継続
            pass

    @retry(
        retry=retry_if_exception_type(RateLimitError),
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=2, max=60)
    )
    def _call_claude(
        self,
        model_name: str,
        prompt: str,
        config: Dict,
        **kwargs
    ) -> Dict[str, Any]:
        """Anthropic API呼び出し"""
        try:
            # ✅ DEBUG: 送信するプロンプトの先頭部分をログに出力
            from loguru import logger
            logger.debug(f"[Anthropic CALL] Model: {model_name}, Prompt start: {prompt[:300]}...")

            # Anthropicの最大トークン数制限を適用
            max_tokens = config.get("max_tokens", 8192)
            # Anthropic models: max 64000 tokens
            if max_tokens > 64000:
                max_tokens = 64000

            response = self.anthropic_client.messages.create(
                model=model_name,
                max_tokens=max_tokens,
                temperature=config.get("temperature", 0.0),
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )

            # ✅ DEBUG: Anthropic からの生の応答コンテンツ全体をログに出力
            raw_content = response.content[0].text
            logger.debug(f"[Anthropic RAW RESP] Content length: {len(raw_content)} chars")
            # 応答が長すぎる場合があるため、先頭2000文字のみをログに記録
            logger.debug(f"[Anthropic RAW RESP] Content preview: {raw_content[:2000]}")

            # トークン使用量を取得
            usage = {}
            if hasattr(response, 'usage') and response.usage:
                usage = {
                    "prompt_tokens": getattr(response.usage, 'input_tokens', 0),
                    "completion_tokens": getattr(response.usage, 'output_tokens', 0),
                    "total_tokens": getattr(response.usage, 'input_tokens', 0) + getattr(response.usage, 'output_tokens', 0)
                }
                logger.info(f"[Anthropic] トークン使用量: prompt={usage['prompt_tokens']}, completion={usage['completion_tokens']}, total={usage['total_tokens']}")

            return {
                "success": True,
                "content": raw_content,
                "model": model_name,
                "provider": "claude",
                "usage": usage
            }

        except RateLimitError:
            # RateLimitErrorは再スローしてtenacityにリトライさせる
            raise
        except Exception as e:
            return {"success": False, "error": str(e), "model": model_name, "provider": "claude"}

    def _call_openai(
        self,
        model_name: str,
        prompt: str,
        config: Dict,
        **kwargs
    ) -> Dict[str, Any]:
        """OpenAI API呼び出し"""
        try:
            # ✅ GPT-5.1では max_completion_tokens を使用、旧モデルでは max_tokens（後方互換性）
            max_completion_tokens = config.get("max_completion_tokens")
            max_tokens = config.get("max_tokens", 16384)

            # パラメータを動的に構築
            api_params = {
                "model": model_name,
                "messages": [
                    {"role": "user", "content": prompt}
                ]
            }

            # temperatureはGPT-5.1などの一部モデルでサポートされていないため、configに含まれている場合のみ設定
            if "temperature" in config:
                api_params["temperature"] = config["temperature"]

            # max_completion_tokens が設定されていればそれを使用、なければ max_tokens
            if max_completion_tokens:
                api_params["max_completion_tokens"] = max_completion_tokens
            else:
                api_params["max_tokens"] = max_tokens

            response = self.openai_client.chat.completions.create(**api_params)

            # トークン使用量を取得
            usage = {}
            if hasattr(response, 'usage') and response.usage:
                usage = {
                    "prompt_tokens": getattr(response.usage, 'prompt_tokens', 0),
                    "completion_tokens": getattr(response.usage, 'completion_tokens', 0),
                    "total_tokens": getattr(response.usage, 'total_tokens', 0)
                }
                logger.info(f"[OpenAI] トークン使用量: prompt={usage['prompt_tokens']}, completion={usage['completion_tokens']}, total={usage['total_tokens']}")

            return {
                "success": True,
                "content": response.choices[0].message.content,
                "model": model_name,
                "provider": "openai",
                "usage": usage
            }

        except Exception as e:
            return {"success": False, "error": str(e), "model": model_name, "provider": "openai"}

    def generate_embedding(self, text: str) -> List[float]:
        """
        Embedding生成

        Args:
            text: Embeddingを生成するテキスト

        Returns:
            1536次元のembeddingベクトル
        """
        config = get_model_config("embeddings")

        if not self.openai_client:
            raise ConnectionError("OpenAI client not initialized for embedding generation.")

        # text-embedding-3-smallモデルで1536次元を明示的に指定
        response = self.openai_client.embeddings.create(
            model=config["model"],
            input=text,
            dimensions=config.get("dimensions", 1536)  # デフォルト1536次元
        )

        return response.data[0].embedding

    def generate_with_vision(
        self,
        prompt: str,
        image_path: str,
        model: str = "gemini-2.0-flash-exp",
        temperature: float = 0.0,
        max_tokens: int = 65536,
        response_format: Optional[str] = None
    ) -> str:
        """
        画像ファイルを使ってGemini Vision APIを呼び出し

        Args:
            prompt: プロンプト
            image_path: 画像ファイルのパス（PNG, JPEG等）
            model: モデル名
            temperature: 温度パラメータ
            max_tokens: 最大トークン数
            response_format: レスポンスフォーマット（"json", "json_object" など）

        Returns:
            生成されたテキスト

        Raises:
            ValueError: APIキーがない、またはレスポンスが不正な場合
            Exception: その他のエラー
        """
        if not self.gemini_api_key:
            raise ValueError("Gemini API key is missing")

        try:
            model_obj = genai.GenerativeModel(model)

            # ファイルをアップロード
            mime_type, _ = mimetypes.guess_type(image_path)
            if not mime_type:
                mime_type = "image/jpeg"  # デフォルト

            uploaded_file = genai.upload_file(path=image_path, mime_type=mime_type)

            # コンテンツパーツを構築
            content_parts = [prompt, uploaded_file]

            # 安全フィルター設定
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]

            # 生成設定【Ver 5.7】蛇口全開固定
            # 引数に依存せず、物理的に65536を強制
            HARDCODED_MAX_TOKENS = 65536
            generation_config = genai.GenerationConfig(
                max_output_tokens=HARDCODED_MAX_TOKENS,
                temperature=temperature
            )
            logger.info(f"[Gemini Vision] max_output_tokens={HARDCODED_MAX_TOKENS} (ハードコード固定)")

            # response_format が指定されている場合
            if response_format in ["json", "json_object"]:
                generation_config.response_mime_type = "application/json"

            # APIを呼び出し（タイムアウト5分、1回リトライ）
            max_retries = 1
            last_error = None

            for attempt in range(max_retries + 1):  # 0回目（初回）+ 1回（リトライ）
                try:
                    logger.info(f"[Gemini Vision] API呼び出し試行 {attempt + 1}/{max_retries + 1}")
                    response = model_obj.generate_content(
                        content_parts,
                        generation_config=generation_config,
                        safety_settings=safety_settings,
                        request_options={"timeout": 300}  # 5分タイムアウト
                    )
                    break  # 成功したらループを抜ける
                except Exception as e:
                    last_error = e
                    error_str = str(e).lower()
                    # タイムアウトまたはネットワークエラーの場合
                    if any(keyword in error_str for keyword in ['timeout', 'deadline', 'network', 'connection']):
                        if attempt < max_retries:
                            logger.warning(f"[Gemini Vision] タイムアウト/ネットワークエラー。リトライします（試行 {attempt + 1}/{max_retries + 1}）: {e}")
                            continue
                        else:
                            logger.error(f"[Gemini Vision] タイムアウト/ネットワークエラー。リトライ上限に達しました: {e}")
                            raise
                    else:
                        # タイムアウト以外のエラーは即座に失敗
                        logger.error(f"[Gemini Vision] API エラー（リトライ不可）: {e}")
                        raise
            else:
                # ループが最後まで実行された（全てのリトライが失敗）
                if last_error:
                    raise last_error

            # アップロードファイルを削除
            try:
                genai.delete_file(name=uploaded_file.name)
            except Exception:
                pass

            # レスポンスの検証
            if not response.candidates:
                raise ValueError("Gemini returned no candidates")

            candidate = response.candidates[0]

            # finish_reason をチェック
            finish_reason_name = candidate.finish_reason.name if hasattr(candidate.finish_reason, 'name') else str(candidate.finish_reason)
            logger.info(f"[Gemini Vision] finish_reason: {finish_reason_name} ({candidate.finish_reason})")

            # テキストを取得（finish_reasonに関わらず取得）
            text_content = candidate.content.parts[0].text if candidate.content.parts else ""

            # finish_reason == 2 (MAX_TOKENS): トークン上限に達した場合
            # 注意: Gemini APIでは MAX_TOKENS = 2（3ではない）
            if candidate.finish_reason == 2:
                error_msg = f"MAX_TOKENS上限に達しました。出力が途中で切れています。({len(text_content)}文字)"
                logger.error(f"[Gemini Vision] {error_msg}")
                logger.error(f"[Gemini Vision] 途中で切れた出力（最後の500文字）: {text_content[-500:]}")
                raise MaxTokensExceededError(
                    message=error_msg,
                    partial_output=text_content,
                    finish_reason_name=finish_reason_name
                )

            # finish_reason != 1 (STOP以外のその他のエラー)
            if candidate.finish_reason != 1:  # 1 = STOP (正常終了)
                error_details = {
                    "finish_reason": candidate.finish_reason,
                    "finish_reason_name": finish_reason_name
                }
                if hasattr(candidate, 'safety_ratings') and candidate.safety_ratings:
                    error_details["safety_ratings"] = [
                        {
                            "category": rating.category.name if hasattr(rating.category, 'name') else str(rating.category),
                            "probability": rating.probability.name if hasattr(rating.probability, 'name') else str(rating.probability)
                        }
                        for rating in candidate.safety_ratings
                    ]
                logger.error(f"Gemini Vision失敗: {error_details}")
                raise ValueError(f"Gemini finish_reason: {finish_reason_name} ({candidate.finish_reason})")

            # テキスト長とトークン使用量をログ出力
            logger.info(f"[Gemini Vision] 応答テキスト長: {len(text_content)}文字")

            # トークン使用量を取得・保存
            self.last_usage = {}
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                self.last_usage = {
                    "prompt_tokens": getattr(response.usage_metadata, 'prompt_token_count', 0),
                    "completion_tokens": getattr(response.usage_metadata, 'candidates_token_count', 0),
                    "total_tokens": getattr(response.usage_metadata, 'total_token_count', 0),
                    "model": model
                }
                logger.info(f"[Gemini Vision] トークン使用量: prompt={self.last_usage['prompt_tokens']}, completion={self.last_usage['completion_tokens']}, total={self.last_usage['total_tokens']}")

            return text_content

        except Exception as e:
            logger.error(f"Gemini Vision API エラー: {e}")
            raise

    def transcribe_image(
        self,
        image_path: Path,
        prompt: str = "この画像内の表組みやリストを、Markdown形式で正確に書き起こしてください。",
        model: str = "gemini-2.5-pro"
    ) -> Dict[str, Any]:
        """
        画像ファイルをGemini Visionで文字起こし

        Args:
            image_path: 画像ファイルのパス（PNG, JPEG等）
            prompt: Geminiに送るプロンプト
            model: 使用するGeminiモデル（デフォルト: gemini-2.5-pro）

        Returns:
            {"success": bool, "content": str, "model": str, "provider": str}
        """
        if not self.gemini_api_key:
            return {"success": False, "error": "Gemini API key is missing", "model": "gemini-2.5-flash"}

        # 指定されたGeminiモデルを使用
        return self._call_gemini(
            model_name=model,
            prompt=prompt,
            file_path=image_path,
            config={
                "max_tokens": 65536,  # Gemini 2.5の最大出力トークン数（65,536）
                "temperature": 0.0
            }
        )
```

## shared/__init__.py

```python
# shared package
```

## pyproject.toml

```python
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "document-management-system"
version = "1.0.0"
description = "Document Management System"
requires-python = ">=3.10"

[tool.setuptools.packages.find]
where = ["."]
include = ["shared*", "scripts*", "services*", "frontend*", "database*"]

[tool.setuptools.package-data]
"*" = ["*.json", "*.yaml", "*.yml", "*.html", "*.css", "*.js"]
```

