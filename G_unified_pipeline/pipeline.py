"""
çµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (Stage E-K) - è¨­å®šãƒ™ãƒ¼ã‚¹ç‰ˆ

è¨­è¨ˆæ›¸: DESIGN_UNIFIED_PIPELINE.md v2.0 ã«æº–æ‹ 
å‡¦ç†é †åº: Stage E â†’ F â†’ G â†’ H â†’ I â†’ J â†’ K

ç‰¹å¾´:
- doc_type / workspace ã«å¿œã˜ã¦è‡ªå‹•çš„ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆ
- config/ å†…ã® YAML ã¨ Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã§è¨­å®šç®¡ç†
"""
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger

from C_ai_common.llm_client.llm_client import LLMClient
from A_common.database.client import DatabaseClient

from .config_loader import ConfigLoader
from .stage_e_preprocessing import StageEPreprocessor
from .stage_f_visual import StageFVisualAnalyzer
from .stage_g_formatting import StageGTextFormatter
from .stage_h_structuring import StageHStructuring
from .stage_h_kakeibo import StageHKakeibo
from .stage_i_synthesis import StageISynthesis
from .stage_j_chunking import StageJChunking
from .stage_k_embedding import StageKEmbedding

# å®¶è¨ˆç°¿å°‚ç”¨ã®DBä¿å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
from K_kakeibo.kakeibo_db_handler import KakeiboDBHandler


class UnifiedDocumentPipeline:
    """çµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (Stage E-K) - è¨­å®šãƒ™ãƒ¼ã‚¹ç‰ˆ"""

    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        db_client: Optional[DatabaseClient] = None,
        config_dir: Optional[Path] = None
    ):
        """
        Args:
            llm_client: LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            db_client: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            config_dir: è¨­å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: G_unified_pipeline/config/ï¼‰
        """
        self.llm_client = llm_client or LLMClient()
        self.db = db_client or DatabaseClient(use_service_role=True)  # RLSãƒã‚¤ãƒ‘ã‚¹ã®ãŸã‚Service Roleä½¿ç”¨

        # è¨­å®šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
        self.config = ConfigLoader(config_dir)

        # å„ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’åˆæœŸåŒ–
        self.stage_e = StageEPreprocessor(self.llm_client)
        self.stage_f = StageFVisualAnalyzer(self.llm_client)
        self.stage_g = StageGTextFormatter(self.llm_client)
        self.stage_h = StageHStructuring(self.llm_client)
        self.stage_h_kakeibo = StageHKakeibo(self.db)  # å®¶è¨ˆç°¿å°‚ç”¨Stage H
        self.stage_i = StageISynthesis(self.llm_client)
        self.stage_j = StageJChunking()
        self.stage_k = StageKEmbedding(self.llm_client, self.db)

        # å®¶è¨ˆç°¿å°‚ç”¨ã®DBä¿å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        self.kakeibo_db_handler = KakeiboDBHandler(self.db)

        logger.info("âœ… UnifiedDocumentPipeline åˆæœŸåŒ–å®Œäº†ï¼ˆè¨­å®šãƒ™ãƒ¼ã‚¹ï¼‰")

    async def process_document(
        self,
        file_path: Path,
        file_name: str,
        doc_type: str,
        workspace: str,
        mime_type: str,
        source_id: str,
        existing_document_id: Optional[str] = None,
        extra_metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ï¼ˆStage E-Kï¼‰

        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«å
            doc_type: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆè¨­å®šãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«ä½¿ç”¨ï¼‰
            workspace: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹
            mime_type: MIMEã‚¿ã‚¤ãƒ—
            source_id: ã‚½ãƒ¼ã‚¹ID
            existing_document_id: æ›´æ–°ã™ã‚‹æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            extra_metadata: è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆClassroomå›ºæœ‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãªã©ï¼‰

        Returns:
            å‡¦ç†çµæœ {'success': bool, 'document_id': str, ...}
        """
        try:
            logger.info(f"ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†é–‹å§‹: {file_name} (doc_type={doc_type}, workspace={workspace})")

            # ============================================
            # Stage E: Pre-processing
            # ============================================
            logger.info("[Stage E] Pre-processingé–‹å§‹...")
            extracted_text = self.stage_e.process(file_path, mime_type)
            logger.info(f"[Stage Eå®Œäº†] æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆé•·: {len(extracted_text)}æ–‡å­—")

            # ============================================
            # Stage F: Visual Analysis (å…¨ä»¶å®Ÿè¡Œ)
            # ============================================
            # è¨­å®šã‹ã‚‰ Stage F ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            stage_f_config = self.config.get_stage_config('stage_f', doc_type, workspace)
            prompt_f = stage_f_config['prompt']
            model_f = stage_f_config['model']

            logger.info(f"[Stage F] Visual Analysisé–‹å§‹... (model={model_f})")
            vision_raw = self.stage_f.process(
                file_path=file_path,
                prompt=prompt_f,
                model=model_f
            )
            logger.info(f"[Stage Få®Œäº†] Visionçµæœ: {len(vision_raw)}æ–‡å­—")

            # ============================================
            # Stage G: Text Formatting + Integration (å…¨ä»¶å®Ÿè¡Œ)
            # ============================================
            # è¨­å®šã‹ã‚‰ Stage G ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            stage_g_config = self.config.get_stage_config('stage_g', doc_type, workspace)
            prompt_g = stage_g_config['prompt']
            model_g = stage_g_config['model']

            logger.info(f"[Stage G] Text Formatting + Integrationé–‹å§‹... (model={model_g})")
            combined_text = self.stage_g.process(
                vision_raw=vision_raw,
                extracted_text=extracted_text,
                prompt_template=prompt_g,
                model=model_g,
                mode="integrate"  # çµ±åˆãƒ¢ãƒ¼ãƒ‰
            )
            logger.info(f"[Stage Gå®Œäº†] çµ±åˆãƒ†ã‚­ã‚¹ãƒˆ: {len(combined_text)}æ–‡å­—")

            # ============================================
            # Stage H: Structuring
            # ============================================
            # è¨­å®šã‹ã‚‰ Stage H ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            stage_h_config = self.config.get_stage_config('stage_h', doc_type, workspace)
            custom_handler = stage_h_config.get('custom_handler')

            # å®¶è¨ˆç°¿å°‚ç”¨å‡¦ç†ã®å ´åˆ
            if custom_handler == 'kakeibo':
                logger.info(f"[Stage H] å®¶è¨ˆç°¿æ§‹é€ åŒ–é–‹å§‹... (custom_handler=kakeibo)")

                # Stage G ã®å‡ºåŠ›ã‚’è¾æ›¸ã«å¤‰æ›ï¼ˆcombined_text ãŒ JSON æ–‡å­—åˆ—ã®å ´åˆï¼‰
                import json
                import re
                try:
                    # Markdownã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ (```json ... ```) ã‚’é™¤å»
                    json_text = combined_text.strip()
                    if json_text.startswith('```'):
                        # æœ€åˆã¨æœ€å¾Œã®```ã‚’é™¤å»
                        json_text = re.sub(r'^```(?:json)?\s*\n', '', json_text)
                        json_text = re.sub(r'\n```\s*$', '', json_text)

                    logger.debug(f"[Stage H] JSON ãƒ‘ãƒ¼ã‚¹å‰ã®æœ€åˆã®500æ–‡å­—:\n{json_text[:500]}")
                    stage_g_output = json.loads(json_text)
                except (json.JSONDecodeError, TypeError) as e:
                    logger.error(f"[Stage H] combined_text ãŒ JSON å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {e}")
                    logger.error(f"[Stage H] combined_text ã®å†…å®¹:\n{combined_text[:1000]}")
                    raise ValueError("Stage G output must be JSON for kakeibo processing")

                # å®¶è¨ˆç°¿å°‚ç”¨ Stage H ã§å‡¦ç†
                stageH_result = self.stage_h_kakeibo.process(stage_g_output)

                # å®¶è¨ˆç°¿å°‚ç”¨ã®DBä¿å­˜
                logger.info("[DBä¿å­˜] å®¶è¨ˆç°¿ãƒ‡ãƒ¼ã‚¿ã‚’DBã«ä¿å­˜...")
                kakeibo_save_result = self.kakeibo_db_handler.save_receipt(
                    stage_h_output=stageH_result,
                    file_name=file_name,
                    drive_file_id=source_id,
                    model_name=stage_h_config['model'],
                    source_folder=workspace
                )
                logger.info(f"[DBä¿å­˜å®Œäº†] receipt_id={kakeibo_save_result['receipt_id']}")

                # å®¶è¨ˆç°¿ã¯ source_documents ã«ä¿å­˜ã›ãšã€ã“ã“ã§çµ‚äº†
                return {
                    'success': True,
                    'receipt_id': kakeibo_save_result['receipt_id'],
                    'transaction_ids': kakeibo_save_result['transaction_ids'],
                    'standardized_ids': kakeibo_save_result['standardized_ids'],
                    'doc_type': 'kakeibo'
                }

            # é€šå¸¸ã® Stage H å‡¦ç†
            else:
                prompt_h = stage_h_config['prompt']
                model_h = stage_h_config['model']

                logger.info(f"[Stage H] æ§‹é€ åŒ–é–‹å§‹... (model={model_h})")
                stageH_result = self.stage_h.process(
                    file_name=file_name,
                    doc_type=doc_type,
                    workspace=workspace,
                    combined_text=combined_text,
                    prompt=prompt_h,
                    model=model_h
                )

                document_date = stageH_result.get('document_date')
                tags = stageH_result.get('tags', [])
                stageH_metadata = stageH_result.get('metadata', {})
                logger.info(f"[Stage Hå®Œäº†]")

            # ============================================
            # Stage I: Synthesis
            # ============================================
            # è¨­å®šã‹ã‚‰ Stage I ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            stage_i_config = self.config.get_stage_config('stage_i', doc_type, workspace)

            # skip ãƒ•ãƒ©ã‚°ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if stage_i_config.get('skip'):
                logger.info("[Stage I] ã‚¹ã‚­ãƒƒãƒ— (skip=true)")
                summary = ""
                relevant_date = None
            else:
                prompt_i = stage_i_config['prompt']
                model_i = stage_i_config['model']

                logger.info(f"[Stage I] çµ±åˆãƒ»è¦ç´„é–‹å§‹... (model={model_i})")
                stageI_result = self.stage_i.process(
                    combined_text=combined_text,
                    stageH_result=stageH_result,
                    prompt=prompt_i,
                    model=model_i
                )

                summary = stageI_result.get('summary', '')
                relevant_date = stageI_result.get('relevant_date')
                logger.info(f"[Stage Iå®Œäº†]")

            # ============================================
            # Stage J: Chunking
            # ============================================
            logger.info("[Stage J] ãƒãƒ£ãƒ³ã‚¯åŒ–é–‹å§‹...")
            chunks = self.stage_j.process(
                display_subject=extra_metadata.get('display_subject', file_name) if extra_metadata else file_name,
                summary=summary,
                tags=tags,
                document_date=document_date,
                metadata=stageH_metadata
            )
            logger.info(f"[Stage Jå®Œäº†] ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")

            # ============================================
            # DBä¿å­˜: source_documents
            # ============================================
            document_id = existing_document_id
            try:
                doc_data = {
                    'source_id': source_id,
                    'source_type': 'unified_pipeline',
                    'file_name': file_name,
                    'workspace': workspace,
                    'doc_type': doc_type,
                    'attachment_text': combined_text,
                    'summary': summary,
                    'tags': tags,
                    'document_date': document_date,
                    'metadata': stageH_metadata,
                    'processing_status': 'completed'
                }

                # extra_metadata ã‚’ãƒãƒ¼ã‚¸
                if extra_metadata:
                    if isinstance(doc_data['metadata'], dict):
                        doc_data['metadata'].update(extra_metadata)
                    else:
                        doc_data['metadata'] = extra_metadata

                # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ›´æ–° or æ–°è¦ä½œæˆ
                if existing_document_id:
                    logger.info(f"[DBæ›´æ–°] æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°: {existing_document_id}")
                    result = self.db.client.table('Rawdata_FILE_AND_MAIL').update(doc_data).eq('id', existing_document_id).execute()
                    if not result.data:
                        logger.error("[DBæ›´æ–°ã‚¨ãƒ©ãƒ¼] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°å¤±æ•—")
                        return {'success': False, 'error': 'Document update failed'}
                else:
                    logger.info("[DBä¿å­˜] æ–°è¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ")
                    result = self.db.client.table('Rawdata_FILE_AND_MAIL').insert(doc_data).execute()
                    if result.data and len(result.data) > 0:
                        document_id = result.data[0]['id']
                        logger.info(f"[DBä¿å­˜] source_documents ID: {document_id}")
                    else:
                        logger.error("[DBä¿å­˜ã‚¨ãƒ©ãƒ¼] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆå¤±æ•—")
                        return {'success': False, 'error': 'Document creation failed'}

            except Exception as e:
                logger.error(f"[DBä¿å­˜ã‚¨ãƒ©ãƒ¼] {e}")
                return {'success': False, 'error': str(e)}

            # ============================================
            # Stage K: Embedding
            # ============================================
            logger.info("[Stage K] ãƒ™ã‚¯ãƒˆãƒ«åŒ–é–‹å§‹...")

            # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆã¯ã€å¤ã„ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤
            if existing_document_id:
                try:
                    logger.info(f"[Stage K] æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤: document_id={document_id}")
                    self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
                except Exception as e:
                    logger.warning(f"[Stage K è­¦å‘Š] æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {e}")

            # æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
            self.stage_k.process(chunks, document_id)
            logger.info(f"[Stage Kå®Œäº†] {len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜")

            return {
                'success': True,
                'document_id': document_id,
                'summary': summary,
                'tags': tags,
                'chunks_count': len(chunks)
            }

        except Exception as e:
            logger.error(f"[ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼] {e}", exc_info=True)
            return {'success': False, 'error': str(e)}
