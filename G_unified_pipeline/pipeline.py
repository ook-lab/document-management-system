"""
çµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (Stage E-K) - è¨­å®šãƒ™ãƒ¼ã‚¹ç‰ˆ

è¨­è¨ˆæ›¸: DESIGN_UNIFIED_PIPELINE.md v2.0 ã«æº–æ‹ 
å‡¦ç†é †åº: Stage E â†’ F â†’ G â†’ H â†’ I â†’ J â†’ K

ç‰¹å¾´:
- doc_type / workspace ã«å¿œã˜ã¦è‡ªå‹•çš„ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆ
- config/ å†…ã® YAML ã¨ Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã§è¨­å®šç®¡ç†
"""
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger

from C_ai_common.llm_client.llm_client import LLMClient
from A_common.database.client import DatabaseClient

from .config_loader import ConfigLoader
from .stage_e_preprocessing import StageEPreprocessor
from .stage_f_visual import StageFVisualAnalyzer
from .stage_g_formatting import StageGTextFormatter
from .stage_h_structuring import StageHStructuring
from .stage_i_synthesis import StageISynthesis
from .stage_j_chunking import StageJChunking
from .stage_k_embedding import StageKEmbedding


class UnifiedDocumentPipeline:
    """çµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (Stage E-K) - è¨­å®šãƒ™ãƒ¼ã‚¹ç‰ˆ"""

    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        db_client: Optional[DatabaseClient] = None,
        config_dir: Optional[Path] = None
    ):
        """
        Args:
            llm_client: LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            db_client: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            config_dir: è¨­å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: G_unified_pipeline/config/ï¼‰
        """
        self.llm_client = llm_client or LLMClient()
        self.db = db_client or DatabaseClient()

        # è¨­å®šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
        self.config = ConfigLoader(config_dir)

        # å„ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’åˆæœŸåŒ–
        self.stage_e = StageEPreprocessor(self.llm_client)
        self.stage_f = StageFVisualAnalyzer(self.llm_client)
        self.stage_g = StageGTextFormatter(self.llm_client)
        self.stage_h = StageHStructuring(self.llm_client)
        self.stage_i = StageISynthesis(self.llm_client)
        self.stage_j = StageJChunking()
        self.stage_k = StageKEmbedding(self.llm_client, self.db)

        logger.info("âœ… UnifiedDocumentPipeline åˆæœŸåŒ–å®Œäº†ï¼ˆè¨­å®šãƒ™ãƒ¼ã‚¹ï¼‰")

    async def process_document(
        self,
        file_path: Path,
        file_name: str,
        doc_type: str,
        workspace: str,
        mime_type: str,
        source_id: str,
        existing_document_id: Optional[str] = None,
        extra_metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ï¼ˆStage E-Kï¼‰

        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«å
            doc_type: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆè¨­å®šãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«ä½¿ç”¨ï¼‰
            workspace: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹
            mime_type: MIMEã‚¿ã‚¤ãƒ—
            source_id: ã‚½ãƒ¼ã‚¹ID
            existing_document_id: æ›´æ–°ã™ã‚‹æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            extra_metadata: è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆClassroomå›ºæœ‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãªã©ï¼‰

        Returns:
            å‡¦ç†çµæœ {'success': bool, 'document_id': str, ...}
        """
        try:
            logger.info(f"ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†é–‹å§‹: {file_name} (doc_type={doc_type}, workspace={workspace})")

            # ============================================
            # Stage E: Pre-processing
            # ============================================
            logger.info("[Stage E] Pre-processingé–‹å§‹...")
            extracted_text = self.stage_e.process(file_path, mime_type)
            logger.info(f"[Stage Eå®Œäº†] æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆé•·: {len(extracted_text)}æ–‡å­—")

            # ============================================
            # Stage F: Visual Analysis (æ¡ä»¶ä»˜ã)
            # ============================================
            vision_raw = ""
            needs_vision = self._should_run_vision(mime_type, extracted_text)

            if needs_vision and file_path.exists():
                # è¨­å®šã‹ã‚‰ Stage F ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
                stage_f_config = self.config.get_stage_config('stage_f', doc_type, workspace)
                prompt_f = stage_f_config['prompt']
                model_f = stage_f_config['model']

                logger.info(f"[Stage F] Visual Analysisé–‹å§‹... (model={model_f})")
                vision_raw = self.stage_f.process(
                    file_path=file_path,
                    prompt=prompt_f,
                    model=model_f
                )
                logger.info(f"[Stage Få®Œäº†] Visionçµæœ: {len(vision_raw)}æ–‡å­—")

            # ============================================
            # Stage G: Text Formatting (æ¡ä»¶ä»˜ã)
            # ============================================
            vision_formatted = ""
            if vision_raw:
                # è¨­å®šã‹ã‚‰ Stage G ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
                stage_g_config = self.config.get_stage_config('stage_g', doc_type, workspace)
                prompt_g = stage_g_config['prompt']
                model_g = stage_g_config['model']

                logger.info(f"[Stage G] Text Formattingé–‹å§‹... (model={model_g})")
                vision_formatted = self.stage_g.process(
                    vision_raw=vision_raw,
                    prompt_template=prompt_g,
                    model=model_g
                )
                logger.info(f"[Stage Gå®Œäº†] æ•´å½¢ãƒ†ã‚­ã‚¹ãƒˆ: {len(vision_formatted)}æ–‡å­—")

            # çµ±åˆãƒ†ã‚­ã‚¹ãƒˆ
            combined_text = f"{extracted_text}\n\n{vision_formatted}".strip()

            # ============================================
            # Stage H: Structuring
            # ============================================
            # è¨­å®šã‹ã‚‰ Stage H ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            stage_h_config = self.config.get_stage_config('stage_h', doc_type, workspace)
            prompt_h = stage_h_config['prompt']
            model_h = stage_h_config['model']

            logger.info(f"[Stage H] æ§‹é€ åŒ–é–‹å§‹... (model={model_h})")
            stageH_result = self.stage_h.process(
                file_name=file_name,
                doc_type=doc_type,
                workspace=workspace,
                combined_text=combined_text,
                prompt=prompt_h,
                model=model_h
            )

            document_date = stageH_result.get('document_date')
            tags = stageH_result.get('tags', [])
            stageH_metadata = stageH_result.get('metadata', {})
            logger.info(f"[Stage Hå®Œäº†]")

            # ============================================
            # Stage I: Synthesis
            # ============================================
            # è¨­å®šã‹ã‚‰ Stage I ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            stage_i_config = self.config.get_stage_config('stage_i', doc_type, workspace)
            prompt_i = stage_i_config['prompt']
            model_i = stage_i_config['model']

            logger.info(f"[Stage I] çµ±åˆãƒ»è¦ç´„é–‹å§‹... (model={model_i})")
            stageI_result = self.stage_i.process(
                combined_text=combined_text,
                stageH_result=stageH_result,
                prompt=prompt_i,
                model=model_i
            )

            summary = stageI_result.get('summary', '')
            relevant_date = stageI_result.get('relevant_date')
            logger.info(f"[Stage Iå®Œäº†]")

            # ============================================
            # Stage J: Chunking
            # ============================================
            logger.info("[Stage J] ãƒãƒ£ãƒ³ã‚¯åŒ–é–‹å§‹...")
            chunks = self.stage_j.process(
                display_subject=extra_metadata.get('display_subject', file_name) if extra_metadata else file_name,
                summary=summary,
                tags=tags,
                document_date=document_date,
                metadata=stageH_metadata
            )
            logger.info(f"[Stage Jå®Œäº†] ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")

            # ============================================
            # DBä¿å­˜: source_documents
            # ============================================
            document_id = existing_document_id
            try:
                doc_data = {
                    'source_id': source_id,
                    'source_type': 'unified_pipeline',
                    'file_name': file_name,
                    'workspace': workspace,
                    'doc_type': doc_type,
                    'attachment_text': combined_text,
                    'summary': summary,
                    'tags': tags,
                    'document_date': document_date,
                    'metadata': stageH_metadata,
                    'processing_status': 'completed'
                }

                # extra_metadata ã‚’ãƒãƒ¼ã‚¸
                if extra_metadata:
                    if isinstance(doc_data['metadata'], dict):
                        doc_data['metadata'].update(extra_metadata)
                    else:
                        doc_data['metadata'] = extra_metadata

                # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ›´æ–° or æ–°è¦ä½œæˆ
                if existing_document_id:
                    logger.info(f"[DBæ›´æ–°] æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°: {existing_document_id}")
                    result = self.db.client.table('source_documents').update(doc_data).eq('id', existing_document_id).execute()
                    if not result.data:
                        logger.error("[DBæ›´æ–°ã‚¨ãƒ©ãƒ¼] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°å¤±æ•—")
                        return {'success': False, 'error': 'Document update failed'}
                else:
                    logger.info("[DBä¿å­˜] æ–°è¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ")
                    result = self.db.client.table('source_documents').insert(doc_data).execute()
                    if result.data and len(result.data) > 0:
                        document_id = result.data[0]['id']
                        logger.info(f"[DBä¿å­˜] source_documents ID: {document_id}")
                    else:
                        logger.error("[DBä¿å­˜ã‚¨ãƒ©ãƒ¼] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆå¤±æ•—")
                        return {'success': False, 'error': 'Document creation failed'}

            except Exception as e:
                logger.error(f"[DBä¿å­˜ã‚¨ãƒ©ãƒ¼] {e}")
                return {'success': False, 'error': str(e)}

            # ============================================
            # Stage K: Embedding
            # ============================================
            logger.info("[Stage K] ãƒ™ã‚¯ãƒˆãƒ«åŒ–é–‹å§‹...")

            # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆã¯ã€å¤ã„ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤
            if existing_document_id:
                try:
                    logger.info(f"[Stage K] æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤: document_id={document_id}")
                    self.db.client.table('search_index').delete().eq('document_id', document_id).execute()
                except Exception as e:
                    logger.warning(f"[Stage K è­¦å‘Š] æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {e}")

            # æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
            self.stage_k.process(chunks, document_id)
            logger.info(f"[Stage Kå®Œäº†] {len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜")

            return {
                'success': True,
                'document_id': document_id,
                'summary': summary,
                'tags': tags,
                'chunks_count': len(chunks)
            }

        except Exception as e:
            logger.error(f"[ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼] {e}", exc_info=True)
            return {'success': False, 'error': str(e)}

    def _should_run_vision(self, mime_type: str, extracted_text: str) -> bool:
        """Stage F (Vision) ã‚’å®Ÿè¡Œã™ã¹ãã‹åˆ¤å®š"""
        # æ¡ä»¶1: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«
        if mime_type and mime_type.startswith('image/'):
            return True

        # æ¡ä»¶2: Pre-processingã§ãƒ†ã‚­ã‚¹ãƒˆãŒã»ã¨ã‚“ã©æŠ½å‡ºã§ããªã‹ã£ãŸï¼ˆ100æ–‡å­—æœªæº€ï¼‰
        if len(extracted_text.strip()) < 100:
            return True

        return False
