"""
çµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (Stage E-K) - è¨­å®šãƒ™ãƒ¼ã‚¹ç‰ˆ

è¨­è¨ˆæ›¸: DESIGN_UNIFIED_PIPELINE.md v2.0 ã«æº–æ‹ 
å‡¦ç†é †åº: Stage E â†’ F â†’ G â†’ H â†’ I â†’ J â†’ K

ç‰¹å¾´:
- doc_type / workspace ã«å¿œã˜ã¦è‡ªå‹•çš„ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆ
- config/ å†…ã® YAML ã¨ Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã§è¨­å®šç®¡ç†
"""
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger

from C_ai_common.llm_client.llm_client import LLMClient
from A_common.database.client import DatabaseClient
from A_common.connectors.google_drive import GoogleDriveConnector

from .config_loader import ConfigLoader
from .stage_e_preprocessing import StageEPreprocessor
from .stage_f_visual import StageFVisualAnalyzer
from .stage_h_structuring import StageHStructuring
from .stage_h_kakeibo import StageHKakeibo
from .stage_i_synthesis import StageISynthesis
from .stage_j_chunking import StageJChunking
from .stage_k_embedding import StageKEmbedding

# å®¶è¨ˆç°¿å°‚ç”¨ã®DBä¿å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ (ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«)
try:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from K_kakeibo.kakeibo_db_handler import KakeiboDBHandler
    KAKEIBO_AVAILABLE = True
except ImportError:
    logger.warning("K_kakeibo module not available, kakeibo features will be disabled")
    KakeiboDBHandler = None
    KAKEIBO_AVAILABLE = False


class UnifiedDocumentPipeline:
    """çµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (Stage E-K) - è¨­å®šãƒ™ãƒ¼ã‚¹ç‰ˆ"""

    @staticmethod
    def _sanitize_text(text: str) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰nullæ–‡å­—ã‚’é™¤å»

        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            ã‚µãƒ‹ã‚¿ã‚¤ã‚ºæ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not text:
            return text
        # nullæ–‡å­— (\u0000) ã‚’é™¤å»
        return text.replace('\u0000', '')

    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        db_client: Optional[DatabaseClient] = None,
        config_dir: Optional[Path] = None,
        enable_hybrid_ocr: Optional[bool] = None
    ):
        """
        Args:
            llm_client: LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            db_client: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            config_dir: è¨­å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: G_unified_pipeline/config/ï¼‰
            enable_hybrid_ocr: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰OCRï¼ˆSurya + PaddleOCRï¼‰ã‚’æœ‰åŠ¹åŒ–ï¼ˆNoneã®å ´åˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼‰
        """
        self.llm_client = llm_client or LLMClient()
        self.db = db_client or DatabaseClient(use_service_role=True)  # RLSãƒã‚¤ãƒ‘ã‚¹ã®ãŸã‚Service Roleä½¿ç”¨
        self.drive_connector = GoogleDriveConnector()  # Google Drive ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°ç”¨

        # è¨­å®šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
        self.config = ConfigLoader(config_dir)

        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰OCRã®æœ‰åŠ¹/ç„¡åŠ¹ã‚’æ±ºå®š
        if enable_hybrid_ocr is None:
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è¨­å®šï¼‰
            enable_hybrid_ocr = self.config.get_hybrid_ocr_enabled('default')

        # å„ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’åˆæœŸåŒ–
        self.stage_e = StageEPreprocessor(self.llm_client)
        self.stage_f = StageFVisualAnalyzer(self.llm_client, enable_hybrid_ocr=enable_hybrid_ocr)
        self.stage_h = StageHStructuring(self.llm_client)
        self.stage_h_kakeibo = StageHKakeibo(self.db)  # å®¶è¨ˆç°¿å°‚ç”¨Stage H
        self.stage_i = StageISynthesis(self.llm_client)
        self.stage_j = StageJChunking()
        self.stage_k = StageKEmbedding(self.llm_client, self.db)

        # å®¶è¨ˆç°¿å°‚ç”¨ã®DBä¿å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        self.kakeibo_db_handler = KakeiboDBHandler(self.db) if KAKEIBO_AVAILABLE else None

        logger.info(f"âœ… UnifiedDocumentPipeline åˆæœŸåŒ–å®Œäº†ï¼ˆè¨­å®šãƒ™ãƒ¼ã‚¹, ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰OCR={'æœ‰åŠ¹' if enable_hybrid_ocr else 'ç„¡åŠ¹'}ï¼‰")

    async def process_document(
        self,
        file_path: Path,
        file_name: str,
        doc_type: str,
        workspace: str,
        mime_type: str,
        source_id: str,
        existing_document_id: Optional[str] = None,
        extra_metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ï¼ˆStage E-Kï¼‰

        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«å
            doc_type: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆè¨­å®šãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«ä½¿ç”¨ï¼‰
            workspace: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹
            mime_type: MIMEã‚¿ã‚¤ãƒ—
            source_id: ã‚½ãƒ¼ã‚¹ID
            existing_document_id: æ›´æ–°ã™ã‚‹æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            extra_metadata: è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆClassroomå›ºæœ‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãªã©ï¼‰

        Returns:
            å‡¦ç†çµæœ {'success': bool, 'document_id': str, ...}
        """
        try:
            logger.info(f"ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†é–‹å§‹: {file_name} (doc_type={doc_type}, workspace={workspace})")

            # ============================================
            # Stage E: Pre-processing
            # ============================================
            logger.info("[Stage E] Pre-processingé–‹å§‹...")

            # extra_metadata ã‹ã‚‰æ—¢ã«æŠ½å‡ºæ¸ˆã¿ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆattachment_textï¼‰ã‚’å–å¾—
            # HTMLãƒ•ã‚¡ã‚¤ãƒ«ç­‰ã€Ingestionæ™‚ã«ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæ¸ˆã¿ã®å ´åˆã«ä½¿ç”¨
            pre_extracted_text = extra_metadata.get('attachment_text', '') if extra_metadata else ''

            stage_e_result = self.stage_e.extract_text(
                file_path,
                mime_type,
                pre_extracted_text=pre_extracted_text
            )

            # Stage E ã®çµæœã‚’ãƒã‚§ãƒƒã‚¯
            if not stage_e_result.get('success'):
                error_msg = f"Stage Eå¤±æ•—: {stage_e_result.get('error', 'ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼')}"
                logger.error(f"[Stage Eå¤±æ•—] {error_msg}")
                return {'success': False, 'error': error_msg}

            extracted_text = stage_e_result.get('content', '')
            # ãƒ­ã‚°å‡ºåŠ›ã¯ Stage E å†…ã§æ—¢ã«å®Ÿæ–½æ¸ˆã¿

            # ============================================
            # Stage F: Visual Analysis (gemini-2.5-pro ã§å®Œç’§ã«ä»•ä¸Šã’ã‚‹)
            # ============================================
            # è¨­å®šã‹ã‚‰ Stage F ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            stage_f_config = self.config.get_stage_config('stage_f', doc_type, workspace)
            prompt_f = stage_f_config['prompt']
            model_f = stage_f_config['model']

            logger.info(f"[Stage F] Visual Analysisé–‹å§‹... (model={model_f})")
            vision_raw = self.stage_f.process(
                file_path=file_path,
                prompt=prompt_f,
                model=model_f,
                extracted_text=extracted_text,
                workspace=workspace
            )
            logger.info(f"[Stage Få®Œäº†] Visionçµæœ: {len(vision_raw)}æ–‡å­—")

            # ============================================
            # Stage F çµæœãƒ‘ãƒ¼ã‚¹: JSON ã‹ã‚‰æ§‹é€ åŒ–æƒ…å ±ã‚’å–å¾—
            # ============================================
            import json
            try:
                vision_json = json.loads(vision_raw)
                ocr_text = vision_json.get('full_text', '')
                stage_f_structure = {
                    'sections': vision_json.get('layout_info', {}).get('sections', []),
                    'tables': vision_json.get('layout_info', {}).get('tables', []),
                    'visual_elements': vision_json.get('visual_elements', {}),
                    'full_text': ocr_text
                }

                # combined_textã®æ§‹ç¯‰ï¼ˆè¤‡æ•°ã‚½ãƒ¼ã‚¹ã‹ã‚‰çµ±åˆï¼‰
                text_parts = []

                # 1. æŠ•ç¨¿æ–‡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆClassroomç­‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
                if extra_metadata:
                    display_post_text = extra_metadata.get('display_post_text', '')
                    if display_post_text and display_post_text.strip():
                        text_parts.append(f"[æŠ•ç¨¿æ–‡]\n{display_post_text}")
                        logger.info(f"[Stage Fâ†’H] display_post_textè¿½åŠ : {len(display_post_text)}æ–‡å­—")

                # 2. OCRæŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ
                if ocr_text and ocr_text.strip():
                    text_parts.append(f"[OCRæŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ]\n{ocr_text}")

                # 3. ç”»åƒã®è¦–è¦šçš„èª¬æ˜ï¼ˆvisual_elements.notesï¼‰
                visual_elements = vision_json.get('visual_elements', {})
                notes = visual_elements.get('notes', [])
                if notes:
                    notes_text = '\n'.join(notes)
                    text_parts.append(f"[ç”»åƒã®è¦–è¦šçš„èª¬æ˜]\n{notes_text}")
                    logger.info(f"[Stage Fâ†’H] visual_elements.notesè¿½åŠ : {len(notes_text)}æ–‡å­—")

                # çµ±åˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
                combined_text = '\n\n'.join(text_parts)

                logger.info(f"[Stage Fâ†’H] æ§‹é€ åŒ–æƒ…å ±ã‚’æŠ½å‡º:")
                logger.info(f"  â”œâ”€ combined_text: {len(combined_text)}æ–‡å­—")
                logger.info(f"  â”œâ”€ OCR full_text: {len(ocr_text)}æ–‡å­—")
                logger.info(f"  â”œâ”€ sections: {len(stage_f_structure.get('sections', []))}å€‹")
                logger.info(f"  â””â”€ tables: {len(stage_f_structure.get('tables', []))}å€‹")
            except json.JSONDecodeError as e:
                logger.warning(f"[Stage Fâ†’H] JSONè§£æå¤±æ•—: {e}")
                combined_text = vision_raw
                stage_f_structure = None

            # ç©ºã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç©ºã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯è­¦å‘Šã®ã¿ã€ã‚¨ãƒ©ãƒ¼ã§ã¯ãªã„ï¼‰
            if not combined_text or not combined_text.strip():
                logger.warning(f"[Stage Fâ†’H] çµ±åˆãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ãªã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å¯èƒ½æ€§ï¼‰")
                combined_text = ""  # ç©ºæ–‡å­—åˆ—ã¨ã—ã¦ç¶™ç¶š

            # ============================================
            # Stage H: Structuring
            # ============================================
            # è¨­å®šã‹ã‚‰ Stage H ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            stage_h_config = self.config.get_stage_config('stage_h', doc_type, workspace)
            custom_handler = stage_h_config.get('custom_handler')

            # å®¶è¨ˆç°¿å°‚ç”¨å‡¦ç†ã®å ´åˆ
            if custom_handler == 'kakeibo':
                logger.info(f"[Stage H] å®¶è¨ˆç°¿æ§‹é€ åŒ–é–‹å§‹... (custom_handler=kakeibo)")

                # Stage F ã®å‡ºåŠ›ã‚’è¾æ›¸ã«å¤‰æ›ï¼ˆcombined_text ãŒ JSON æ–‡å­—åˆ—ã®å ´åˆï¼‰
                import json
                import re
                try:
                    # Markdownã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ (```json ... ```) ã‚’é™¤å»
                    json_text = combined_text.strip()
                    if json_text.startswith('```'):
                        # æœ€åˆã¨æœ€å¾Œã®```ã‚’é™¤å»
                        json_text = re.sub(r'^```(?:json)?\s*\n', '', json_text)
                        json_text = re.sub(r'\n```\s*$', '', json_text)

                    logger.debug(f"[Stage H] JSON ãƒ‘ãƒ¼ã‚¹å‰ã®æœ€åˆã®500æ–‡å­—:\n{json_text[:500]}")
                    stage_f_output = json.loads(json_text)
                except (json.JSONDecodeError, TypeError) as e:
                    logger.error(f"[Stage H] combined_text ãŒ JSON å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {e}")
                    logger.error(f"[Stage H] combined_text ã®å†…å®¹:\n{combined_text[:1000]}")
                    raise ValueError("Stage F output must be JSON for kakeibo processing")

                # å®¶è¨ˆç°¿å°‚ç”¨ Stage H ã§å‡¦ç†
                stageH_result = self.stage_h_kakeibo.process(stage_f_output)

                # å®¶è¨ˆç°¿å°‚ç”¨ã®DBä¿å­˜
                if self.kakeibo_db_handler:
                    logger.info("[DBä¿å­˜] å®¶è¨ˆç°¿ãƒ‡ãƒ¼ã‚¿ã‚’DBã«ä¿å­˜...")
                    kakeibo_save_result = self.kakeibo_db_handler.save_receipt(
                        stage_h_output=stageH_result,
                        file_name=file_name,
                        drive_file_id=source_id,
                        model_name=stage_h_config['model'],
                        source_folder=workspace
                    )
                    logger.info(f"[DBä¿å­˜å®Œäº†] receipt_id={kakeibo_save_result['receipt_id']}")
                else:
                    logger.warning("K_kakeibo module not available, skipping kakeibo DB save")

                # å®¶è¨ˆç°¿ã¯ Rawdata_FILE_AND_MAIL ã«ä¿å­˜ã›ãšã€ã“ã“ã§çµ‚äº†
                return {
                    'success': True,
                    'receipt_id': kakeibo_save_result['receipt_id'],
                    'transaction_ids': kakeibo_save_result['transaction_ids'],
                    'log_id': kakeibo_save_result['log_id'],
                    'doc_type': 'kakeibo'
                }

            # é€šå¸¸ã® Stage H å‡¦ç†
            else:
                prompt_h = stage_h_config['prompt']
                model_h = stage_h_config['model']

                logger.info(f"[Stage H] æ§‹é€ åŒ–é–‹å§‹... (model={model_h})")
                stageH_result = self.stage_h.process(
                    file_name=file_name,
                    doc_type=doc_type,
                    workspace=workspace,
                    combined_text=combined_text,
                    prompt=prompt_h,
                    model=model_h,
                    stage_f_structure=stage_f_structure  # æ§‹é€ åŒ–æƒ…å ±ã‚’æ¸¡ã™
                )

                # Stage H ã®çµæœã‚’ãƒã‚§ãƒƒã‚¯
                if not stageH_result or not isinstance(stageH_result, dict):
                    error_msg = "Stage Hå¤±æ•—: æ§‹é€ åŒ–çµæœãŒä¸æ­£ã§ã™"
                    logger.error(f"[Stage Hå¤±æ•—] {error_msg}")
                    return {'success': False, 'error': error_msg}

                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã®å‡¦ç†ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆï¼‰
                stageH_metadata = stageH_result.get('metadata', {})
                if stageH_metadata.get('extraction_failed'):
                    logger.warning("[Stage Hè­¦å‘Š] ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚’ä½¿ç”¨ï¼‰")
                    # ã‚¨ãƒ©ãƒ¼ã§ã¯ãªãã€ç©ºã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ç¶™ç¶š

                document_date = stageH_result.get('document_date')
                tags = stageH_result.get('tags', [])
                logger.info(f"[Stage Hå®Œäº†]")

            # ============================================
            # Stage I: Synthesis
            # ============================================
            # è¨­å®šã‹ã‚‰ Stage I ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            stage_i_config = self.config.get_stage_config('stage_i', doc_type, workspace)

            # skip ãƒ•ãƒ©ã‚°ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if stage_i_config.get('skip'):
                logger.info("[Stage I] ã‚¹ã‚­ãƒƒãƒ— (skip=true)")
                summary = ""
                relevant_date = None
            else:
                prompt_i = stage_i_config['prompt']
                model_i = stage_i_config['model']

                logger.info(f"[Stage I] çµ±åˆãƒ»è¦ç´„é–‹å§‹... (model={model_i})")
                stageI_result = self.stage_i.process(
                    combined_text=combined_text,
                    stageH_result=stageH_result,
                    prompt=prompt_i,
                    model=model_i
                )

                # Stage I ã®çµæœã‚’ãƒã‚§ãƒƒã‚¯
                if not stageI_result or not isinstance(stageI_result, dict):
                    error_msg = "Stage Iå¤±æ•—: çµ±åˆãƒ»è¦ç´„çµæœãŒä¸æ­£ã§ã™"
                    logger.error(f"[Stage Iå¤±æ•—] {error_msg}")
                    return {'success': False, 'error': error_msg}

                title = stageI_result.get('title', '')
                summary = stageI_result.get('summary', '')

                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã®å‡¦ç†ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆï¼‰
                if summary == 'å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ':
                    logger.warning("[Stage Iè­¦å‘Š] ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚’ä½¿ç”¨ï¼‰")
                    summary = ''  # ç©ºã®è¦ç´„ã¨ã—ã¦ç¶™ç¶š

                relevant_date = stageI_result.get('relevant_date')

                # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆã¨ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                calendar_events = stageI_result.get('calendar_events', [])
                tasks = stageI_result.get('tasks', [])

                # metadataã«è¿½åŠ 
                stageH_metadata['calendar_events'] = calendar_events
                stageH_metadata['tasks'] = tasks

                logger.info(f"[Stage Iå®Œäº†] calendar_events={len(calendar_events)}ä»¶, tasks={len(tasks)}ä»¶")

                # ============================================
                # Google Drive ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã«åŸºã¥ãï¼‰
                # ============================================
                if title and source_id:
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’æŠ½å‡º
                    import os
                    file_extension = os.path.splitext(file_name)[1]  # ä¾‹: ".pdf"

                    # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒˆãƒ« + æ‹¡å¼µå­ï¼‰
                    new_file_name = title + file_extension

                    # Google Drive ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ›´æ–°
                    try:
                        self.drive_connector.rename_file(source_id, new_file_name)
                        logger.info(f"[Google Drive] ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°æˆåŠŸ: {new_file_name}")
                    except Exception as e:
                        # ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°å¤±æ•—ã¯ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ã¿ï¼ˆå‡¦ç†ã¯ç¶™ç¶šï¼‰
                        logger.warning(f"[Google Drive] ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°å¤±æ•—: {e}")

            # ============================================
            # Stage J: Chunking
            # ============================================
            logger.info("[Stage J] ãƒãƒ£ãƒ³ã‚¯åŒ–é–‹å§‹...")
            chunks = self.stage_j.process(
                display_subject=extra_metadata.get('display_subject', file_name) if extra_metadata else file_name,
                summary=summary,
                tags=tags,
                document_date=document_date,
                metadata=stageH_metadata
            )
            logger.info(f"[Stage Jå®Œäº†] ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")

            # ============================================
            # DBä¿å­˜: Rawdata_FILE_AND_MAIL
            # ============================================
            document_id = existing_document_id
            try:
                # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã® attachment_text, metadata, display_* ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—ï¼ˆnullã§ä¸Šæ›¸ãã—ãªã„ãŸã‚ï¼‰
                existing_attachment_text = None
                existing_metadata = {}
                existing_display_fields = {}
                if existing_document_id:
                    try:
                        existing_doc = self.db.client.table('Rawdata_FILE_AND_MAIL').select(
                            'attachment_text, metadata, display_sender, display_sender_email, display_subject, display_sent_at, display_post_text'
                        ).eq('id', existing_document_id).execute()
                        if existing_doc.data and len(existing_doc.data) > 0:
                            doc = existing_doc.data[0]
                            existing_attachment_text = doc.get('attachment_text', '')
                            # æ—¢å­˜ metadata ã‚’ä¿æŒï¼ˆmessage_id, thread_id, subject ãªã©ï¼‰
                            existing_metadata = doc.get('metadata', {})
                            if isinstance(existing_metadata, str):
                                import json
                                existing_metadata = json.loads(existing_metadata)
                            # display_* ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä¿æŒ
                            existing_display_fields = {
                                'display_sender': doc.get('display_sender'),
                                'display_sender_email': doc.get('display_sender_email'),
                                'display_subject': doc.get('display_subject'),
                                'display_sent_at': doc.get('display_sent_at'),
                                'display_post_text': doc.get('display_post_text')
                            }
                            logger.debug(f"[DBä¿å­˜] æ—¢å­˜attachment_textå–å¾—: {len(existing_attachment_text or '')}æ–‡å­—")
                            logger.debug(f"[DBä¿å­˜] æ—¢å­˜metadataå–å¾—: {list(existing_metadata.keys())}")
                            logger.debug(f"[DBä¿å­˜] æ—¢å­˜display_*ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å–å¾—: sender={existing_display_fields.get('display_sender')}, subject={existing_display_fields.get('display_subject')}")
                    except Exception as e:
                        logger.warning(f"[DBä¿å­˜è­¦å‘Š] æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å–å¾—å¤±æ•—: {e}")

                # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼ˆnullæ–‡å­—ã‚’é™¤å»ï¼‰
                sanitized_combined_text = self._sanitize_text(combined_text)
                sanitized_summary = self._sanitize_text(summary)
                sanitized_extracted_text = self._sanitize_text(extracted_text)

                # Stage F ã®å‡ºåŠ›ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆJSONã‹ã‚‰å„è¦ç´ ã‚’æŠ½å‡ºï¼‰
                stage_f_text_ocr = None
                stage_f_layout_ocr = None
                stage_f_visual_elements = None
                try:
                    if vision_raw and stage_f_structure:
                        # full_text ã‚’ text OCR ã¨ã—ã¦ä¿å­˜
                        stage_f_text_ocr = self._sanitize_text(stage_f_structure.get('full_text', ''))
                        # sections + tables ã‚’ layout OCR ã¨ã—ã¦ä¿å­˜
                        import json
                        stage_f_layout_ocr = json.dumps({
                            'sections': stage_f_structure.get('sections', []),
                            'tables': stage_f_structure.get('tables', [])
                        }, ensure_ascii=False, indent=2)
                        # visual_elements ã‚’ãã®ã¾ã¾ä¿å­˜
                        stage_f_visual_elements = json.dumps(
                            stage_f_structure.get('visual_elements', {}),
                            ensure_ascii=False,
                            indent=2
                        )
                except Exception as e:
                    logger.warning(f"[DBä¿å­˜è­¦å‘Š] Stage Få‡ºåŠ›ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {e}")

                # Stage EãŒç©ºã®å ´åˆã€Stage Fã®full_textã‚’E4/E5ã«ä½¿ç”¨
                if not sanitized_extracted_text and stage_f_text_ocr:
                    logger.info("[DBä¿å­˜] Stage EãŒç©ºã®ãŸã‚ã€Stage Fã®full_textã‚’E4/E5ã«ä½¿ç”¨")
                    sanitized_extracted_text = stage_f_text_ocr

                # titleã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
                sanitized_title = self._sanitize_text(title)

                # attachment_text ã®æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯
                # - Stage EãŒæ­£å½“ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ãŸå ´åˆï¼ˆsanitized_combined_text ãŒç©ºã§ãªã„ï¼‰â†’ ä½¿ç”¨ï¼ˆæ­£å½“ãªä¸Šæ›¸ãï¼‰
                # - Stage EãŒå¤±æ•—ã—ãŸå ´åˆï¼ˆsanitized_combined_text ãŒç©ºï¼‰â†’ æ—¢å­˜å€¤ã‚’ä¿æŒï¼ˆnullã§ä¸Šæ›¸ãã—ãªã„ï¼‰
                final_attachment_text = sanitized_combined_text
                if not sanitized_combined_text and existing_attachment_text:
                    final_attachment_text = existing_attachment_text
                    logger.info(f"[DBä¿å­˜] Stage EãŒç©ºã®ãŸã‚ã€æ—¢å­˜attachment_textã‚’ä¿æŒ: {len(final_attachment_text)}æ–‡å­—")

                # metadata ã®ãƒãƒ¼ã‚¸ãƒ­ã‚¸ãƒƒã‚¯
                # æ—¢å­˜ã® metadataï¼ˆmessage_id, thread_id, subject ãªã©ï¼‰ã‚’ä¿æŒã—ã¤ã¤ã€
                # Stage H ã® metadataï¼ˆLLMãŒç”Ÿæˆã—ãŸæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’è¿½åŠ 
                final_metadata = {}
                if existing_document_id and existing_metadata:
                    # æ—¢å­˜ã® metadata ã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹
                    final_metadata = existing_metadata.copy()
                    logger.info(f"[DBä¿å­˜] æ—¢å­˜metadataã‚’ä¿æŒ: {list(existing_metadata.keys())}")
                # Stage H ã® metadata ã‚’è¿½åŠ ãƒ»æ›´æ–°
                if stageH_metadata:
                    final_metadata.update(stageH_metadata)
                    logger.info(f"[DBä¿å­˜] Stage H metadataã‚’ãƒãƒ¼ã‚¸: {list(stageH_metadata.keys())}")

                doc_data = {
                    'source_id': source_id,
                    'source_type': 'unified_pipeline',
                    'file_name': file_name,
                    'workspace': workspace,
                    'doc_type': doc_type,
                    'title': sanitized_title,
                    'attachment_text': final_attachment_text,
                    'summary': sanitized_summary,
                    'tags': tags,
                    'document_date': document_date,
                    'metadata': final_metadata,
                    'processing_status': 'completed',
                    # å„ã‚¹ãƒ†ãƒ¼ã‚¸ã®å‡ºåŠ›ã‚’ä¿å­˜
                    # E1-E3: ç¾åœ¨ã¯æœªå®Ÿè£…ã®ãŸã‚ã€E4ã¨åŒã˜å€¤ã‚’ä¿å­˜ï¼ˆå°†æ¥çš„ã«å€‹åˆ¥ã‚¨ãƒ³ã‚¸ãƒ³ã‚’å®Ÿè£…äºˆå®šï¼‰
                    'stage_e1_text': sanitized_extracted_text,  # Stage E-1: PyPDF2ï¼ˆæœªå®Ÿè£…ã€E4ã®å€¤ã‚’ä½¿ç”¨ï¼‰
                    'stage_e2_text': sanitized_extracted_text,  # Stage E-2: pdfminerï¼ˆæœªå®Ÿè£…ã€E4ã®å€¤ã‚’ä½¿ç”¨ï¼‰
                    'stage_e3_text': sanitized_extracted_text,  # Stage E-3: PyMuPDFï¼ˆæœªå®Ÿè£…ã€E4ã®å€¤ã‚’ä½¿ç”¨ï¼‰
                    'stage_e4_text': sanitized_extracted_text,  # Stage E-4: pdfplumber/ç”»åƒOCR
                    'stage_e5_text': sanitized_extracted_text,  # Stage E-5: æœ€çµ‚çµ±åˆï¼ˆç¾åœ¨ã¯E4ã¨åŒã˜ï¼‰
                    'stage_f_text_ocr': stage_f_text_ocr,        # Stage F: Text OCR
                    'stage_f_layout_ocr': stage_f_layout_ocr,    # Stage F: Layout OCR
                    'stage_f_visual_elements': stage_f_visual_elements,  # Stage F: Visual Elements
                    'stage_h_normalized': sanitized_combined_text,  # Stage H ã¸ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
                    'stage_i_structured': json.dumps(stageH_result, ensure_ascii=False, indent=2) if stageH_result else None,  # Stage H ã®å‡ºåŠ›
                    'stage_j_chunks_json': json.dumps(chunks, ensure_ascii=False, indent=2)  # Stage J ã®å‡ºåŠ›
                }

                # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆã€display_* ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä¿æŒï¼ˆGmail ingestionæ™‚ã«è¨­å®šã•ã‚ŒãŸå€¤ã‚’ä¸Šæ›¸ãã—ãªã„ãŸã‚ï¼‰
                if existing_document_id and existing_display_fields:
                    for key, value in existing_display_fields.items():
                        if value is not None:  # Noneã§ãªã„å€¤ã®ã¿ä¿æŒ
                            doc_data[key] = value
                    logger.debug(f"[DBä¿å­˜] display_*ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä¿æŒ: {list(existing_display_fields.keys())}")

                # extra_metadata ã‚’ãƒãƒ¼ã‚¸
                if extra_metadata:
                    # display_*ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯æœ€ä¸Šä½ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ã—ã¦ä¿å­˜
                    display_fields = ['display_subject', 'display_sender', 'display_sender_email', 'display_sent_at', 'display_post_text', 'display_type']
                    for field in display_fields:
                        if field in extra_metadata and extra_metadata[field] is not None:
                            doc_data[field] = extra_metadata[field]
                            logger.debug(f"[DBä¿å­˜] extra_metadataã‹ã‚‰{field}ã‚’è¨­å®š: {extra_metadata[field]}")

                    # display_*ä»¥å¤–ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯metadataã«ãƒãƒ¼ã‚¸
                    other_metadata = {k: v for k, v in extra_metadata.items() if k not in display_fields}
                    if other_metadata:
                        if isinstance(doc_data['metadata'], dict):
                            doc_data['metadata'].update(other_metadata)
                        else:
                            doc_data['metadata'] = other_metadata

                # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ›´æ–° or æ–°è¦ä½œæˆ
                if existing_document_id:
                    logger.info(f"[DBæ›´æ–°] æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°: {existing_document_id}")
                    # IDã‚’é™¤å¤–ã—ã¦UPDATEï¼ˆIDã¯å¤‰æ›´ä¸å¯ï¼‰
                    update_data = {k: v for k, v in doc_data.items() if k != 'id'}
                    result = self.db.client.table('Rawdata_FILE_AND_MAIL').update(update_data).eq('id', existing_document_id).execute()
                    if result.data and len(result.data) > 0:
                        document_id = result.data[0]['id']
                        logger.info(f"[DBæ›´æ–°å®Œäº†] Rawdata_FILE_AND_MAIL ID: {document_id}")
                    else:
                        logger.error("[DBæ›´æ–°ã‚¨ãƒ©ãƒ¼] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°å¤±æ•—")
                        return {'success': False, 'error': 'Document update failed'}
                else:
                    logger.info("[DBä¿å­˜] æ–°è¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ")
                    result = self.db.client.table('Rawdata_FILE_AND_MAIL').insert(doc_data).execute()
                    if result.data and len(result.data) > 0:
                        document_id = result.data[0]['id']
                        logger.info(f"[DBä¿å­˜] Rawdata_FILE_AND_MAIL ID: {document_id}")
                    else:
                        logger.error("[DBä¿å­˜ã‚¨ãƒ©ãƒ¼] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆå¤±æ•—")
                        return {'success': False, 'error': 'Document creation failed'}

            except Exception as e:
                logger.error(f"[DBä¿å­˜ã‚¨ãƒ©ãƒ¼] {e}")
                return {'success': False, 'error': str(e)}

            # ============================================
            # Stage K: Embedding
            # ============================================
            logger.info("[Stage K] ãƒ™ã‚¯ãƒˆãƒ«åŒ–é–‹å§‹...")

            # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆã¯ã€å¤ã„ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤
            if existing_document_id:
                try:
                    logger.info(f"[Stage K] æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤: document_id={document_id}")
                    self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
                except Exception as e:
                    logger.warning(f"[Stage K è­¦å‘Š] æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {e}")

            # æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
            stage_k_result = self.stage_k.embed_and_save(document_id, chunks)

            # Stage K ã®çµæœã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå³æ ¼ãƒ¢ãƒ¼ãƒ‰: 1ã¤ã§ã‚‚å¤±æ•—ã—ãŸã‚‰å…¨ä½“å¤±æ•—ï¼‰
            if not stage_k_result.get('success'):
                error_msg = f"Stage Kå¤±æ•—: {stage_k_result.get('failed_count', 0)}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å¤±æ•—"
                logger.error(f"[Stage Kå¤±æ•—] {error_msg}")
                return {'success': False, 'error': error_msg}

            # éƒ¨åˆ†çš„å¤±æ•—ã¯è­¦å‘Šã¨ã—ã¦æ‰±ã†ï¼ˆä¸€éƒ¨ã®ãƒãƒ£ãƒ³ã‚¯ã¯ä¿å­˜æ¸ˆã¿ï¼‰
            failed_count = stage_k_result.get('failed_count', 0)
            saved_count = stage_k_result.get('saved_count', 0)
            if failed_count > 0:
                logger.warning(f"[Stage Kè­¦å‘Š] éƒ¨åˆ†çš„ãªå¤±æ•—: {failed_count}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å¤±æ•—ï¼ˆ{saved_count}ãƒãƒ£ãƒ³ã‚¯ã¯ä¿å­˜æ¸ˆã¿ï¼‰")
                # å¤±æ•—ã—ãŸãŒã€ä¸€éƒ¨ã¯æˆåŠŸã—ã¦ã„ã‚‹ã®ã§ç¶™ç¶š

            logger.info(f"[Stage Kå®Œäº†] {stage_k_result.get('saved_count', 0)}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜")

            return {
                'success': True,
                'document_id': document_id,
                'summary': summary,
                'tags': tags,
                'chunks_count': stage_k_result.get('saved_count', 0)
            }

        except Exception as e:
            logger.error(f"[ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼] {e}", exc_info=True)
            return {'success': False, 'error': str(e)}
