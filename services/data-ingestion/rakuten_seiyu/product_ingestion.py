"""
æ¥½å¤©è¥¿å‹ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼ å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦Supabaseã«ä¿å­˜ã—ã¾ã™ã€‚

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
1. Cookieã‚’ä½¿ç”¨ã—ã¦å•†å“ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
2. å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
3. JANã‚³ãƒ¼ãƒ‰ã§æ—¢å­˜å•†å“ã‚’ãƒã‚§ãƒƒã‚¯
4. Supabaseã«ä¿å­˜ï¼ˆæ–°è¦ or æ›´æ–°ï¼‰
"""

import os
import sys
import logging
from pathlib import Path
from typing import List, Dict, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from B_ingestion.common.base_product_ingestion import BaseProductIngestionPipeline
from B_ingestion.rakuten_seiyu.rakuten_seiyu_scraper_playwright import RakutenSeiyuScraperPlaywright

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class RakutenSeiyuProductIngestionPipeline(BaseProductIngestionPipeline):
    """æ¥½å¤©è¥¿å‹å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå…±é€šåŸºç›¤ã‚¯ãƒ©ã‚¹ç¶™æ‰¿ï¼‰"""

    def __init__(self, rakuten_id: str, password: str, headless: bool = True):
        """
        Args:
            rakuten_id: æ¥½å¤©ID
            password: ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
            headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹ã‹
        """
        super().__init__(organization_name="æ¥½å¤©è¥¿å‹ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼", headless=headless)
        self.rakuten_id = rakuten_id
        self.password = password

        logger.info("RakutenSeiyuProductIngestionPipelineåˆæœŸåŒ–å®Œäº†ï¼ˆService Roleä½¿ç”¨ï¼‰")

    async def start(self) -> bool:
        """
        ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’èµ·å‹•ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ï¼ˆæ¥½å¤©è¥¿å‹å›ºæœ‰ï¼‰

        Returns:
            æˆåŠŸã—ãŸã‚‰True
        """
        try:
            self.scraper = RakutenSeiyuScraperPlaywright()
            await self.scraper.start(headless=self.headless)

            # ãƒ­ã‚°ã‚¤ãƒ³
            success = await self.scraper.login(self.rakuten_id, self.password)
            if not success:
                logger.error("âŒ ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—")
                await self.scraper.close()
                return False

            logger.info("âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•ãƒ»ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†")
            return True

        except Exception as e:
            logger.error(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False

    async def close(self):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’çµ‚äº†"""
        if self.scraper:
            await self.scraper.close()

    async def discover_categories(self) -> List[Dict[str, Any]]:
        """
        å®Ÿéš›ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’å–å¾—ï¼ˆæ¯å›å‹•çš„ã«å–å¾—ï¼‰

        Returns:
            ã‚«ãƒ†ã‚´ãƒªãƒ¼æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        logger.info("ğŸ” ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’æ¢ç´¢ä¸­...")

        page = self.scraper.page

        # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
        await page.goto(self.scraper.base_url, wait_until="domcontentloaded")
        await page.wait_for_timeout(2000)

        # ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        category_selectors = [
            'a[href*="/search/"]',
            '[class*="category"] a',
        ]

        all_categories = []

        for selector in category_selectors:
            try:
                links = await page.query_selector_all(selector)

                for link in links:
                    try:
                        href = await link.get_attribute('href')
                        text = await link.inner_text()

                        if not href or href.startswith('javascript:') or not text:
                            continue

                        # ã‚«ãƒ†ã‚´ãƒªãƒ¼IDã‚’æŠ½å‡º
                        category_id = None
                        if '/search/' in href:
                            parts = href.split('/search/')
                            if len(parts) > 1:
                                category_id = parts[1].split('?')[0].split('/')[0]

                        # å®Œå…¨ãªURLã‚’æ§‹ç¯‰
                        full_url = href if href.startswith('http') else f"https://netsuper.rakuten.co.jp{href}"

                        category_info = {
                            'name': text.strip(),
                            'url': full_url,
                            'category_id': category_id
                        }

                        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                        if not any(cat['url'] == full_url for cat in all_categories):
                            all_categories.append(category_info)

                    except Exception:
                        continue

            except Exception:
                continue

        logger.info(f"âœ… {len(all_categories)}ä»¶ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’ç™ºè¦‹")
        return all_categories


async def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    logger.info("æ¥½å¤©è¥¿å‹å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")

    rakuten_id = os.getenv("RAKUTEN_ID")
    password = os.getenv("RAKUTEN_PASSWORD")

    if not rakuten_id or not password:
        logger.error("âŒ ç’°å¢ƒå¤‰æ•° RAKUTEN_ID ã¨ RAKUTEN_PASSWORD ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return

    pipeline = RakutenSeiyuProductIngestionPipeline(
        rakuten_id=rakuten_id,
        password=password,
        headless=False
    )

    try:
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•
        success = await pipeline.start()
        if not success:
            logger.error("âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•å¤±æ•—")
            return

        # ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’å‹•çš„ã«å–å¾—
        categories = await pipeline.discover_categories()
        if categories:
            logger.info(f"ç™ºè¦‹ã—ãŸã‚«ãƒ†ã‚´ãƒªãƒ¼: {len(categories)}ä»¶")
            for cat in categories[:5]:  # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º
                logger.info(f"  - {cat['name']}: {cat['url']}")

        logger.info("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†")

    finally:
        await pipeline.close()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
