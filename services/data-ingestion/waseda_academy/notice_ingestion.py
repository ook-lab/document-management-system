"""
æ—©ç¨²ç”°ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼OnlineãŠçŸ¥ã‚‰ã›å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

HTML â†’ PDFæŠ½å‡ºãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ â†’ Google Drive â†’ Supabase (pending)

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
1. HTMLãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆwindow.appPropsã®JSONï¼‰ã‹ã‚‰ãŠçŸ¥ã‚‰ã›ä¸€è¦§ã‚’å–å¾—
2. Supabaseã§æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æ–°ç€ãŠçŸ¥ã‚‰ã›ã‚’æŠ½å‡º
3. PDFãƒªãƒ³ã‚¯ã‹ã‚‰PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦Google Driveã«ä¿å­˜
4. Supabaseã«åŸºæœ¬æƒ…å ±ã‚’ç™»éŒ²ï¼ˆprocessing_status='pending'ï¼‰
5. åˆ¥é€” process_queued_documents.py ã§å‡¦ç†ï¼ˆPDFæŠ½å‡ºã€Stage E-Kï¼‰
"""
import os
import sys
import re
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from loguru import logger

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¨ data-ingestion ã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
root_dir = Path(__file__).parent.parent.parent.parent
ingestion_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))
sys.path.insert(0, str(ingestion_dir))

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from shared.common.connectors.google_drive import GoogleDriveConnector
from shared.common.database.client import DatabaseClient
from waseda_academy.browser_automation import WasedaAcademyBrowser


class WasedaNoticeIngestionPipeline:
    """æ—©ç¨²ç”°ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼OnlineãŠçŸ¥ã‚‰ã›å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(
        self,
        pdf_folder_id: Optional[str] = None,
        session_cookies: Optional[Dict[str, str]] = None,
        owner_id: Optional[str] = None
    ):
        """
        Args:
            pdf_folder_id: PDFä¿å­˜å…ˆã®Driveãƒ•ã‚©ãƒ«ãƒ€IDï¼ˆNoneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            session_cookies: æ—©ç¨²ç”°ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼Onlineã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒƒã‚­ãƒ¼ï¼ˆPDFå–å¾—ç”¨ï¼‰
            owner_id: ã‚ªãƒ¼ãƒŠãƒ¼IDï¼ˆSupabase Auth ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã€çœç•¥æ™‚ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
        """
        self.pdf_folder_id = pdf_folder_id or os.getenv("WASEDA_PDF_FOLDER_ID")
        self.session_cookies = session_cookies or {}
        self.base_url = "https://online.waseda-ac.co.jp"

        # Phase 3: owner_id ã‚’å–å¾—ï¼ˆå¿…é ˆï¼‰
        self.owner_id = owner_id or os.getenv('DEFAULT_OWNER_ID')
        if not self.owner_id:
            raise ValueError(
                "owner_id ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å¼•æ•°ã§æŒ‡å®šã™ã‚‹ã‹ã€DEFAULT_OWNER_ID ã‚’ .env ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚"
            )

        # ã‚³ãƒã‚¯ã‚¿ã®åˆæœŸåŒ–
        self.drive = GoogleDriveConnector()
        self.db = DatabaseClient(use_service_role=True)

        logger.info(f"WasedaNoticeIngestionPipelineåˆæœŸåŒ–å®Œäº†")
        logger.info(f"  - PDF folder: {self.pdf_folder_id}")

    async def fetch_html_with_browser(self) -> List[str]:
        """
        ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã‚’ä½¿ç”¨ã—ã¦å…¨ãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—

        Returns:
            å„ãƒšãƒ¼ã‚¸ã®HTMLãƒªã‚¹ãƒˆã€å¤±æ•—æ™‚ã¯ç©ºãƒªã‚¹ãƒˆ
        """
        try:
            browser = WasedaAcademyBrowser(headless=True)
            html_pages, _ = await browser.run_automated_session()
            return html_pages if html_pages else []
        except Exception as e:
            logger.error(f"ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return []

    def extract_notice_data(self, html_content) -> List[Dict[str, Any]]:
        """
        HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãŠçŸ¥ã‚‰ã›ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹
        ãƒ‡ãƒ¼ã‚¿ã¯window.appPropsã¨ã„ã†JavaScriptå¤‰æ•°å†…ã®JSONã¨ã—ã¦åŸ‹ã‚è¾¼ã¾ã‚Œã¦ã„ã‚‹

        Args:
            html_content: HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„å…¨ä½“

        Returns:
            ãŠçŸ¥ã‚‰ã›ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        # ãƒªã‚¹ãƒˆï¼ˆè¤‡æ•°ãƒšãƒ¼ã‚¸ï¼‰ã§ã‚‚å˜ä¸€æ–‡å­—åˆ—ã§ã‚‚å—ã‘ä»˜ã‘ã‚‹
        pages = html_content if isinstance(html_content, list) else [html_content]

        all_notices = []
        seen_ids = set()
        for i, page_html in enumerate(pages, 1):
            match = re.search(r'window\.appProps\s*=\s*(\{.*?\});', page_html, re.DOTALL)
            if not match:
                logger.warning(f"p={i}: window.appPropsãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                continue
            try:
                app_props = json.loads(match.group(1))
                notices = app_props['page']['noticeList']['_0']['notices']
                new_count = 0
                for n in notices:
                    nid = n.get('id')
                    if nid and nid not in seen_ids:
                        seen_ids.add(nid)
                        all_notices.append(n)
                        new_count += 1
                logger.info(f"  [p={i}] {new_count}ä»¶")
            except (json.JSONDecodeError, KeyError) as e:
                logger.error(f"p={i} JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")

        logger.info(f"ãŠçŸ¥ã‚‰ã›ã‚’åˆè¨ˆ{len(all_notices)}ä»¶æŠ½å‡ºã—ã¾ã—ãŸ")
        return all_notices

    async def check_existing_notices(self, notice_ids: List[str]) -> set:
        """
        Supabaseã§æ—¢å­˜ã®ãŠçŸ¥ã‚‰ã›IDã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆfile_id ã‚«ãƒ©ãƒ ã§ç…§åˆï¼‰

        Args:
            notice_ids: ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãŠçŸ¥ã‚‰ã›IDã®ãƒªã‚¹ãƒˆ

        Returns:
            æ—¢ã«å­˜åœ¨ã™ã‚‹ãŠçŸ¥ã‚‰ã›IDã®ã‚»ãƒƒãƒˆ
        """
        try:
            result = self.db.client.table('Rawdata_FILE_AND_MAIL').select('file_id').in_(
                'file_id', notice_ids
            ).execute()

            existing_ids = {doc['file_id'] for doc in (result.data or []) if doc.get('file_id')}
            logger.info(f"æ—¢å­˜ã®ãŠçŸ¥ã‚‰ã›: {len(existing_ids)}ä»¶")
            return existing_ids

        except Exception as e:
            logger.error(f"Supabaseæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return set()

    async def download_pdfs_with_browser(
        self,
        pdf_info_list: List[Dict[str, str]]
    ) -> Dict[str, bytes]:
        """
        ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã‚’ä½¿ç”¨ã—ã¦è¤‡æ•°ã®PDFã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

        Args:
            pdf_info_list: [{'notice_id': 'xxx', 'pdf_url': '/notice/xxx/pdf/0', 'pdf_title': 'ã‚¿ã‚¤ãƒˆãƒ«'}, ...]

        Returns:
            {notice_id: pdf_data}ã®è¾æ›¸
        """
        try:
            browser = WasedaAcademyBrowser(headless=True)
            pdfs = await browser.download_pdfs_batch(pdf_info_list)
            return pdfs
        except Exception as e:
            logger.error(f"PDFãƒãƒƒãƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {}

    def save_pdf_to_drive(
        self,
        pdf_data: bytes,
        pdf_title: str,
        notice_id: str
    ) -> Optional[str]:
        """
        PDFã‚’Google Driveã«ä¿å­˜

        Args:
            pdf_data: PDFã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            pdf_title: PDFã®ã‚¿ã‚¤ãƒˆãƒ«
            notice_id: ãŠçŸ¥ã‚‰ã›ID

        Returns:
            Driveã®ãƒ•ã‚¡ã‚¤ãƒ«IDã€å¤±æ•—æ™‚ã¯None
        """
        file_name = f"{pdf_title}.pdf"

        # Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        file_id = self.drive.upload_file(
            file_content=pdf_data,
            file_name=file_name,
            mime_type='application/pdf',
            folder_id=self.pdf_folder_id
        )

        if file_id:
            logger.info(f"PDFã‚’Driveã«ä¿å­˜: {file_name}")
        else:
            logger.error(f"PDFã®ä¿å­˜ã«å¤±æ•—: {file_name}")

        return file_id, file_name

    async def process_single_notice(
        self,
        notice: Dict[str, Any],
        pdf_data_dict: Dict[str, bytes]
    ) -> Dict[str, Any]:
        """
        1ä»¶ã®ãŠçŸ¥ã‚‰ã›ã‚’å‡¦ç†ï¼ˆPDFã®ã¿ï¼‰

        Args:
            notice: ãŠçŸ¥ã‚‰ã›ãƒ‡ãƒ¼ã‚¿
            pdf_data_dict: {notice_id: pdf_data}ã®è¾æ›¸ï¼ˆäº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ï¼‰

        Returns:
            å‡¦ç†çµæœã®è¾æ›¸
        """
        result = {
            'notice_id': notice.get('id'),
            'success': False,
            'pdf_file_ids': [],
            'document_ids': [],
            'error': None
        }

        try:
            notice_id = notice.get('id')
            title = notice.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')
            date = notice.get('date', '')
            message = notice.get('message', '')
            source = notice.get('source', {})
            category = notice.get('category', {})

            logger.info(f"ãŠçŸ¥ã‚‰ã›å‡¦ç†é–‹å§‹: {title}")

            # æ—¥ä»˜ã‚’ datetime ã«å¤‰æ›ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: 2025.12.16ï¼‰
            sent_at = None
            if date:
                try:
                    sent_at = datetime.strptime(date, '%Y.%m.%d').isoformat()
                except ValueError:
                    logger.warning(f"æ—¥ä»˜ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {date}")

            # PDFãƒªãƒ³ã‚¯ãŒãªã„å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ¬ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦ä¿å­˜
            pdfs = notice.get('pdfs', [])
            if not pdfs:
                logger.info(f"PDFãƒªãƒ³ã‚¯ãªã—ã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§ç™»éŒ²: {title}")
                metadata = {
                    'notice_title': title,
                    'notice_date': date,
                    'notice_source': source.get('label', 'ä¸æ˜'),
                    'notice_category': category.get('label', 'ãã®ä»–'),
                    'notice_message': message,
                }
                doc_data = {
                    'file_id': notice_id,
                    'doc_type': 'æ—©ç¨²ã‚¢ã‚«ã‚ªãƒ³ãƒ©ã‚¤ãƒ³',
                    'workspace': 'waseda_academy',
                    'person': ['è‚²å“‰'],
                    'organization': ['æ—©ç¨²ç”°ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼'],
                    'metadata': metadata,
                    'processing_status': 'pending',
                    'display_subject': title,
                    'display_sent_at': sent_at,
                    'display_sender': source.get('label', 'ä¸æ˜'),
                    'display_post_text': message,
                    'owner_id': self.owner_id
                }
                try:
                    doc_result = await self.db.insert_document('Rawdata_FILE_AND_MAIL', doc_data)
                    if doc_result:
                        result['document_ids'].append(doc_result.get('id'))
                        logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ä¿å­˜å®Œäº†: {title}")
                except Exception as db_error:
                    logger.error(f"Supabaseä¿å­˜ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼‰: {db_error}")
                    result['error'] = str(db_error)
                result['success'] = True
                return result

            # å„PDFã‚’å‡¦ç†
            for pdf in pdfs:
                pdf_title = pdf.get('title', 'untitled')
                pdf_url = pdf.get('url', '')

                if not pdf_url:
                    logger.warning(f"PDFã®URLãŒç©º: {pdf_title}")
                    continue

                # 1. äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®PDFãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                pdf_data = pdf_data_dict.get(notice_id)
                if not pdf_data:
                    logger.warning(f"PDFãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {pdf_title}")
                    continue

                # 2. PDFã‚’Google Driveã«ä¿å­˜
                file_id, actual_file_name = self.save_pdf_to_drive(pdf_data, pdf_title, notice_id)
                if not file_id:
                    logger.error(f"PDFã®ä¿å­˜ã«å¤±æ•—: {pdf_title}")
                    continue

                result['pdf_file_ids'].append(file_id)

                # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æº–å‚™
                # å®Œå…¨ãªPDF URLã‚’æ§‹ç¯‰
                if pdf_url.startswith('http'):
                    full_pdf_url = pdf_url
                elif pdf_url.startswith('/'):
                    full_pdf_url = f"{self.base_url}{pdf_url}"
                else:
                    full_pdf_url = f"{self.base_url}/{pdf_url}"

                metadata = {
                    'notice_title': title,
                    'notice_date': date,
                    'notice_source': source.get('label', 'ä¸æ˜'),
                    'notice_category': category.get('label', 'ãã®ä»–'),
                    'notice_message': message,
                    'pdf_url': full_pdf_url,
                    'pdf_title': pdf_title
                }

                # 5. Supabaseã«åŸºæœ¬æƒ…å ±ã®ã¿ä¿å­˜
                doc_data = {
                    'file_url': f"https://drive.google.com/file/d/{file_id}/view",
                    'file_id': notice_id,
                    'file_name': actual_file_name,
                    'doc_type': 'æ—©ç¨²ã‚¢ã‚«ã‚ªãƒ³ãƒ©ã‚¤ãƒ³',  # å›ºå®šå€¤
                    'workspace': 'waseda_academy',
                    'person': ['è‚²å“‰'],  # æ‹…å½“è€…ï¼ˆé…åˆ—å½¢å¼ï¼‰
                    'organization': ['æ—©ç¨²ç”°ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼'],  # çµ„ç¹”ï¼ˆé…åˆ—å½¢å¼ï¼‰
                    'metadata': metadata,
                    'processing_status': 'pending',
                    # è¡¨ç¤ºç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                    'display_subject': title,
                    'display_sent_at': sent_at,
                    'display_sender': source.get('label', 'ä¸æ˜'),
                    'display_post_text': message,
                    # Phase 3: owner_id å¿…é ˆ
                    'owner_id': self.owner_id
                }

                try:
                    # Supabaseã«ä¿å­˜
                    doc_result = await self.db.insert_document('Rawdata_FILE_AND_MAIL', doc_data)
                    if doc_result:
                        doc_id = doc_result.get('id')
                        result['document_ids'].append(doc_id)
                        logger.info(f"Supabaseä¿å­˜å®Œäº†ï¼ˆpendingçŠ¶æ…‹ï¼‰: {doc_id}")
                        logger.info(f"  â†’ process_queued_documents.py ã§å‡¦ç†ã—ã¦ãã ã•ã„")

                except Exception as db_error:
                    logger.error(f"Supabaseä¿å­˜ã‚¨ãƒ©ãƒ¼: {db_error}")
                    result['error'] = str(db_error)

            result['success'] = True
            logger.info(f"ãŠçŸ¥ã‚‰ã›å‡¦ç†å®Œäº†: {title} ({len(result['pdf_file_ids'])} PDFs)")

        except Exception as e:
            logger.error(f"ãŠçŸ¥ã‚‰ã›å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            result['error'] = str(e)

        return result



async def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    import sys

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
    pipeline = WasedaNoticeIngestionPipeline()

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ
    use_browser = "--browser" in sys.argv or "--auto" in sys.argv

    html_content = None

    if use_browser:
        # ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã§å…¨ãƒšãƒ¼ã‚¸HTMLã‚’å–å¾—
        logger.info("ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ãƒ¢ãƒ¼ãƒ‰: ãƒ­ã‚°ã‚¤ãƒ³ â†’ å…¨ãƒšãƒ¼ã‚¸HTMLå–å¾—")
        html_content = await pipeline.fetch_html_with_browser()

        if not html_content:
            logger.error("HTMLã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return

        # 1ãƒšãƒ¼ã‚¸ç›®ã‚’ãƒ‡ãƒãƒƒã‚°ç”¨ã«ä¿å­˜
        temp_html_file = Path(__file__).parent.parent.parent / "waseda_notice_page.html"
        with open(temp_html_file, 'w', encoding='utf-8') as f:
            f.write(html_content[0])
        logger.info(f"å–å¾—ã—ãŸHTMLã‚’ä¿å­˜ï¼ˆ1ãƒšãƒ¼ã‚¸ç›®ï¼‰: {temp_html_file}")
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        html_file = Path(__file__).parent.parent.parent / "pasted_content.txt"

        if not html_file.exists():
            logger.error(f"HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {html_file}")
            logger.info("ãƒ’ãƒ³ãƒˆ: --browser ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã‚’ä½¿ç”¨ã§ãã¾ã™")
            logger.info("  python -m B_ingestion.waseda_academy.notice_ingestion --browser")
            return

        with open(html_file, 'r', encoding='utf-8') as f:
            html_content = f.read()

    # ãŠçŸ¥ã‚‰ã›ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    current_notices = pipeline.extract_notice_data(html_content)
    if not current_notices:
        logger.warning("ãŠçŸ¥ã‚‰ã›ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return

    # æ—¢å­˜ã®ãŠçŸ¥ã‚‰ã›IDã‚’Supabaseã‹ã‚‰å–å¾—
    notice_ids = [n.get('id') for n in current_notices if n.get('id')]
    existing_ids = await pipeline.check_existing_notices(notice_ids)

    # æ–°ç€ãŠçŸ¥ã‚‰ã›ã‚’æŠ½å‡º
    new_notices = [n for n in current_notices if n.get('id') not in existing_ids]

    logger.info(f"ç¾åœ¨ã®ãŠçŸ¥ã‚‰ã›: {len(current_notices)}ä»¶")
    logger.info(f"æ—¢å­˜ã®ãŠçŸ¥ã‚‰ã›: {len(existing_ids)}ä»¶")
    logger.info(f"æ–°ç€ãŠçŸ¥ã‚‰ã›: {len(new_notices)}ä»¶")

    if not new_notices:
        logger.info("æ–°ç€ãŠçŸ¥ã‚‰ã›ã¯ã‚ã‚Šã¾ã›ã‚“")
        return

    # æ–°ç€ãŠçŸ¥ã‚‰ã›ã‹ã‚‰PDFæƒ…å ±ã‚’åé›†
    pdf_info_list = []
    for notice in new_notices:
        notice_id = notice.get('id')
        pdfs = notice.get('pdfs', [])
        for pdf in pdfs:
            pdf_title = pdf.get('title', 'untitled')
            pdf_url = pdf.get('url', '')
            if pdf_url:
                pdf_info_list.append({
                    'notice_id': notice_id,
                    'pdf_url': pdf_url,
                    'pdf_title': pdf_title
                })

    logger.info(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®PDF: {len(pdf_info_list)}ä»¶")

    # PDFã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ï¼‰
    pdf_data_dict = {}
    if pdf_info_list:
        logger.info("ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã§PDFã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        pdf_data_dict = await pipeline.download_pdfs_with_browser(pdf_info_list)
        logger.info(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(pdf_data_dict)}/{len(pdf_info_list)}ä»¶")

    # æ–°ç€ãŠçŸ¥ã‚‰ã›ã‚’å‡¦ç†ï¼ˆPDFãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ï¼‰
    results = []
    for i, notice in enumerate(new_notices, 1):
        logger.info(f"[{i}/{len(new_notices)}] å‡¦ç†ä¸­...")
        result = await pipeline.process_single_notice(notice, pdf_data_dict)
        results.append(result)

    # ã‚µãƒãƒªãƒ¼
    success_count = sum(1 for r in results if r['success'])
    total_pdfs = sum(len(r['pdf_file_ids']) for r in results)
    total_docs = sum(len(r['document_ids']) for r in results)

    logger.info("=" * 60)
    logger.info("å‡¦ç†å®Œäº†")
    logger.info(f"  æˆåŠŸ: {success_count}/{len(results)}")
    logger.info(f"  å¤±æ•—: {len(results) - success_count}/{len(results)}")
    logger.info(f"  å‡¦ç†ã—ãŸPDF: {total_pdfs}ä»¶")
    logger.info(f"  ç™»éŒ²ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {total_docs}ä»¶ï¼ˆpendingçŠ¶æ…‹ï¼‰")
    logger.info("=" * 60)
    logger.info("")
    logger.info("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    logger.info("  python process_queued_documents.py --workspace=waseda_academy")
    logger.info("=" * 60)

    # çµæœã‚’è¡¨ç¤º
    print("\n" + "=" * 80)
    print("ğŸ“¢ æ—©ç¨²ç”°ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼ãŠçŸ¥ã‚‰ã›å–å¾—çµæœ")
    print("=" * 80)

    for result in results:
        print(f"\nNotice ID: {result['notice_id']}")
        print(f"  Success: {result['success']}")
        print(f"  PDFs: {len(result['pdf_file_ids'])}")
        for file_id in result['pdf_file_ids']:
            print(f"    - https://drive.google.com/file/d/{file_id}/view")
        print(f"  Documents: {len(result['document_ids'])} (pending)")
        if result['error']:
            print(f"  âŒ Error: {result['error']}")

    print("\n" + "=" * 80)
    print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  python process_queued_documents.py --workspace=waseda_academy")
    print("=" * 80)


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
