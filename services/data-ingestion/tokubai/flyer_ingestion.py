"""
ãƒˆã‚¯ãƒã‚¤ãƒãƒ©ã‚·å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

ãƒˆã‚¯ãƒã‚¤ã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‹ã‚‰ãƒãƒ©ã‚·ç”»åƒã‚’å–å¾—ã—ã€Google Driveã¨Supabaseã«ç™»éŒ²ã™ã‚‹ã€‚

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
1. ãƒˆã‚¯ãƒã‚¤ã®åº—èˆ—ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒãƒ©ã‚·ä¸€è¦§ã‚’å–å¾—
2. Supabaseã§æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æ–°ç€ãƒãƒ©ã‚·ã‚’æŠ½å‡º
3. ãƒãƒ©ã‚·ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦Google Driveã«ä¿å­˜
4. Supabaseã«åŸºæœ¬æƒ…å ±ã‚’ç™»éŒ²ï¼ˆprocessing_status='pending'ï¼‰
5. åˆ¥é€” process_queued_documents.py ã§å‡¦ç†ï¼ˆç”»åƒæŠ½å‡ºã€Stage A/B/Cï¼‰
"""
import os
import sys
import hashlib
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from loguru import logger

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from shared.common.connectors.google_drive import GoogleDriveConnector
from shared.common.database.client import DatabaseClient
from B_ingestion.tokubai.tokubai_scraper import TokubaiScraper


class TokubaiFlyerIngestionPipeline:
    """ãƒˆã‚¯ãƒã‚¤ãƒãƒ©ã‚·å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(
        self,
        store_url: Optional[str] = None,
        flyer_folder_id: Optional[str] = None,
        store_name: Optional[str] = None
    ):
        """
        Args:
            store_url: ãƒˆã‚¯ãƒã‚¤ã®åº—èˆ—URLï¼ˆNoneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            flyer_folder_id: ãƒãƒ©ã‚·ä¿å­˜å…ˆã®Driveãƒ•ã‚©ãƒ«ãƒ€IDï¼ˆNoneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            store_name: åº—èˆ—åï¼ˆNoneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
        """
        self.store_url = store_url or os.getenv("TOKUBAI_STORE_URL")
        self.flyer_folder_id = flyer_folder_id or os.getenv("TOKUBAI_FLYER_FOLDER_ID")
        self.store_name = store_name or os.getenv("TOKUBAI_STORE_NAME", "ãƒˆã‚¯ãƒã‚¤")

        if not self.store_url:
            raise ValueError("åº—èˆ—URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•° TOKUBAI_STORE_URL ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

        if not self.flyer_folder_id:
            raise ValueError("ãƒ•ã‚©ãƒ«ãƒ€IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•° TOKUBAI_FLYER_FOLDER_ID ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

        # ã‚³ãƒã‚¯ã‚¿ã®åˆæœŸåŒ–
        self.scraper = TokubaiScraper(self.store_url)
        self.drive = GoogleDriveConnector()
        self.db = DatabaseClient()

        logger.info(f"TokubaiFlyerIngestionPipelineåˆæœŸåŒ–å®Œäº†")
        logger.info(f"  - Store name: {self.store_name}")
        logger.info(f"  - Store URL: {self.store_url}")
        logger.info(f"  - Flyer folder: {self.flyer_folder_id}")

    async def check_existing_flyers(self, flyer_ids: List[str]) -> set:
        """
        Supabaseã§æ—¢å­˜ã®ãƒãƒ©ã‚·IDã‚’ãƒã‚§ãƒƒã‚¯

        Args:
            flyer_ids: ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãƒãƒ©ã‚·IDã®ãƒªã‚¹ãƒˆ

        Returns:
            æ—¢ã«å­˜åœ¨ã™ã‚‹ãƒãƒ©ã‚·IDã®ã‚»ãƒƒãƒˆ
        """
        try:
            # Rawdata_FLYER_shops ãƒ†ãƒ¼ãƒ–ãƒ«ã§æ—¢å­˜ã®ãƒãƒ©ã‚·IDã‚’å–å¾—
            result = self.db.client.table('Rawdata_FLYER_shops').select('flyer_id').in_(
                'flyer_id', flyer_ids
            ).execute()

            # flyer_id ã‚’æŠ½å‡º
            existing_ids = set()
            if result.data:
                for doc in result.data:
                    flyer_id = doc.get('flyer_id')
                    if flyer_id:
                        existing_ids.add(flyer_id)

            logger.info(f"æ—¢å­˜ã®ãƒãƒ©ã‚·: {len(existing_ids)}ä»¶")
            return existing_ids

        except Exception as e:
            logger.error(f"Supabaseæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return set()

    def save_image_to_drive(
        self,
        image_data: bytes,
        flyer_id: str,
        page_num: int,
        flyer_title: str
    ) -> Optional[tuple]:
        """
        ãƒãƒ©ã‚·ç”»åƒã‚’Google Driveã«ä¿å­˜

        Args:
            image_data: ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            flyer_id: ãƒãƒ©ã‚·ID
            page_num: ãƒšãƒ¼ã‚¸ç•ªå·
            flyer_title: ãƒãƒ©ã‚·ã®ã‚¿ã‚¤ãƒˆãƒ«

        Returns:
            (Driveã®ãƒ•ã‚¡ã‚¤ãƒ«ID, ãƒ•ã‚¡ã‚¤ãƒ«å)ã®ã‚¿ãƒ—ãƒ«ã€å¤±æ•—æ™‚ã¯None
        """
        # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        safe_title = "".join(c for c in flyer_title if c.isalnum() or c in (' ', '-', '_', 'ã€€')).strip()
        if not safe_title:
            safe_title = "tokubai_flyer"

        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_name = f"{timestamp}_{safe_title}_{flyer_id}_p{page_num}.webp"

        # ç”»åƒå½¢å¼ã‚’åˆ¤å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
        # å®Ÿéš›ã®Content-Typeã‚„ãƒ•ã‚¡ã‚¤ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰åˆ¤å®šã™ã‚‹æ–¹ãŒæ­£ç¢º
        if image_data.startswith(b'\xff\xd8\xff'):
            file_name = file_name.replace('.webp', '.jpg')
            mime_type = 'image/jpeg'
        elif image_data.startswith(b'\x89PNG'):
            file_name = file_name.replace('.webp', '.png')
            mime_type = 'image/png'
        elif image_data.startswith(b'RIFF') and b'WEBP' in image_data[:20]:
            mime_type = 'image/webp'
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯webp
            mime_type = 'image/webp'

        # Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        file_id = self.drive.upload_file(
            file_content=image_data,
            file_name=file_name,
            mime_type=mime_type,
            folder_id=self.flyer_folder_id
        )

        if file_id:
            logger.info(f"ãƒãƒ©ã‚·ç”»åƒã‚’Driveã«ä¿å­˜: {file_name}")
        else:
            logger.error(f"ãƒãƒ©ã‚·ç”»åƒã®ä¿å­˜ã«å¤±æ•—: {file_name}")

        return (file_id, file_name) if file_id else None

    async def process_single_flyer(
        self,
        flyer_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        1ä»¶ã®ãƒãƒ©ã‚·ã‚’å‡¦ç†ï¼ˆç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰â†’Driveä¿å­˜â†’Supabaseç™»éŒ²ï¼‰

        Args:
            flyer_info: ãƒãƒ©ã‚·æƒ…å ±
                {'title': 'ã‚¿ã‚¤ãƒˆãƒ«', 'url': '/ãƒãƒ©ã‚·URL', 'flyer_id': 'xxx', 'period': 'æœŸé–“'}

        Returns:
            å‡¦ç†çµæœã®è¾æ›¸
        """
        result = {
            'flyer_id': flyer_info.get('flyer_id'),
            'success': False,
            'image_file_ids': [],
            'document_ids': [],
            'error': None
        }

        try:
            flyer_id = flyer_info['flyer_id']
            title = flyer_info.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')
            period = flyer_info.get('period', '')
            flyer_url = flyer_info.get('url', '')

            logger.info(f"ãƒãƒ©ã‚·å‡¦ç†é–‹å§‹: {title} (ID: {flyer_id})")

            # 1. ãƒãƒ©ã‚·ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒURLã‚’å–å¾—
            images = self.scraper.get_flyer_images(flyer_info)

            if not images:
                logger.warning(f"ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€ã‚¹ã‚­ãƒƒãƒ—: {title}")
                result['success'] = True
                return result

            logger.info(f"ç”»åƒã‚’{len(images)}ä»¶å–å¾—ã—ã¾ã—ãŸ")

            # 2. å„ç”»åƒã‚’å‡¦ç†
            for img_info in images:
                img_url = img_info['url']
                page_num = img_info['page']

                # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                image_data = self.scraper.download_image(img_url)
                if not image_data:
                    logger.warning(f"ç”»åƒã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {img_url}")
                    continue

                # Google Driveã«ä¿å­˜
                drive_result = self.save_image_to_drive(image_data, flyer_id, page_num, title)
                if not drive_result:
                    logger.error(f"ç”»åƒã®ä¿å­˜ã«å¤±æ•—: page {page_num}")
                    continue

                file_id, actual_file_name = drive_result
                result['image_file_ids'].append(file_id)

                # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æº–å‚™
                full_flyer_url = f"https://tokubai.co.jp{flyer_url}" if flyer_url.startswith('/') else flyer_url

                # 4. Supabaseã«åŸºæœ¬æƒ…å ±ã®ã¿ä¿å­˜ï¼ˆRawdata_FLYER_shopsãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰
                doc_data = {
                    # åŸºæœ¬æƒ…å ±
                    'source_type': 'flyer',
                    'workspace': 'shopping',
                    'doc_type': 'physical shop',
                    'organization': self.store_name,  # åº—èˆ—å

                    # ãƒãƒ©ã‚·å›ºæœ‰æƒ…å ±
                    'flyer_id': f"{flyer_id}_p{page_num}",  # ãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªID
                    'flyer_title': title,
                    'flyer_period': period,
                    'flyer_url': full_flyer_url,
                    'page_number': page_num,

                    # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
                    'source_id': file_id,
                    'source_url': f"https://drive.google.com/file/d/{file_id}/view",
                    'file_name': actual_file_name,
                    'file_type': 'image',
                    'content_hash': hashlib.sha256(image_data).hexdigest(),

                    # OCRãƒ»ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ï¼ˆå¾Œã§å‡¦ç†ï¼‰
                    'attachment_text': '',
                    'summary': '',

                    # åˆ†é¡ãƒ»ã‚¿ã‚°
                    'tags': ['ãƒãƒ©ã‚·', 'è²·ã„ç‰©'],

                    # æ—¥ä»˜
                    'document_date': datetime.now().date().isoformat(),

                    # å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
                    'processing_status': 'pending',  # ç”»åƒå‡¦ç†å¾…ã¡
                    'processing_stage': 'tokubai_flyer_downloaded',

                    # è¡¨ç¤ºç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                    'display_subject': f"{title} (ãƒšãƒ¼ã‚¸ {page_num})",
                    'display_sent_at': datetime.now().isoformat(),
                    'display_sender': 'ãƒˆã‚¯ãƒã‚¤',
                    'display_post_text': period,

                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                    'metadata': {
                        'image_url': img_url,
                        'store_url': self.store_url,
                        'original_flyer_id': flyer_id
                    },

                    # ãã®ä»–
                    'person': 'å…±æœ‰'
                }

                try:
                    # Supabaseã«ä¿å­˜
                    doc_result = await self.db.insert_document('Rawdata_FLYER_shops', doc_data)
                    if doc_result:
                        doc_id = doc_result.get('id')
                        result['document_ids'].append(doc_id)
                        logger.info(f"Supabaseä¿å­˜å®Œäº†ï¼ˆpendingçŠ¶æ…‹ï¼‰: {doc_id}")

                except Exception as db_error:
                    logger.error(f"Supabaseä¿å­˜ã‚¨ãƒ©ãƒ¼: {db_error}")
                    result['error'] = str(db_error)

            result['success'] = True
            logger.info(f"ãƒãƒ©ã‚·å‡¦ç†å®Œäº†: {title} ({len(result['image_file_ids'])} images)")

        except Exception as e:
            logger.error(f"ãƒãƒ©ã‚·å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            result['error'] = str(e)

        return result


def load_stores_config(config_path: Optional[Path] = None) -> Dict[str, Any]:
    """
    åº—èˆ—è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€

    Args:
        config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ï¼‰

    Returns:
        è¨­å®šãƒ‡ãƒ¼ã‚¿
    """
    if config_path is None:
        config_path = Path(__file__).parent / "stores_config.json"

    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        logger.info(f"åº—èˆ—è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {config_path}")
        return config
    except Exception as e:
        logger.error(f"åº—èˆ—è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {"stores": [], "default_folder_id": None}


async def process_store(store_config: Dict[str, str], folder_id: str) -> Dict[str, Any]:
    """
    1åº—èˆ—ã®ãƒãƒ©ã‚·ã‚’å‡¦ç†

    Args:
        store_config: åº—èˆ—è¨­å®š {'name': 'åº—èˆ—å', 'url': 'URL', 'enabled': True}
        folder_id: Google Driveãƒ•ã‚©ãƒ«ãƒ€ID

    Returns:
        å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼
    """
    store_name = store_config['name']
    store_url = store_config['url']

    logger.info("=" * 60)
    logger.info(f"åº—èˆ—å‡¦ç†é–‹å§‹: {store_name}")
    logger.info("=" * 60)

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
    try:
        pipeline = TokubaiFlyerIngestionPipeline(
            store_url=store_url,
            flyer_folder_id=folder_id,
            store_name=store_name
        )
    except ValueError as e:
        logger.error(f"åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ ({store_name}): {e}")
        return {
            'store_name': store_name,
            'success': False,
            'error': str(e),
            'results': []
        }

    # 1. åº—èˆ—ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨ãƒãƒ©ã‚·æƒ…å ±ã‚’å–å¾—
    logger.info("åº—èˆ—ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒãƒ©ã‚·ä¸€è¦§ã‚’å–å¾—ä¸­...")
    all_flyers = pipeline.scraper.get_all_flyers()

    if not all_flyers:
        logger.warning("ãƒãƒ©ã‚·ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return {
            'store_name': store_name,
            'success': True,
            'new_flyers': 0,
            'results': []
        }

    logger.info(f"ãƒãƒ©ã‚·ã‚’{len(all_flyers)}ä»¶å–å¾—ã—ã¾ã—ãŸ")

    # 2. æ—¢å­˜ã®ãƒãƒ©ã‚·IDã‚’Supabaseã‹ã‚‰å–å¾—
    flyer_ids = [f.get('flyer_id') for f in all_flyers if f.get('flyer_id')]
    existing_ids = await pipeline.check_existing_flyers(flyer_ids)

    # 3. æ–°ç€ãƒãƒ©ã‚·ã‚’æŠ½å‡º
    new_flyers = [f for f in all_flyers if f.get('flyer_id') not in existing_ids]

    logger.info(f"ç¾åœ¨ã®ãƒãƒ©ã‚·: {len(all_flyers)}ä»¶")
    logger.info(f"æ—¢å­˜ã®ãƒãƒ©ã‚·: {len(existing_ids)}ä»¶")
    logger.info(f"æ–°ç€ãƒãƒ©ã‚·: {len(new_flyers)}ä»¶")

    if not new_flyers:
        logger.info("æ–°ç€ãƒãƒ©ã‚·ã¯ã‚ã‚Šã¾ã›ã‚“")
        return {
            'store_name': store_name,
            'success': True,
            'new_flyers': 0,
            'results': []
        }

    # 4. æ–°ç€ãƒãƒ©ã‚·ã‚’å‡¦ç†
    results = []
    for i, flyer in enumerate(new_flyers, 1):
        logger.info(f"[{i}/{len(new_flyers)}] å‡¦ç†ä¸­: {flyer.get('title', 'ç„¡é¡Œ')}")
        result = await pipeline.process_single_flyer(flyer)
        results.append(result)

    # 5. ã‚µãƒãƒªãƒ¼
    success_count = sum(1 for r in results if r['success'])
    total_images = sum(len(r['image_file_ids']) for r in results)
    total_docs = sum(len(r['document_ids']) for r in results)

    logger.info("=" * 60)
    logger.info(f"{store_name} ã®å‡¦ç†å®Œäº†")
    logger.info(f"  æˆåŠŸ: {success_count}/{len(results)}")
    logger.info(f"  å¤±æ•—: {len(results) - success_count}/{len(results)}")
    logger.info(f"  å‡¦ç†ã—ãŸç”»åƒ: {total_images}ä»¶")
    logger.info(f"  ç™»éŒ²ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {total_docs}ä»¶ï¼ˆpendingçŠ¶æ…‹ï¼‰")
    logger.info("=" * 60)

    return {
        'store_name': store_name,
        'success': True,
        'new_flyers': len(new_flyers),
        'success_count': success_count,
        'total_images': total_images,
        'total_docs': total_docs,
        'results': results
    }


async def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    logger.info("=" * 60)
    logger.info("ãƒˆã‚¯ãƒã‚¤ãƒãƒ©ã‚·å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹ï¼ˆè¤‡æ•°åº—èˆ—å¯¾å¿œï¼‰")
    logger.info("=" * 60)

    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    config = load_stores_config()

    if not config.get('stores'):
        logger.error("åº—èˆ—è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        logger.info("stores_config.json ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        return

    # æœ‰åŠ¹ãªåº—èˆ—ã®ã¿ã‚’å‡¦ç†
    enabled_stores = [s for s in config['stores'] if s.get('enabled', True)]
    logger.info(f"å‡¦ç†å¯¾è±¡åº—èˆ—: {len(enabled_stores)}ä»¶")

    # ãƒ•ã‚©ãƒ«ãƒ€IDã‚’å–å¾—ï¼ˆç’°å¢ƒå¤‰æ•°ã¾ãŸã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰
    folder_id = os.getenv("TOKUBAI_FLYER_FOLDER_ID") or config.get('default_folder_id')

    if not folder_id:
        logger.error("ãƒ•ã‚©ãƒ«ãƒ€IDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        logger.info("ç’°å¢ƒå¤‰æ•° TOKUBAI_FLYER_FOLDER_ID ã¾ãŸã¯ stores_config.json ã§è¨­å®šã—ã¦ãã ã•ã„")
        return

    # å„åº—èˆ—ã‚’å‡¦ç†
    all_store_results = []
    for i, store in enumerate(enabled_stores, 1):
        logger.info(f"\n[{i}/{len(enabled_stores)}] åº—èˆ—å‡¦ç†é–‹å§‹: {store['name']}")
        store_result = await process_store(store, folder_id)
        all_store_results.append(store_result)

    # å…¨ä½“ã®ã‚µãƒãƒªãƒ¼
    logger.info("\n" + "=" * 60)
    logger.info("å…¨åº—èˆ—ã®å‡¦ç†å®Œäº†")
    logger.info("=" * 60)

    total_new_flyers = sum(r.get('new_flyers', 0) for r in all_store_results)
    total_images = sum(r.get('total_images', 0) for r in all_store_results)
    total_docs = sum(r.get('total_docs', 0) for r in all_store_results)

    logger.info(f"  å‡¦ç†ã—ãŸåº—èˆ—: {len(all_store_results)}ä»¶")
    logger.info(f"  æ–°ç€ãƒãƒ©ã‚·: {total_new_flyers}ä»¶")
    logger.info(f"  å‡¦ç†ã—ãŸç”»åƒ: {total_images}ä»¶")
    logger.info(f"  ç™»éŒ²ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {total_docs}ä»¶ï¼ˆpendingçŠ¶æ…‹ï¼‰")
    logger.info("=" * 60)

    # çµæœã‚’è¡¨ç¤º
    print("\n" + "=" * 80)
    print("ğŸ›’ ãƒˆã‚¯ãƒã‚¤ãƒãƒ©ã‚·å–å¾—çµæœï¼ˆè¤‡æ•°åº—èˆ—ï¼‰")
    print("=" * 80)

    for store_result in all_store_results:
        print(f"\nåº—èˆ—: {store_result['store_name']}")
        print(f"  æ–°ç€ãƒãƒ©ã‚·: {store_result.get('new_flyers', 0)}ä»¶")
        if store_result.get('results'):
            print(f"  æˆåŠŸ: {store_result.get('success_count', 0)}")
            print(f"  ç”»åƒ: {store_result.get('total_images', 0)}ä»¶")
            print(f"  ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {store_result.get('total_docs', 0)}ä»¶")

    print("\n" + "=" * 80)
    print(f"åˆè¨ˆ: æ–°ç€ãƒãƒ©ã‚· {total_new_flyers}ä»¶ã€ç”»åƒ {total_images}ä»¶ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {total_docs}ä»¶")
    print("=" * 80)
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  python process_queued_documents.py --workspace=household")
    print("=" * 80)


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
