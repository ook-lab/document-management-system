"""
æ±æ€¥ã‚¹ãƒˆã‚¢ ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼ å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦Supabaseã«ä¿å­˜ã—ã¾ã™ã€‚

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
1. ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦é…é”ã‚¨ãƒªã‚¢ã‚’é¸æŠ
2. ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸ã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
3. JANã‚³ãƒ¼ãƒ‰ã§æ—¢å­˜å•†å“ã‚’ãƒã‚§ãƒƒã‚¯
4. Supabaseã«ä¿å­˜ï¼ˆæ–°è¦ or æ›´æ–°ï¼‰
"""

import os
import sys
import logging
from pathlib import Path
from typing import List, Dict, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
root_dir = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(root_dir))

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from common.base_product_ingestion import BaseProductIngestionPipeline
from tokyu_store.tokyu_store_scraper_playwright import TokyuStoreScraperPlaywright

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class TokyuStoreProductIngestionPipeline(BaseProductIngestionPipeline):
    """æ±æ€¥ã‚¹ãƒˆã‚¢å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå…±é€šåŸºç›¤ã‚¯ãƒ©ã‚¹ç¶™æ‰¿ï¼‰"""

    def __init__(self, login_id: str, password: str, zip_code: str = "158-0094", headless: bool = True):
        """
        Args:
            login_id: æ±æ€¥ã‚¹ãƒˆã‚¢ãƒ­ã‚°ã‚¤ãƒ³IDï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼‰
            password: ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
            zip_code: é…é”ã‚¨ãƒªã‚¢éƒµä¾¿ç•ªå·
            headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹ã‹
        """
        super().__init__(organization_name="æ±æ€¥ã‚¹ãƒˆã‚¢ ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼", headless=headless)
        self.login_id = login_id
        self.password = password
        self.zip_code = zip_code

        logger.info("TokyuStoreProductIngestionPipelineåˆæœŸåŒ–å®Œäº†ï¼ˆService Roleä½¿ç”¨ï¼‰")

    async def start(self) -> bool:
        """
        ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’èµ·å‹•ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ï¼ˆæ±æ€¥ã‚¹ãƒˆã‚¢å›ºæœ‰ï¼‰

        Returns:
            æˆåŠŸã—ãŸã‚‰True
        """
        try:
            self.scraper = TokyuStoreScraperPlaywright()
            await self.scraper.start(headless=self.headless)

            # ãƒ­ã‚°ã‚¤ãƒ³
            success = await self.scraper.login(self.login_id, self.password)
            if not success:
                logger.error("âŒ ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—")
                await self.scraper.close()
                return False

            # é…é”ã‚¨ãƒªã‚¢é¸æŠ
            success = await self.scraper.select_delivery_area(self.zip_code)
            if not success:
                logger.warning("âš ï¸ é…é”ã‚¨ãƒªã‚¢é¸æŠã«å¤±æ•—ã—ã¾ã—ãŸãŒç¶šè¡Œã—ã¾ã™")

            logger.info("âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•ãƒ»ãƒ­ã‚°ã‚¤ãƒ³ãƒ»é…é”ã‚¨ãƒªã‚¢é¸æŠå®Œäº†")
            return True

        except Exception as e:
            logger.error(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False

    async def close(self):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’çµ‚äº†"""
        if self.scraper:
            await self.scraper.close()

    async def discover_categories(self) -> List[Dict[str, str]]:
        """
        ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’å‹•çš„ã«å–å¾—

        Returns:
            ã‚«ãƒ†ã‚´ãƒªãƒ¼æƒ…å ±ã®ãƒªã‚¹ãƒˆ [{"name": "ã‚«ãƒ†ã‚´ãƒªãƒ¼å", "url": "URL"}]
        """
        try:
            logger.info("ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’å–å¾—ä¸­...")

            # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã¾ãŸã¯ã‚«ãƒ†ã‚´ãƒªãƒ¼ä¸€è¦§ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            await self.scraper.page.goto(
                f"{self.scraper.base_url}/shop/default.aspx",
                wait_until="domcontentloaded",
                timeout=60000
            )
            await self.scraper.page.wait_for_timeout(2000)

            # ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’é–‹ã
            try:
                category_modal_button = await self.scraper.page.query_selector('.category-modal-open, a:has-text("ã‚«ãƒ†ã‚´ãƒª")')
                if category_modal_button:
                    await category_modal_button.click()
                    await self.scraper.page.wait_for_timeout(1000)
                    logger.info("ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’é–‹ãã¾ã—ãŸ")
            except Exception as e:
                logger.warning(f"ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’é–‹ã‘ã¾ã›ã‚“ã§ã—ãŸ: {e}")

            # ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ¢ãƒ¼ãƒ€ãƒ«å†…ã®ä¸»è¦ã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼‰
            category_links = await self.scraper.page.query_selector_all(
                'h3.category-name a, .category-name a'
            )

            categories = []
            for link in category_links:
                try:
                    name = await link.inner_text()
                    href = await link.get_attribute('href')

                    if href and name:
                        # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        if not href.startswith('http'):
                            href = f"{self.scraper.base_url}{href}" if href.startswith('/') else f"{self.scraper.base_url}/{href}"

                        categories.append({
                            "name": name.strip(),
                            "url": href
                        })
                except Exception as e:
                    logger.warning(f"ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒªãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            logger.info(f"âœ… {len(categories)}ä»¶ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’ç™ºè¦‹")
            return categories

        except Exception as e:
            logger.error(f"ã‚«ãƒ†ã‚´ãƒªãƒ¼å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return []


async def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    logger.info("æ±æ€¥ã‚¹ãƒˆã‚¢å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")

    login_id = os.getenv("TOKYU_STORE_LOGIN_ID")
    password = os.getenv("TOKYU_STORE_PASSWORD")
    zip_code = os.getenv("DELIVERY_ZIP_CODE", "158-0094")

    if not login_id or not password:
        logger.error("âŒ ç’°å¢ƒå¤‰æ•° TOKYU_STORE_LOGIN_ID ã¨ TOKYU_STORE_PASSWORD ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return

    pipeline = TokyuStoreProductIngestionPipeline(
        login_id=login_id,
        password=password,
        zip_code=zip_code,
        headless=False
    )

    try:
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•
        success = await pipeline.start()
        if not success:
            logger.error("âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•å¤±æ•—")
            return

        # ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’å‹•çš„ã«å–å¾—
        categories = await pipeline.discover_categories()
        if categories:
            logger.info(f"ç™ºè¦‹ã—ãŸã‚«ãƒ†ã‚´ãƒªãƒ¼: {len(categories)}ä»¶")
            for cat in categories[:5]:  # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º
                logger.info(f"  - {cat['name']}: {cat['url']}")

        logger.info("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†")

    finally:
        await pipeline.close()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
