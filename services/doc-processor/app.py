"""
Flask Web Application - Document Processing System
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆå‡¦ç†å°‚ç”¨ï¼‰
"""
import os
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ™‚ç”¨ï¼‰
# Dockerç’°å¢ƒã§ã¯ PYTHONPATH=/app ãŒè¨­å®šæ¸ˆã¿ãªã®ã§ã€parent.parentãŒ/ã«ãªã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
project_root = Path(__file__).parent.parent
if str(project_root) != '/' and str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from datetime import datetime, timezone
from flask import Flask, render_template, request, jsonify
from flask_cors import CORS
from loguru import logger
import psutil
import time

# ========== å®šæ•°å®šç¾© ==========
# ãƒ­ãƒƒã‚¯é–¢é€£
LOCK_TIMEOUT_SECONDS = 300  # ãƒ­ãƒƒã‚¯ã®æœ‰åŠ¹æœŸé™ï¼ˆ5åˆ†ï¼‰
LOCK_RETRY_COUNT = 3  # ãƒ­ãƒƒã‚¯è¨­å®šæ™‚ã®æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
LOCK_RETRY_DELAY = 1.0  # ãƒªãƒˆãƒ©ã‚¤é–“éš”ï¼ˆç§’ï¼‰

# ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–é–¢é€£
RESOURCE_UPDATE_INTERVAL = 5.0  # ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±æ›´æ–°é–“éš”ï¼ˆç§’ï¼‰
MAX_LOG_ENTRIES = 300  # ä¿æŒã™ã‚‹ãƒ­ã‚°ã®æœ€å¤§ä»¶æ•°
MAX_LOG_ENTRIES_SUPABASE = 150  # Supabaseã«ä¿å­˜ã™ã‚‹ãƒ­ã‚°ã®æœ€å¤§ä»¶æ•°

# ä¸¦åˆ—å‡¦ç†é–¢é€£
DEFAULT_MAX_PARALLEL = 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¸¦åˆ—æ•°
MIN_PARALLEL = 1  # æœ€å°ä¸¦åˆ—æ•°
MAX_PARALLEL_LIMIT = 100  # æœ€å¤§ä¸¦åˆ—æ•°ä¸Šé™

# ãƒ¡ãƒ¢ãƒªé–¾å€¤ï¼ˆ16GBç’°å¢ƒç”¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
MEMORY_LOW_THRESHOLD = 60.0  # ä½™è£•ã‚ã‚Šï¼ˆä¸¦åˆ—æ•°å¢—åŠ å¯ï¼‰
MEMORY_HIGH_THRESHOLD = 85.0  # é€¼è¿«ï¼ˆæ¸›é€Ÿé–‹å§‹ï¼‰
MEMORY_CRITICAL_THRESHOLD = 90.0  # å±é™ºï¼ˆä¸¦åˆ—æ•°å‰Šæ¸›ï¼‰
MEMORY_RECOVER_THRESHOLD = 70.0  # å›å¾©ï¼ˆæ¸›é€Ÿç·©å’Œï¼‰

# ã‚¹ãƒ­ãƒƒãƒˆãƒ«é–¢é€£
THROTTLE_STEP = 0.5  # ã‚¹ãƒ­ãƒƒãƒˆãƒ«èª¿æ•´ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆç§’ï¼‰
MAX_THROTTLE_DELAY = 3.0  # æœ€å¤§ã‚¹ãƒ­ãƒƒãƒˆãƒ«é…å»¶ï¼ˆç§’ï¼‰

# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆé–¢é€£
DOCUMENT_PROCESS_TIMEOUT = int(os.getenv('DOC_PROCESS_TIMEOUT', '1800'))  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰

app = Flask(__name__)

# CORSè¨­å®š: ç’°å¢ƒå¤‰æ•°ã§è¨±å¯ã‚ªãƒªã‚¸ãƒ³ã‚’æŒ‡å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æœ¬ç•ªç’°å¢ƒã®ã¿ï¼‰
ALLOWED_ORIGINS = os.getenv('ALLOWED_ORIGINS', 'https://doc-processor-*.run.app,https://docs.ookubotechnologies.com').split(',')
# é–‹ç™ºç’°å¢ƒã§ã¯å…¨è¨±å¯
if os.getenv('FLASK_ENV') == 'development' or os.getenv('DEBUG') == 'true':
    CORS(app)
else:
    CORS(app, origins=ALLOWED_ORIGINS, supports_credentials=True)

# ========== èªè¨¼è¨­å®š ==========
API_KEY = os.getenv('DOC_PROCESSOR_API_KEY', '')
REQUIRE_AUTH = os.getenv('REQUIRE_AUTH', 'true').lower() == 'true'

def require_api_key(f):
    """APIã‚­ãƒ¼èªè¨¼ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    from functools import wraps
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not REQUIRE_AUTH:
            return f(*args, **kwargs)

        # APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯èªè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé–‹ç™ºç’°å¢ƒç”¨ï¼‰
        if not API_KEY:
            logger.warning("DOC_PROCESSOR_API_KEY is not set. Skipping authentication.")
            return f(*args, **kwargs)

        # ãƒ˜ãƒƒãƒ€ãƒ¼ã¾ãŸã¯ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
        provided_key = request.headers.get('X-API-Key') or request.args.get('api_key')
        if provided_key != API_KEY:
            return jsonify({'success': False, 'error': 'Unauthorized'}), 401

        return f(*args, **kwargs)
    return decorated_function

def safe_error_response(error: Exception, status_code: int = 500):
    """å®‰å…¨ãªã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’éš ã™ï¼‰"""
    is_development = os.getenv('FLASK_ENV') == 'development' or os.getenv('DEBUG') == 'true'
    if is_development:
        return jsonify({'success': False, 'error': str(error)}), status_code
    else:
        # æœ¬ç•ªç’°å¢ƒã§ã¯è©³ç´°ã‚’éš ã™
        logger.error(f"Error: {error}", exc_info=True)
        return jsonify({'success': False, 'error': 'Internal server error'}), status_code

# Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå‡¦ç†ãƒ­ãƒƒã‚¯ç”¨ï¼‰
_supabase_client = None

def get_supabase_client():
    """Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""
    global _supabase_client
    if _supabase_client is None:
        from shared.common.database.client import DatabaseClient
        db = DatabaseClient(use_service_role=True)
        _supabase_client = db.client
    return _supabase_client


def get_processing_lock():
    """Supabaseã‹ã‚‰å‡¦ç†ãƒ­ãƒƒã‚¯çŠ¶æ…‹ã‚’å–å¾—"""
    try:
        client = get_supabase_client()
        result = client.table('processing_lock').select('*').eq('id', 1).execute()
        if result.data:
            lock = result.data[0]
            # LOCK_TIMEOUT_SECONDSä»¥ä¸Šæ›´æ–°ãŒãªã‘ã‚Œã°æœŸé™åˆ‡ã‚Œã¨ã¿ãªã™
            if lock.get('is_processing') and lock.get('updated_at'):
                from datetime import datetime, timezone
                updated_at = datetime.fromisoformat(lock['updated_at'].replace('Z', '+00:00'))
                now = datetime.now(timezone.utc)
                if (now - updated_at).total_seconds() > LOCK_TIMEOUT_SECONDS:
                    logger.warning(f"å‡¦ç†ãƒ­ãƒƒã‚¯ãŒæœŸé™åˆ‡ã‚Œï¼ˆ{(now - updated_at).total_seconds():.0f}ç§’çµŒéï¼‰ã€‚è‡ªå‹•ãƒªã‚»ãƒƒãƒˆã€‚")
                    set_processing_lock(False)
                    return False
            return lock.get('is_processing', False)
        return False
    except Exception as e:
        logger.error(f"å‡¦ç†ãƒ­ãƒƒã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def set_processing_lock(is_processing: bool, max_retries: int = 3, timeout_sec: float = 10.0):
    """Supabaseã«å‡¦ç†ãƒ­ãƒƒã‚¯çŠ¶æ…‹ã‚’è¨­å®šï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰

    Args:
        is_processing: ãƒ­ãƒƒã‚¯çŠ¶æ…‹
        max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰
        timeout_sec: 1å›ã‚ãŸã‚Šã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ç§’ï¼‰

    Returns:
        bool: æˆåŠŸã—ãŸå ´åˆTrue
    """
    import time as time_module

    for attempt in range(max_retries):
        try:
            start_time = time_module.time()
            client = get_supabase_client()

            data = {
                'is_processing': is_processing,
                'updated_at': datetime.now(timezone.utc).isoformat()
            }
            if is_processing:
                data['started_at'] = datetime.now(timezone.utc).isoformat()
                # å‡¦ç†é–‹å§‹æ™‚: processingçŠ¶æ…‹ã®ã¾ã¾æ®‹ã£ã¦ã„ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’pendingã«ãƒªã‚»ãƒƒãƒˆ
                reset_stuck_documents()
            else:
                # å‡¦ç†çµ‚äº†æ™‚: å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ã‚¯ãƒªã‚¢ & processingçŠ¶æ…‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
                clear_all_workers()
                reset_stuck_documents()

            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
            elapsed = time_module.time() - start_time
            if elapsed > timeout_sec:
                raise TimeoutError(f"Processing lock operation timed out after {elapsed:.1f}s")

            client.table('processing_lock').update(data).eq('id', 1).execute()
            logger.info(f"å‡¦ç†ãƒ­ãƒƒã‚¯è¨­å®š: {is_processing}")
            return True

        except Exception as e:
            logger.warning(f"å‡¦ç†ãƒ­ãƒƒã‚¯è¨­å®šã‚¨ãƒ©ãƒ¼ (attempt {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time_module.sleep(1.0)  # ãƒªãƒˆãƒ©ã‚¤å‰ã«1ç§’å¾…æ©Ÿ
            else:
                logger.error(f"å‡¦ç†ãƒ­ãƒƒã‚¯è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆ{max_retries}å›è©¦è¡Œï¼‰: {e}")
                return False

    return False


def update_processing_lock():
    """å‡¦ç†ä¸­ã®ãƒ­ãƒƒã‚¯ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ›´æ–°ï¼ˆãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆï¼‰"""
    try:
        client = get_supabase_client()
        client.table('processing_lock').update({
            'updated_at': datetime.now(timezone.utc).isoformat()
        }).eq('id', 1).execute()
        return True
    except Exception as e:
        logger.error(f"ãƒ­ãƒƒã‚¯æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        return False


# ========== ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†ï¼ˆè¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¯¾å¿œï¼‰ ==========

import uuid
_instance_id = str(uuid.uuid4())[:8]  # ã“ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ID


def register_worker(doc_id: str, doc_title: str) -> bool:
    """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ç™»éŒ²ï¼ˆå‡¦ç†é–‹å§‹æ™‚ï¼‰- active_tasksã®ã¿ä½¿ç”¨"""
    # active_tasksã¸ã®è¿½åŠ ã¯å‘¼ã³å‡ºã—å…ƒã§å®Ÿæ–½æ¸ˆã¿
    # ã“ã“ã§ã¯ä½•ã‚‚ã—ãªã„ï¼ˆäº’æ›æ€§ã®ãŸã‚é–¢æ•°ã¯æ®‹ã™ï¼‰
    return True


def unregister_worker(doc_id: str) -> bool:
    """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’è§£é™¤ï¼ˆå‡¦ç†çµ‚äº†æ™‚ï¼‰- active_tasksã®ã¿ä½¿ç”¨"""
    # active_tasksã‹ã‚‰ã®å‰Šé™¤ã¯å‘¼ã³å‡ºã—å…ƒã§å®Ÿæ–½æ¸ˆã¿
    # ã“ã“ã§ã¯ä½•ã‚‚ã—ãªã„ï¼ˆäº’æ›æ€§ã®ãŸã‚é–¢æ•°ã¯æ®‹ã™ï¼‰
    return True


def clear_all_workers() -> bool:
    """å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ã‚¯ãƒªã‚¢ï¼ˆå‡¦ç†çµ‚äº†æ™‚ï¼‰- active_tasksã®ã¿ä½¿ç”¨"""
    global active_tasks
    # active_tasksã‚’ã‚¯ãƒªã‚¢
    count = len(active_tasks)
    active_tasks.clear()
    logger.info(f"å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸï¼ˆå‰Šé™¤ä»¶æ•°: {count}ä»¶ï¼‰")
    return True




def reset_stuck_documents() -> int:
    """
    processingçŠ¶æ…‹ã§ã‚¹ã‚¿ãƒƒã‚¯ã—ã¦ã„ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’pendingã«ãƒªã‚»ãƒƒãƒˆ

    ã‚¹ã‚¿ãƒƒã‚¯åˆ¤å®š:
    - processing_status='processing'
    - ã‹ã¤ active_tasks ã«å­˜åœ¨ã—ãªã„

    Returns:
        ãƒªã‚»ãƒƒãƒˆã—ãŸä»¶æ•°
    """
    global active_tasks
    try:
        from shared.common.database.client import DatabaseClient
        db = DatabaseClient(use_service_role=True)

        # processingçŠ¶æ…‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        result = db.client.table('Rawdata_FILE_AND_MAIL').select('id').eq('processing_status', 'processing').execute()
        processing_ids = [row['id'] for row in result.data] if result.data else []

        if not processing_ids:
            return 0

        # active_tasksã‹ã‚‰å®Ÿéš›ã«å‡¦ç†ä¸­ã®doc_idã‚’å–å¾—
        active_doc_ids = list(active_tasks.keys())

        # ã‚¹ã‚¿ãƒƒã‚¯ã—ã¦ã„ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ = processingã ãŒactive_tasksã«å­˜åœ¨ã—ãªã„
        stuck_ids = [doc_id for doc_id in processing_ids if doc_id not in active_doc_ids]

        if stuck_ids:
            # pendingã«ãƒªã‚»ãƒƒãƒˆ
            db.client.table('Rawdata_FILE_AND_MAIL').update({
                'processing_status': 'pending'
            }).in_('id', stuck_ids).execute()
            logger.info(f"ã‚¹ã‚¿ãƒƒã‚¯çŠ¶æ…‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸï¼ˆ{len(stuck_ids)}ä»¶ï¼‰")
            return len(stuck_ids)

        return 0
    except Exception as e:
        logger.error(f"reset_stuck_documents ã‚¨ãƒ©ãƒ¼: {e}")
        return 0

def update_worker_count() -> int:
    """ç¾åœ¨ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’è¿”ã™ - active_tasksã®ã¿ä½¿ç”¨"""
    global active_tasks
    # active_tasksã‹ã‚‰ç›´æ¥ã‚«ã‚¦ãƒ³ãƒˆï¼ˆDBæ›´æ–°ä¸è¦ï¼‰
    return len(active_tasks)


def update_progress_to_supabase(current_index: int, total_count: int, current_file: str,
                                 success_count: int, error_count: int, logs: list):
    """é€²æ—æƒ…å ±ã‚’Supabaseã«ä¿å­˜ï¼ˆè¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å…±æœ‰ç”¨ï¼‰"""
    global resource_manager, active_tasks
    try:
        client = get_supabase_client()
        # æœ€æ–°ã®ãƒ­ã‚°ã®ã¿ä¿å­˜
        latest_logs = logs[-MAX_LOG_ENTRIES_SUPABASE:] if len(logs) > MAX_LOG_ENTRIES_SUPABASE else logs

        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚‚å«ã‚ã‚‹ï¼ˆCloud Runç’°å¢ƒå¯¾å¿œï¼‰
        cpu_percent = get_cgroup_cpu()
        memory_info = get_cgroup_memory()

        # å®Ÿéš›ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’active_tasksã‹ã‚‰å–å¾—
        actual_workers = len(active_tasks)

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªresource_managerãŒå­˜åœ¨ã™ã‚Œã°ã€ãƒªã‚½ãƒ¼ã‚¹èª¿æ•´ã‚’å®Ÿè¡Œ
        if resource_manager is not None:
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã«åŸºã¥ã„ã¦ãƒªã‚½ãƒ¼ã‚¹èª¿æ•´ï¼ˆå®Ÿéš›ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’ä½¿ç”¨ï¼‰
            adjust_result = resource_manager.adjust_resources(memory_info['percent'], actual_workers)
            # èª¿æ•´çµæœã‚’processing_statusã«åæ˜ ï¼ˆwhileãƒ«ãƒ¼ãƒ—ã§å‚ç…§ã•ã‚Œã‚‹ï¼‰
            processing_status['resource_control']['throttle_delay'] = adjust_result['throttle_delay']
            processing_status['resource_control']['adjustment_count'] = resource_manager.adjustment_count

        # resource_managerã‹ã‚‰æœ€æ–°å€¤ã‚’å–å¾—ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        if resource_manager is not None:
            throttle_delay = resource_manager.throttle_delay
            max_parallel = resource_manager.max_parallel
            adjustment_count = resource_manager.adjustment_count
        else:
            resource_control = processing_status.get('resource_control', {})
            throttle_delay = resource_control.get('throttle_delay', 0.0)
            max_parallel = resource_control.get('max_parallel', 1)
            adjustment_count = resource_control.get('adjustment_count', 0)

        client.table('processing_lock').update({
            'current_index': current_index,
            'total_count': total_count,
            'current_file': current_file,
            'success_count': success_count,
            'error_count': error_count,
            'logs': latest_logs,
            'cpu_percent': cpu_percent,
            'memory_percent': memory_info['percent'],
            'memory_used_gb': memory_info['used_gb'],
            'memory_total_gb': memory_info['total_gb'],
            'throttle_delay': throttle_delay,
            'max_parallel': max_parallel,
            'current_workers': actual_workers,  # active_tasksã‹ã‚‰å–å¾—ã—ãŸå®Ÿéš›ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
            'adjustment_count': adjustment_count,
            'updated_at': datetime.now(timezone.utc).isoformat()
        }).eq('id', 1).execute()
        return True
    except Exception as e:
        logger.error(f"é€²æ—æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def get_progress_from_supabase() -> dict:
    """Supabaseã‹ã‚‰é€²æ—æƒ…å ±ã‚’å–å¾—ï¼ˆè¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å…±æœ‰ç”¨ï¼‰"""
    try:
        client = get_supabase_client()
        result = client.table('processing_lock').select('*').eq('id', 1).execute()
        if result.data:
            data = result.data[0]
            return {
                'current_index': data.get('current_index', 0),
                'total_count': data.get('total_count', 0),
                'current_file': data.get('current_file', ''),
                'success_count': data.get('success_count', 0),
                'error_count': data.get('error_count', 0),
                'logs': data.get('logs', []),
                # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±
                'cpu_percent': data.get('cpu_percent', 0.0),
                'memory_percent': data.get('memory_percent', 0.0),
                'memory_used_gb': data.get('memory_used_gb', 0.0),
                'memory_total_gb': data.get('memory_total_gb', 0.0),
                'throttle_delay': data.get('throttle_delay', 0.0),
                'adjustment_count': data.get('adjustment_count', 0)
            }
        return {
            'current_index': 0,
            'total_count': 0,
            'current_file': '',
            'success_count': 0,
            'error_count': 0,
            'logs': [],
            'cpu_percent': 0.0,
            'memory_percent': 0.0,
            'memory_used_gb': 0.0,
            'memory_total_gb': 0.0,
            'throttle_delay': 0.0,
            'adjustment_count': 0
        }
    except Exception as e:
        logger.error(f"é€²æ—å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'current_index': 0,
            'total_count': 0,
            'current_file': '',
            'success_count': 0,
            'error_count': 0,
            'logs': [],
            'cpu_percent': 0.0,
            'memory_percent': 0.0,
            'memory_used_gb': 0.0,
            'memory_total_gb': 0.0,
            'throttle_delay': 0.0,
            'adjustment_count': 0
        }


def get_worker_status() -> dict:
    """ç¾åœ¨ã®ãƒ¯ãƒ¼ã‚«ãƒ¼çŠ¶æ³ã‚’å–å¾— - active_tasksã®ã¿ä½¿ç”¨"""
    global resource_manager, active_tasks
    try:
        # active_tasksã‹ã‚‰å®Ÿéš›ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å–å¾—
        current_workers = len(active_tasks)

        # resource_managerã‹ã‚‰max_parallelã‚’å–å¾—
        max_parallel = resource_manager.max_parallel if resource_manager else 1

        # processing_lockã‹ã‚‰is_processingã®ã¿å–å¾—
        client = get_supabase_client()
        lock_result = client.table('processing_lock').select('is_processing').eq('id', 1).execute()
        lock_data = lock_result.data[0] if lock_result.data else {}

        # active_tasksã®å†…å®¹ã‚’workerså½¢å¼ã«å¤‰æ›
        workers = [
            {
                'doc_id': doc_id,
                'doc_title': task_info.get('title', ''),
                'started_at': task_info.get('started_at', '')
            }
            for doc_id, task_info in active_tasks.items()
        ]

        return {
            'max_parallel': max_parallel,
            'current_workers': current_workers,  # active_tasksã‹ã‚‰å–å¾—ã—ãŸå®Ÿéš›ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
            'is_processing': lock_data.get('is_processing', False),
            'workers': workers
        }
    except Exception as e:
        logger.error(f"ãƒ¯ãƒ¼ã‚«ãƒ¼çŠ¶æ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {'max_parallel': 1, 'current_workers': 0, 'is_processing': False, 'workers': []}


def adjust_max_parallel(memory_percent: float) -> int:
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã«åŸºã¥ã„ã¦max_parallelã‚’èª¿æ•´

    é‡è¦: å®Ÿè¡Œæ•°ãŒä¸Šé™ã«é”ã—ã¦ã„ã‚‹å ´åˆã®ã¿ä¸¦åˆ—æ•°ã‚’å¢—ã‚„ã™
    ï¼ˆå®Ÿéš›ã«ãƒ•ãƒ«ç¨¼åƒã—ã¦ã„ã‚‹çŠ¶æ…‹ã§ä½™è£•ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰å¢—ã‚„ã™ï¼‰
    """
    global resource_manager, active_tasks
    try:
        if not resource_manager:
            return 3

        current_max = resource_manager.max_parallel
        current_workers = len(active_tasks)

        new_max = current_max

        # ãƒ¡ãƒ¢ãƒªã«ä½™è£•ãŒã‚ã‚Šã€ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒä¸Šé™ã«é”ã—ã¦ã„ã‚‹å ´åˆã®ã¿å¢—åŠ 
        # current_workers >= current_max - 1 ã§ã€ã»ã¼ãƒ•ãƒ«ç¨¼åƒã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        if memory_percent < 60 and current_workers >= current_max - 1:
            new_max = min(current_max + 1, 20)  # æœ€å¤§20
        # ãƒ¡ãƒ¢ãƒªãŒé€¼è¿«ã—ã¦ã„ã‚‹å ´åˆã¯æ¸›å°‘
        elif memory_percent > 85:
            new_max = max(current_max - 1, 1)  # æœ€å°1

        if new_max != current_max:
            resource_manager.max_parallel = new_max
            logger.info(f"max_parallelèª¿æ•´: {current_max} â†’ {new_max} (ãƒ¡ãƒ¢ãƒª: {memory_percent:.1f}%)")

        return new_max
    except Exception as e:
        logger.error(f"max_parallelèª¿æ•´ã‚¨ãƒ©ãƒ¼: {e}")
        return 3


def can_start_new_worker() -> bool:
    """æ–°ã—ã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’é–‹å§‹ã§ãã‚‹ã‹ç¢ºèª - active_tasksã®ã¿ä½¿ç”¨"""
    global resource_manager, active_tasks
    current_workers = len(active_tasks)
    max_parallel = resource_manager.max_parallel if resource_manager else 1
    return current_workers < max_parallel


# ========== å‡¦ç†é€²æ—ã®ç®¡ç†ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€è¡¨ç¤ºç”¨ï¼‰ ==========
processing_status = {
    'is_processing': False,
    'current_index': 0,
    'total_count': 0,
    'current_file': '',
    'success_count': 0,
    'error_count': 0,
    'logs': [],
    # ã‚¹ãƒ†ãƒ¼ã‚¸é€²æ—ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ã®é€²æ—ï¼‰
    'current_stage': '',
    'stage_progress': 0.0,  # 0.0 - 1.0
    # ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡æƒ…å ±
    'resource_control': {
        'throttle_delay': 0.0,
        'adjustment_count': 0
    }
}

# CPUä½¿ç”¨ç‡è¨ˆç®—ç”¨ã®å‰å›ã®å€¤
_last_cpu_stats = {'usage_usec': 0, 'timestamp': time.time()}

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªãƒªã‚½ãƒ¼ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ï¼ˆupdate_progress_to_supabaseã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ç”¨ï¼‰
resource_manager = None

# ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¿ã‚¹ã‚¯ç®¡ç†ï¼ˆè¾æ›¸å‹: {doc_id: {'title': str, 'started_at': str}}ï¼‰
active_tasks = {}


def get_cgroup_memory():
    """cgroupã‹ã‚‰ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚’å–å¾—ï¼ˆCloud Run Gen2 cgroup v2å¯¾å¿œï¼‰"""
    import os

    try:
        current = None
        max_mem = None
        inactive_file = 0

        # cgroup v2 ãƒ‘ã‚¹ã‚’å„ªå…ˆçš„ã«è©¦è¡Œï¼ˆCloud Run Gen2ï¼‰
        v2_current_path = '/sys/fs/cgroup/memory.current'
        v2_max_path = '/sys/fs/cgroup/memory.max'
        v2_stat_path = '/sys/fs/cgroup/memory.stat'

        # cgroup v1 ãƒ‘ã‚¹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
        v1_current_path = '/sys/fs/cgroup/memory/memory.usage_in_bytes'
        v1_max_path = '/sys/fs/cgroup/memory/memory.limit_in_bytes'
        v1_stat_path = '/sys/fs/cgroup/memory/memory.stat'

        # cgroup v2 ã‚’è©¦è¡Œ
        if os.path.exists(v2_current_path):
            logger.debug("[MEMORY] Using cgroup v2")
            with open(v2_current_path, 'r') as f:
                current = int(f.read().strip())
            with open(v2_max_path, 'r') as f:
                max_val = f.read().strip()
                # "max" ã¯ç„¡åˆ¶é™ã‚’æ„å‘³ã™ã‚‹
                if max_val == 'max':
                    max_mem = psutil.virtual_memory().total
                else:
                    max_mem = int(max_val)

            # cgroup v2 ã® memory.stat ã‹ã‚‰ inactive_file ã‚’å–å¾—
            if os.path.exists(v2_stat_path):
                with open(v2_stat_path, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) == 2:
                            key, value = parts
                            if key == 'inactive_file':
                                inactive_file = int(value)

        # cgroup v1 ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        elif os.path.exists(v1_current_path):
            logger.debug("[MEMORY] Using cgroup v1")
            with open(v1_current_path, 'r') as f:
                current = int(f.read().strip())
            with open(v1_max_path, 'r') as f:
                max_mem = int(f.read().strip())
                # éå¸¸ã«å¤§ãã„å€¤ã®å ´åˆã¯ç„¡åˆ¶é™
                if max_mem > 1e15:
                    max_mem = psutil.virtual_memory().total

            # cgroup v1 ã® memory.stat ã‹ã‚‰ inactive_file ã‚’å–å¾—
            if os.path.exists(v1_stat_path):
                with open(v1_stat_path, 'r') as f:
                    stat_content = f.read()
                    for line in stat_content.split('\n'):
                        parts = line.strip().split()
                        if len(parts) == 2:
                            key, value = parts
                            # total_inactive_file ã¾ãŸã¯ inactive_file ã‚’æ¢ã™
                            if key in ('total_inactive_file', 'inactive_file'):
                                inactive_file = int(value)
                                logger.debug(f"[MEMORY] Found {key}={value}")
                                break
                    # ãƒ‡ãƒãƒƒã‚°: è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã€æœ€åˆã®10è¡Œã‚’ãƒ­ã‚°
                    if inactive_file == 0:
                        lines = stat_content.split('\n')[:10]
                        logger.warning(f"[MEMORY] inactive_file not found. First 10 lines: {lines}")

        if current is None or max_mem is None:
            raise FileNotFoundError("cgroup memory files not found")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’é™¤ã„ãŸå®Ÿéš›ã®ä½¿ç”¨é‡ï¼ˆKubernetesæ¨™æº–ï¼‰
        actual_used = current - inactive_file
        percent = (actual_used / max_mem) * 100
        used_gb = actual_used / (1024 ** 3)
        total_gb = max_mem / (1024 ** 3)

        logger.debug(f"[MEMORY] current={current/(1024**3):.2f}GB, inactive={inactive_file/(1024**3):.2f}GB, max={max_mem/(1024**3):.2f}GB, percent={percent:.1f}%")

        return {
            'percent': round(percent, 1),
            'used_gb': round(used_gb, 2),
            'total_gb': round(total_gb, 2)
        }
    except Exception as e:
        # cgroupãŒä½¿ãˆãªã„å ´åˆã¯psutilã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        logger.warning(f"cgroup memoryèª­ã¿å–ã‚Šå¤±æ•—ã€psutilã‚’ä½¿ç”¨: {e}")
        try:
            memory = psutil.virtual_memory()
            return {
                'percent': round(memory.percent, 1),
                'used_gb': round(memory.used / (1024 ** 3), 2),
                'total_gb': round(memory.total / (1024 ** 3), 2)
            }
        except Exception as e2:
            # psutilã‚‚å¤±æ•—ã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
            logger.error(f"psutil memoryèª­ã¿å–ã‚Šã‚‚å¤±æ•—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨: {e2}")
            return {
                'percent': 50.0,  # å®‰å…¨å´ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                'used_gb': 8.0,
                'total_gb': 16.0
            }


def get_cgroup_cpu():
    """CPUä½¿ç”¨ç‡ã‚’å–å¾—ï¼ˆCloud Runå¯¾å¿œï¼‰"""
    global _last_cpu_stats

    try:
        # Cloud Run Gen2: cgroup v2 ã¾ãŸã¯ v1 ã®ãƒ‘ã‚¹ã‚’è©¦è¡Œ
        usage_nsec = None
        cpu_usage_paths = [
            '/sys/fs/cgroup/cpu.stat',  # cgroup v2 (Gen2)
            '/sys/fs/cgroup/cpuacct/cpuacct.usage'  # cgroup v1 (Gen1)
        ]

        for path in cpu_usage_paths:
            try:
                if path.endswith('cpu.stat'):
                    # cgroup v2: cpu.statã‹ã‚‰ usage_usec ã‚’èª­ã¿å–ã‚‹
                    with open(path, 'r') as f:
                        for line in f:
                            if line.startswith('usage_usec'):
                                usage_nsec = int(line.split()[1]) * 1000  # ãƒã‚¤ã‚¯ãƒ­ç§’â†’ãƒŠãƒç§’
                                break
                else:
                    # cgroup v1: cpuacct.usageã‹ã‚‰ç›´æ¥èª­ã¿å–ã‚‹
                    with open(path, 'r') as f:
                        usage_nsec = int(f.read().strip())

                if usage_nsec is not None:
                    break
            except Exception:
                continue

        if usage_nsec is None:
            # cgroupã‹ã‚‰èª­ã¿å–ã‚Œãªã„å ´åˆã¯psutilã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return round(psutil.cpu_percent(interval=0.1), 1)

        current_time = time.time()

        # åˆå›å‘¼ã³å‡ºã—ã®å ´åˆã¯åˆæœŸåŒ–ã—ã¦psutilã§å–å¾—
        if _last_cpu_stats['usage_usec'] == 0:
            _last_cpu_stats = {'usage_usec': usage_nsec, 'timestamp': current_time}
            # psutilã§å³åº§ã®å€¤ã‚’è¿”ã™
            return round(psutil.cpu_percent(interval=0.1), 1)

        # å‰å›ã¨ã®å·®åˆ†ã‹ã‚‰ä½¿ç”¨ç‡ã‚’è¨ˆç®—
        time_delta = current_time - _last_cpu_stats['timestamp']
        usage_delta = usage_nsec - _last_cpu_stats['usage_usec']

        if time_delta > 0:
            # usage_deltaã¯ãƒŠãƒç§’ã€time_deltaã¯ç§’
            # CPUä½¿ç”¨ç‡ = (ä½¿ç”¨æ™‚é–“ã®å¢—åŠ  / çµŒéæ™‚é–“) / CPUæ•°
            cpu_count = psutil.cpu_count() or 4
            cpu_percent = (usage_delta / (time_delta * 1_000_000_000)) * 100 / cpu_count
            cpu_percent = max(0.0, min(cpu_percent, 100.0))  # 0-100%ã®ç¯„å›²
        else:
            cpu_percent = 0.0

        # æ¬¡å›ã®ãŸã‚ã«ä¿å­˜
        _last_cpu_stats = {'usage_usec': usage_nsec, 'timestamp': current_time}

        return round(cpu_percent, 1)
    except Exception as e:
        # å¤±æ•—æ™‚ã¯psutilã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        logger.debug(f"cgroup CPUèª­ã¿å–ã‚Šå¤±æ•—ã€psutilã‚’ä½¿ç”¨: {e}")
        return round(psutil.cpu_percent(interval=0.1), 1)


class AdaptiveResourceManager:
    """ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼

    ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã«åŸºã¥ã„ã¦ä¸¦åˆ—æ•°ã¨ã‚¹ãƒ­ãƒƒãƒˆãƒ«é…å»¶ã‚’å‹•çš„ã«èª¿æ•´ã—ã¾ã™ã€‚

    åˆ¶å¾¡ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆç·ãƒ¡ãƒ¢ãƒªé‡ã«å¿œã˜ã¦é–¾å€¤ã‚’è‡ªå‹•èª¿æ•´ï¼‰:

    16GBç’°å¢ƒï¼ˆä¿å®ˆçš„ï¼‰:
    - ä¸¦åˆ—æ•°åˆ¶å¾¡: memory < 60% â†’ å¢—åŠ , > 90% â†’ å‰Šæ¸›
    - ã‚¹ãƒ­ãƒƒãƒˆãƒ«: > 85% â†’ æ¸›é€Ÿé–‹å§‹, < 70% â†’ æ¸›é€Ÿç·©å’Œ

    32GBç’°å¢ƒï¼ˆæ”»ã‚ã®è¨­å®šï¼‰:
    - ä¸¦åˆ—æ•°åˆ¶å¾¡: memory < 70% â†’ å¢—åŠ , > 95% â†’ å‰Šæ¸›
    - ã‚¹ãƒ­ãƒƒãƒˆãƒ«: > 90% â†’ æ¸›é€Ÿé–‹å§‹, < 80% â†’ æ¸›é€Ÿç·©å’Œ

    32GBã®åˆ©ç‚¹:
    - 95%ã¾ã§ä½¿ã£ã¦ã‚‚æ®‹ã‚Š1.6GBï¼ˆ16GBã®90%ã¨åŒç­‰ï¼‰
    - ä¸¦åˆ—æ•°ã‚’æœ€å¤§100ã¾ã§å¢—ã‚„ã›ã‚‹å¯èƒ½æ€§
    - å‡¦ç†æ™‚é–“çŸ­ç¸®ã§ã‚³ã‚¹ãƒˆãƒ¡ãƒªãƒƒãƒˆ
    """

    def __init__(self, initial_max_parallel=1, min_parallel=1, max_parallel=100, history_size=3):
        """åˆæœŸåŒ–

        Args:
            initial_max_parallel: åˆæœŸä¸¦åˆ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰
            min_parallel: æœ€å°ä¸¦åˆ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰
            max_parallel: æœ€å¤§ä¸¦åˆ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰
            history_size: ç§»å‹•å¹³å‡ç”¨ã®å±¥æ­´ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰
        """
        self.max_parallel = initial_max_parallel
        self.min_parallel = min_parallel
        self.max_parallel_limit = max_parallel
        self.throttle_delay = 0.0  # ã‚¹ãƒ­ãƒƒãƒˆãƒ«é…å»¶ï¼ˆç§’ï¼‰

        # ç·ãƒ¡ãƒ¢ãƒªé‡ã‚’å–å¾—ã—ã¦é–¾å€¤ã‚’å‹•çš„ã«è¨­å®š
        total_memory_gb = 16.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        try:
            mem_info = get_cgroup_memory()
            total_memory_gb = mem_info['total_gb']
        except:
            pass

        # ã—ãã„å€¤ã‚’ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºã«å¿œã˜ã¦èª¿æ•´
        if total_memory_gb >= 32:
            # 32GBä»¥ä¸Š: æ”»ã‚ã®è¨­å®šï¼ˆæ®‹ã‚Š1.6GB = 95%ã¾ã§è¨±å®¹ï¼‰
            self.memory_low = 70.0   # ä½™è£•ã‚ã‚Š â†’ ä¸¦åˆ—æ•°å¢—åŠ 
            self.memory_high = 90.0  # é€¼è¿« â†’ æ¸›é€Ÿé–‹å§‹
            self.memory_critical = 95.0  # å±é™º â†’ ä¸¦åˆ—æ•°å‰Šæ¸›
            self.memory_recover = 80.0  # å›å¾© â†’ æ¸›é€Ÿç·©å’Œ
        elif total_memory_gb >= 24:
            # 24GB: ä¸­é–“è¨­å®š
            self.memory_low = 65.0
            self.memory_high = 87.0
            self.memory_critical = 92.0
            self.memory_recover = 75.0
        else:
            # 16GBä»¥ä¸‹: ä¿å®ˆçš„è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            self.memory_low = 60.0   # ä½™è£•ã‚ã‚Š â†’ ä¸¦åˆ—æ•°å¢—åŠ 
            self.memory_high = 85.0  # é€¼è¿« â†’ æ¸›é€Ÿé–‹å§‹
            self.memory_critical = 90.0  # å±é™º â†’ ä¸¦åˆ—æ•°å‰Šæ¸›
            self.memory_recover = 70.0  # å›å¾© â†’ æ¸›é€Ÿç·©å’Œ

        from loguru import logger
        self.logger = logger
        self.logger.info(f"[INIT] AdaptiveResourceManager: total_memory={total_memory_gb:.1f}GB, thresholds=[{self.memory_low}%, {self.memory_high}%, {self.memory_critical}%]")

        # èª¿æ•´ã‚¹ãƒ†ãƒƒãƒ—
        self.parallel_step = 1
        self.throttle_step = 0.5  # ç§’
        self.max_throttle = 3.0   # æœ€å¤§ã‚¹ãƒ­ãƒƒãƒˆãƒ«é…å»¶

        # ç§»å‹•å¹³å‡ç”¨ã®å±¥æ­´
        self.history_size = history_size
        self.memory_history = []

        # çµ±è¨ˆ
        self.adjustment_count = 0
        self.last_adjustment_time = time.time()

        from loguru import logger
        self.logger = logger

    def adjust_resources(self, memory_percent, current_workers=0):
        """ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ã«åŸºã¥ã„ã¦ä¸¦åˆ—æ•°ã¨ã‚¹ãƒ­ãƒƒãƒˆãƒ«ã‚’èª¿æ•´

        Args:
            memory_percent: ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ï¼ˆ%ï¼‰
            current_workers: ç¾åœ¨ã®å®Ÿè¡Œä¸­ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°

        Returns:
            dict: èª¿æ•´æƒ…å ± {'max_parallel': int, 'throttle_delay': float, 'adjusted': bool}
        """
        # å±¥æ­´ã«è¿½åŠ 
        self.memory_history.append(memory_percent)
        if len(self.memory_history) > self.history_size:
            self.memory_history.pop(0)

        # ç§»å‹•å¹³å‡ã‚’è¨ˆç®—ï¼ˆç›´è¿‘3å›ã®å¹³å‡ï¼‰
        memory_avg = sum(self.memory_history) / len(self.memory_history)

        # ç§»å‹•å¹³å‡ã§åˆ¤æ–­ï¼ˆç¬é–“å€¤ã§ã¯ãªãï¼‰
        memory_percent = memory_avg

        original_parallel = self.max_parallel
        original_throttle = self.throttle_delay
        adjusted = False

        # ãƒ•ã‚§ãƒ¼ã‚º3: ä¸¦åˆ—æ•°å‰Šæ¸›ï¼ˆç·Šæ€¥æ™‚ï¼‰- æœ€å„ªå…ˆ
        if memory_percent > self.memory_critical:
            if self.max_parallel > self.min_parallel:
                self.max_parallel = max(self.max_parallel - self.parallel_step, self.min_parallel)
                adjusted = True
                self.logger.warning(
                    f"[ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡] ğŸš¨ ãƒ¡ãƒ¢ãƒªé€¼è¿« ({memory_percent:.1f}%) â†’ ä¸¦åˆ—æ•°å‰Šæ¸›: {original_parallel} â†’ {self.max_parallel}"
                )

        # ãƒ•ã‚§ãƒ¼ã‚º2: æ¸›é€Ÿåˆ¶å¾¡
        if memory_percent > self.memory_high:
            # é€¼è¿« â†’ æ¸›é€Ÿ
            if self.throttle_delay < self.max_throttle:
                self.throttle_delay = min(self.throttle_delay + self.throttle_step, self.max_throttle)
                adjusted = True
                self.logger.info(
                    f"[ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡] âš ï¸ ãƒ¡ãƒ¢ãƒªé«˜ä½¿ç”¨ ({memory_percent:.1f}%) â†’ æ¸›é€Ÿ: {original_throttle:.1f}ç§’ â†’ {self.throttle_delay:.1f}ç§’"
                )
        elif memory_percent < self.memory_recover and self.throttle_delay > 0:
            # å›å¾© â†’ æ¸›é€Ÿç·©å’Œ
            self.throttle_delay = max(self.throttle_delay - self.throttle_step, 0.0)
            adjusted = True
            self.logger.info(
                f"[ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡] âœ… ãƒ¡ãƒ¢ãƒªå›å¾© ({memory_percent:.1f}%) â†’ æ¸›é€Ÿç·©å’Œ: {original_throttle:.1f}ç§’ â†’ {self.throttle_delay:.1f}ç§’"
            )

        # max_parallel = current_workers + 1 (å¸¸ã«å®Ÿè¡Œæ•°+1ãŒä¸Šé™)
        # å®Ÿè¡Œæ•°ãŒæ¸›ã‚Œã°max_parallelã‚‚æ¸›ã‚‹ã€å¢—ãˆã‚Œã°å¢—ãˆã‚‹
        new_max_parallel = min(current_workers + 1, self.max_parallel_limit)
        new_max_parallel = max(new_max_parallel, self.min_parallel)  # æœ€å°å€¤ã‚’ä¿è¨¼

        if new_max_parallel != self.max_parallel:
            old_max = self.max_parallel
            self.max_parallel = new_max_parallel
            adjusted = True
            self.logger.info(
                f"[ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡] max_parallelèª¿æ•´: {old_max} â†’ {self.max_parallel} (å®Ÿè¡Œæ•°: {current_workers})"
            )

        if adjusted:
            self.adjustment_count += 1
            self.last_adjustment_time = time.time()

        return {
            'max_parallel': self.max_parallel,
            'throttle_delay': self.throttle_delay,
            'adjusted': adjusted,
            'memory_percent': memory_percent
        }

    async def apply_throttle(self):
        """ã‚¹ãƒ­ãƒƒãƒˆãƒ«é…å»¶ã‚’é©ç”¨"""
        if self.throttle_delay > 0:
            import asyncio
            await asyncio.sleep(self.throttle_delay)

    def get_status(self):
        """ç¾åœ¨ã®çŠ¶æ…‹ã‚’å–å¾—"""
        return {
            'max_parallel': self.max_parallel,
            'throttle_delay': self.throttle_delay,
            'adjustment_count': self.adjustment_count,
            'last_adjustment': self.last_adjustment_time
        }


# loguruã®ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼šãƒ­ã‚°ã‚’processing_statusã«é€ä¿¡
def log_to_processing_status(message):
    """loguruã®ãƒ­ã‚°ã‚’processing_statusã«è¿½åŠ  + ã‚¹ãƒ†ãƒ¼ã‚¸æ¤œå‡º"""
    log_record = message.record
    level = log_record['level'].name
    msg = log_record['message']

    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆINFOã®ã¿è¡¨ç¤ºï¼‰
    if level in ['INFO', 'WARNING', 'ERROR']:
        timestamp = datetime.now().strftime('%H:%M:%S')
        formatted_msg = f"[{timestamp}] {msg}"
        processing_status['logs'].append(formatted_msg)

        # ã‚¹ãƒ†ãƒ¼ã‚¸æ¤œå‡ºï¼ˆãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’æ¨æ¸¬ï¼‰
        msg_lower = msg.lower()
        if 'stage h' in msg_lower or 'æ§‹é€ åŒ–' in msg_lower:
            processing_status['current_stage'] = 'Stage H: æ§‹é€ åŒ–'
            processing_status['stage_progress'] = 0.3
        elif 'stage j' in msg_lower or 'ãƒãƒ£ãƒ³ã‚¯' in msg_lower:
            processing_status['current_stage'] = 'Stage J: ãƒãƒ£ãƒ³ã‚¯åŒ–'
            processing_status['stage_progress'] = 0.5
        elif 'stage k' in msg_lower or 'embedding' in msg_lower or 'embed' in msg_lower:
            processing_status['current_stage'] = 'Stage K: Embedding'
            processing_status['stage_progress'] = 0.7
        elif 'æˆåŠŸ' in msg or 'âœ…' in msg:
            processing_status['current_stage'] = 'å®Œäº†'
            processing_status['stage_progress'] = 1.0
        elif 'ã‚¨ãƒ©ãƒ¼' in msg or 'âŒ' in msg:
            processing_status['current_stage'] = 'ã‚¨ãƒ©ãƒ¼'
            processing_status['stage_progress'] = 1.0
        elif 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰' in msg_lower or 'download' in msg_lower:
            processing_status['current_stage'] = 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­'
            processing_status['stage_progress'] = 0.1

        # ãƒ­ã‚°ã¯æœ€å¤§ä»¶æ•°ã¾ã§ä¿æŒ
        if len(processing_status['logs']) > MAX_LOG_ENTRIES:
            processing_status['logs'] = processing_status['logs'][-MAX_LOG_ENTRIES:]


# loguruã«ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰å†…ã§å€‹åˆ¥ã«è¿½åŠ ã™ã‚‹ãŸã‚ã“ã“ã§ã¯è¿½åŠ ã—ãªã„ï¼‰
# logger.add(log_to_processing_status, format="{message}")


@app.route('/')
def index():
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ - å‡¦ç†ç”»é¢ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ"""
    return render_template('processing.html',
        supabase_url=os.getenv('SUPABASE_URL', ''),
        supabase_anon_key=os.getenv('SUPABASE_KEY', ''))


@app.route('/processing')
def processing():
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸"""
    return render_template('processing.html',
        supabase_url=os.getenv('SUPABASE_URL', ''),
        supabase_anon_key=os.getenv('SUPABASE_KEY', ''))


@app.route('/api/health', methods=['GET'])
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return jsonify({
        'status': 'ok',
        'message': 'Document Processing System is running',
        'version': '2025-01-11-v2'  # ãƒ‡ãƒ—ãƒ­ã‚¤ç¢ºèªç”¨
    })


@app.route('/api/process/progress', methods=['GET'])
def get_process_progress():
    """
    å‡¦ç†é€²æ—ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‚’å–å¾—ï¼ˆSupabaseã‹ã‚‰å…±æœ‰çŠ¶æ…‹ã‚’å–å¾—ï¼‰
    å…¨ã¦ã®æƒ…å ±ã¯ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒSupabaseã«ä¿å­˜ã—ãŸå€¤ã‚’ä½¿ç”¨
    """
    try:
        # Supabaseã‹ã‚‰ãƒ¯ãƒ¼ã‚«ãƒ¼çŠ¶æ³ã‚’å–å¾—ï¼ˆè¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¯¾å¿œï¼‰
        worker_status = get_worker_status()

        # Supabaseã‹ã‚‰é€²æ—æƒ…å ±ã‚’å–å¾—ï¼ˆè¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å…±æœ‰ï¼‰
        # CPU/ãƒ¡ãƒ¢ãƒª/ã‚¹ãƒ­ãƒƒãƒˆãƒ«ç­‰ã‚‚ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®å€¤ã‚’å–å¾—
        progress = get_progress_from_supabase()

        # ç¾åœ¨å‡¦ç†ä¸­ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®é€²æ—ã‚’å–å¾—
        current_stage = ''
        stage_progress = 0.0
        try:
            # processingä¸­ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ï¼ˆæœ€æ–°1ä»¶ï¼‰
            client = get_supabase_client()
            processing_doc = client.table('Rawdata_FILE_AND_MAIL').select('processing_stage, processing_progress').eq('processing_status', 'processing').order('created_at', desc=False).limit(1).execute()
            if processing_doc.data and len(processing_doc.data) > 0:
                current_stage = processing_doc.data[0].get('processing_stage', '')
                stage_progress = processing_doc.data[0].get('processing_progress', 0.0)
        except Exception as e:
            logger.error(f"é€²æ—å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

        return jsonify({
            'success': True,
            'processing': worker_status['is_processing'],
            'current_index': progress['current_index'],
            'total_count': progress['total_count'],
            'current_file': progress['current_file'],
            'success_count': progress['success_count'],
            'error_count': progress['error_count'],
            'logs': progress['logs'],  # æœ€æ–°150ä»¶
            # ã‚¹ãƒ†ãƒ¼ã‚¸é€²æ—ï¼ˆSupabaseã‹ã‚‰å–å¾—ï¼‰
            'current_stage': current_stage,
            'stage_progress': stage_progress,
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ï¼ˆSupabaseã‹ã‚‰å–å¾—ï¼‰
            'system': {
                'cpu_percent': progress['cpu_percent'],
                'memory_percent': progress['memory_percent'],
                'memory_used_gb': progress['memory_used_gb'],
                'memory_total_gb': progress['memory_total_gb']
            },
            'resource_control': {
                'current_workers': worker_status['current_workers'],
                'max_parallel': worker_status['max_parallel'],
                'throttle_delay': progress['throttle_delay'],
                'adjustment_count': progress['adjustment_count']
            },
            'workers': worker_status['workers']  # å‡¦ç†ä¸­ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§
        })
    except Exception as e:
        return safe_error_response(e)


@app.route('/api/workspaces', methods=['GET'])
def get_workspaces():
    """
    ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—
    """
    try:
        from shared.common.database.client import DatabaseClient
        db = DatabaseClient()

        # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—
        query = db.client.table('Rawdata_FILE_AND_MAIL').select('workspace').execute()

        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŠ½å‡º
        workspaces = set()
        for row in query.data:
            workspace = row.get('workspace')
            if workspace:
                workspaces.add(workspace)

        # ã‚½ãƒ¼ãƒˆã—ã¦ãƒªã‚¹ãƒˆåŒ–
        workspace_list = sorted(list(workspaces))

        return jsonify({
            'success': True,
            'workspaces': workspace_list
        })

    except Exception as e:
        logger.error(f"ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return safe_error_response(e)


@app.route('/api/process/stats', methods=['GET'])
def get_process_stats():
    """
    å‡¦ç†ã‚­ãƒ¥ãƒ¼ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—

    ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®æ„å‘³:
    - pending: æœªå‡¦ç†ï¼ˆã¾ã ãƒãƒƒãƒã«é¸ã°ã‚Œã¦ã„ãªã„ï¼‰
    - processing: ã“ã®ãƒãƒƒãƒã®å‡¦ç†å¯¾è±¡ã¨ã—ã¦é¸æŠæ¸ˆã¿ï¼ˆé‡è¤‡é˜²æ­¢ãƒãƒ¼ã‚¯ï¼‰
    - completed: å‡¦ç†å®Œäº†
    - error: å‡¦ç†ã‚¨ãƒ©ãƒ¼
    """
    try:
        from shared.common.database.client import DatabaseClient
        db = DatabaseClient()

        workspace = request.args.get('workspace', 'all')

        query = db.client.table('Rawdata_FILE_AND_MAIL').select('processing_status, workspace')

        if workspace != 'all':
            query = query.eq('workspace', workspace)

        response = query.execute()

        stats = {
            'pending': 0,
            'processing': 0,  # ãƒãƒƒãƒã«é¸ã°ã‚ŒãŸä»¶æ•°
            'completed': 0,
            'error': 0  # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰è¡¨ç¤ºç”¨ã¯ 'error'
        }

        for doc in response.data:
            status = doc.get('processing_status')
            # nullã¯ç„¡è¦–ï¼ˆpending/processing/completed/failedã®ã¿ã‚«ã‚¦ãƒ³ãƒˆï¼‰
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã¯ 'failed' ã®ã¾ã¾ã€ãƒ•ãƒ­ãƒ³ãƒˆã«ã¯ 'error' ã¨ã—ã¦é€ã‚‹
            if status == 'failed':
                stats['error'] = stats.get('error', 0) + 1
            elif status in stats:
                stats[status] = stats.get(status, 0) + 1

        return jsonify({
            'success': True,
            'stats': stats
        })

    except Exception as e:
        logger.error(f"çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return safe_error_response(e)


@app.route('/api/process/start', methods=['POST'])
@require_api_key
def start_processing():
    """
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚’é–‹å§‹ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œï¼‰
    """
    global processing_status

    # Supabaseã§ãƒ­ãƒƒã‚¯çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆè¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¯¾å¿œï¼‰
    if get_processing_lock():
        return jsonify({
            'success': False,
            'error': 'æ—¢ã«å‡¦ç†ãŒå®Ÿè¡Œä¸­ã§ã™'
        }), 400

    try:
        import threading
        import asyncio

        data = request.get_json()
        workspace = data.get('workspace', 'all')
        limit = data.get('limit', 100)
        preserve_workspace = data.get('preserve_workspace', True)

        # Supabaseã«ãƒ­ãƒƒã‚¯ã‚’è¨­å®š
        set_processing_lock(True)

        # loguruãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ ï¼ˆprocess_queued_documents.pyã®ãƒ­ã‚°ã‚’ã‚­ãƒ£ãƒƒãƒï¼‰
        handler_id = logger.add(log_to_processing_status, format="{message}")

        # é€²æ—çŠ¶æ³ã‚’åˆæœŸåŒ–ï¼ˆå‡¦ç†é–‹å§‹ã‚’ã™ãã«è¡¨ç¤ºï¼‰
        processing_status['is_processing'] = True
        processing_status['current_index'] = 0
        processing_status['total_count'] = 0
        processing_status['current_file'] = 'åˆæœŸåŒ–ä¸­...'
        processing_status['success_count'] = 0
        processing_status['error_count'] = 0
        processing_status['current_stage'] = ''
        processing_status['stage_progress'] = 0.0
        processing_status['logs'] = [
            f"[{datetime.now().strftime('%H:%M:%S')}] å‡¦ç†é–‹å§‹æº–å‚™ä¸­...",
            f"[{datetime.now().strftime('%H:%M:%S')}] ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹: {workspace}, åˆ¶é™: {limit}ä»¶"
        ]
        # ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡ã‚’åˆæœŸåŒ–
        processing_status['resource_control'] = {
            'throttle_delay': 0.0,
            'adjustment_count': 0
        }

        # Supabaseã«åˆæœŸçŠ¶æ…‹ã‚’ä¿å­˜
        update_progress_to_supabase(0, 0, 'åˆæœŸåŒ–ä¸­...', 0, 0, processing_status['logs'])

        # DocumentProcessorã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        update_progress_to_supabase(0, 0, 'ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿ä¸­...', 0, 0, processing_status['logs'])
        from process_queued_documents import DocumentProcessor

        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] ãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–ä¸­...")
        update_progress_to_supabase(0, 0, 'ãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–ä¸­...', 0, 0, processing_status['logs'])
        processor = DocumentProcessor()

        # pending ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—ä¸­...")
        update_progress_to_supabase(0, 0, 'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—ä¸­...', 0, 0, processing_status['logs'])
        docs = processor.get_pending_documents(workspace, limit)

        if not docs:
            processing_status['is_processing'] = False
            processing_status['current_file'] = ''
            processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] å‡¦ç†å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            set_processing_lock(False)  # ãƒ­ãƒƒã‚¯è§£æ”¾
            return jsonify({
                'success': True,
                'message': 'å‡¦ç†å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“',
                'processed': 0
            })

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’æ›´æ–°
        processing_status['total_count'] = len(docs)
        processing_status['current_file'] = ''
        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] {len(docs)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã¾ã—ãŸ")
        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        update_progress_to_supabase(0, len(docs), 'ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†é–‹å§‹', 0, 0, processing_status['logs'])

        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†é–¢æ•°
        def background_processing():
            global processing_status
            print("[DEBUG] background_processing() é–‹å§‹")
            logger.info("[DEBUG] background_processing() é–‹å§‹")

            # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ç”¨ã®Timer
            update_timer = None

            def periodic_resource_update():
                """5ç§’ã”ã¨ã«ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æ›´æ–°ï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œï¼‰"""
                nonlocal update_timer
                if not processing_status['is_processing']:
                    logger.info("[PERIODIC_UPDATE] å‡¦ç†çµ‚äº†ã«ã¤ãã‚¿ã‚¤ãƒãƒ¼åœæ­¢")
                    return

                try:
                    logger.debug("[PERIODIC_UPDATE] ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±æ›´æ–°é–‹å§‹")
                    # update_progress_to_supabaseã‚’å‘¼ã³å‡ºã—
                    update_progress_to_supabase(
                        processing_status['current_index'],
                        processing_status['total_count'],
                        processing_status['current_file'],
                        processing_status['success_count'],
                        processing_status['error_count'],
                        processing_status['logs']
                    )
                    logger.debug("[PERIODIC_UPDATE] ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±æ›´æ–°å®Œäº†")
                except Exception as e:
                    logger.error(f"[PERIODIC_UPDATE] ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                finally:
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚æ¬¡ã®ã‚¿ã‚¤ãƒãƒ¼ã‚’ç¢ºå®Ÿã«è¨­å®š
                    if processing_status['is_processing']:
                        update_timer = threading.Timer(5.0, periodic_resource_update)
                        update_timer.daemon = True
                        update_timer.start()

            # å®šæœŸæ›´æ–°ã‚’é–‹å§‹ï¼ˆ5ç§’é–“éš”ï¼‰
            update_timer = threading.Timer(5.0, periodic_resource_update)
            update_timer.daemon = True
            update_timer.start()
            logger.info("[PERIODIC_UPDATE] ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹ï¼ˆ5ç§’é–“éš”ï¼‰")

            async def process_all():
                print("[DEBUG] process_all() é–‹å§‹")
                logger.info("[DEBUG] process_all() é–‹å§‹")
                # ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ï¼‰
                # ãƒªã‚½ãƒ¼ã‚¹é©å¿œå‹ä¸¦åˆ—åˆ¶å¾¡
                # - åˆæœŸä¸¦åˆ—æ•°: 1ï¼ˆæœ€å°åŒæ™‚å®Ÿè¡Œæ•°ï¼‰
                # - æœ€å°ä¸¦åˆ—æ•°: 1ï¼ˆãƒªã‚½ãƒ¼ã‚¹é€¼è¿«æ™‚ã‚‚1ã¯ç¶­æŒï¼‰
                # - æœ€å¤§ä¸¦åˆ—æ•°: 100ï¼ˆä½™è£•ãŒã‚ã‚‹å ´åˆã¯ã“ã“ã¾ã§å¢—ã‚„ã™ï¼‰
                # update_progress_to_supabaseã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦åˆæœŸåŒ–
                global resource_manager
                resource_manager = AdaptiveResourceManager(initial_max_parallel=1, min_parallel=1, max_parallel=100)

                # ä¸¦åˆ—æ•°åˆ¶å¾¡ç”¨ã®ã‚»ãƒãƒ•ã‚©ï¼ˆå‹•çš„ã«èª¿æ•´ï¼‰
                # NOTE: ã‚»ãƒãƒ•ã‚©ã¯å›ºå®šå€¤ãªã®ã§ã€ã‚¿ã‚¹ã‚¯å†…ã§åˆ¶å¾¡ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã®active_tasksï¼ˆè¾æ›¸ï¼‰ã¨ã¯åˆ¥ã«ã€asyncio.Taskã®ãƒªã‚¹ãƒˆã‚’ç®¡ç†
                pending_async_tasks = []
                processed_count = 0

                # å€‹åˆ¥ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¿ã‚¹ã‚¯
                async def process_single_document(doc, index):
                    """å€‹åˆ¥ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†"""
                    global active_tasks
                    nonlocal processed_count

                    # åœæ­¢ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
                    if not processing_status['is_processing']:
                        return False

                    file_name = doc.get('file_name', 'unknown')
                    title = doc.get('title', '') or '(ã‚¿ã‚¤ãƒˆãƒ«æœªç”Ÿæˆ)'
                    doc_id = doc.get('id', str(index))

                    # active_tasksã«ãƒ¯ãƒ¼ã‚«ãƒ¼æƒ…å ±ã‚’è¿½åŠ 
                    active_tasks[doc_id] = {
                        'title': title,
                        'started_at': datetime.now(timezone.utc).isoformat()
                    }
                    # Supabaseã«ãƒ¯ãƒ¼ã‚«ãƒ¼ç™»éŒ²
                    register_worker(doc_id, title)

                    try:
                        # å‡¦ç†å‰ã®ãƒ¡ãƒ¢ãƒªã‚’è¨˜éŒ²
                        mem_before = get_cgroup_memory()

                        # é€²æ—ã‚’æ›´æ–°
                        processing_status['current_index'] = index
                        processing_status['current_file'] = title
                        processing_status['logs'].append(
                            f"[{datetime.now().strftime('%H:%M:%S')}] [{index}/{len(docs)}] å‡¦ç†ä¸­: {title}"
                        )

                        # ãƒ­ã‚°ã¯æœ€å¤§ä»¶æ•°ã¾ã§ä¿æŒ
                        if len(processing_status['logs']) > MAX_LOG_ENTRIES:
                            processing_status['logs'] = processing_status['logs'][-MAX_LOG_ENTRIES:]

                        # ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—ã—ã¦ãƒªã‚½ãƒ¼ã‚¹èª¿æ•´
                        memory_info = get_cgroup_memory()
                        memory_percent = memory_info['percent']
                        worker_status = get_worker_status()
                        current_workers = worker_status['current_workers']

                        # ãƒªã‚½ãƒ¼ã‚¹èª¿æ•´ï¼ˆä¸¦åˆ—æ•°ã‚’å‹•çš„ã«èª¿æ•´ï¼‰
                        status = resource_manager.adjust_resources(memory_percent, current_workers)
                        processing_status['resource_control']['throttle_delay'] = status['throttle_delay']
                        processing_status['resource_control']['adjustment_count'] = resource_manager.adjustment_count

                        # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã‚’å®šç¾©
                        def progress_callback(stage):
                            """å„ã‚¹ãƒ†ãƒ¼ã‚¸é–‹å§‹æ™‚ã«Supabaseã‚’æ›´æ–°"""
                            logger.info(f"[PROGRESS] Stage {stage} é–‹å§‹")
                            processing_status['current_file'] = f"{title} (Stage {stage})"
                            update_progress_to_supabase(
                                processing_status['current_index'],
                                processing_status['total_count'],
                                processing_status['current_file'],
                                processing_status['success_count'],
                                processing_status['error_count'],
                                processing_status['logs']
                            )

                        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ï¼ˆé€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æ¸¡ã™ï¼‰
                        success = await processor.process_document(doc, preserve_workspace, progress_callback=progress_callback)

                        # å‡¦ç†å¾Œã®ãƒ¡ãƒ¢ãƒªã‚’è¨˜éŒ²ï¼ˆæœ€åˆã®3ä»¶ã®ã¿ãƒ­ã‚°å‡ºåŠ›ï¼‰
                        if index <= 3:
                            mem_after = get_cgroup_memory()
                            mem_delta = mem_after['used_gb'] - mem_before['used_gb']
                            logger.info(f"[MEMORY PER DOC] Doc#{index}: before={mem_before['used_gb']:.2f}GB, after={mem_after['used_gb']:.2f}GB, delta={mem_delta:.2f}GB, parallel={len(active_tasks)}")

                        # ã‚¹ãƒ­ãƒƒãƒˆãƒ«é…å»¶ã‚’é©ç”¨
                        await resource_manager.apply_throttle()

                        processed_count += 1

                        if success:
                            processing_status['success_count'] += 1
                            processing_status['logs'].append(
                                f"[{datetime.now().strftime('%H:%M:%S')}] âœ… æˆåŠŸ: {title}"
                            )
                        else:
                            processing_status['error_count'] += 1
                            processing_status['logs'].append(
                                f"[{datetime.now().strftime('%H:%M:%S')}] âŒ ã‚¨ãƒ©ãƒ¼: {title}"
                            )

                        # Supabaseã«çµæœã‚’ä¿å­˜
                        update_progress_to_supabase(
                            index, len(docs), title,
                            processing_status['success_count'],
                            processing_status['error_count'],
                            processing_status['logs']
                        )

                        return success
                    finally:
                        # active_tasksã‹ã‚‰ãƒ¯ãƒ¼ã‚«ãƒ¼æƒ…å ±ã‚’å‰Šé™¤
                        if doc_id in active_tasks:
                            del active_tasks[doc_id]
                        # Supabaseã‹ã‚‰ãƒ¯ãƒ¼ã‚«ãƒ¼è§£é™¤
                        unregister_worker(doc_id)

                # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã¯threading.Timerã§å®šæœŸå®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ä¸è¦

                try:
                    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†
                    for i, doc in enumerate(docs, 1):
                        logger.info(f"[FOR_LOOP] ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ {i}/{len(docs)} é–‹å§‹, pending_async_tasks={len(pending_async_tasks)}, max_parallel={resource_manager.max_parallel}")

                        # åœæ­¢ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
                        if not processing_status['is_processing']:
                            processing_status['logs'].append(
                                f"[{datetime.now().strftime('%H:%M:%S')}] âš ï¸ å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ"
                            )
                            break

                        # ä¸¦åˆ—æ•°åˆ¶å¾¡ï¼špending_async_tasksãŒ max_parallel æœªæº€ã«ãªã‚‹ã¾ã§å¾…æ©Ÿ
                        # å‹•çš„ã«èª¿æ•´ã•ã‚Œã‚‹ max_parallel ã‚’ä½¿ç”¨ï¼ˆãƒªã‚½ãƒ¼ã‚¹ã«å¿œã˜ã¦1ï½100ã§å¤‰å‹•ï¼‰
                        if len(pending_async_tasks) >= resource_manager.max_parallel:
                            logger.info(f"[WHILE_ENTER] pending_async_tasks={len(pending_async_tasks)} >= max_parallel={resource_manager.max_parallel}, å¾…æ©Ÿé–‹å§‹")
                        while len(pending_async_tasks) >= resource_manager.max_parallel:
                            # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‚’å‰Šé™¤
                            done_tasks = [t for t in pending_async_tasks if t.done()]
                            for t in done_tasks:
                                pending_async_tasks.remove(t)

                            if len(pending_async_tasks) >= resource_manager.max_parallel:
                                # ã¾ã ä¸¦åˆ—æ•°ãŒä¸Šé™ã«é”ã—ã¦ã„ã‚‹å ´åˆã¯å°‘ã—å¾…æ©Ÿ
                                await asyncio.sleep(0.1)

                        # æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
                        task = asyncio.create_task(process_single_document(doc, i))
                        pending_async_tasks.append(task)

                        # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã«åˆ¶å¾¡ã‚’æ¸¡ã™
                        await asyncio.sleep(0)

                    # ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ
                    if pending_async_tasks:
                        await asyncio.gather(*pending_async_tasks, return_exceptions=True)

                finally:
                    # å‡¦ç†å®Œäº†ï¼ˆSupabaseã‚‚æ›´æ–°ï¼‰
                    processing_status['is_processing'] = False
                    set_processing_lock(False)
                    logger.info("[PROCESS] å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†å®Œäº†")

            try:
                asyncio.run(process_all())

                # å‡¦ç†å®Œäº†
                processing_status['is_processing'] = False
                processing_status['current_file'] = ''
                processing_status['logs'].append(
                    f"[{datetime.now().strftime('%H:%M:%S')}] å‡¦ç†å®Œäº†: æˆåŠŸ={processing_status['success_count']}, ã‚¨ãƒ©ãƒ¼={processing_status['error_count']}"
                )

                # Supabaseã«å®Œäº†çŠ¶æ…‹ã‚’ä¿å­˜
                update_progress_to_supabase(
                    processing_status['current_index'],
                    processing_status['total_count'],
                    '',
                    processing_status['success_count'],
                    processing_status['error_count'],
                    processing_status['logs']
                )
            except Exception as e:
                processing_status['is_processing'] = False
                processing_status['logs'].append(
                    f"[{datetime.now().strftime('%H:%M:%S')}] âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}"
                )
                print(f"[ERROR] ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                # å®šæœŸæ›´æ–°ã‚¿ã‚¤ãƒãƒ¼ã‚’åœæ­¢
                processing_status['is_processing'] = False  # ã‚¿ã‚¤ãƒãƒ¼ã®ãƒ«ãƒ¼ãƒ—æ¡ä»¶ã‚’Falseã«
                set_processing_lock(False)  # Supabaseã‚‚æ›´æ–°
                if update_timer is not None:
                    update_timer.cancel()
                    logger.info("[PERIODIC_UPDATE] ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")

                # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªresource_managerã‚’ãƒªã‚»ãƒƒãƒˆ
                global resource_manager
                resource_manager = None
                logger.info("[RESOURCE] resource_managerã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")

                # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤
                logger.remove(handler_id)
                # Supabaseãƒ­ãƒƒã‚¯è§£æ”¾
                set_processing_lock(False)

        # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å‡¦ç†ã‚’é–‹å§‹
        # daemon=False: Cloud RunãŒã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è½ã¨ã—ã¦ã‚‚ã€å‡¦ç†å®Œäº†ã¾ã§å¾…æ©Ÿ
        thread = threading.Thread(target=background_processing, daemon=False)
        thread.start()

        # ã™ãã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
        return jsonify({
            'success': True,
            'message': 'å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã—ãŸ',
            'total_count': len(docs)
        })

    except Exception as e:
        processing_status['is_processing'] = False
        set_processing_lock(False)  # ãƒ­ãƒƒã‚¯è§£æ”¾
        logger.error(f"å‡¦ç†é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
        return safe_error_response(e)


@app.route('/api/process/stop', methods=['POST'])
@require_api_key
def stop_processing():
    """
    å‡¦ç†ã‚’åœæ­¢
    """
    global processing_status

    # Supabaseã®ãƒ­ãƒƒã‚¯ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã¯åˆ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§è¦‹ãªã„ï¼‰
    if not get_processing_lock():
        return jsonify({
            'success': False,
            'error': 'å®Ÿè¡Œä¸­ã®å‡¦ç†ãŒã‚ã‚Šã¾ã›ã‚“'
        }), 400

    # åœæ­¢ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‹Supabaseä¸¡æ–¹ï¼‰
    processing_status['is_processing'] = False
    set_processing_lock(False)  # Supabaseãƒ­ãƒƒã‚¯è§£æ”¾ + ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒªã‚¢
    processing_status['logs'].append(
        f"[{datetime.now().strftime('%H:%M:%S')}] âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦åœæ­¢ã•ã‚Œã¾ã—ãŸ"
    )

    return jsonify({
        'success': True,
        'message': 'å‡¦ç†ã‚’åœæ­¢ã—ã¾ã—ãŸ'
    })


@app.route('/api/process/reset', methods=['POST'])
@require_api_key
def reset_processing():
    """
    å‡¦ç†ãƒ•ãƒ©ã‚°ã‚’å¼·åˆ¶ãƒªã‚»ãƒƒãƒˆï¼ˆSupabase + ãƒ­ãƒ¼ã‚«ãƒ«ä¸¡æ–¹ï¼‰
    """
    global processing_status

    # Supabaseãƒ­ãƒƒã‚¯è§£æ”¾
    set_processing_lock(False)

    # Supabaseã®å…¨çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
    try:
        client = get_supabase_client()
        client.table('processing_lock').update({
            'current_index': 0,
            'total_count': 0,
            'success_count': 0,
            'error_count': 0,
            'current_file': '',
            'current_workers': 0,
            'max_parallel': 1,
            'throttle_delay': 0.0,
            'adjustment_count': 0,
            'cpu_percent': 0.0,
            'memory_percent': 0.0,
            'memory_used_gb': 0.0,
            'memory_total_gb': 0.0,
            'logs': []
        }).eq('id', 1).execute()
    except Exception as e:
        logger.error(f"Supabaseãƒªã‚»ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")

    # ãƒ­ãƒ¼ã‚«ãƒ«çŠ¶æ…‹ã‚‚ãƒªã‚»ãƒƒãƒˆ
    processing_status['is_processing'] = False
    processing_status['current_index'] = 0
    processing_status['total_count'] = 0
    processing_status['current_file'] = ''
    processing_status['success_count'] = 0
    processing_status['error_count'] = 0
    processing_status['current_stage'] = ''
    processing_status['stage_progress'] = 0.0
    processing_status['logs'] = [
        f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ”„ å‡¦ç†ãƒ•ãƒ©ã‚°ã‚’å¼·åˆ¶ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸï¼ˆSupabase + ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰"
    ]
    processing_status['resource_control'] = {
        'throttle_delay': 0.0,
        'adjustment_count': 0
    }

    return jsonify({
        'success': True,
        'message': 'å‡¦ç†ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ'
    })


if __name__ == '__main__':
    # é–‹ç™ºç’°å¢ƒã§ã®å®Ÿè¡Œ
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
