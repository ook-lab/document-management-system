"""
Flask Web Application - Document Processing System
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆå‡¦ç†å°‚ç”¨ï¼‰
"""
import os
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ™‚ç”¨ï¼‰
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from datetime import datetime, timezone
from flask import Flask, render_template, request, jsonify
from flask_cors import CORS
from loguru import logger
import psutil
import time

app = Flask(__name__)
CORS(app)

# Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå‡¦ç†ãƒ­ãƒƒã‚¯ç”¨ï¼‰
_supabase_client = None

def get_supabase_client():
    """Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""
    global _supabase_client
    if _supabase_client is None:
        from shared.common.database.client import DatabaseClient
        db = DatabaseClient(use_service_role=True)
        _supabase_client = db.client
    return _supabase_client


def get_processing_lock():
    """Supabaseã‹ã‚‰å‡¦ç†ãƒ­ãƒƒã‚¯çŠ¶æ…‹ã‚’å–å¾—"""
    try:
        client = get_supabase_client()
        result = client.table('processing_lock').select('*').eq('id', 1).execute()
        if result.data:
            lock = result.data[0]
            # 5åˆ†ä»¥ä¸Šæ›´æ–°ãŒãªã‘ã‚Œã°æœŸé™åˆ‡ã‚Œã¨ã¿ãªã™
            if lock.get('is_processing') and lock.get('updated_at'):
                from datetime import datetime, timezone
                updated_at = datetime.fromisoformat(lock['updated_at'].replace('Z', '+00:00'))
                now = datetime.now(timezone.utc)
                if (now - updated_at).total_seconds() > 300:  # 5åˆ†
                    logger.warning(f"å‡¦ç†ãƒ­ãƒƒã‚¯ãŒæœŸé™åˆ‡ã‚Œï¼ˆ{(now - updated_at).total_seconds():.0f}ç§’çµŒéï¼‰ã€‚è‡ªå‹•ãƒªã‚»ãƒƒãƒˆã€‚")
                    set_processing_lock(False)
                    return False
            return lock.get('is_processing', False)
        return False
    except Exception as e:
        logger.error(f"å‡¦ç†ãƒ­ãƒƒã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def set_processing_lock(is_processing: bool):
    """Supabaseã«å‡¦ç†ãƒ­ãƒƒã‚¯çŠ¶æ…‹ã‚’è¨­å®š"""
    try:
        client = get_supabase_client()
        data = {
            'is_processing': is_processing,
            'updated_at': datetime.now(timezone.utc).isoformat()
        }
        if is_processing:
            data['started_at'] = datetime.now(timezone.utc).isoformat()
            # å‡¦ç†é–‹å§‹æ™‚: processingçŠ¶æ…‹ã®ã¾ã¾æ®‹ã£ã¦ã„ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’pendingã«ãƒªã‚»ãƒƒãƒˆ
            reset_stuck_documents()
        else:
            # å‡¦ç†çµ‚äº†æ™‚: å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ã‚¯ãƒªã‚¢ & processingçŠ¶æ…‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
            clear_all_workers()
            reset_stuck_documents()
        client.table('processing_lock').update(data).eq('id', 1).execute()
        logger.info(f"å‡¦ç†ãƒ­ãƒƒã‚¯è¨­å®š: {is_processing}")
        return True
    except Exception as e:
        logger.error(f"å‡¦ç†ãƒ­ãƒƒã‚¯è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        return False


def update_processing_lock():
    """å‡¦ç†ä¸­ã®ãƒ­ãƒƒã‚¯ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ›´æ–°ï¼ˆãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆï¼‰"""
    try:
        client = get_supabase_client()
        client.table('processing_lock').update({
            'updated_at': datetime.now(timezone.utc).isoformat()
        }).eq('id', 1).execute()
        return True
    except Exception as e:
        logger.error(f"ãƒ­ãƒƒã‚¯æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        return False


# ========== ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†ï¼ˆè¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¯¾å¿œï¼‰ ==========

import uuid
_instance_id = str(uuid.uuid4())[:8]  # ã“ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ID


def register_worker(doc_id: str, doc_title: str) -> bool:
    """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ç™»éŒ²ï¼ˆå‡¦ç†é–‹å§‹æ™‚ï¼‰"""
    try:
        client = get_supabase_client()
        worker_id = f"{_instance_id}-{doc_id[:8]}"
        client.table('processing_workers').upsert({
            'instance_id': worker_id,
            'doc_id': doc_id,
            'doc_title': doc_title,
            'started_at': datetime.now(timezone.utc).isoformat(),
            'updated_at': datetime.now(timezone.utc).isoformat()
        }).execute()
        # current_workersã‚’æ›´æ–°
        update_worker_count()
        return True
    except Exception as e:
        logger.error(f"ãƒ¯ãƒ¼ã‚«ãƒ¼ç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def unregister_worker(doc_id: str) -> bool:
    """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’è§£é™¤ï¼ˆå‡¦ç†çµ‚äº†æ™‚ï¼‰"""
    try:
        client = get_supabase_client()
        worker_id = f"{_instance_id}-{doc_id[:8]}"
        client.table('processing_workers').delete().eq('instance_id', worker_id).execute()
        # current_workersã‚’æ›´æ–°
        update_worker_count()
        logger.info(f"ãƒ¯ãƒ¼ã‚«ãƒ¼è§£é™¤: {worker_id}")
        return True
    except Exception as e:
        logger.error(f"ãƒ¯ãƒ¼ã‚«ãƒ¼è§£é™¤ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def clear_all_workers() -> bool:
    """å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ã‚¯ãƒªã‚¢ï¼ˆå‡¦ç†çµ‚äº†æ™‚ï¼‰"""
    try:
        client = get_supabase_client()
        # ã‚¹ãƒ†ãƒƒãƒ—1: å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã®instance_idã‚’å–å¾—
        result = client.table('processing_workers').select('instance_id').execute()
        instance_ids = [row['instance_id'] for row in result.data] if result.data else []

        # ã‚¹ãƒ†ãƒƒãƒ—2: instance_idãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‰Šé™¤
        if instance_ids:
            client.table('processing_workers').delete().in_('instance_id', instance_ids).execute()

        # ã‚¹ãƒ†ãƒƒãƒ—3: current_workersã‚’0ã«æ›´æ–°
        client.table('processing_lock').update({
            'current_workers': 0
        }).eq('id', 1).execute()

        logger.info(f"å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸï¼ˆå‰Šé™¤ä»¶æ•°: {len(instance_ids)}ä»¶ï¼‰")
        return True
    except Exception as e:
        logger.error(f"å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}")
        return False




def reset_stuck_documents() -> int:
    """
    processingçŠ¶æ…‹ã§ã‚¹ã‚¿ãƒƒã‚¯ã—ã¦ã„ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’pendingã«ãƒªã‚»ãƒƒãƒˆ

    ã‚¹ã‚¿ãƒƒã‚¯åˆ¤å®š:
    - processing_status='processing'
    - ã‹ã¤ processing_workers ãƒ†ãƒ¼ãƒ–ãƒ«ã«å­˜åœ¨ã—ãªã„

    Returns:
        ãƒªã‚»ãƒƒãƒˆã—ãŸä»¶æ•°
    """
    try:
        from shared.common.database.client import DatabaseClient
        db = DatabaseClient(use_service_role=True)

        # processingçŠ¶æ…‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        result = db.client.table('Rawdata_FILE_AND_MAIL').select('id').eq('processing_status', 'processing').execute()
        processing_ids = [row['id'] for row in result.data] if result.data else []

        if not processing_ids:
            return 0

        # processing_workersã‹ã‚‰å®Ÿéš›ã«å‡¦ç†ä¸­ã®doc_idã‚’å–å¾—
        workers_result = db.client.table('processing_workers').select('doc_id').execute()
        active_doc_ids = [row['doc_id'] for row in workers_result.data] if workers_result.data else []

        # ã‚¹ã‚¿ãƒƒã‚¯ã—ã¦ã„ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ = processingã ãŒãƒ¯ãƒ¼ã‚«ãƒ¼ã«å­˜åœ¨ã—ãªã„
        stuck_ids = [doc_id for doc_id in processing_ids if doc_id not in active_doc_ids]

        if stuck_ids:
            # pendingã«ãƒªã‚»ãƒƒãƒˆ
            db.client.table('Rawdata_FILE_AND_MAIL').update({
                'processing_status': 'pending'
            }).in_('id', stuck_ids).execute()
            logger.info(f"ã‚¹ã‚¿ãƒƒã‚¯çŠ¶æ…‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸï¼ˆ{len(stuck_ids)}ä»¶ï¼‰")
            return len(stuck_ids)

        return 0
    except Exception as e:
        logger.error(f"reset_stuck_documents ã‚¨ãƒ©ãƒ¼: {e}")
        return 0

def update_worker_count() -> int:
    """ç¾åœ¨ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¦Supabaseã«ä¿å­˜"""
    try:
        client = get_supabase_client()
        result = client.table('processing_workers').select('instance_id').execute()
        count = len(result.data) if result.data else 0
        client.table('processing_lock').update({
            'current_workers': count
        }).eq('id', 1).execute()
        return count
    except Exception as e:
        logger.error(f"ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        return 0


def update_progress_to_supabase(current_index: int, total_count: int, current_file: str,
                                 success_count: int, failed_count: int, logs: list):
    """é€²æ—æƒ…å ±ã‚’Supabaseã«ä¿å­˜ï¼ˆè¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å…±æœ‰ç”¨ï¼‰"""
    try:
        client = get_supabase_client()
        # æœ€æ–°150ä»¶ã®ãƒ­ã‚°ã®ã¿ä¿å­˜
        latest_logs = logs[-150:] if len(logs) > 150 else logs

        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚‚å«ã‚ã‚‹ï¼ˆCloud Runç’°å¢ƒå¯¾å¿œï¼‰
        cpu_percent = get_cgroup_cpu()
        memory_info = get_cgroup_memory()

        # ã‚¹ãƒ­ãƒƒãƒˆãƒ«æƒ…å ±ã‚’å–å¾—
        throttle_delay = processing_status.get('resource_control', {}).get('throttle_delay', 0.0)

        client.table('processing_lock').update({
            'current_index': current_index,
            'total_count': total_count,
            'current_file': current_file,
            'success_count': success_count,
            'failed_count': failed_count,
            'logs': latest_logs,
            'cpu_percent': cpu_percent,
            'memory_percent': memory_info['percent'],
            'memory_used_gb': memory_info['used_gb'],
            'memory_total_gb': memory_info['total_gb'],
            'throttle_delay': throttle_delay,
            'updated_at': datetime.now(timezone.utc).isoformat()
        }).eq('id', 1).execute()
        return True
    except Exception as e:
        logger.error(f"é€²æ—æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def get_progress_from_supabase() -> dict:
    """Supabaseã‹ã‚‰é€²æ—æƒ…å ±ã‚’å–å¾—ï¼ˆè¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å…±æœ‰ç”¨ï¼‰"""
    try:
        client = get_supabase_client()
        result = client.table('processing_lock').select('*').eq('id', 1).execute()
        if result.data:
            data = result.data[0]
            return {
                'current_index': data.get('current_index', 0),
                'total_count': data.get('total_count', 0),
                'current_file': data.get('current_file', ''),
                'success_count': data.get('success_count', 0),
                'failed_count': data.get('failed_count', 0),
                'logs': data.get('logs', [])
            }
        return {
            'current_index': 0,
            'total_count': 0,
            'current_file': '',
            'success_count': 0,
            'failed_count': 0,
            'logs': []
        }
    except Exception as e:
        logger.error(f"é€²æ—å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'current_index': 0,
            'total_count': 0,
            'current_file': '',
            'success_count': 0,
            'failed_count': 0,
            'logs': []
        }


def get_worker_status() -> dict:
    """ç¾åœ¨ã®ãƒ¯ãƒ¼ã‚«ãƒ¼çŠ¶æ³ã‚’å–å¾—"""
    try:
        client = get_supabase_client()
        # processing_lockã‹ã‚‰max_parallelã¨current_workersã‚’å–å¾—
        lock_result = client.table('processing_lock').select('*').eq('id', 1).execute()
        # processing_workersã‹ã‚‰è©³ç´°ã‚’å–å¾—
        workers_result = client.table('processing_workers').select('*').execute()

        lock_data = lock_result.data[0] if lock_result.data else {}
        workers = workers_result.data if workers_result.data else []

        return {
            'max_parallel': lock_data.get('max_parallel', 10),
            'current_workers': len(workers),
            'is_processing': lock_data.get('is_processing', False),
            'workers': workers
        }
    except Exception as e:
        logger.error(f"ãƒ¯ãƒ¼ã‚«ãƒ¼çŠ¶æ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {'max_parallel': 3, 'current_workers': 0, 'is_processing': False, 'workers': []}


def adjust_max_parallel(memory_percent: float) -> int:
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã«åŸºã¥ã„ã¦max_parallelã‚’èª¿æ•´

    é‡è¦: å®Ÿè¡Œæ•°ãŒä¸Šé™ã«é”ã—ã¦ã„ã‚‹å ´åˆã®ã¿ä¸¦åˆ—æ•°ã‚’å¢—ã‚„ã™
    ï¼ˆå®Ÿéš›ã«ãƒ•ãƒ«ç¨¼åƒã—ã¦ã„ã‚‹çŠ¶æ…‹ã§ä½™è£•ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰å¢—ã‚„ã™ï¼‰
    """
    try:
        client = get_supabase_client()
        status = get_worker_status()
        current_max = status['max_parallel']
        current_workers = status['current_workers']

        new_max = current_max

        # ãƒ¡ãƒ¢ãƒªã«ä½™è£•ãŒã‚ã‚Šã€ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒä¸Šé™ã«é”ã—ã¦ã„ã‚‹å ´åˆã®ã¿å¢—åŠ 
        # current_workers >= current_max - 1 ã§ã€ã»ã¼ãƒ•ãƒ«ç¨¼åƒã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        if memory_percent < 60 and current_workers >= current_max - 1:
            new_max = min(current_max + 1, 20)  # æœ€å¤§20
        # ãƒ¡ãƒ¢ãƒªãŒé€¼è¿«ã—ã¦ã„ã‚‹å ´åˆã¯æ¸›å°‘
        elif memory_percent > 85:
            new_max = max(current_max - 1, 1)  # æœ€å°1

        if new_max != current_max:
            client.table('processing_lock').update({
                'max_parallel': new_max
            }).eq('id', 1).execute()
            logger.info(f"max_parallelèª¿æ•´: {current_max} â†’ {new_max} (ãƒ¡ãƒ¢ãƒª: {memory_percent:.1f}%)")

        return new_max
    except Exception as e:
        logger.error(f"max_parallelèª¿æ•´ã‚¨ãƒ©ãƒ¼: {e}")
        return 3


def can_start_new_worker() -> bool:
    """æ–°ã—ã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’é–‹å§‹ã§ãã‚‹ã‹ç¢ºèª"""
    status = get_worker_status()
    return status['current_workers'] < status['max_parallel']


# ========== å‡¦ç†é€²æ—ã®ç®¡ç†ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€è¡¨ç¤ºç”¨ï¼‰ ==========
processing_status = {
    'is_processing': False,
    'current_index': 0,
    'total_count': 0,
    'current_file': '',
    'success_count': 0,
    'failed_count': 0,
    'logs': [],
    # ã‚¹ãƒ†ãƒ¼ã‚¸é€²æ—ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ã®é€²æ—ï¼‰
    'current_stage': '',
    'stage_progress': 0.0,  # 0.0 - 1.0
    # ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡æƒ…å ±
    'resource_control': {
        'max_parallel': 3,
        'current_parallel': 0,
        'throttle_delay': 0.0,
        'adjustment_count': 0
    }
}

# CPUä½¿ç”¨ç‡è¨ˆç®—ç”¨ã®å‰å›ã®å€¤
_last_cpu_stats = {'usage_usec': 0, 'timestamp': time.time()}


def get_cgroup_memory():
    """cgroupã‹ã‚‰ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚’å–å¾—ï¼ˆCloud Runå¯¾å¿œï¼‰"""
    try:
        # Cloud Run: cgroup v1 ã®ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’å–å¾—
        with open('/sys/fs/cgroup/memory/memory.usage_in_bytes', 'r') as f:
            current = int(f.read().strip())
        with open('/sys/fs/cgroup/memory/memory.limit_in_bytes', 'r') as f:
            max_mem = int(f.read().strip())

            # éå¸¸ã«å¤§ãã„å€¤ã®å ´åˆã¯ç„¡åˆ¶é™ãªã®ã§ã€ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨
            # (é€šå¸¸ã€9223372036854771712 ãªã©ã®å€¤)
            if max_mem > 1e15:
                max_mem = psutil.virtual_memory().total

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªã‚’é™¤å¤–ã—ã¦æ­£ç¢ºãªä½¿ç”¨é‡ã‚’è¨ˆç®—
        # Kubernetesã®æ¨™æº–: working_set = usage - total_inactive_file
        inactive_file = 0
        total_cache = 0
        try:
            # cgroup v2 ã¨ v1 ã®ä¸¡æ–¹ã«å¯¾å¿œ
            import os
            memory_stat_path = '/sys/fs/cgroup/memory.stat'  # v2
            if not os.path.exists(memory_stat_path):
                memory_stat_path = '/sys/fs/cgroup/memory/memory.stat'  # v1

            with open(memory_stat_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) == 2:
                        key, value = parts
                        if key == 'total_inactive_file':
                            inactive_file = int(value)
                        elif key == 'total_cache' or key == 'cache':
                            total_cache = int(value)

            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã§ä¸¡æ–¹ã®è¨ˆç®—æ–¹æ³•ã‚’ç¢ºèª
            if inactive_file > 0 or total_cache > 0:
                logger.info(f"[MEMORY DEBUG] usage={current/(1024**3):.2f}GB, inactive_file={inactive_file/(1024**3):.2f}GB, total_cache={total_cache/(1024**3):.2f}GB")
                logger.info(f"[MEMORY DEBUG] working_set(inactiveé™¤å¤–)={(current-inactive_file)/(1024**3):.2f}GB, working_set(å…¨cacheé™¤å¤–)={(current-total_cache)/(1024**3):.2f}GB")
        except Exception as e:
            logger.debug(f"[MEMORY DEBUG] memory.statèª­ã¿å–ã‚Šå¤±æ•—: {e}")

        # Kubernetesæ¨™æº–ã®è¨ˆç®—å¼ã‚’ä½¿ç”¨ï¼ˆtotal_inactive_fileã‚’é™¤å¤–ï¼‰
        # ã“ã‚Œã¯ã€Œã™ãã«è§£æ”¾å¯èƒ½ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ã‚’é™¤ã„ãŸå®Ÿä½¿ç”¨é‡
        cache_memory = inactive_file

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’é™¤ã„ãŸå®Ÿéš›ã®ä½¿ç”¨é‡
        actual_used = current - cache_memory
        percent = (actual_used / max_mem) * 100
        used_gb = actual_used / (1024 ** 3)
        total_gb = max_mem / (1024 ** 3)

        return {
            'percent': round(percent, 1),
            'used_gb': round(used_gb, 2),
            'total_gb': round(total_gb, 2)
        }
    except Exception as e:
        # cgroupãŒä½¿ãˆãªã„å ´åˆã¯psutilã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        logger.debug(f"cgroup memoryèª­ã¿å–ã‚Šå¤±æ•—ã€psutilã‚’ä½¿ç”¨: {e}")
        memory = psutil.virtual_memory()
        return {
            'percent': round(memory.percent, 1),
            'used_gb': round(memory.used / (1024 ** 3), 2),
            'total_gb': round(memory.total / (1024 ** 3), 2)
        }


def get_cgroup_cpu():
    """CPUä½¿ç”¨ç‡ã‚’å–å¾—ï¼ˆCloud Runå¯¾å¿œï¼‰"""
    global _last_cpu_stats

    try:
        # Cloud Run Gen2: cgroup v2 ã¾ãŸã¯ v1 ã®ãƒ‘ã‚¹ã‚’è©¦è¡Œ
        usage_nsec = None
        cpu_usage_paths = [
            '/sys/fs/cgroup/cpu.stat',  # cgroup v2 (Gen2)
            '/sys/fs/cgroup/cpuacct/cpuacct.usage'  # cgroup v1 (Gen1)
        ]

        for path in cpu_usage_paths:
            try:
                if path.endswith('cpu.stat'):
                    # cgroup v2: cpu.statã‹ã‚‰ usage_usec ã‚’èª­ã¿å–ã‚‹
                    with open(path, 'r') as f:
                        for line in f:
                            if line.startswith('usage_usec'):
                                usage_nsec = int(line.split()[1]) * 1000  # ãƒã‚¤ã‚¯ãƒ­ç§’â†’ãƒŠãƒç§’
                                break
                else:
                    # cgroup v1: cpuacct.usageã‹ã‚‰ç›´æ¥èª­ã¿å–ã‚‹
                    with open(path, 'r') as f:
                        usage_nsec = int(f.read().strip())

                if usage_nsec is not None:
                    break
            except Exception:
                continue

        if usage_nsec is None:
            # cgroupã‹ã‚‰èª­ã¿å–ã‚Œãªã„å ´åˆã¯psutilã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return round(psutil.cpu_percent(interval=0.1), 1)

        current_time = time.time()

        # åˆå›å‘¼ã³å‡ºã—ã®å ´åˆã¯åˆæœŸåŒ–ã—ã¦psutilã§å–å¾—
        if _last_cpu_stats['usage_usec'] == 0:
            _last_cpu_stats = {'usage_usec': usage_nsec, 'timestamp': current_time}
            # psutilã§å³åº§ã®å€¤ã‚’è¿”ã™
            return round(psutil.cpu_percent(interval=0.1), 1)

        # å‰å›ã¨ã®å·®åˆ†ã‹ã‚‰ä½¿ç”¨ç‡ã‚’è¨ˆç®—
        time_delta = current_time - _last_cpu_stats['timestamp']
        usage_delta = usage_nsec - _last_cpu_stats['usage_usec']

        if time_delta > 0:
            # usage_deltaã¯ãƒŠãƒç§’ã€time_deltaã¯ç§’
            # CPUä½¿ç”¨ç‡ = (ä½¿ç”¨æ™‚é–“ã®å¢—åŠ  / çµŒéæ™‚é–“) / CPUæ•°
            cpu_count = psutil.cpu_count() or 4
            cpu_percent = (usage_delta / (time_delta * 1_000_000_000)) * 100 / cpu_count
            cpu_percent = max(0.0, min(cpu_percent, 100.0))  # 0-100%ã®ç¯„å›²
        else:
            cpu_percent = 0.0

        # æ¬¡å›ã®ãŸã‚ã«ä¿å­˜
        _last_cpu_stats = {'usage_usec': usage_nsec, 'timestamp': current_time}

        return round(cpu_percent, 1)
    except Exception as e:
        # å¤±æ•—æ™‚ã¯psutilã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        logger.debug(f"cgroup CPUèª­ã¿å–ã‚Šå¤±æ•—ã€psutilã‚’ä½¿ç”¨: {e}")
        return round(psutil.cpu_percent(interval=0.1), 1)


class AdaptiveResourceManager:
    """ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼

    ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã«åŸºã¥ã„ã¦ä¸¦åˆ—æ•°ã¨ã‚¹ãƒ­ãƒƒãƒˆãƒ«é…å»¶ã‚’å‹•çš„ã«èª¿æ•´ã—ã¾ã™ã€‚

    åˆ¶å¾¡ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆç·ãƒ¡ãƒ¢ãƒªé‡ã«å¿œã˜ã¦é–¾å€¤ã‚’è‡ªå‹•èª¿æ•´ï¼‰:

    16GBç’°å¢ƒï¼ˆä¿å®ˆçš„ï¼‰:
    - ä¸¦åˆ—æ•°åˆ¶å¾¡: memory < 60% â†’ å¢—åŠ , > 90% â†’ å‰Šæ¸›
    - ã‚¹ãƒ­ãƒƒãƒˆãƒ«: > 85% â†’ æ¸›é€Ÿé–‹å§‹, < 70% â†’ æ¸›é€Ÿç·©å’Œ

    32GBç’°å¢ƒï¼ˆæ”»ã‚ã®è¨­å®šï¼‰:
    - ä¸¦åˆ—æ•°åˆ¶å¾¡: memory < 70% â†’ å¢—åŠ , > 95% â†’ å‰Šæ¸›
    - ã‚¹ãƒ­ãƒƒãƒˆãƒ«: > 90% â†’ æ¸›é€Ÿé–‹å§‹, < 80% â†’ æ¸›é€Ÿç·©å’Œ

    32GBã®åˆ©ç‚¹:
    - 95%ã¾ã§ä½¿ã£ã¦ã‚‚æ®‹ã‚Š1.6GBï¼ˆ16GBã®90%ã¨åŒç­‰ï¼‰
    - ä¸¦åˆ—æ•°ã‚’20-30ã¾ã§å¢—ã‚„ã›ã‚‹å¯èƒ½æ€§
    - å‡¦ç†æ™‚é–“çŸ­ç¸®ã§ã‚³ã‚¹ãƒˆãƒ¡ãƒªãƒƒãƒˆ
    """

    def __init__(self, initial_max_parallel=3, min_parallel=1, max_parallel=5, history_size=3):
        """åˆæœŸåŒ–

        Args:
            initial_max_parallel: åˆæœŸä¸¦åˆ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰
            min_parallel: æœ€å°ä¸¦åˆ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰
            max_parallel: æœ€å¤§ä¸¦åˆ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰
            history_size: ç§»å‹•å¹³å‡ç”¨ã®å±¥æ­´ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰
        """
        self.max_parallel = initial_max_parallel
        self.min_parallel = min_parallel
        self.max_parallel_limit = max_parallel
        self.throttle_delay = 0.0  # ã‚¹ãƒ­ãƒƒãƒˆãƒ«é…å»¶ï¼ˆç§’ï¼‰

        # ç·ãƒ¡ãƒ¢ãƒªé‡ã‚’å–å¾—ã—ã¦é–¾å€¤ã‚’å‹•çš„ã«è¨­å®š
        total_memory_gb = 16.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        try:
            mem_info = get_cgroup_memory()
            total_memory_gb = mem_info['total_gb']
        except:
            pass

        # ã—ãã„å€¤ã‚’ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºã«å¿œã˜ã¦èª¿æ•´
        if total_memory_gb >= 32:
            # 32GBä»¥ä¸Š: æ”»ã‚ã®è¨­å®šï¼ˆæ®‹ã‚Š1.6GB = 95%ã¾ã§è¨±å®¹ï¼‰
            self.memory_low = 70.0   # ä½™è£•ã‚ã‚Š â†’ ä¸¦åˆ—æ•°å¢—åŠ 
            self.memory_high = 90.0  # é€¼è¿« â†’ æ¸›é€Ÿé–‹å§‹
            self.memory_critical = 95.0  # å±é™º â†’ ä¸¦åˆ—æ•°å‰Šæ¸›
            self.memory_recover = 80.0  # å›å¾© â†’ æ¸›é€Ÿç·©å’Œ
        elif total_memory_gb >= 24:
            # 24GB: ä¸­é–“è¨­å®š
            self.memory_low = 65.0
            self.memory_high = 87.0
            self.memory_critical = 92.0
            self.memory_recover = 75.0
        else:
            # 16GBä»¥ä¸‹: ä¿å®ˆçš„è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            self.memory_low = 60.0   # ä½™è£•ã‚ã‚Š â†’ ä¸¦åˆ—æ•°å¢—åŠ 
            self.memory_high = 85.0  # é€¼è¿« â†’ æ¸›é€Ÿé–‹å§‹
            self.memory_critical = 90.0  # å±é™º â†’ ä¸¦åˆ—æ•°å‰Šæ¸›
            self.memory_recover = 70.0  # å›å¾© â†’ æ¸›é€Ÿç·©å’Œ

        from loguru import logger
        self.logger = logger
        self.logger.info(f"[INIT] AdaptiveResourceManager: total_memory={total_memory_gb:.1f}GB, thresholds=[{self.memory_low}%, {self.memory_high}%, {self.memory_critical}%]")

        # èª¿æ•´ã‚¹ãƒ†ãƒƒãƒ—
        self.parallel_step = 1
        self.throttle_step = 0.5  # ç§’
        self.max_throttle = 3.0   # æœ€å¤§ã‚¹ãƒ­ãƒƒãƒˆãƒ«é…å»¶

        # ç§»å‹•å¹³å‡ç”¨ã®å±¥æ­´
        self.history_size = history_size
        self.memory_history = []

        # çµ±è¨ˆ
        self.adjustment_count = 0
        self.last_adjustment_time = time.time()

        from loguru import logger
        self.logger = logger

    def adjust_resources(self, memory_percent, current_workers=0):
        """ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ã«åŸºã¥ã„ã¦ä¸¦åˆ—æ•°ã¨ã‚¹ãƒ­ãƒƒãƒˆãƒ«ã‚’èª¿æ•´

        Args:
            memory_percent: ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ï¼ˆ%ï¼‰
            current_workers: ç¾åœ¨ã®å®Ÿè¡Œä¸­ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°

        Returns:
            dict: èª¿æ•´æƒ…å ± {'max_parallel': int, 'throttle_delay': float, 'adjusted': bool}
        """
        # å±¥æ­´ã«è¿½åŠ 
        self.memory_history.append(memory_percent)
        if len(self.memory_history) > self.history_size:
            self.memory_history.pop(0)

        # ç§»å‹•å¹³å‡ã‚’è¨ˆç®—ï¼ˆç›´è¿‘3å›ã®å¹³å‡ï¼‰
        memory_avg = sum(self.memory_history) / len(self.memory_history)

        # ç§»å‹•å¹³å‡ã§åˆ¤æ–­ï¼ˆç¬é–“å€¤ã§ã¯ãªãï¼‰
        memory_percent = memory_avg

        original_parallel = self.max_parallel
        original_throttle = self.throttle_delay
        adjusted = False

        # ãƒ•ã‚§ãƒ¼ã‚º3: ä¸¦åˆ—æ•°å‰Šæ¸›ï¼ˆç·Šæ€¥æ™‚ï¼‰- æœ€å„ªå…ˆ
        if memory_percent > self.memory_critical:
            if self.max_parallel > self.min_parallel:
                self.max_parallel = max(self.max_parallel - self.parallel_step, self.min_parallel)
                adjusted = True
                self.logger.warning(
                    f"[ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡] ğŸš¨ ãƒ¡ãƒ¢ãƒªé€¼è¿« ({memory_percent:.1f}%) â†’ ä¸¦åˆ—æ•°å‰Šæ¸›: {original_parallel} â†’ {self.max_parallel}"
                )

        # ãƒ•ã‚§ãƒ¼ã‚º2: æ¸›é€Ÿåˆ¶å¾¡
        if memory_percent > self.memory_high:
            # é€¼è¿« â†’ æ¸›é€Ÿ
            if self.throttle_delay < self.max_throttle:
                self.throttle_delay = min(self.throttle_delay + self.throttle_step, self.max_throttle)
                adjusted = True
                self.logger.info(
                    f"[ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡] âš ï¸ ãƒ¡ãƒ¢ãƒªé«˜ä½¿ç”¨ ({memory_percent:.1f}%) â†’ æ¸›é€Ÿ: {original_throttle:.1f}ç§’ â†’ {self.throttle_delay:.1f}ç§’"
                )
        elif memory_percent < self.memory_recover and self.throttle_delay > 0:
            # å›å¾© â†’ æ¸›é€Ÿç·©å’Œ
            self.throttle_delay = max(self.throttle_delay - self.throttle_step, 0.0)
            adjusted = True
            self.logger.info(
                f"[ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡] âœ… ãƒ¡ãƒ¢ãƒªå›å¾© ({memory_percent:.1f}%) â†’ æ¸›é€Ÿç·©å’Œ: {original_throttle:.1f}ç§’ â†’ {self.throttle_delay:.1f}ç§’"
            )

        # ãƒ•ã‚§ãƒ¼ã‚º1: ä¸¦åˆ—æ•°å¢—åŠ ï¼ˆä½™è£•ãŒã‚ã‚‹å ´åˆï¼‰
        # é‡è¦: å®Ÿè¡Œæ•°ãŒä¸Šé™ã«é”ã—ã¦ã„ã‚‹å ´åˆã®ã¿å¢—ã‚„ã™ï¼ˆå®Ÿéš›ã«ãƒ•ãƒ«ç¨¼åƒã—ã¦ã„ã‚‹çŠ¶æ…‹ã§ä½™è£•ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼‰
        if memory_percent < self.memory_low and self.throttle_delay == 0:
            # å®Ÿè¡Œæ•°ãŒä¸Šé™ã«é”ã—ã¦ã„ã‚‹ ã‹ã¤ ä½™è£•ã‚ã‚Š ã‹ã¤ æ¸›é€Ÿãªã— â†’ ä¸¦åˆ—æ•°ã‚’å¢—ã‚„ã™
            if current_workers >= self.max_parallel - 1 and self.max_parallel < self.max_parallel_limit:
                self.max_parallel = min(self.max_parallel + self.parallel_step, self.max_parallel_limit)
                adjusted = True
                self.logger.info(
                    f"[ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡] â¬†ï¸ ãƒ¡ãƒ¢ãƒªä½™è£• ({memory_percent:.1f}%) + ãƒ•ãƒ«ç¨¼åƒä¸­ ({current_workers}/{original_parallel}) â†’ ä¸¦åˆ—æ•°å¢—åŠ : {original_parallel} â†’ {self.max_parallel}"
                )

        if adjusted:
            self.adjustment_count += 1
            self.last_adjustment_time = time.time()

        return {
            'max_parallel': self.max_parallel,
            'throttle_delay': self.throttle_delay,
            'adjusted': adjusted,
            'memory_percent': memory_percent
        }

    async def apply_throttle(self):
        """ã‚¹ãƒ­ãƒƒãƒˆãƒ«é…å»¶ã‚’é©ç”¨"""
        if self.throttle_delay > 0:
            import asyncio
            await asyncio.sleep(self.throttle_delay)

    def get_status(self):
        """ç¾åœ¨ã®çŠ¶æ…‹ã‚’å–å¾—"""
        return {
            'max_parallel': self.max_parallel,
            'throttle_delay': self.throttle_delay,
            'adjustment_count': self.adjustment_count,
            'last_adjustment': self.last_adjustment_time
        }


# loguruã®ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼šãƒ­ã‚°ã‚’processing_statusã«é€ä¿¡
def log_to_processing_status(message):
    """loguruã®ãƒ­ã‚°ã‚’processing_statusã«è¿½åŠ  + ã‚¹ãƒ†ãƒ¼ã‚¸æ¤œå‡º"""
    log_record = message.record
    level = log_record['level'].name
    msg = log_record['message']

    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆINFOã®ã¿è¡¨ç¤ºï¼‰
    if level in ['INFO', 'WARNING', 'ERROR']:
        timestamp = datetime.now().strftime('%H:%M:%S')
        formatted_msg = f"[{timestamp}] {msg}"
        processing_status['logs'].append(formatted_msg)

        # ã‚¹ãƒ†ãƒ¼ã‚¸æ¤œå‡ºï¼ˆãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’æ¨æ¸¬ï¼‰
        msg_lower = msg.lower()
        if 'stage h' in msg_lower or 'æ§‹é€ åŒ–' in msg_lower:
            processing_status['current_stage'] = 'Stage H: æ§‹é€ åŒ–'
            processing_status['stage_progress'] = 0.3
        elif 'stage j' in msg_lower or 'ãƒãƒ£ãƒ³ã‚¯' in msg_lower:
            processing_status['current_stage'] = 'Stage J: ãƒãƒ£ãƒ³ã‚¯åŒ–'
            processing_status['stage_progress'] = 0.5
        elif 'stage k' in msg_lower or 'embedding' in msg_lower or 'embed' in msg_lower:
            processing_status['current_stage'] = 'Stage K: Embedding'
            processing_status['stage_progress'] = 0.7
        elif 'æˆåŠŸ' in msg or 'âœ…' in msg:
            processing_status['current_stage'] = 'å®Œäº†'
            processing_status['stage_progress'] = 1.0
        elif 'å¤±æ•—' in msg or 'âŒ' in msg:
            processing_status['current_stage'] = 'å¤±æ•—'
            processing_status['stage_progress'] = 1.0
        elif 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰' in msg_lower or 'download' in msg_lower:
            processing_status['current_stage'] = 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­'
            processing_status['stage_progress'] = 0.1

        # ãƒ­ã‚°ã¯æœ€å¤§300ä»¶ã¾ã§ä¿æŒ
        if len(processing_status['logs']) > 300:
            processing_status['logs'] = processing_status['logs'][-300:]


# loguruã«ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰å†…ã§å€‹åˆ¥ã«è¿½åŠ ã™ã‚‹ãŸã‚ã“ã“ã§ã¯è¿½åŠ ã—ãªã„ï¼‰
# logger.add(log_to_processing_status, format="{message}")


@app.route('/')
def index():
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ - å‡¦ç†ç”»é¢ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ"""
    return render_template('processing.html',
        supabase_url=os.getenv('SUPABASE_URL', ''),
        supabase_anon_key=os.getenv('SUPABASE_KEY', ''))


@app.route('/processing')
def processing():
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸"""
    return render_template('processing.html',
        supabase_url=os.getenv('SUPABASE_URL', ''),
        supabase_anon_key=os.getenv('SUPABASE_KEY', ''))


@app.route('/api/health', methods=['GET'])
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return jsonify({
        'status': 'ok',
        'message': 'Document Processing System is running'
    })


@app.route('/api/process/progress', methods=['GET'])
def get_process_progress():
    """
    å‡¦ç†é€²æ—ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‚’å–å¾—ï¼ˆSupabaseã‹ã‚‰å…±æœ‰çŠ¶æ…‹ã‚’å–å¾—ï¼‰
    """
    try:
        # cgroupã‹ã‚‰ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—ï¼ˆCloud Runå¯¾å¿œï¼‰
        cpu_percent = get_cgroup_cpu()
        memory_info = get_cgroup_memory()

        # Supabaseã‹ã‚‰ãƒ¯ãƒ¼ã‚«ãƒ¼çŠ¶æ³ã‚’å–å¾—ï¼ˆè¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¯¾å¿œï¼‰
        worker_status = get_worker_status()

        # Supabaseã‹ã‚‰é€²æ—æƒ…å ±ã‚’å–å¾—ï¼ˆè¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å…±æœ‰ï¼‰
        progress = get_progress_from_supabase()

        # ç¾åœ¨å‡¦ç†ä¸­ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®é€²æ—ã‚’å–å¾—
        current_stage = ''
        stage_progress = 0.0
        try:
            # processingä¸­ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ï¼ˆæœ€æ–°1ä»¶ï¼‰
            processing_doc = db.client.table('Rawdata_FILE_AND_MAIL').select('processing_stage, processing_progress').eq('processing_status', 'processing').order('created_at', desc=False).limit(1).execute()
            if processing_doc.data and len(processing_doc.data) > 0:
                current_stage = processing_doc.data[0].get('processing_stage', '')
                stage_progress = processing_doc.data[0].get('processing_progress', 0.0)
        except Exception as e:
            logger.error(f"é€²æ—å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

        return jsonify({
            'success': True,
            'processing': worker_status['is_processing'],
            'current_index': progress['current_index'],
            'total_count': progress['total_count'],
            'current_file': progress['current_file'],
            'success_count': progress['success_count'],
            'failed_count': progress['failed_count'],
            'logs': progress['logs'],  # æœ€æ–°150ä»¶
            # ã‚¹ãƒ†ãƒ¼ã‚¸é€²æ—ï¼ˆSupabaseã‹ã‚‰å–å¾—ï¼‰
            'current_stage': current_stage,
            'stage_progress': stage_progress,
            'system': {
                'cpu_percent': cpu_percent,
                'memory_percent': memory_info['percent'],
                'memory_used_gb': memory_info['used_gb'],
                'memory_total_gb': memory_info['total_gb']
            },
            'resource_control': {
                'current_parallel': worker_status['current_workers'],
                'max_parallel': worker_status['max_parallel'],
                'throttle_delay': 0.0,
                'adjustment_count': 0
            },
            'workers': worker_status['workers']  # å‡¦ç†ä¸­ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/workspaces', methods=['GET'])
def get_workspaces():
    """
    ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—
    """
    try:
        from shared.common.database.client import DatabaseClient
        db = DatabaseClient()

        # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—
        query = db.client.table('Rawdata_FILE_AND_MAIL').select('workspace').execute()

        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŠ½å‡º
        workspaces = set()
        for row in query.data:
            workspace = row.get('workspace')
            if workspace:
                workspaces.add(workspace)

        # ã‚½ãƒ¼ãƒˆã—ã¦ãƒªã‚¹ãƒˆåŒ–
        workspace_list = sorted(list(workspaces))

        return jsonify({
            'success': True,
            'workspaces': workspace_list
        })

    except Exception as e:
        print(f"[ERROR] ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/process/stats', methods=['GET'])
def get_process_stats():
    """
    å‡¦ç†ã‚­ãƒ¥ãƒ¼ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—

    ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®æ„å‘³:
    - pending: æœªå‡¦ç†ï¼ˆã¾ã ãƒãƒƒãƒã«é¸ã°ã‚Œã¦ã„ãªã„ï¼‰
    - processing: ã“ã®ãƒãƒƒãƒã®å‡¦ç†å¯¾è±¡ã¨ã—ã¦é¸æŠæ¸ˆã¿ï¼ˆé‡è¤‡é˜²æ­¢ãƒãƒ¼ã‚¯ï¼‰
    - completed: å‡¦ç†å®Œäº†
    - failed: å‡¦ç†å¤±æ•—
    """
    try:
        from shared.common.database.client import DatabaseClient
        db = DatabaseClient()

        workspace = request.args.get('workspace', 'all')

        query = db.client.table('Rawdata_FILE_AND_MAIL').select('processing_status, workspace')

        if workspace != 'all':
            query = query.eq('workspace', workspace)

        response = query.execute()

        stats = {
            'pending': 0,
            'processing': 0,  # ãƒãƒƒãƒã«é¸ã°ã‚ŒãŸä»¶æ•°
            'completed': 0,
            'failed': 0
        }

        for doc in response.data:
            status = doc.get('processing_status')
            # nullã¯ç„¡è¦–ï¼ˆpending/processing/completed/failedã®ã¿ã‚«ã‚¦ãƒ³ãƒˆï¼‰
            if status in stats:
                stats[status] = stats.get(status, 0) + 1

        return jsonify({
            'success': True,
            'stats': stats
        })

    except Exception as e:
        logger.error(f"çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/process/start', methods=['POST'])
def start_processing():
    """
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚’é–‹å§‹ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œï¼‰
    """
    global processing_status

    # Supabaseã§ãƒ­ãƒƒã‚¯çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆè¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¯¾å¿œï¼‰
    if get_processing_lock():
        return jsonify({
            'success': False,
            'error': 'æ—¢ã«å‡¦ç†ãŒå®Ÿè¡Œä¸­ã§ã™'
        }), 400

    try:
        import threading
        import asyncio

        data = request.get_json()
        workspace = data.get('workspace', 'all')
        limit = data.get('limit', 100)
        preserve_workspace = data.get('preserve_workspace', True)

        # Supabaseã«ãƒ­ãƒƒã‚¯ã‚’è¨­å®š
        set_processing_lock(True)

        # loguruãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ ï¼ˆprocess_queued_documents.pyã®ãƒ­ã‚°ã‚’ã‚­ãƒ£ãƒƒãƒï¼‰
        handler_id = logger.add(log_to_processing_status, format="{message}")

        # é€²æ—çŠ¶æ³ã‚’åˆæœŸåŒ–ï¼ˆå‡¦ç†é–‹å§‹ã‚’ã™ãã«è¡¨ç¤ºï¼‰
        processing_status['is_processing'] = True
        processing_status['current_index'] = 0
        processing_status['total_count'] = 0
        processing_status['current_file'] = 'åˆæœŸåŒ–ä¸­...'
        processing_status['success_count'] = 0
        processing_status['failed_count'] = 0
        processing_status['current_stage'] = ''
        processing_status['stage_progress'] = 0.0
        processing_status['logs'] = [
            f"[{datetime.now().strftime('%H:%M:%S')}] å‡¦ç†é–‹å§‹æº–å‚™ä¸­...",
            f"[{datetime.now().strftime('%H:%M:%S')}] ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹: {workspace}, åˆ¶é™: {limit}ä»¶"
        ]

        # Supabaseã«åˆæœŸçŠ¶æ…‹ã‚’ä¿å­˜
        update_progress_to_supabase(0, 0, 'åˆæœŸåŒ–ä¸­...', 0, 0, processing_status['logs'])

        # DocumentProcessorã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        update_progress_to_supabase(0, 0, 'ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿ä¸­...', 0, 0, processing_status['logs'])
        from process_queued_documents import DocumentProcessor

        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] ãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–ä¸­...")
        update_progress_to_supabase(0, 0, 'ãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–ä¸­...', 0, 0, processing_status['logs'])
        processor = DocumentProcessor()

        # pending ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—ä¸­...")
        update_progress_to_supabase(0, 0, 'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—ä¸­...', 0, 0, processing_status['logs'])
        docs = processor.get_pending_documents(workspace, limit)

        if not docs:
            processing_status['is_processing'] = False
            processing_status['current_file'] = ''
            processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] å‡¦ç†å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            set_processing_lock(False)  # ãƒ­ãƒƒã‚¯è§£æ”¾
            return jsonify({
                'success': True,
                'message': 'å‡¦ç†å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“',
                'processed': 0
            })

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’æ›´æ–°
        processing_status['total_count'] = len(docs)
        processing_status['current_file'] = ''
        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] {len(docs)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã¾ã—ãŸ")
        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        update_progress_to_supabase(0, len(docs), 'ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†é–‹å§‹', 0, 0, processing_status['logs'])

        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†é–¢æ•°
        def background_processing():
            global processing_status

            async def process_all():
                # ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ãƒªã‚½ãƒ¼ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
                # ç¾åœ¨: 16GBã§ max_parallel=10
                # TODO: 32GBã®å ´åˆã¯ max_parallel=20-30 ã«å¢—ã‚„ã™ã“ã¨ã§ã‚³ã‚¹ãƒˆãƒ¡ãƒªãƒƒãƒˆãŒæœŸå¾…ã§ãã‚‹
                # [MEMORY PER DOC] ãƒ­ã‚°ã§1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚ãŸã‚Šã®ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã‚’ç¢ºèªã—ã¦ã‹ã‚‰èª¿æ•´
                resource_manager = AdaptiveResourceManager(initial_max_parallel=3, min_parallel=1, max_parallel=10)

                # ä¸¦åˆ—æ•°åˆ¶å¾¡ç”¨ã®ã‚»ãƒãƒ•ã‚©ï¼ˆå‹•çš„ã«èª¿æ•´ï¼‰
                # NOTE: ã‚»ãƒãƒ•ã‚©ã¯å›ºå®šå€¤ãªã®ã§ã€ã‚¿ã‚¹ã‚¯å†…ã§åˆ¶å¾¡ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
                active_tasks = []
                processed_count = 0

                # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦– + ãƒ­ã‚°åŒæœŸã‚¿ã‚¹ã‚¯
                async def monitor_resources():
                    """ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã‚¿ã‚¹ã‚¯ï¼šãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ä¸¦åˆ—æ•°ã‚’èª¿æ•´ + ãƒ­ã‚°ã‚’å®šæœŸåŒæœŸ"""
                    while processing_status['is_processing']:
                        try:
                            memory_info = get_cgroup_memory()
                            memory_percent = memory_info['percent']

                            # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å–å¾—ï¼ˆä¸¦åˆ—æ•°èª¿æ•´ã«å¿…è¦ï¼‰
                            worker_status = get_worker_status()
                            current_workers = worker_status['current_workers']

                            # Supabaseã®max_parallelã‚’èª¿æ•´ï¼ˆè¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å…±æœ‰ï¼‰
                            new_max = adjust_max_parallel(memory_percent)

                            # ãƒ­ãƒ¼ã‚«ãƒ«ã®ãƒªã‚½ãƒ¼ã‚¹èª¿æ•´ã‚‚å®Ÿè¡Œï¼ˆcurrent_workersã‚’æ¸¡ã™ï¼‰
                            status = resource_manager.adjust_resources(memory_percent, current_workers)
                            # Supabaseã®å€¤ã§ä¸Šæ›¸ã
                            resource_manager.max_parallel_limit = new_max

                            # ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡æƒ…å ±ã‚’æ›´æ–°
                            processing_status['resource_control']['max_parallel'] = new_max
                            processing_status['resource_control']['current_parallel'] = current_workers
                            processing_status['resource_control']['throttle_delay'] = status['throttle_delay']
                            processing_status['resource_control']['adjustment_count'] = resource_manager.adjustment_count

                            # ãƒ­ã‚°ã‚’Supabaseã«å®šæœŸåŒæœŸï¼ˆ2ç§’ã”ã¨ï¼‰
                            update_progress_to_supabase(
                                processing_status['current_index'],
                                processing_status['total_count'],
                                processing_status['current_file'],
                                processing_status['success_count'],
                                processing_status['failed_count'],
                                processing_status['logs']
                            )

                        except Exception as e:
                            logger.error(f"ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")

                        await asyncio.sleep(2)  # 2ç§’ã”ã¨ã«ç›£è¦– + ãƒ­ã‚°åŒæœŸ

                # å€‹åˆ¥ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¿ã‚¹ã‚¯
                async def process_single_document(doc, index):
                    """å€‹åˆ¥ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†"""
                    nonlocal processed_count

                    # åœæ­¢ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
                    if not processing_status['is_processing']:
                        return False

                    file_name = doc.get('file_name', 'unknown')
                    title = doc.get('title', '') or '(ã‚¿ã‚¤ãƒˆãƒ«æœªç”Ÿæˆ)'
                    doc_id = doc.get('id', str(index))

                    # Supabaseã«ãƒ¯ãƒ¼ã‚«ãƒ¼ç™»éŒ²
                    register_worker(doc_id, title)

                    try:
                        # å‡¦ç†å‰ã®ãƒ¡ãƒ¢ãƒªã‚’è¨˜éŒ²
                        mem_before = get_cgroup_memory()

                        # é€²æ—ã‚’æ›´æ–°
                        processing_status['current_index'] = index
                        processing_status['current_file'] = title
                        processing_status['logs'].append(
                            f"[{datetime.now().strftime('%H:%M:%S')}] [{index}/{len(docs)}] å‡¦ç†ä¸­: {title}"
                        )

                        # ãƒ­ã‚°ã¯æœ€å¤§300ä»¶ã¾ã§ä¿æŒ
                        if len(processing_status['logs']) > 300:
                            processing_status['logs'] = processing_status['logs'][-300:]

                        # Supabaseã«é€²æ—ã‚’ä¿å­˜
                        update_progress_to_supabase(
                            index, len(docs), title,
                            processing_status['success_count'],
                            processing_status['failed_count'],
                            processing_status['logs']
                        )

                        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
                        success = await processor.process_document(doc, preserve_workspace)

                        # å‡¦ç†å¾Œã®ãƒ¡ãƒ¢ãƒªã‚’è¨˜éŒ²ï¼ˆæœ€åˆã®3ä»¶ã®ã¿ãƒ­ã‚°å‡ºåŠ›ï¼‰
                        if index <= 3:
                            mem_after = get_cgroup_memory()
                            mem_delta = mem_after['used_gb'] - mem_before['used_gb']
                            logger.info(f"[MEMORY PER DOC] Doc#{index}: before={mem_before['used_gb']:.2f}GB, after={mem_after['used_gb']:.2f}GB, delta={mem_delta:.2f}GB, parallel={len(active_tasks)}")

                        # ã‚¹ãƒ­ãƒƒãƒˆãƒ«é…å»¶ã‚’é©ç”¨
                        await resource_manager.apply_throttle()

                        processed_count += 1

                        if success:
                            processing_status['success_count'] += 1
                            processing_status['logs'].append(
                                f"[{datetime.now().strftime('%H:%M:%S')}] âœ… æˆåŠŸ: {title}"
                            )
                        else:
                            processing_status['failed_count'] += 1
                            processing_status['logs'].append(
                                f"[{datetime.now().strftime('%H:%M:%S')}] âŒ å¤±æ•—: {title}"
                            )

                        # Supabaseã«çµæœã‚’ä¿å­˜
                        update_progress_to_supabase(
                            index, len(docs), title,
                            processing_status['success_count'],
                            processing_status['failed_count'],
                            processing_status['logs']
                        )

                        return success
                    finally:
                        # Supabaseã‹ã‚‰ãƒ¯ãƒ¼ã‚«ãƒ¼è§£é™¤
                        unregister_worker(doc_id)

                # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
                monitor_task = asyncio.create_task(monitor_resources())

                try:
                    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†
                    for i, doc in enumerate(docs, 1):
                        # åœæ­¢ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
                        if not processing_status['is_processing']:
                            processing_status['logs'].append(
                                f"[{datetime.now().strftime('%H:%M:%S')}] âš ï¸ å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ"
                            )
                            break

                        # ä¸¦åˆ—æ•°åˆ¶å¾¡ï¼šactive_tasksãŒ max_parallel æœªæº€ã«ãªã‚‹ã¾ã§å¾…æ©Ÿ
                        # Supabaseã§å…±æœ‰ã•ã‚ŒãŸmax_parallel_limitã‚’ä½¿ç”¨
                        while len(active_tasks) >= resource_manager.max_parallel_limit:
                            # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‚’å‰Šé™¤
                            done_tasks = [t for t in active_tasks if t.done()]
                            for t in done_tasks:
                                active_tasks.remove(t)

                            if len(active_tasks) >= resource_manager.max_parallel_limit:
                                # ã¾ã ä¸¦åˆ—æ•°ãŒä¸Šé™ã«é”ã—ã¦ã„ã‚‹å ´åˆã¯å°‘ã—å¾…æ©Ÿ
                                await asyncio.sleep(0.1)

                        # æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
                        task = asyncio.create_task(process_single_document(doc, i))
                        active_tasks.append(task)

                        # ç¾åœ¨ã®ä¸¦åˆ—æ•°ã‚’æ›´æ–°
                        processing_status['resource_control']['current_parallel'] = len(active_tasks)

                    # ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ
                    if active_tasks:
                        await asyncio.gather(*active_tasks, return_exceptions=True)

                finally:
                    # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã‚¿ã‚¹ã‚¯ã‚’åœæ­¢
                    processing_status['is_processing'] = False  # ç›£è¦–ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†
                    await monitor_task

            try:
                asyncio.run(process_all())

                # å‡¦ç†å®Œäº†
                processing_status['is_processing'] = False
                processing_status['current_file'] = ''
                processing_status['logs'].append(
                    f"[{datetime.now().strftime('%H:%M:%S')}] å‡¦ç†å®Œäº†: æˆåŠŸ={processing_status['success_count']}, å¤±æ•—={processing_status['failed_count']}"
                )

                # Supabaseã«å®Œäº†çŠ¶æ…‹ã‚’ä¿å­˜
                update_progress_to_supabase(
                    processing_status['current_index'],
                    processing_status['total_count'],
                    '',
                    processing_status['success_count'],
                    processing_status['failed_count'],
                    processing_status['logs']
                )
            except Exception as e:
                processing_status['is_processing'] = False
                processing_status['logs'].append(
                    f"[{datetime.now().strftime('%H:%M:%S')}] âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}"
                )
                print(f"[ERROR] ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤
                logger.remove(handler_id)
                # Supabaseãƒ­ãƒƒã‚¯è§£æ”¾
                set_processing_lock(False)

        # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å‡¦ç†ã‚’é–‹å§‹
        thread = threading.Thread(target=background_processing, daemon=True)
        thread.start()

        # ã™ãã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
        return jsonify({
            'success': True,
            'message': 'å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã—ãŸ',
            'total_count': len(docs)
        })

    except Exception as e:
        processing_status['is_processing'] = False
        set_processing_lock(False)  # ãƒ­ãƒƒã‚¯è§£æ”¾
        print(f"[ERROR] å‡¦ç†é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/process/stop', methods=['POST'])
def stop_processing():
    """
    å‡¦ç†ã‚’åœæ­¢
    """
    global processing_status

    # ãƒ­ãƒ¼ã‚«ãƒ«ã¾ãŸã¯Supabaseã®ãƒ­ãƒƒã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
    if not processing_status['is_processing'] and not get_processing_lock():
        return jsonify({
            'success': False,
            'error': 'å®Ÿè¡Œä¸­ã®å‡¦ç†ãŒã‚ã‚Šã¾ã›ã‚“'
        }), 400

    # åœæ­¢ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
    processing_status['is_processing'] = False
    set_processing_lock(False)  # Supabaseãƒ­ãƒƒã‚¯è§£æ”¾
    processing_status['logs'].append(
        f"[{datetime.now().strftime('%H:%M:%S')}] âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦åœæ­¢ã•ã‚Œã¾ã—ãŸ"
    )

    return jsonify({
        'success': True,
        'message': 'å‡¦ç†ã‚’åœæ­¢ã—ã¾ã—ãŸ'
    })


@app.route('/api/process/reset', methods=['POST'])
def reset_processing():
    """
    å‡¦ç†ãƒ•ãƒ©ã‚°ã‚’å¼·åˆ¶ãƒªã‚»ãƒƒãƒˆï¼ˆSupabase + ãƒ­ãƒ¼ã‚«ãƒ«ä¸¡æ–¹ï¼‰
    """
    global processing_status

    # Supabaseãƒ­ãƒƒã‚¯è§£æ”¾
    set_processing_lock(False)

    # ãƒ­ãƒ¼ã‚«ãƒ«çŠ¶æ…‹ã‚‚ãƒªã‚»ãƒƒãƒˆ
    processing_status['is_processing'] = False
    processing_status['current_index'] = 0
    processing_status['total_count'] = 0
    processing_status['current_file'] = ''
    processing_status['success_count'] = 0
    processing_status['failed_count'] = 0
    processing_status['current_stage'] = ''
    processing_status['stage_progress'] = 0.0
    processing_status['logs'] = [
        f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ”„ å‡¦ç†ãƒ•ãƒ©ã‚°ã‚’å¼·åˆ¶ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸï¼ˆSupabase + ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰"
    ]
    processing_status['resource_control'] = {
        'current_parallel': 0,
        'max_parallel': 3,
        'throttle_delay': 0.0,
        'adjustment_count': 0
    }

    return jsonify({
        'success': True,
        'message': 'å‡¦ç†ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ'
    })


if __name__ == '__main__':
    # é–‹ç™ºç’°å¢ƒã§ã®å®Ÿè¡Œ
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
