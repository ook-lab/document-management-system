"""
ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã‚­ãƒ¥ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ã‚ãšã€Rawdata_FILE_AND_MAIL.processing_status ã§ç›´æ¥ç®¡ç†

å‡¦ç†å†…å®¹:
1. processing_status='pending' ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
2. çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆStage E-Kï¼‰ã§å‡¦ç†
3. æˆåŠŸ: processing_status='completed'
4. å¤±æ•—: processing_status='failed'

ä½¿ã„æ–¹:
    # å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‡¦ç†
    python process_queued_documents_v3.py --limit=100

    # ç‰¹å®šã®ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®ã¿
    python process_queued_documents_v3.py --workspace=ema_classroom --limit=20

    # pendingã«ãƒªã‚»ãƒƒãƒˆï¼ˆå†å‡¦ç†ç”¨ï¼‰
    python process_queued_documents_v3.py --reset-to-pending --workspace=all
"""

import asyncio
from typing import List, Dict, Any, Optional
import sys
from datetime import datetime
from pathlib import Path
import mimetypes

from loguru import logger
from A_common.database.client import DatabaseClient
from A_common.connectors.google_drive import GoogleDriveConnector
from G_unified_pipeline import UnifiedDocumentPipeline


class DocumentProcessor:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""

    VIDEO_EXTENSIONS = ['.mp4', '.mov', '.avi', '.mkv', '.webm', '.flv', '.wmv', '.m4v', '.mpeg', '.mpg']

    def __init__(self):
        self.db = DatabaseClient()
        self.pipeline = UnifiedDocumentPipeline(db_client=self.db)
        self.drive = GoogleDriveConnector()
        self.temp_dir = Path("./temp")
        self.temp_dir.mkdir(parents=True, exist_ok=True)

    def get_pending_documents(self, workspace: str = 'all', limit: int = 100) -> List[Dict[str, Any]]:
        """
        processing_status='pending' ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—

        Args:
            workspace: å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ ('all' ã§å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹)
            limit: å–å¾—ã™ã‚‹æœ€å¤§ä»¶æ•°

        Returns:
            ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        """
        query = self.db.client.table('Rawdata_FILE_AND_MAIL').select('*').eq('processing_status', 'pending')

        if workspace != 'all':
            query = query.eq('workspace', workspace)

        result = query.limit(limit).execute()
        return result.data if result.data else []

    def mark_as_processing(self, document_id: str):
        """å‡¦ç†ä¸­ã«ãƒãƒ¼ã‚¯"""
        try:
            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                'processing_status': 'processing'
            }).eq('id', document_id).execute()
        except Exception as e:
            logger.error( f"å‡¦ç†ä¸­ãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

    def mark_as_completed(self, document_id: str):
        """å®Œäº†ã«ãƒãƒ¼ã‚¯"""
        try:
            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                'processing_status': 'completed'
            }).eq('id', document_id).execute()
        except Exception as e:
            logger.error( f"å®Œäº†ãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

    def mark_as_failed(self, document_id: str, error_message: str = ""):
        """å¤±æ•—ã«ãƒãƒ¼ã‚¯"""
        try:
            update_data = {'processing_status': 'failed'}

            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜
            if error_message:
                # æ—¢å­˜ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                doc_result = self.db.client.table('Rawdata_FILE_AND_MAIL').select('metadata').eq('id', document_id).execute()
                if doc_result.data and len(doc_result.data) > 0:
                    metadata = doc_result.data[0].get('metadata', {}) or {}
                else:
                    metadata = {}

                metadata['last_error'] = error_message
                metadata['last_error_time'] = datetime.now().isoformat()
                update_data['metadata'] = metadata

            self.db.client.table('Rawdata_FILE_AND_MAIL').update(update_data).eq('id', document_id).execute()
        except Exception as e:
            logger.error( f"å¤±æ•—ãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

    def get_queue_stats(self, workspace: str = 'all') -> Dict[str, int]:
        """
        çµ±è¨ˆæƒ…å ±ã‚’å–å¾—

        Args:
            workspace: å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ ('all' ã§å…¨ã¦)

        Returns:
            çµ±è¨ˆæƒ…å ±ã®è¾æ›¸
        """
        try:
            query = self.db.client.table('Rawdata_FILE_AND_MAIL').select('processing_status, workspace')

            if workspace != 'all':
                query = query.eq('workspace', workspace)

            response = query.execute()

            stats = {
                'pending': 0,
                'processing': 0,
                'completed': 0,
                'failed': 0,
                'null': 0  # æœªå‡¦ç†ï¼ˆprocessing_statusãŒnullï¼‰
            }

            for doc in response.data:
                status = doc.get('processing_status')
                if status is None:
                    stats['null'] += 1
                else:
                    stats[status] = stats.get(status, 0) + 1

            stats['total'] = len(response.data)

            # æˆåŠŸç‡ã‚’è¨ˆç®—
            processed = stats['completed'] + stats['failed']
            if processed > 0:
                stats['success_rate'] = round(stats['completed'] / processed * 100, 1)
            else:
                stats['success_rate'] = 0.0

            return stats

        except Exception as e:
            logger.error(f" çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def print_queue_stats(self, workspace: str = 'all'):
        """
        çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º

        Args:
            workspace: å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ ('all' ã§å…¨ã¦)
        """
        stats = self.get_queue_stats(workspace)

        if not stats:
            logger.info("çµ±è¨ˆæƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return

        logger.info("\n" + "="*80)
        if workspace == 'all':
            logger.info("ğŸ“Š å…¨ä½“çµ±è¨ˆ")
        else:
            logger.info(f"ğŸ“Š çµ±è¨ˆ (workspace: {workspace})")
        logger.info("="*80)
        logger.info(f"å¾…æ©Ÿä¸­ (pending):      {stats.get('pending', 0):>5}ä»¶")
        logger.info(f"å‡¦ç†ä¸­ (processing):   {stats.get('processing', 0):>5}ä»¶")
        logger.info(f"å®Œäº†   (completed):    {stats.get('completed', 0):>5}ä»¶")
        logger.info(f"å¤±æ•—   (failed):       {stats.get('failed', 0):>5}ä»¶")
        logger.info(f"æœªå‡¦ç† (null):         {stats.get('null', 0):>5}ä»¶")
        logger.info("-" * 80)
        logger.info(f"åˆè¨ˆ:                  {stats.get('total', 0):>5}ä»¶")

        # æˆåŠŸç‡ã‚’è¡¨ç¤º
        processed = stats.get('completed', 0) + stats.get('failed', 0)
        if processed > 0:
            logger.info(f"æˆåŠŸç‡:                {stats.get('success_rate', 0):>5.1f}% ({stats.get('completed', 0)}/{processed})")

        logger.info("="*80 + "\n")

    async def process_document(
        self,
        doc: Dict[str, Any],
        preserve_workspace: bool = True
    ) -> bool:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†

        Args:
            doc: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
            preserve_workspace: workspaceã‚’ä¿æŒã™ã‚‹ã‹

        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        document_id = doc['id']
        file_name = doc.get('file_name', 'unknown')
        title = doc.get('title', '')
        display_name = title if title else '(ã‚¿ã‚¤ãƒˆãƒ«æœªç”Ÿæˆ)'
        source_type = doc.get('source_type', '')

        try:
            # å‡¦ç†ä¸­ã«ãƒãƒ¼ã‚¯
            self.mark_as_processing(document_id)

            # source_idã®æœ‰ç„¡ã§åˆ¤æ–­ï¼ˆsource_typeã«ã¯ä¾å­˜ã—ãªã„ï¼‰
            drive_file_id = doc.get('source_id')

            if drive_file_id:
                # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚ã‚Šï¼ˆDrive File IDãŒå­˜åœ¨ï¼‰
                result = await self._process_with_attachment(doc, preserve_workspace)
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼ˆæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ï¼‰
                result = await self._process_text_only(doc, preserve_workspace)

            # çµæœãŒboolã®å ´åˆï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
            if isinstance(result, bool):
                success = result
                error_msg = "å‡¦ç†å¤±æ•—ï¼ˆè©³ç´°ãªã—ï¼‰" if not success else None
            else:
                # çµæœãŒdictã®å ´åˆï¼ˆè©³ç´°ã‚¨ãƒ©ãƒ¼ä»˜ãï¼‰
                success = result.get('success', False)
                error_msg = result.get('error', "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼") if not success else None

            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
            if success:
                # screenshot_url ãŒã‚ã‚‹å ´åˆï¼šPNGã‚’å‰Šé™¤ã—ã¦ã‚¯ãƒªã‚¢
                screenshot_url = doc.get('screenshot_url')
                if screenshot_url:
                    try:
                        # screenshot_url ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’æŠ½å‡º
                        import re
                        match = re.search(r'/d/([a-zA-Z0-9_-]+)', screenshot_url)
                        if match:
                            png_file_id = match.group(1)

                            # PNGã‚’ã‚´ãƒŸç®±ã«ç§»å‹•ï¼ˆå…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–ã§ã¯å®Œå…¨å‰Šé™¤ä¸å¯ï¼‰
                            from A_common.connectors.google_drive import GoogleDriveConnector
                            drive = GoogleDriveConnector()
                            drive.trash_file(png_file_id)
                            logger.info(f"[OK] OCRç”¨PNGã‚’ã‚´ãƒŸç®±ã«ç§»å‹•: {png_file_id}")

                            # screenshot_url ã‚’ã‚¯ãƒªã‚¢
                            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                                'screenshot_url': None
                            }).eq('id', document_id).execute()
                            logger.info(f"[OK] screenshot_url ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")

                    except Exception as e:
                        logger.warning(f" PNGå‰Šé™¤å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ï¼ˆå‡¦ç†ã¯ç¶™ç¶šï¼‰: {e}")

                self.mark_as_completed(document_id)
                logger.info(f"[OK] å‡¦ç†æˆåŠŸ: {display_name}")
            else:
                self.mark_as_failed(document_id, error_msg)
                logger.error( f"[FAIL] å‡¦ç†å¤±æ•—: {display_name} - {error_msg}")

            return success

        except Exception as e:
            error_msg = f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}"
            logger.error( f"[FAIL] {error_msg}")
            self.mark_as_failed(document_id, error_msg)
            return False

    async def _process_text_only(
        self,
        doc: Dict[str, Any],
        preserve_workspace: bool = True
    ) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ï¼ˆçµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®Stage H-Kéƒ¨åˆ†ã®ã¿ä½¿ç”¨ï¼‰"""
        from A_common.processing.metadata_chunker import MetadataChunker

        document_id = doc['id']
        file_name = doc.get('file_name', 'text_only')
        workspace_to_use = doc.get('workspace', 'unknown') if preserve_workspace else 'unknown'

        display_subject = doc.get('display_subject', '')
        display_post_text = doc.get('display_post_text', '')
        attachment_text = doc.get('attachment_text', '')

        # ãƒ†ã‚­ã‚¹ãƒˆçµåˆ
        text_parts = []
        if display_subject:
            text_parts.append(f"ã€ä»¶åã€‘\n{display_subject}")
        if display_post_text:
            text_parts.append(f"ã€æœ¬æ–‡ã€‘\n{display_post_text}")
        if attachment_text:
            text_parts.append(f"ã€æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã€‘\n{attachment_text}")

        combined_text = '\n\n'.join(text_parts)

        if not combined_text.strip():
            error_msg = "ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        # çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã® Stage H-K ã‚’ä½¿ç”¨
        # config ã‹ã‚‰è¨­å®šã‚’å–å¾—
        stage_h_config = self.pipeline.config.get_stage_config('stage_h', doc.get('doc_type', 'other'), workspace_to_use)

        # Stage H: æ§‹é€ åŒ–
        stageh_result = self.pipeline.stage_h.process(
            file_name=file_name,
            doc_type=doc.get('doc_type', 'unknown'),
            workspace=workspace_to_use,
            combined_text=combined_text,
            prompt=stage_h_config['prompt'],
            model=stage_h_config['model']
        )

        # Stage H ã®çµæœã‚’ãƒã‚§ãƒƒã‚¯
        if not stageh_result or not isinstance(stageh_result, dict):
            error_msg = "Stage Hå¤±æ•—: æ§‹é€ åŒ–çµæœãŒä¸æ­£ã§ã™"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        stageh_metadata = stageh_result.get('metadata', {})
        if stageh_metadata.get('extraction_failed'):
            error_msg = "Stage Hå¤±æ•—: JSONæŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        document_date = stageh_result.get('document_date')
        tags = stageh_result.get('tags', [])

        # Stage I ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãªã®ã§è¦ç´„ä¸è¦ï¼‰

        # Stage J: ãƒãƒ£ãƒ³ã‚¯åŒ–
        metadata_chunker = MetadataChunker()
        document_data = {
            'file_name': file_name,
            'summary': '',
            'document_date': document_date,
            'tags': tags,
            'doc_type': doc.get('doc_type'),
            'display_subject': display_subject,
            'display_post_text': display_post_text,
            'display_sender': doc.get('display_sender'),
            'display_type': doc.get('display_type'),
            'display_sent_at': doc.get('display_sent_at'),
            'classroom_sender_email': doc.get('classroom_sender_email'),
            'attachment_text': attachment_text,
            'persons': stageh_metadata.get('persons', []) if isinstance(stageh_metadata, dict) else [],
            'organizations': stageh_metadata.get('organizations', []) if isinstance(stageh_metadata, dict) else [],
            'people': stageh_metadata.get('people', []) if isinstance(stageh_metadata, dict) else [],
            # Stage H ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            'text_blocks': stageh_metadata.get('text_blocks', []) if isinstance(stageh_metadata, dict) else [],
            'structured_tables': stageh_metadata.get('structured_tables', []) if isinstance(stageh_metadata, dict) else [],
            'weekly_schedule': stageh_metadata.get('weekly_schedule', []) if isinstance(stageh_metadata, dict) else [],
            'other_text': stageh_metadata.get('other_text', []) if isinstance(stageh_metadata, dict) else []
        }

        chunks = metadata_chunker.create_metadata_chunks(document_data)

        # æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤
        try:
            self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
        except Exception as e:
            logger.warning( f"æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {e}")

        # Stage K: Embedding + ä¿å­˜
        stage_k_result = self.pipeline.stage_k.embed_and_save(document_id, chunks)

        if not stage_k_result.get('success'):
            error_msg = f"Stage Kå¤±æ•—: {stage_k_result.get('failed_count', 0)}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å¤±æ•—"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        # éƒ¨åˆ†çš„å¤±æ•—ã‚‚ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦æ‰±ã†ï¼ˆå³æ ¼ãƒ¢ãƒ¼ãƒ‰ï¼‰
        failed_count = stage_k_result.get('failed_count', 0)
        if failed_count > 0:
            error_msg = f"Stage Kéƒ¨åˆ†å¤±æ•—: {failed_count}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å¤±æ•—"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        logger.info(f"ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å®Œäº†: {stage_k_result.get('saved_count', 0)}/{len(chunks)}ä»¶")

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°
        try:
            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                'tags': tags,
                'document_date': document_date,
                'metadata': stageh_metadata
            }).eq('id', document_id).execute()
        except Exception as e:
            error_msg = f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        return {'success': True}

    async def _process_with_attachment(
        self,
        doc: Dict[str, Any],
        preserve_workspace: bool = True
    ) -> bool:
        """æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚ã‚Šãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†"""
        document_id = doc['id']
        file_name = doc.get('file_name', 'unknown')
        drive_file_id = doc.get('source_id')

        if not drive_file_id:
            logger.error( "source_idï¼ˆDrive File IDï¼‰ãŒã‚ã‚Šã¾ã›ã‚“")
            return False

        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
        file_extension = Path(file_name).suffix.lower()
        if file_extension in self.VIDEO_EXTENSIONS:
            logger.info(f"â­ï¸  å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {file_name}")
            # å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—æ‰±ã„ã§æˆåŠŸã¨ã™ã‚‹
            return True

        # screenshot_url ãŒã‚ã‚Œã°PNGã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆOCRç”¨ï¼‰ã€ãªã‘ã‚Œã°é€šå¸¸ãƒ•ã‚¡ã‚¤ãƒ«
        screenshot_url = doc.get('screenshot_url')
        screenshot_file_id = None
        download_file_id = drive_file_id
        download_file_name = file_name

        if screenshot_url:
            # screenshot_url ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’æŠ½å‡º
            import re
            match = re.search(r'/d/([a-zA-Z0-9_-]+)', screenshot_url)
            if match:
                screenshot_file_id = match.group(1)
                download_file_id = screenshot_file_id
                # PNGãƒ•ã‚¡ã‚¤ãƒ«åã«å¤‰æ›´
                base_name = Path(file_name).stem
                download_file_name = f"{base_name}.png"
                logger.info(f"[OCRç”¨] PNGã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {download_file_name} (screenshot_urlä½¿ç”¨)")
            else:
                logger.warning( f"screenshot_url ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’æŠ½å‡ºã§ãã¾ã›ã‚“: {screenshot_url}")

        # Driveã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        try:
            self.drive.download_file(download_file_id, download_file_name, str(self.temp_dir))
            local_path = self.temp_dir / download_file_name
        except Exception as e:
            # 404ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰ã®å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            error_str = str(e)
            if 'File not found' in error_str or '404' in error_str:
                logger.warning( f"Driveã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ãƒ†ã‚­ã‚¹ãƒˆã®ã¿å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {file_name}")
                return await self._process_text_only(doc, preserve_workspace)
            else:
                logger.error( f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                return False

        # MIMEã‚¿ã‚¤ãƒ—ã‚’æ¨æ¸¬
        mime_type = doc.get('mimeType')
        if not mime_type:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãªã„å ´åˆã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬
            mime_type, _ = mimetypes.guess_type(file_name)
        if not mime_type:
            # ãã‚Œã§ã‚‚ä¸æ˜ãªå ´åˆã¯æ±ç”¨ãƒã‚¤ãƒŠãƒªã¨ã—ã¦æ‰±ã†
            mime_type = 'application/octet-stream'

        # çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å‡¦ç†
        try:
            workspace_to_use = doc.get('workspace', 'unknown') if preserve_workspace else 'unknown'

            result = await self.pipeline.process_document(
                file_path=Path(local_path),
                file_name=file_name,
                doc_type=doc.get('doc_type', 'other'),
                workspace=workspace_to_use,
                mime_type=mime_type,
                source_id=drive_file_id,
                existing_document_id=document_id,
                extra_metadata={
                    'display_subject': doc.get('display_subject'),
                    'display_post_text': doc.get('display_post_text'),
                    'attachment_text': doc.get('attachment_text'),
                    'display_sender': doc.get('display_sender'),
                    'display_sender_email': doc.get('display_sender_email'),
                    'display_type': doc.get('display_type'),
                    'display_sent_at': doc.get('display_sent_at'),
                    'classroom_sender_email': doc.get('classroom_sender_email')
                }
            )

            # çµæœå…¨ä½“ã‚’è¿”ã™ï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å«ã‚€ï¼‰
            return result

        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            if local_path.exists():
                local_path.unlink()
                logger.debug( f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {local_path}")

    async def run(
        self,
        workspace: str = 'all',
        limit: int = 100,
        preserve_workspace: bool = True
    ):
        """
        å‡¦ç†ã‚’å®Ÿè¡Œ

        Args:
            workspace: å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹
            limit: å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•°
            preserve_workspace: workspaceã‚’ä¿æŒã™ã‚‹ã‹
        """
        logger.info("="*80)
        logger.info("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰")
        logger.info("="*80)

        # pending ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        docs = self.get_pending_documents(workspace, limit)

        if not docs:
            logger.info("å‡¦ç†å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            return

        logger.info(f"å‡¦ç†å¯¾è±¡: {len(docs)}ä»¶")
        logger.info("")

        # çµ±è¨ˆ
        stats = {'success': 0, 'failed': 0, 'total': len(docs)}

        # é †æ¬¡å‡¦ç†
        for i, doc in enumerate(docs, 1):
            file_name = doc.get('file_name', 'unknown')
            title = doc.get('title', '')
            # ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚Œã°ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤ºã€ãªã‘ã‚Œã°ã€Œã‚¿ã‚¤ãƒˆãƒ«æœªç”Ÿæˆã€
            display_name = title if title else '(ã‚¿ã‚¤ãƒˆãƒ«æœªç”Ÿæˆ)'
            logger.info(f"\n{'='*80}")
            logger.info(f"[{i}/{len(docs)}] å‡¦ç†é–‹å§‹: {display_name}")
            logger.info(f"Document ID: {doc['id']}")

            success = await self.process_document(doc, preserve_workspace)

            if success:
                stats['success'] += 1
            else:
                stats['failed'] += 1

            logger.info(f"é€²æ—: æˆåŠŸ={stats['success']}, å¤±æ•—={stats['failed']}, æ®‹ã‚Š={len(docs)-i}")

        # æœ€çµ‚çµæœ
        logger.info("\n" + "="*80)
        logger.info("å‡¦ç†å®Œäº†")
        logger.info("="*80)
        logger.info(f"[OK] æˆåŠŸ: {stats['success']}ä»¶")
        logger.error(f"[FAIL] å¤±æ•—: {stats['failed']}ä»¶")
        logger.info(f"[TOTAL] åˆè¨ˆ: {stats['total']}ä»¶")
        logger.info("="*80)


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰')
    parser.add_argument('--workspace', default='all', help='å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: all)')
    parser.add_argument('--limit', type=int, default=100, help='å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100)')
    parser.add_argument('--no-preserve-workspace', action='store_true', help='workspaceã‚’ä¿æŒã—ãªã„')
    parser.add_argument('--stats', action='store_true', help='çµ±è¨ˆæƒ…å ±ã®ã¿ã‚’è¡¨ç¤º')

    args = parser.parse_args()

    processor = DocumentProcessor()

    # çµ±è¨ˆæƒ…å ±ã®ã¿è¡¨ç¤º
    if args.stats:
        processor.print_queue_stats(workspace=args.workspace)
        return

    # é€šå¸¸ã®å‡¦ç†
    await processor.run(
        workspace=args.workspace,
        limit=args.limit,
        preserve_workspace=not args.no_preserve_workspace
    )


if __name__ == '__main__':
    asyncio.run(main())
