"""
ã‚­ãƒ¥ãƒ¼å‡¦ç†å‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å‡¦ç†çŠ¶æ…‹ç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ« (document_reprocessing_queue) ã‚’ä½¿ç”¨ã—ãŸçµ±åˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
é‡è¤‡å‡¦ç†ã‚’é˜²ãã€å‡¦ç†é€²æ—ã‚’è¿½è·¡ã—ã€ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒªãƒˆãƒ©ã‚¤ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚

å‡¦ç†å†…å®¹:
1. ã™ã¹ã¦ã®ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã¾ãŸã¯æŒ‡å®šã•ã‚ŒãŸãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚­ãƒ¥ãƒ¼ã«ç™»éŒ²
2. ã‚­ãƒ¥ãƒ¼ã‹ã‚‰é †æ¬¡ã‚¿ã‚¹ã‚¯ã‚’å–å¾—ã—ã¦å‡¦ç†
3. å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆPre-processing â†’ Stage B â†’ Stage C â†’ Stage A â†’ Chunkingï¼‰ã§å‡¦ç†
4. attachment_textã€æ§‹é€ åŒ–metadataã€search_indexã‚’ç”Ÿæˆ
5. å‡¦ç†çŠ¶æ…‹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ç®¡ç†ï¼ˆpending â†’ processing â†’ completed/failedï¼‰

å¯¾å¿œã™ã‚‹ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—:
- classroom: Google Classroomæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ä»˜ãï¼ˆDrive URLçµŒç”±ï¼‰
- classroom_text: Google Classroomãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ•ç¨¿
- text_only: ä¸€èˆ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- drive: Google Driveãƒ•ã‚¡ã‚¤ãƒ«
- email_attachment: ãƒ¡ãƒ¼ãƒ«æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«

ä½¿ã„æ–¹:
    # å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‡¦ç†ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    python process_queued_documents.py --limit=100

    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆç¢ºèªã®ã¿ï¼‰
    python process_queued_documents.py --dry-run

    # ç‰¹å®šã®ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®ã¿å‡¦ç†
    python process_queued_documents.py --workspace=ema_classroom --limit=20
    python process_queued_documents.py --workspace=ikuya_classroom --limit=20

    # ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã®ã¿ï¼ˆå‡¦ç†ã¯å®Ÿè¡Œã—ãªã„ï¼‰
    python process_queued_documents.py --populate-only --limit=50

    # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å‡¦ç†å®Ÿè¡Œ
    python process_queued_documents.py --process-queue --limit=10

    # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’ä¿æŒã—ãªã„ï¼ˆAIåˆ¤å®šã«ä»»ã›ã‚‹ï¼‰
    python process_queued_documents.py --no-preserve-workspace
"""

import asyncio
from typing import List, Dict, Any, Optional
from loguru import logger
import json
import sys
from datetime import datetime

from A_common.database.client import DatabaseClient
from B_ingestion.two_stage_ingestion import TwoStageIngestionPipeline


class ClassroomReprocessorV2:
    """Google Classroomãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†å‡¦ç†ï¼ˆå‡¦ç†çŠ¶æ…‹ç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œç‰ˆï¼‰"""

    # å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ãŒå¤šã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ï¼‰
    VIDEO_EXTENSIONS = ['.mp4', '.mov', '.avi', '.mkv', '.webm', '.flv', '.wmv', '.m4v', '.mpeg', '.mpg']

    def __init__(self, worker_id: str = "reprocessor_v2"):
        self.db = DatabaseClient()
        self.pipeline = TwoStageIngestionPipeline()
        self.worker_id = worker_id

    def populate_queue_from_workspace(
        self,
        workspace: str = 'all',
        limit: int = 100,
        reason: str = 'classroom_reprocessing',
        preserve_workspace: bool = True
    ) -> int:
        """
        æŒ‡å®šã•ã‚ŒãŸworkspaceã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 

        Args:
            workspace: å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ ('all' ã§å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹)
            limit: è¿½åŠ ã™ã‚‹æœ€å¤§ä»¶æ•°
            reason: å†å‡¦ç†ã®ç†ç”±
            preserve_workspace: workspaceã‚’ä¿æŒã™ã‚‹ã‹

        Returns:
            è¿½åŠ ã—ãŸä»¶æ•°
        """
        logger.info(f"ã‚­ãƒ¥ãƒ¼ã¸ã®è¿½åŠ ã‚’é–‹å§‹: workspace={workspace}")

        # å¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ï¼ˆprocessing_status='completed'ã¯é™¤å¤–ï¼‰
        if workspace == 'all':
            # å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å¯¾è±¡
            result = self.db.client.table('source_documents').select('*').neq('processing_status', 'completed').limit(limit).execute()
        else:
            # ç‰¹å®šã®ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®ã¿
            result = self.db.client.table('source_documents').select('*').eq(
                'workspace', workspace
            ).neq('processing_status', 'completed').limit(limit).execute()

        documents = result.data if result.data else []
        logger.info(f"å¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {len(documents)}ä»¶")

        if not documents:
            logger.info("è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            return 0

        added_count = 0
        skipped_count = 0

        for doc in documents:
            doc_id = doc['id']
            file_name = doc.get('file_name', 'unknown')

            # æ—¢ã«ã‚­ãƒ¥ãƒ¼ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            existing = self.db.client.table('document_reprocessing_queue').select('id, status').eq(
                'document_id', doc_id
            ).eq('status', 'pending').execute()

            if existing.data:
                logger.debug(f"ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢ã«ã‚­ãƒ¥ãƒ¼ã«ç™»éŒ²æ¸ˆã¿ï¼‰: {file_name}")
                skipped_count += 1
                continue

            # ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
            try:
                queue_data = {
                    'document_id': doc_id,
                    'reprocess_reason': reason,
                    'reprocess_type': 'full',
                    'priority': 0,
                    'preserve_workspace': preserve_workspace,
                    'original_file_name': file_name,
                    'original_workspace': doc.get('workspace'),
                    'original_doc_type': doc.get('doc_type'),
                    'original_source_id': doc.get('source_id'),
                    'created_by': self.worker_id
                }

                self.db.client.table('document_reprocessing_queue').insert(queue_data).execute()
                added_count += 1
                logger.debug(f"ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ : {file_name}")

            except Exception as e:
                logger.error(f"ã‚­ãƒ¥ãƒ¼è¿½åŠ ã‚¨ãƒ©ãƒ¼: {file_name} - {e}")

        logger.info(f"ã‚­ãƒ¥ãƒ¼è¿½åŠ å®Œäº†: {added_count}ä»¶è¿½åŠ , {skipped_count}ä»¶ã‚¹ã‚­ãƒƒãƒ—")
        return added_count

    async def process_queue(self, limit: int = 100) -> Dict[str, int]:
        """
        ã‚­ãƒ¥ãƒ¼ã‹ã‚‰é †æ¬¡ã‚¿ã‚¹ã‚¯ã‚’å–å¾—ã—ã¦å‡¦ç†

        Args:
            limit: å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•°

        Returns:
            å‡¦ç†çµæœã®çµ±è¨ˆï¼ˆæˆåŠŸæ•°ã€å¤±æ•—æ•°ãªã©ï¼‰
        """
        logger.info(f"ã‚­ãƒ¥ãƒ¼å‡¦ç†é–‹å§‹: æœ€å¤§{limit}ä»¶")

        stats = {
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'total': 0
        }

        for i in range(limit):
            # æ¬¡ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
            task = self._get_next_task()

            if not task:
                logger.info("å‡¦ç†ã™ã‚‹ã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“")
                break

            stats['total'] += 1
            queue_id = task['queue_id']
            document_id = task['document_id']
            file_name = task['file_name']
            preserve_workspace = task.get('preserve_workspace', True)

            logger.info(f"\n{'='*80}")
            logger.info(f"[{i+1}/{limit}] å‡¦ç†é–‹å§‹: {file_name}")
            logger.info(f"Queue ID: {queue_id}")
            logger.info(f"Document ID: {document_id}")

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å†å‡¦ç†
            success = await self._reprocess_document(
                queue_id=queue_id,
                document_id=document_id,
                file_name=file_name,
                preserve_workspace=preserve_workspace
            )

            if success:
                stats['success'] += 1
            else:
                stats['failed'] += 1

            # é€²æ—è¡¨ç¤º
            logger.info(f"é€²æ—: æˆåŠŸ={stats['success']}, å¤±æ•—={stats['failed']}, åˆè¨ˆ={stats['total']}")

        return stats

    def _get_next_task(self) -> Optional[Dict[str, Any]]:
        """
        æ¬¡ã®å‡¦ç†å¯¾è±¡ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å–å¾—
        ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢æ•° get_next_reprocessing_task ã‚’ä½¿ç”¨

        Returns:
            ã‚¿ã‚¹ã‚¯æƒ…å ±ã€ã¾ãŸã¯None
        """
        try:
            response = self.db.client.rpc(
                'get_next_reprocessing_task',
                {'p_worker_id': self.worker_id}
            ).execute()

            if response.data and len(response.data) > 0:
                return response.data[0]
            return None

        except Exception as e:
            logger.error(f"æ¬¡ã‚¿ã‚¹ã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    async def _reprocess_document(
        self,
        queue_id: str,
        document_id: str,
        file_name: str,
        preserve_workspace: bool = True
    ) -> bool:
        """
        å˜ä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å†å‡¦ç†ã—ã€çµæœã‚’ã‚­ãƒ¥ãƒ¼ã«è¨˜éŒ²

        Args:
            queue_id: ã‚­ãƒ¥ãƒ¼ID
            document_id: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«å
            preserve_workspace: workspaceã‚’ä¿æŒã™ã‚‹ã‹

        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—
            doc = self.db.get_document_by_id(document_id)
            if not doc:
                error_msg = "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                logger.error(error_msg)
                self._mark_task_failed(queue_id, error_msg)
                return False

            source_type = doc.get('source_type', '')

            # ============================================
            # Classroomæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ä»˜ããƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆclassroomï¼‰ã®å‡¦ç†
            # ============================================
            if source_type == 'classroom':
                logger.info(f"ğŸ“ Classroomæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ä»˜ããƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œå‡ºï¼ˆ{source_type}ï¼‰")
                return await self._reprocess_classroom_document_with_attachment(
                    queue_id=queue_id,
                    document_id=document_id,
                    doc=doc,
                    preserve_workspace=preserve_workspace
                )

            # ============================================
            # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆclassroom_text, text_onlyï¼‰ã®å‡¦ç†
            # ============================================
            if source_type in ['classroom_text', 'text_only']:
                logger.info(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œå‡ºï¼ˆ{source_type}ï¼‰")
                return await self._reprocess_text_only_document(
                    queue_id=queue_id,
                    document_id=document_id,
                    doc=doc,
                    preserve_workspace=preserve_workspace
                )

            # ============================================
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆdrive, email_attachmentç­‰ï¼‰ã®å‡¦ç†
            # ============================================
            # ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’å–å¾—
            file_id = self._extract_file_id(doc)
            if not file_id:
                error_msg = "ãƒ•ã‚¡ã‚¤ãƒ«IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                logger.error(f"{error_msg}: {file_name}")
                logger.error(f"  source_id: {doc.get('source_id')}")
                self._mark_task_failed(queue_id, error_msg)
                return False

            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ID: {file_id}")

            # å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ãŒå¤šã„ãŸã‚ï¼‰
            file_ext = '.' + file_name.lower().split('.')[-1] if '.' in file_name else ''

            if file_ext in self.VIDEO_EXTENSIONS:
                logger.info(f"ğŸ¬ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º: {file_name}")
                logger.info(f"  â†’ ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»å‰Šæ¸›ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                self._mark_task_completed(queue_id, success=True)
                return True

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
            file_meta = {
                'id': file_id,
                'name': file_name,
                'mimeType': self._guess_mime_type(file_name),
                'doc_type': doc.get('doc_type', 'other')  # doc_typeã‚’è¿½åŠ ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: otherï¼‰
            }

            # workspaceã‚’æ±ºå®š
            workspace_to_use = doc.get('workspace', 'unknown') if preserve_workspace else 'unknown'
            logger.info(f"Workspace: {workspace_to_use} (preserve={preserve_workspace})")

            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å‡¦ç†
            result = await self.pipeline.process_file(
                file_meta=file_meta,
                workspace=workspace_to_use,
                force_reprocess=True
            )

            if result and result.get('success'):
                logger.success(f"âœ… å†å‡¦ç†æˆåŠŸ: {file_name}")
                self._mark_task_completed(queue_id, success=True)
                return True

            elif result and result.get('error') and 'duplicate key' in str(result.get('error')):
                # é‡è¤‡ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€å¤ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã—ã¦å†è©¦è¡Œ
                logger.warning(f"é‡è¤‡æ¤œå‡ºã€å¤ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã—ã¦å†è©¦è¡Œ")
                self.db.client.table('source_documents').delete().eq('id', document_id).execute()

                # å†è©¦è¡Œ
                result = await self.pipeline.process_file(
                    file_meta=file_meta,
                    workspace=workspace_to_use,
                    force_reprocess=True
                )

                if result and result.get('success'):
                    logger.success(f"âœ… å†å‡¦ç†æˆåŠŸï¼ˆå†è©¦è¡Œï¼‰: {file_name}")
                    self._mark_task_completed(queue_id, success=True)
                    return True
                else:
                    error_msg = f"å†è©¦è¡Œã‚‚å¤±æ•—: {result.get('error', 'unknown')}"
                    logger.error(f"âŒ {error_msg}")
                    self._mark_task_failed(queue_id, error_msg)
                    return False
            else:
                error_msg = result.get('error', 'unknown error') if result else 'no result'
                logger.error(f"âŒ å†å‡¦ç†å¤±æ•—: {error_msg}")
                self._mark_task_failed(queue_id, error_msg)
                return False

        except Exception as e:
            error_msg = f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.exception(e)
            self._mark_task_failed(queue_id, error_msg, error_details={'exception': str(e)})
            return False

    def _mark_task_completed(self, queue_id: str, success: bool):
        """ã‚¿ã‚¹ã‚¯ã‚’å®Œäº†ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
        try:
            self.db.client.rpc(
                'mark_reprocessing_task_completed',
                {
                    'p_queue_id': queue_id,
                    'p_success': success
                }
            ).execute()
        except Exception as e:
            logger.error(f"ã‚¿ã‚¹ã‚¯å®Œäº†ãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

    def _mark_task_failed(
        self,
        queue_id: str,
        error_message: str,
        error_details: Optional[Dict] = None
    ):
        """ã‚¿ã‚¹ã‚¯ã‚’å¤±æ•—ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
        try:
            self.db.client.rpc(
                'mark_reprocessing_task_completed',
                {
                    'p_queue_id': queue_id,
                    'p_success': False,
                    'p_error_message': error_message,
                    'p_error_details': json.dumps(error_details) if error_details else None
                }
            ).execute()
        except Exception as e:
            logger.error(f"ã‚¿ã‚¹ã‚¯å¤±æ•—ãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

    async def _reprocess_text_only_document(
        self,
        queue_id: str,
        document_id: str,
        doc: Dict[str, Any],
        preserve_workspace: bool = True
    ) -> bool:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆclassroom_textï¼‰ã‚’å†å‡¦ç†

        Args:
            queue_id: ã‚­ãƒ¥ãƒ¼ID
            document_id: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
            doc: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
            preserve_workspace: workspaceã‚’ä¿æŒã™ã‚‹ã‹

        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        from D_stage_a_classifier.classifier import StageAClassifier
        from F_stage_c_extractor.extractor import StageCExtractor
        from A_common.config.yaml_loader import get_classification_yaml_string

        file_name = doc.get('file_name', 'text_only')
        source_type = doc.get('source_type', '')

        # å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰å€‹åˆ¥ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆçµåˆã—ãªã„ï¼‰
        display_subject = doc.get('display_subject', '')
        display_post_text = doc.get('display_post_text', '')
        attachment_text = doc.get('attachment_text', '')

        # ClassroomæŠ•ç¨¿ï¼ˆæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ï¼‰ã®å ´åˆã®æ¤œè¨¼
        if source_type == 'classroom_text':
            if not (display_subject or display_post_text):
                error_msg = "display_subjectã‚‚display_post_textã‚‚ç©ºã§ã™"
                logger.error(f"{error_msg}: {file_name}")
                self._mark_task_failed(queue_id, error_msg)
                return False
            total_length = len(display_subject) + len(display_post_text)
            logger.info(f"ğŸ“ Classroomãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: ä»¶å={len(display_subject)}æ–‡å­—, æœ¬æ–‡={len(display_post_text)}æ–‡å­—")
        else:
            # text_only ãªã©ã€é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆ
            if not attachment_text:
                error_msg = "attachment_textãŒç©ºã§ã™"
                logger.error(f"{error_msg}: {file_name}")
                self._mark_task_failed(queue_id, error_msg)
                return False
            total_length = len(attachment_text)
            logger.info(f"ğŸ“ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ: {total_length}æ–‡å­—")

        logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆç·é‡: {total_length}æ–‡å­—")

        try:
            # Stage 1ã¨Stage 2ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
            stage1_classifier = StageAClassifier(llm_client=self.pipeline.llm_client)
            stage2_extractor = StageCExtractor(llm_client=self.pipeline.llm_client)
            yaml_string = get_classification_yaml_string()

            # workspaceã‚’æ±ºå®š
            workspace_to_use = doc.get('workspace', 'unknown') if preserve_workspace else 'unknown'

            # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
            text_parts = []
            if display_subject:
                text_parts.append(f"ã€ä»¶åã€‘\n{display_subject}")
            if display_post_text:
                text_parts.append(f"ã€æœ¬æ–‡ã€‘\n{display_post_text}")
            if attachment_text:
                text_parts.append(f"ã€æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã€‘\n{attachment_text}")
            combined_text = '\n\n'.join(text_parts)

            # ============================================
            # Stage C: Claudeæ§‹é€ åŒ–ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼‰
            # ============================================
            logger.info("[Stage C] Claudeæ§‹é€ åŒ–é–‹å§‹...")

            # stage1_resultã«ã¯doc_typeã¨workspaceã®ã¿ã‚’æ¸¡ã™
            stage1_result_for_stagec = {
                'doc_type': doc.get('doc_type', 'unknown'),
                'workspace': doc.get('workspace', 'unknown')
            }

            stagec_result = stage2_extractor.extract_metadata(
                file_name=file_name,
                stage1_result=stage1_result_for_stagec,  # doc_typeã¨workspaceã®ã¿
                workspace=doc.get('workspace', 'unknown'),
                attachment_text=attachment_text if attachment_text else None,
                display_subject=display_subject if display_subject else None,
                display_post_text=display_post_text if display_post_text else None,
            )

            # Stage Cã®çµæœã‚’å–å¾—
            document_date = stagec_result.get('document_date')
            tags = stagec_result.get('tags', [])
            stagec_metadata = stagec_result.get('metadata', {})

            logger.info(f"[Stage C] å®Œäº†: metadata_fields={len(stagec_metadata)}")

            # ============================================
            # Stage A: Geminiçµ±åˆãƒ»è¦ç´„ï¼ˆStage Cã®çµæœã‚’æ´»ç”¨ï¼‰
            # ============================================
            logger.info("[Stage A] Geminiçµ±åˆãƒ»è¦ç´„é–‹å§‹...")

            summary = ''
            relevant_date = None

            try:
                from pathlib import Path as PathLib
                stageA_result = await stage1_classifier.classify(
                    file_path=PathLib("dummy"),  # ãƒ€ãƒŸãƒ¼ãƒ‘ã‚¹ï¼ˆä½¿ç”¨ã•ã‚Œãªã„ï¼‰
                    doc_types_yaml=yaml_string,
                    mime_type="text/plain",
                    text_content=combined_text,
                    stagec_result=stagec_result  # Stage Cã®çµæœã‚’æ¸¡ã™
                )

                summary = stageA_result.get('summary', '')
                relevant_date = stageA_result.get('relevant_date')

                logger.info(f"[Stage A] å®Œäº†: summary={summary[:50] if summary else ''}...")

            except Exception as e:
                logger.error(f"[Stage A] å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                # Stage Cã®summaryã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨
                summary = stagec_result.get('summary', '')
                relevant_date = stagec_result.get('document_date')
                logger.info("[Stage A] å¤±æ•— â†’ Stage Cã®summaryã‚’ä½¿ç”¨")

            # çµæœã®çµ±åˆ
            doc_type = doc.get('doc_type', 'unknown')  # å…ƒã®doc_typeã‚’ä¿æŒï¼ˆå¤‰æ›´ã—ãªã„ï¼‰
            metadata = stagec_metadata

            logger.info(f"[å‡¦ç†å®Œäº†] doc_type={doc_type}")

            # ============================================
            # ãƒãƒ£ãƒ³ã‚¯åŒ–å‡¦ç†ï¼ˆæ–°è¦è¿½åŠ ï¼‰
            # ============================================
            logger.info("[ãƒãƒ£ãƒ³ã‚¯åŒ–] é–‹å§‹...")

            # æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤ï¼ˆå†å‡¦ç†ã®å ´åˆï¼‰
            try:
                delete_result = self.db.client.table('search_index').delete().eq('document_id', document_id).execute()
                deleted_count = len(delete_result.data) if delete_result.data else 0
                logger.info(f"  æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤: {deleted_count}å€‹")
            except Exception as e:
                logger.warning(f"  æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {e}")

            # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆã™ã¹ã¦ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹ï¼‰
            document_data = {
                'file_name': file_name,
                'summary': summary,
                'document_date': document_date,
                'tags': tags,
                'doc_type': doc.get('doc_type'),
                'display_subject': display_subject,
                'display_post_text': display_post_text,
                'display_sender': doc.get('display_sender'),
                'display_type': doc.get('display_type'),
                'display_sent_at': doc.get('display_sent_at'),
                'classroom_sender_email': doc.get('classroom_sender_email'),
                'attachment_text': attachment_text,  # classroom_textã®å ´åˆã¯None
                'persons': metadata.get('persons', []) if isinstance(metadata, dict) else [],
                'organizations': metadata.get('organizations', []) if isinstance(metadata, dict) else [],
                'people': metadata.get('people', []) if isinstance(metadata, dict) else []
            }

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ
            from A_common.processing.metadata_chunker import MetadataChunker
            metadata_chunker = MetadataChunker()
            metadata_chunks = metadata_chunker.create_metadata_chunks(document_data)

            current_chunk_index = 0
            for meta_chunk in metadata_chunks:
                meta_text = meta_chunk.get('chunk_text', '')
                meta_type = meta_chunk.get('chunk_type', 'metadata')
                meta_weight = meta_chunk.get('search_weight', 1.0)

                if not meta_text:
                    continue

                # Embeddingç”Ÿæˆ
                meta_embedding = self.pipeline.llm_client.generate_embedding(meta_text)

                # search_indexã«ä¿å­˜
                meta_doc = {
                    'document_id': document_id,
                    'chunk_index': current_chunk_index,
                    'chunk_content': meta_text,
                    'chunk_size': len(meta_text),
                    'chunk_type': meta_type,
                    'embedding': meta_embedding,
                    'search_weight': meta_weight
                }

                try:
                    self.db.client.table('search_index').insert(meta_doc).execute()
                    current_chunk_index += 1
                except Exception as e:
                    logger.error(f"  ãƒãƒ£ãƒ³ã‚¯ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

            logger.info(f"[ãƒãƒ£ãƒ³ã‚¯åŒ–] å®Œäº†: {current_chunk_index}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ä½œæˆ")

            # ============================================
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
            # ============================================
            update_data = {
                'summary': summary,
                'metadata': metadata,
                'processing_status': 'completed',
                'processing_stage': 'stagec_and_stagea_complete',
                'stagea_classifier_model': 'gemini-2.5-flash',
                'stageb_vision_model': None,  # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆStage Bæœªä½¿ç”¨ï¼‰
                'stagec_extractor_model': 'claude-haiku-4-5-20251001',
                'text_extraction_model': None,  # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆæŠ½å‡ºä¸è¦ï¼‰
                'relevant_date': relevant_date
            }

            if document_date:
                update_data['document_date'] = document_date
            if tags:
                update_data['tags'] = tags

            response = self.db.client.table('source_documents').update(update_data).eq('id', document_id).execute()

            if response.data:
                logger.success(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†å‡¦ç†æˆåŠŸ: {file_name}")
                logger.info(f"  ãƒãƒ£ãƒ³ã‚¯æ•°: {current_chunk_index}")
                self._mark_task_completed(queue_id, success=True)
                return True
            else:
                error_msg = "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°å¤±æ•—"
                logger.error(error_msg)
                self._mark_task_failed(queue_id, error_msg)
                return False

        except Exception as e:
            error_msg = f"ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.exception(e)
            self._mark_task_failed(queue_id, error_msg, error_details={'exception': str(e)})
            return False

    async def _reprocess_classroom_document_with_attachment(
        self,
        queue_id: str,
        document_id: str,
        doc: Dict[str, Any],
        preserve_workspace: bool = True
    ) -> bool:
        """
        æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ä»˜ãClassroomãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆsource_type='classroom'ï¼‰ã‚’å†å‡¦ç†

        å‡¦ç†ãƒ•ãƒ­ãƒ¼:
        1. Pre-processing: Drive URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        2. Stage B: ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º + Visionå‡¦ç†
        3. Stage C: Claudeæ§‹é€ åŒ–ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼‰
        4. Stage A: Geminiçµ±åˆãƒ»è¦ç´„ï¼ˆStage Cã®çµæœã‚’æ´»ç”¨ï¼‰
        5. ãƒãƒ£ãƒ³ã‚¯åŒ–: subject + post_text + attachment_text
        6. Supabaseä¿å­˜

        Args:
            queue_id: ã‚­ãƒ¥ãƒ¼ID
            document_id: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
            doc: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
            preserve_workspace: workspaceã‚’ä¿æŒã™ã‚‹ã‹

        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        from D_stage_a_classifier.classifier import StageAClassifier
        from F_stage_c_extractor.extractor import StageCExtractor
        from A_common.config.yaml_loader import get_classification_yaml_string

        file_name = doc.get('file_name', 'classroom_attachment')
        display_subject = doc.get('display_subject', '')
        display_post_text = doc.get('display_post_text', '')

        # å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ãŒå¤šã„ãŸã‚ï¼‰
        # ãŸã ã—ã€æŠ•ç¨¿æœ¬æ–‡ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ç™»éŒ²
        file_ext = '.' + file_name.lower().split('.')[-1] if '.' in file_name else ''

        if file_ext in self.VIDEO_EXTENSIONS:
            logger.info(f"ğŸ¬ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º: {file_name}")
            logger.info(f"  â†’ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«è‡ªä½“ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ãŒã€æŠ•ç¨¿æœ¬æ–‡ã¯å‡¦ç†ã—ã¾ã™")

            # æŠ•ç¨¿æœ¬æ–‡ãŒã‚ã‚‹å ´åˆã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã—ã¦å‡¦ç†
            if display_subject or display_post_text:
                logger.info(f"  ğŸ“ æŠ•ç¨¿æœ¬æ–‡ã‚’æ¤œå‡º: ä»¶å={len(display_subject)}æ–‡å­—, æœ¬æ–‡={len(display_post_text)}æ–‡å­—")
                return await self._process_video_post_text_only(
                    queue_id=queue_id,
                    document_id=document_id,
                    doc=doc,
                    file_name=file_name,
                    display_subject=display_subject,
                    display_post_text=display_post_text,
                    preserve_workspace=preserve_workspace
                )
            else:
                logger.info(f"  â†’ æŠ•ç¨¿æœ¬æ–‡ã‚‚ãªã„ãŸã‚å®Œå…¨ã«ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                self._mark_task_completed(queue_id, success=True)
                return True

        # metadata ã‹ã‚‰ Google Drive ãƒ•ã‚¡ã‚¤ãƒ«ID/URLã‚’å–å¾—
        metadata = doc.get('metadata')
        if metadata is None:
            metadata = {}
        elif isinstance(metadata, str):
            try:
                metadata = json.loads(metadata)
            except:
                metadata = {}

        # ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ï¼ˆå„ªå…ˆé †ä½é †ï¼‰
        # â˜…ä¿®æ­£: source_idã‚’æœ€å„ªå…ˆï¼ˆGASã§ã‚³ãƒ”ãƒ¼ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«IDã€ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«æ¨©é™ä»˜ä¸æ¸ˆã¿ï¼‰
        file_id = None
        source_description = ""

        # 1. source_idï¼ˆã‚³ãƒ”ãƒ¼å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«IDã€ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ï¼‰
        source_id = doc.get('source_id')
        if source_id and not source_id.isdigit():
            file_id = source_id
            source_description = f"source_id: {file_id}"

        # 2. metadata.drive_url ã‹ã‚‰ã®æŠ½å‡ºï¼ˆURLå½¢å¼ï¼‰
        if not file_id:
            drive_url = metadata.get('drive_url')
            if drive_url:
                file_id = self._extract_file_id_from_url(drive_url)
                source_description = f"Drive URL: {drive_url}"

        # 3. metadata.original_classroom_idï¼ˆå…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«IDã€æ¨©é™ãŒãªã„å¯èƒ½æ€§ã‚ã‚Šï¼‰
        if not file_id:
            original_classroom_id = metadata.get('original_classroom_id')
            if original_classroom_id and not original_classroom_id.isdigit():
                file_id = original_classroom_id
                source_description = f"original_classroom_id: {file_id}"

        if not file_id:
            error_msg = "ãƒ•ã‚¡ã‚¤ãƒ«IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆdrive_url, original_classroom_id, source_id ã®ã„ãšã‚Œã‚‚ç„¡åŠ¹ï¼‰"
            logger.error(f"{error_msg}: {file_name}")
            self._mark_task_failed(queue_id, error_msg)
            return False

        logger.info(f"ğŸ“ Classroomæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º")
        logger.info(f"  ä»¶å: {display_subject[:50] if display_subject else '(ãªã—)'}...")
        logger.info(f"  ãƒ•ã‚¡ã‚¤ãƒ«ã‚½ãƒ¼ã‚¹: {source_description}")
        logger.info(f"  ãƒ•ã‚¡ã‚¤ãƒ«ID: {file_id}")

        try:
            # ============================================
            # Pre-processing: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            # ============================================
            logger.info("[Pre-processing] ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰ï¼ˆClassroomãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å«ã‚ã‚‹ï¼‰
            file_meta = {
                'id': file_id,
                'name': file_name,
                'mimeType': self._guess_mime_type(file_name),
                'doc_type': doc.get('doc_type', 'classroom_document'),
                'display_subject': display_subject,
                'display_post_text': display_post_text
            }

            # workspaceã‚’æ±ºå®š
            workspace_to_use = doc.get('workspace', 'unknown') if preserve_workspace else 'unknown'
            logger.info(f"  Workspace: {workspace_to_use} (preserve={preserve_workspace})")

            # ============================================
            # Stage B: ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º + Visionå‡¦ç†
            # ============================================
            logger.info("[Stage B] ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º + Visionå‡¦ç†é–‹å§‹...")

            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®process_fileãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
            # ã“ã‚Œã«ã‚ˆã‚Šã€Pre-processingã€Stage Bã€Stage Cã€Stage AãŒå…¨ã¦å®Ÿè¡Œã•ã‚Œã‚‹
            result = await self.pipeline.process_file(
                file_meta=file_meta,
                workspace=workspace_to_use,
                force_reprocess=True
            )

            if not result or not result.get('success'):
                error_msg = result.get('error', 'unknown error') if result else 'no result'
                logger.error(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†å¤±æ•—: {error_msg}")
                self._mark_task_failed(queue_id, error_msg)
                return False

            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒå‡¦ç†ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã‚’å–å¾—
            processed_doc_id = result.get('document_id')

            # ============================================
            # å…ƒã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ã—ã€æ–°ã—ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã‚’ç¶­æŒ
            # ï¼ˆfile_metaã«Classroomãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å«ã‚ãŸã®ã§ã€ãƒãƒ£ãƒ³ã‚¯å†ç”Ÿæˆã¯ä¸è¦ï¼‰
            # ============================================
            if processed_doc_id != document_id:
                logger.info(f"[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆçµ±åˆ] å…ƒã®ID {document_id} â†’ æ–°ID {processed_doc_id}")
                # å¤ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤
                try:
                    self.db.client.table('source_documents').delete().eq('id', document_id).execute()
                    logger.info("  å¤ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‰Šé™¤å®Œäº†")
                except Exception as e:
                    logger.warning(f"  å¤ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‰Šé™¤ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {e}")

            logger.success(f"âœ… Classroomæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†å‡¦ç†æˆåŠŸ: {file_name}")
            # ãƒãƒ£ãƒ³ã‚¯æ•°ã¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã§è‡ªå‹•çš„ã«è¨˜éŒ²ã•ã‚Œã¾ã™
            self._mark_task_completed(queue_id, success=True)
            return True

        except Exception as e:
            error_msg = f"Classroomæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.exception(e)
            self._mark_task_failed(queue_id, error_msg, error_details={'exception': str(e)})
            return False

    async def _process_video_post_text_only(
        self,
        queue_id: str,
        document_id: str,
        doc: Dict[str, Any],
        file_name: str,
        display_subject: str,
        display_post_text: str,
        preserve_workspace: bool = True
    ) -> bool:
        """
        å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ä»˜ãæŠ•ç¨¿ã®æœ¬æ–‡ã®ã¿ã‚’å‡¦ç†ï¼ˆå‹•ç”»è‡ªä½“ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰

        Args:
            queue_id: ã‚­ãƒ¥ãƒ¼ID
            document_id: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
            doc: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«å
            display_subject: æŠ•ç¨¿ä»¶å
            display_post_text: æŠ•ç¨¿æœ¬æ–‡
            preserve_workspace: workspaceã‚’ä¿æŒã™ã‚‹ã‹

        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        from D_stage_a_classifier.classifier import StageAClassifier
        from F_stage_c_extractor.extractor import StageCExtractor
        from A_common.config.yaml_loader import get_classification_yaml_string

        logger.info(f"ğŸ“ å‹•ç”»æŠ•ç¨¿ã®æœ¬æ–‡å‡¦ç†é–‹å§‹: {file_name}")
        logger.info(f"  ä»¶å: {display_subject[:50] if display_subject else '(ãªã—)'}...")
        logger.info(f"  æœ¬æ–‡: {display_post_text[:50] if display_post_text else '(ãªã—)'}...")

        try:
            # Stage 1ã¨Stage 2ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
            stage1_classifier = StageAClassifier(llm_client=self.pipeline.llm_client)
            stage2_extractor = StageCExtractor(llm_client=self.pipeline.llm_client)
            yaml_string = get_classification_yaml_string()

            # workspaceã‚’æ±ºå®š
            workspace_to_use = doc.get('workspace', 'unknown') if preserve_workspace else 'unknown'

            # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
            text_parts = []
            if display_subject:
                text_parts.append(f"ã€ä»¶åã€‘\n{display_subject}")
            if display_post_text:
                text_parts.append(f"ã€æœ¬æ–‡ã€‘\n{display_post_text}")
            # å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹ã“ã¨ã‚’æ˜è¨˜
            text_parts.append(f"ã€å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã€‘\n{file_name}")
            combined_text = '\n\n'.join(text_parts)

            # ============================================
            # Stage C: Claudeæ§‹é€ åŒ–ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼‰
            # ============================================
            logger.info("[Stage C] Claudeæ§‹é€ åŒ–é–‹å§‹...")

            stage1_result_for_stagec = {
                'doc_type': doc.get('doc_type', 'unknown'),
                'workspace': doc.get('workspace', 'unknown')
            }

            stagec_result = stage2_extractor.extract_metadata(
                file_name=file_name,
                stage1_result=stage1_result_for_stagec,
                workspace=doc.get('workspace', 'unknown'),
                attachment_text=None,  # å‹•ç”»ã¯å‡¦ç†ã—ãªã„
                display_subject=display_subject if display_subject else None,
                display_post_text=display_post_text if display_post_text else None,
            )

            # Stage Cã®çµæœã‚’å–å¾—
            document_date = stagec_result.get('document_date')
            tags = stagec_result.get('tags', [])
            stagec_metadata = stagec_result.get('metadata', {})

            logger.info(f"[Stage C] å®Œäº†: metadata_fields={len(stagec_metadata)}")

            # ============================================
            # Stage A: Geminiçµ±åˆãƒ»è¦ç´„ï¼ˆStage Cã®çµæœã‚’æ´»ç”¨ï¼‰
            # ============================================
            logger.info("[Stage A] Geminiçµ±åˆãƒ»è¦ç´„é–‹å§‹...")

            summary = ''
            relevant_date = None

            try:
                from pathlib import Path as PathLib
                stageA_result = await stage1_classifier.classify(
                    file_path=PathLib("dummy"),
                    doc_types_yaml=yaml_string,
                    mime_type="text/plain",
                    text_content=combined_text,
                    stagec_result=stagec_result
                )

                summary = stageA_result.get('summary', '')
                relevant_date = stageA_result.get('relevant_date')

                logger.info(f"[Stage A] å®Œäº†: summary={summary[:50] if summary else ''}...")

            except Exception as e:
                logger.error(f"[Stage A] å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                summary = stagec_result.get('summary', '')
                relevant_date = stagec_result.get('document_date')
                logger.info("[Stage A] å¤±æ•— â†’ Stage Cã®summaryã‚’ä½¿ç”¨")

            # çµæœã®çµ±åˆ
            doc_type = doc.get('doc_type', 'unknown')
            metadata = stagec_metadata

            logger.info(f"[å‡¦ç†å®Œäº†] doc_type={doc_type}")

            # ============================================
            # ãƒãƒ£ãƒ³ã‚¯åŒ–å‡¦ç†
            # ============================================
            logger.info("[ãƒãƒ£ãƒ³ã‚¯åŒ–] é–‹å§‹...")

            # æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤ï¼ˆå†å‡¦ç†ã®å ´åˆï¼‰
            try:
                delete_result = self.db.client.table('search_index').delete().eq('document_id', document_id).execute()
                deleted_count = len(delete_result.data) if delete_result.data else 0
                logger.info(f"  æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤: {deleted_count}å€‹")
            except Exception as e:
                logger.warning(f"  æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {e}")

            # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆã™ã¹ã¦ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹ï¼‰
            document_data = {
                'file_name': file_name,
                'summary': summary,
                'document_date': document_date,
                'tags': tags,
                'doc_type': doc.get('doc_type'),
                'display_subject': display_subject,
                'display_post_text': display_post_text,
                'display_sender': doc.get('display_sender'),
                'display_type': doc.get('display_type'),
                'display_sent_at': doc.get('display_sent_at'),
                'classroom_sender_email': doc.get('classroom_sender_email'),
                'attachment_text': None,  # å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡¦ç†ã—ãªã„
                'persons': metadata.get('persons', []) if isinstance(metadata, dict) else [],
                'organizations': metadata.get('organizations', []) if isinstance(metadata, dict) else [],
                'people': metadata.get('people', []) if isinstance(metadata, dict) else []
            }

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ
            from A_common.processing.metadata_chunker import MetadataChunker
            metadata_chunker = MetadataChunker()
            metadata_chunks = metadata_chunker.create_metadata_chunks(document_data)

            current_chunk_index = 0
            for meta_chunk in metadata_chunks:
                meta_text = meta_chunk.get('chunk_text', '')
                meta_type = meta_chunk.get('chunk_type', 'metadata')
                meta_weight = meta_chunk.get('search_weight', 1.0)

                if not meta_text:
                    continue

                # Embeddingç”Ÿæˆ
                meta_embedding = self.pipeline.llm_client.generate_embedding(meta_text)

                # search_indexã«ä¿å­˜
                meta_doc = {
                    'document_id': document_id,
                    'chunk_index': current_chunk_index,
                    'chunk_content': meta_text,
                    'chunk_size': len(meta_text),
                    'chunk_type': meta_type,
                    'embedding': meta_embedding,
                    'search_weight': meta_weight
                }

                try:
                    self.db.client.table('search_index').insert(meta_doc).execute()
                    current_chunk_index += 1
                except Exception as e:
                    logger.error(f"  ãƒãƒ£ãƒ³ã‚¯ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

            logger.info(f"[ãƒãƒ£ãƒ³ã‚¯åŒ–] å®Œäº†: {current_chunk_index}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ä½œæˆ")

            # ============================================
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
            # ============================================
            update_data = {
                'summary': summary,
                'metadata': metadata,
                'processing_status': 'completed',
                'processing_stage': 'stagec_and_stagea_complete',
                'stagea_classifier_model': 'gemini-2.5-flash',
                'stageb_vision_model': None,  # å‹•ç”»ã‚¹ã‚­ãƒƒãƒ—ï¼ˆStage Bæœªä½¿ç”¨ï¼‰
                'stagec_extractor_model': 'claude-haiku-4-5-20251001',
                'text_extraction_model': None,  # å‹•ç”»ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæŠ½å‡ºä¸è¦ï¼‰
                'relevant_date': relevant_date,
                'attachment_text': f"ã€å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã€‘{file_name}"  # å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’è¨˜éŒ²
            }

            if document_date:
                update_data['document_date'] = document_date
            if tags:
                update_data['tags'] = tags

            response = self.db.client.table('source_documents').update(update_data).eq('id', document_id).execute()

            if response.data:
                logger.success(f"âœ… å‹•ç”»æŠ•ç¨¿æœ¬æ–‡å‡¦ç†æˆåŠŸ: {file_name}")
                logger.info(f"  ãƒãƒ£ãƒ³ã‚¯æ•°: {current_chunk_index}")
                self._mark_task_completed(queue_id, success=True)
                return True
            else:
                error_msg = "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°å¤±æ•—"
                logger.error(error_msg)
                self._mark_task_failed(queue_id, error_msg)
                return False

        except Exception as e:
            error_msg = f"å‹•ç”»æŠ•ç¨¿æœ¬æ–‡å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.exception(e)
            self._mark_task_failed(queue_id, error_msg, error_details={'exception': str(e)})
            return False

    def _extract_file_id(self, doc: Dict[str, Any]) -> str:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰Google Drive ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’æŠ½å‡º"""
        # â˜…ä¿®æ­£: source_idã‚’æœ€å„ªå…ˆï¼ˆGASã§ã‚³ãƒ”ãƒ¼ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«IDã€ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«æ¨©é™ä»˜ä¸æ¸ˆã¿ï¼‰

        # 1. source_idï¼ˆã‚³ãƒ”ãƒ¼å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«IDã€ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ï¼‰
        source_id = doc.get('source_id', '')
        if source_id and not source_id.isdigit():
            return source_id

        # 2. metadata->original_file_idï¼ˆå…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«IDã€æ¨©é™ãŒãªã„å¯èƒ½æ€§ã‚ã‚Šï¼‰
        metadata = doc.get('metadata')
        if metadata is None:
            metadata = {}
        elif isinstance(metadata, str):
            try:
                metadata = json.loads(metadata)
            except:
                metadata = {}

        if metadata.get('original_file_id'):
            return metadata['original_file_id']

        return ''

    def _extract_file_id_from_url(self, url: str) -> Optional[str]:
        """
        Google Drive URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’æŠ½å‡º

        Args:
            url: Google Drive URL

        Returns:
            ãƒ•ã‚¡ã‚¤ãƒ«IDã€ã¾ãŸã¯None

        Examples:
            https://drive.google.com/file/d/1ABC123/view -> 1ABC123
            https://drive.google.com/open?id=1ABC123 -> 1ABC123
        """
        if not url:
            return None

        import re

        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: /file/d/{file_id}/
        match = re.search(r'/file/d/([a-zA-Z0-9_-]+)', url)
        if match:
            return match.group(1)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ?id={file_id}
        match = re.search(r'[?&]id=([a-zA-Z0-9_-]+)', url)
        if match:
            return match.group(1)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: /d/{file_id}/
        match = re.search(r'/d/([a-zA-Z0-9_-]+)', url)
        if match:
            return match.group(1)

        return None

    def _guess_mime_type(self, file_name: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ MIME ã‚¿ã‚¤ãƒ—ã‚’æ¨æ¸¬"""
        ext = file_name.lower().split('.')[-1] if '.' in file_name else ''

        mime_map = {
            'pdf': 'application/pdf',
            'jpg': 'image/jpeg',
            'jpeg': 'image/jpeg',
            'png': 'image/png',
            'gif': 'image/gif',
            'doc': 'application/msword',
            'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            'xls': 'application/vnd.ms-excel',
            'xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        }

        return mime_map.get(ext, 'application/octet-stream')

    def get_queue_stats(self) -> Dict[str, int]:
        """ã‚­ãƒ¥ãƒ¼ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        try:
            # å„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ä»¶æ•°ã‚’é›†è¨ˆ
            stats = {}
            statuses = ['pending', 'processing', 'completed', 'failed', 'skipped']

            for status in statuses:
                result = self.db.client.table('document_reprocessing_queue').select(
                    '*', count='exact'
                ).eq('status', status).execute()
                stats[status] = result.count if result.count else 0

            stats['total'] = sum(stats.values())

            return stats

        except Exception as e:
            logger.error(f"çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def print_queue_stats(self):
        """ã‚­ãƒ¥ãƒ¼ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        stats = self.get_queue_stats()

        logger.info("\n" + "="*80)
        logger.info("ã‚­ãƒ¥ãƒ¼çµ±è¨ˆ")
        logger.info("="*80)
        logger.info(f"å¾…æ©Ÿä¸­ (pending):   {stats.get('pending', 0):>5}ä»¶")
        logger.info(f"å‡¦ç†ä¸­ (processing): {stats.get('processing', 0):>5}ä»¶")
        logger.info(f"å®Œäº†   (completed):  {stats.get('completed', 0):>5}ä»¶")
        logger.info(f"å¤±æ•—   (failed):     {stats.get('failed', 0):>5}ä»¶")
        logger.info(f"ã‚¹ã‚­ãƒƒãƒ— (skipped):  {stats.get('skipped', 0):>5}ä»¶")
        logger.info("-" * 80)
        logger.info(f"åˆè¨ˆ:                {stats.get('total', 0):>5}ä»¶")
        logger.info("="*80 + "\n")

    async def run(
        self,
        limit: int = 100,
        dry_run: bool = False,
        populate_only: bool = False,
        process_queue_only: bool = False,
        preserve_workspace: bool = True,
        workspace: str = 'all',
        auto_yes: bool = False
    ):
        """
        å†å‡¦ç†ã‚’å®Ÿè¡Œ

        Args:
            limit: å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•°
            dry_run: Trueã®å ´åˆã€å®Ÿéš›ã®å‡¦ç†ã¯è¡Œã‚ãšç¢ºèªã®ã¿
            populate_only: Trueã®å ´åˆã€ã‚­ãƒ¥ãƒ¼è¿½åŠ ã®ã¿ï¼ˆå‡¦ç†ã¯å®Ÿè¡Œã—ãªã„ï¼‰
            process_queue_only: Trueã®å ´åˆã€ã‚­ãƒ¥ãƒ¼å‡¦ç†ã®ã¿ï¼ˆæ–°è¦è¿½åŠ ã—ãªã„ï¼‰
            preserve_workspace: Trueã®å ´åˆã€æ—¢å­˜ã®workspaceã‚’ä¿æŒ
        """
        logger.info("\n" + "="*80)
        logger.info("Google Classroom ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ v2")
        logger.info("="*80)

        if dry_run:
            logger.warning("ğŸ” DRY RUN ãƒ¢ãƒ¼ãƒ‰: å®Ÿéš›ã®å‡¦ç†ã¯è¡Œã„ã¾ã›ã‚“")

        # ã‚­ãƒ¥ãƒ¼ã®ç¾åœ¨ã®çŠ¶æ…‹ã‚’è¡¨ç¤º
        self.print_queue_stats()

        # ã‚­ãƒ¥ãƒ¼ã¸ã®è¿½åŠ 
        if not process_queue_only:
            logger.info(f"\nğŸ“¥ ã‚­ãƒ¥ãƒ¼ã¸ã®è¿½åŠ ã‚’é–‹å§‹...")
            logger.info(f"  å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹: {workspace}")
            logger.info(f"  æœ€å¤§ä»¶æ•°: {limit}")
            logger.info(f"  Workspaceä¿æŒ: {preserve_workspace}")

            if not dry_run:
                added = self.populate_queue_from_workspace(
                    workspace=workspace,
                    limit=limit,
                    preserve_workspace=preserve_workspace
                )
                logger.info(f"âœ… {added}ä»¶ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã—ã¾ã—ãŸ")

                # æ›´æ–°å¾Œã®çµ±è¨ˆã‚’è¡¨ç¤º
                self.print_queue_stats()

        # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å‡¦ç†
        if not populate_only:
            if dry_run:
                logger.info(f"\nğŸ” DRY RUN: {limit}ä»¶ã®å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ")
            else:
                logger.info(f"\nâš™ï¸  ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ã®å‡¦ç†ã‚’é–‹å§‹...")

                # ç¢ºèªï¼ˆauto_yesãŒç„¡åŠ¹ãªå ´åˆã®ã¿ç¢ºèªï¼‰
                if not auto_yes:
                    print("\nå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/N): ", end='')
                    response = input().strip().lower()
                    if response != 'y':
                        logger.info("å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
                        return

                # å‡¦ç†å®Ÿè¡Œ
                stats = await self.process_queue(limit=limit)

                # æœ€çµ‚çµæœ
                logger.info("\n" + "="*80)
                logger.info("å†å‡¦ç†å®Œäº†")
                logger.info("="*80)
                logger.info(f"æˆåŠŸ: {stats['success']}ä»¶")
                logger.info(f"å¤±æ•—: {stats['failed']}ä»¶")
                logger.info(f"åˆè¨ˆ: {stats['total']}ä»¶")
                logger.info("="*80)

                # æœ€çµ‚çš„ãªã‚­ãƒ¥ãƒ¼ã®çŠ¶æ…‹ã‚’è¡¨ç¤º
                self.print_queue_stats()


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‘ãƒ¼ã‚¹
    dry_run = '--dry-run' in sys.argv or '-n' in sys.argv
    populate_only = '--populate-only' in sys.argv
    process_queue_only = '--process-queue' in sys.argv
    preserve_workspace = '--no-preserve-workspace' not in sys.argv
    auto_yes = '--no' not in sys.argv  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è‡ªå‹•æ‰¿èªï¼ˆ--noã§ç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¡¨ç¤ºï¼‰
    limit = 100
    workspace = 'all'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹

    # --limit ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®å‡¦ç†
    for arg in sys.argv:
        if arg.startswith('--limit='):
            try:
                limit = int(arg.split('=')[1])
            except:
                pass
        elif arg.startswith('--workspace='):
            workspace = arg.split('=')[1]

    reprocessor = ClassroomReprocessorV2()
    await reprocessor.run(
        limit=limit,
        dry_run=dry_run,
        populate_only=populate_only,
        process_queue_only=process_queue_only,
        preserve_workspace=preserve_workspace,
        workspace=workspace,
        auto_yes=auto_yes
    )


if __name__ == "__main__":
    asyncio.run(main())
