"""
2æ®µéšå–ã‚Šè¾¼ã¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆv4.0: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰AIç‰ˆï¼‰
Stage A/B/Cå®Ÿè£…ç‰ˆï¼ˆStage C: Claudeè©³ç´°æŠ½å‡ºï¼‰
"""

import os
import asyncio
from typing import Dict, Any, Optional
from pathlib import Path
from datetime import datetime
from loguru import logger
import hashlib
import json
import traceback

from A_common.connectors.google_drive import GoogleDriveConnector
from A_common.processors.pdf import PDFProcessor
from A_common.processors.office import OfficeProcessor
from D_stage_a_classifier.classifier import StageAClassifier
from F_stage_c_extractor.extractor import StageCExtractor
# from C_ai_common.embeddings.embeddings import EmbeddingClient  # 768æ¬¡å…ƒ - ä½¿ç”¨ã—ãªã„
from A_common.database.client import DatabaseClient
from C_ai_common.llm_client.llm_client import LLMClient
from A_common.utils.chunking import TextChunker, ParentChildChunker
from A_common.utils.synthetic_chunks import create_all_synthetic_chunks
from A_common.utils.date_extractor import DateExtractor
from A_common.processing.metadata_chunker import MetadataChunker
from A_common.config.yaml_loader import get_classification_yaml_string
from A_common.config.model_tiers import ModelTier


def flatten_metadata_to_text(metadata: Dict[str, Any]) -> str:
    """
    ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢å¯èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ï¼ˆå†å¸°çš„ã«å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æŠ½å‡ºï¼‰
    activities, recruitment, organization ãªã©ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å®Œå…¨ã«å±•é–‹
    """
    searchable_parts = []

    def extract_values(obj, prefix=''):
        """å†å¸°çš„ã«è¾æ›¸ã‚„ãƒªã‚¹ãƒˆã‹ã‚‰å€¤ã‚’æŠ½å‡º"""
        if isinstance(obj, dict):
            for key, value in obj.items():
                # ãƒ¡ã‚¿æƒ…å ±ã‚„ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ã‚¹ã‚­ãƒƒãƒ—
                if key in ['extractor', 'num_pages', 'extraction_method', 'confidence']:
                    continue
                extract_values(value, f"{prefix}{key}: " if prefix or key not in ['weekly_schedule', 'text_blocks'] else '')
        elif isinstance(obj, list):
            for item in obj:
                extract_values(item, prefix)
        elif obj is not None and str(obj).strip():
            # æœ‰åŠ¹ãªå€¤ã®ã¿è¿½åŠ 
            searchable_parts.append(f"{prefix}{str(obj)}" if prefix else str(obj))

    # é™¤å¤–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆæ¤œç´¢ã«ä¸è¦ã€ã¾ãŸã¯å¤§ãã™ãã‚‹ï¼‰
    excluded_fields = ['weekly_schedule', 'text_blocks', 'extractor', 'num_pages']

    # å…¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å†å¸°çš„ã«æŠ½å‡º
    for key, value in metadata.items():
        if key not in excluded_fields:
            extract_values(value)

    # weekly_schedule ã®å±•é–‹ï¼ˆå¾“æ¥é€šã‚Šï¼‰
    if 'weekly_schedule' in metadata and metadata['weekly_schedule']:
        for day_schedule in metadata['weekly_schedule']:
            if 'date' in day_schedule:
                searchable_parts.append(day_schedule['date'])
            if 'day_of_week' in day_schedule:
                searchable_parts.append(day_schedule['day_of_week'])
            if 'day' in day_schedule:
                searchable_parts.append(f"{day_schedule['day']}æ›œæ—¥")
            if 'events' in day_schedule and day_schedule['events']:
                searchable_parts.extend(day_schedule['events'])
            if 'note' in day_schedule and day_schedule['note']:
                searchable_parts.append(day_schedule['note'])
            if 'class_schedules' in day_schedule:
                for class_schedule in day_schedule['class_schedules']:
                    if 'class' in class_schedule:
                        searchable_parts.append(class_schedule['class'])
                    if 'subjects' in class_schedule and class_schedule['subjects']:
                        searchable_parts.extend(class_schedule['subjects'])
                    if 'periods' in class_schedule and class_schedule['periods']:
                        for period in class_schedule['periods']:
                            if 'subject' in period:
                                searchable_parts.append(period['subject'])
                            if 'time' in period:
                                searchable_parts.append(period['time'])

    # text_blocks ã®å±•é–‹ï¼ˆå¾“æ¥é€šã‚Šï¼‰
    if 'text_blocks' in metadata and metadata['text_blocks']:
        for block in metadata['text_blocks']:
            if 'title' in block and block['title']:
                searchable_parts.append(block['title'])
            if 'content' in block and block['content']:
                searchable_parts.append(block['content'])

    return ' '.join(searchable_parts)

PROCESSING_STATUS = {
    "PENDING": "pending",
    "COMPLETED": "completed",
    "FAILED": "failed",
    "SKIPPED": "skipped"
}

class TwoStageIngestionPipeline:
    """2æ®µéšå–ã‚Šè¾¼ã¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, temp_dir: str = "./temp"):

        self.llm_client = LLMClient()
        self.drive = GoogleDriveConnector()
        self.db = DatabaseClient()
        self.date_extractor = DateExtractor()  # æ—¥ä»˜æŠ½å‡ºãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
        self.yaml_string = get_classification_yaml_string()

        self.pdf_processor = PDFProcessor(llm_client=self.llm_client)
        self.office_processor = OfficeProcessor()

        self.stageA_classifier = StageAClassifier(llm_client=self.llm_client)
        self.stageC_extractor = StageCExtractor(llm_client=self.llm_client)
        # Embeddingã¯LLMClientçµŒç”±ã§ç”Ÿæˆï¼ˆ1536æ¬¡å…ƒï¼‰
        # self.embeddings = EmbeddingClient()  # å‰Šé™¤
        
        self.temp_dir = Path(temp_dir)
        self.temp_dir.mkdir(parents=True, exist_ok=True)

        # Stage Cã¯å®Œå…¨ãƒªãƒ¬ãƒ¼æ–¹å¼ï¼ˆåˆ¤å®šãªã—ã€Stage Aã®çµæœã‚’å¿…ãšStage Cã¸ï¼‰

        logger.info("TwoStageIngestionPipelineåˆæœŸåŒ–å®Œäº† (å®Œå…¨ãƒªãƒ¬ãƒ¼æ–¹å¼: Geminiâ†’Haiku)")
    
    def _extract_text(self, local_path: str, mime_type: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
        
        logger.debug(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºé–‹å§‹: {local_path}, mime_type={mime_type}")
        logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª: {Path(local_path).exists()}")
        logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {Path(local_path).stat().st_size if Path(local_path).exists() else 'N/A'} bytes")
        
        mime_map = {
            "application/pdf": self.pdf_processor.extract_text,
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document": self.office_processor.extract_from_docx,
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet": self.office_processor.extract_from_xlsx,
            "application/vnd.openxmlformats-officedocument.presentationml.presentation": self.office_processor.extract_from_pptx,
        }
        
        processor = mime_map.get(mime_type)
        if processor:
            result = processor(local_path)
            logger.debug(f"æŠ½å‡ºçµæœ: success={result.get('success')}, content_length={len(result.get('content', ''))}")
            return result
        
        return {"content": "", "metadata": {}, "success": False, "error_message": f"Unsupported MIME Type: {mime_type}"}

    def _get_file_type(self, mime_type: str) -> str:
        """MIME Typeã‹ã‚‰file_typeã‚’åˆ¤å®š"""
        mapping = {
            "application/pdf": "pdf",
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document": "docx",
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet": "xlsx",
            "application/vnd.openxmlformats-officedocument.presentationml.presentation": "pptx",
        }
        return mapping.get(mime_type, "other")
    
    def _should_run_stageC(self, stageA_result: Dict[str, Any], extracted_text: str) -> bool:
        """
        Stage Cã‚’å®Ÿè¡Œã™ã¹ãã‹ã©ã†ã‹åˆ¤å®šï¼ˆå®Œå…¨ãƒªãƒ¬ãƒ¼æ–¹å¼ï¼‰

        ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‘
        - Stage A (Gemini): æ–‡æ›¸ã®åˆ†é¡ã¨åŸºæœ¬æƒ…å ±ã®æŠ½å‡º
        - Stage B (Gemini Vision): Visionå‡¦ç†ï¼ˆå¿…è¦ãªå ´åˆï¼‰
        - Stage C (Claude Haiku): Stage Aã®çµæœã‚’å—ã‘ã¦æ§‹é€ åŒ–ãƒ»æ„å‘³ä»˜ã‘

        ãƒ†ã‚­ã‚¹ãƒˆãŒå­˜åœ¨ã™ã‚‹é™ã‚Šã€Stage Aã®çµæœã¯å¿…ãšStage Cï¼ˆHaikuï¼‰ã«æ¸¡ã—ã¦æ§‹é€ åŒ–ã™ã‚‹ã€‚
        ä¿¡é ¼åº¦ã«é–¢ä¿‚ãªãã€åˆ¤å®šãªã—ã®å®Œå…¨ãƒªãƒ¬ãƒ¼æ–¹å¼ã§å‹•ä½œã™ã‚‹ã€‚
        """

        # æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®å ´åˆã®ã¿ã‚¹ã‚­ãƒƒãƒ—
        if not extracted_text or not extracted_text.strip():
            logger.info("[Stage C] ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            return False

        # ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹é™ã‚Šã€æ–‡å­—æ•°ã«é–¢ä¿‚ãªãç„¡æ¡ä»¶ã§Stage Cï¼ˆæ§‹é€ åŒ–ãƒ—ãƒ­ã‚»ã‚¹ï¼‰ã¸
        doc_type = stageA_result.get('doc_type', 'other')
        logger.info(f"[Stage C] æ§‹é€ åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã¸ç§»è¡Œ ({doc_type}, ãƒ†ã‚­ã‚¹ãƒˆé•·={len(extracted_text.strip())}æ–‡å­—)")
        return True

    async def process_file(
        self,
        file_meta: Dict[str, Any],
        workspace: str = "personal",
        force_reprocess: bool = False,
        source_type: str = "drive"
    ) -> Optional[Dict[str, Any]]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’2æ®µéšã§å‡¦ç†"""
        file_id = file_meta['id']
        file_name = file_meta['name']
        mime_type = file_meta.get('mimeType', 'application/octet-stream')
        doc_type = file_meta.get('doc_type', 'other')  # doc_typeã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: otherï¼‰

        # source_typeã¯file_metaã‹ã‚‰å–å¾—ã™ã‚‹ï¼ˆæŒ‡å®šãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’å„ªå…ˆï¼‰
        if 'source_type' in file_meta:
            source_type = file_meta['source_type']

        logger.info(f"=== 2æ®µéšå‡¦ç†é–‹å§‹: {file_name} ===")

        existing = self.db.get_document_by_source_id(file_id)
        if existing and not force_reprocess:
            logger.warning(f"æ—¢ã«å‡¦ç†æ¸ˆã¿ (Source ID): {file_name}")
            return existing

        if existing and force_reprocess:
            logger.info(f"ğŸ”„ å†å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä¸Šæ›¸ãã—ã¾ã™")

        # âœ… ClassroomæŠ•ç¨¿ã®å ´åˆã€display_sent_atã‚’å–å¾—ã—ã¦reference_dateã¨ã—ã¦ä½¿ç”¨
        reference_date = None
        if existing and existing.get('display_sent_at'):
            reference_date = str(existing['display_sent_at']).split('T')[0]  # YYYY-MM-DDå½¢å¼ã«å¤‰æ›
            logger.info(f"[Classroom] reference_date={reference_date} (display_sent_at ã‹ã‚‰å–å¾—)")
        elif 'display_sent_at' in file_meta:
            reference_date = str(file_meta['display_sent_at']).split('T')[0]
            logger.info(f"[Classroom] reference_date={reference_date} (file_meta ã‹ã‚‰å–å¾—)")
        
        local_path = None
        # extraction_resultã‚’åˆæœŸåŒ–ï¼ˆNameErrorå›é¿ï¼‰
        extraction_result = {"success": False, "content": "", "metadata": {}, "error_message": "æœªå®Ÿè¡Œ"}

        try:
            # ============================================
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            # ============================================
            local_path = self.drive.download_file(file_id, file_name, self.temp_dir)
            logger.debug(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {local_path}")

            # ============================================
            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆStage 1ã®å‰ã«å®Ÿè¡Œï¼‰
            # ============================================
            extraction_result = self._extract_text(local_path, mime_type)

            if not extraction_result["success"]:
                logger.warning(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: {file_name}")
                logger.warning(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {extraction_result.get('error_message')}")
                extracted_text = ""
            else:
                extracted_text = extraction_result["content"]

            base_metadata = extraction_result.get("metadata", {})

            # ============================================
            # Stage 1: Geminiåˆ†é¡ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã™ï¼‰
            # ============================================
            logger.info("[Stage 1] Geminiåˆ†é¡é–‹å§‹...")
            stageA_result = await self.stageA_classifier.classify(
                file_path=Path(local_path),
                doc_types_yaml=self.yaml_string,
                mime_type=mime_type,
                text_content=extracted_text
            )

            # Stage1ã¯doc_typeã¨workspaceã‚’è¿”ã•ãªã„ï¼ˆå…¥åŠ›å…ƒã§æ±ºå®šã•ã‚Œã‚‹ãŸã‚ï¼‰
            # workspaceã¯å¼•æ•°ã§æ¸¡ã•ã‚ŒãŸå€¤ã‚’ãã®ã¾ã¾ä½¿ç”¨
            summary = stageA_result.get('summary', '')
            relevant_date = stageA_result.get('relevant_date')
            document_date = None  # Stage Cã§è¨­å®šã•ã‚Œã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼ˆåˆæœŸåŒ–å¿…é ˆï¼‰

            logger.info(f"[Stage A] å®Œäº†: summary={summary[:50] if summary else ''}...")

            # ============================================
            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãŒå¤±æ•—ã—ãŸå ´åˆã§ã‚‚summaryã‚’ä½¿ç”¨
            # ============================================
            if not extraction_result["success"]:
                logger.warning("[ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º] å¤±æ•— â†’ summaryã‚’full_textã¨ã—ã¦ä½¿ç”¨")
                extracted_text = summary

            # ============================================
            # Stage Cåˆ¤å®šãƒ»å®Ÿè¡Œï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—ã§ã‚‚å®Ÿè¡Œï¼‰
            # ============================================
            if self._should_run_stageC(stageA_result, extracted_text):
                logger.info("[Stage C] Claudeè©³ç´°æŠ½å‡ºé–‹å§‹...")
                try:
                    stageC_result = self.stageC_extractor.extract_metadata(
                        file_name=file_name,
                        stage1_result=stageA_result,  # StageCExtractorã¯ stage1_result ã‚’æœŸå¾…
                        workspace=workspace,  # å¼•æ•°ã§æ¸¡ã•ã‚ŒãŸå…ƒã®workspaceã‚’ä½¿ç”¨
                        reference_date=reference_date,  # âœ… ClassroomæŠ•ç¨¿ã®å ´åˆã¯æŠ•ç¨¿æ—¥ã‚’æ¸¡ã™
                        # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã™
                        attachment_text=extracted_text if extracted_text else None,
                    )

                    # Stage Cã®çµæœã‚’åæ˜ ï¼ˆdoc_typeã¯ä½¿ã‚ãªã„ï¼‰
                    summary = stageC_result.get('summary', summary)
                    document_date = stageC_result.get('document_date')
                    event_dates = stageC_result.get('event_dates', [])  # ã‚¤ãƒ™ãƒ³ãƒˆæ—¥ä»˜é…åˆ—ã‚’å–å¾—
                    tags = stageC_result.get('tags', [])
                    tables = stageC_result.get('tables', [])  # è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                    stageC_metadata = stageC_result.get('metadata', {})

                    # metadataã‚’ãƒãƒ¼ã‚¸ï¼ˆStage Cå„ªå…ˆï¼‰
                    metadata = {
                        **base_metadata,
                        **stageC_metadata,
                        'stagec_attempted': True  # æ–°ã—ã„å‘½åè¦å‰‡
                    }
                    if tags:
                        metadata['tags'] = tags
                    if document_date:
                        metadata['document_date'] = document_date
                    if event_dates:
                        metadata['event_dates'] = event_dates  # ã‚¤ãƒ™ãƒ³ãƒˆæ—¥ä»˜é…åˆ—ã‚’metadataã«è¿½åŠ 
                    if tables:
                        metadata['tables'] = tables  # è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’metadataã«è¿½åŠ 

                    processing_stage = 'stagea_and_stagec'  # æ–°ã—ã„å‘½åè¦å‰‡
                    stagec_model = ModelTier.STAGEC_EXTRACTOR["model"]  # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‚ç…§

                    logger.info(f"[Stage C] å®Œäº†: metadata_fields={len(stageC_metadata)}")

                except Exception as e:
                    error_msg = str(e)
                    error_traceback = traceback.format_exc()
                    # KeyErrorå›é¿: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®‰å…¨ã«æ–‡å­—åˆ—åŒ–
                    safe_error_msg = error_msg.replace('{', '{{').replace('}', '}}')
                    safe_traceback = error_traceback.replace('{', '{{').replace('}', '}}')
                    logger.error(f"[Stage C] å‡¦ç†ã‚¨ãƒ©ãƒ¼: {safe_error_msg}\n{safe_traceback}")

                    # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’metadataã«è¨˜éŒ²
                    metadata = {
                        **base_metadata,
                        'stagec_attempted': True,
                        'stagec_error': str(e),
                        'stagec_error_type': type(e).__name__,
                        'stagec_error_timestamp': datetime.now().isoformat()
                    }

                    processing_stage = 'stagec_failed'
                    stagec_model = None
            else:
                # Stage Aã®ã¿ã§å®Œçµ
                processing_stage = 'stagea_only'
                metadata = {**base_metadata, 'stagec_attempted': False}
                stagec_model = None

            # ============================================
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
            # ============================================
            content_hash = hashlib.sha256(extracted_text.encode('utf-8')).hexdigest() if extracted_text else None
            
            # ============================================
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ï¼ˆNullæ–‡å­—é™¤å» + é‡è¤‡ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼‰
            # ============================================
            # Nullæ–‡å­—ã‚’é™¤å»
            if extracted_text:
                extracted_text = extracted_text.replace('\x00', '')
            if summary:
                summary = summary.replace('\x00', '')

            # metadata.tables ã‹ã‚‰ extracted_tables ã‚’æŠ½å‡º
            extracted_tables = None
            if 'tables' in metadata and metadata['tables']:
                extracted_tables = metadata['tables']

            # Visionå‡¦ç†ã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—
            vision_model = base_metadata.get('vision_model', None)  # Gemini Visionç­‰

            # ã‚¤ãƒ™ãƒ³ãƒˆæ—¥ä»˜é…åˆ—ã‚’å–å¾—ï¼ˆStage Cã§æŠ½å‡ºã•ã‚ŒãŸã‚‚ã®ï¼‰
            event_dates_array = event_dates if 'event_dates' in locals() and event_dates else []

            # ============================================
            # ã™ã¹ã¦ã®æ—¥ä»˜ã‚’æŠ½å‡ºï¼ˆæ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹ + AIçµæœï¼‰
            # ã€é‡è¦ã€‘æ—¥ä»˜ã¯æœ€å„ªå…ˆæ¤œç´¢é …ç›®ã¨ã—ã¦ã€æ¼ã‚ŒãªãæŠ½å‡º
            # ============================================
            all_mentioned_dates = []
            try:
                # æ­£è¦è¡¨ç¾ã§æœ¬æ–‡ã‹ã‚‰ã™ã¹ã¦ã®æ—¥ä»˜ã‚’æŠ½å‡º
                regex_extracted_dates = self.date_extractor.extract_all_dates(
                    text=extracted_text,
                    reference_date=reference_date  # ClassroomæŠ•ç¨¿æ—¥ãªã©ã‚’åŸºæº–ã«ç›¸å¯¾æ—¥ä»˜ã‚’è¨ˆç®—
                )

                # AIãŒæŠ½å‡ºã—ãŸevent_datesã¨çµ±åˆ
                all_dates_set = set(regex_extracted_dates)
                if event_dates_array:
                    all_dates_set.update(event_dates_array)

                # document_dateã‚‚è¿½åŠ 
                if document_date:
                    all_dates_set.add(document_date)

                # display_sent_atã‚‚è¿½åŠ ï¼ˆæŠ•ç¨¿æ—¥ã‚‚æ¤œç´¢å¯¾è±¡ï¼‰
                if reference_date:
                    all_dates_set.add(reference_date)

                # ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¦ã‚½ãƒ¼ãƒˆ
                all_mentioned_dates = sorted(list(all_dates_set))

                logger.info(f"[æ—¥ä»˜çµ±åˆ] åˆè¨ˆ{len(all_mentioned_dates)}ä»¶ã®æ—¥ä»˜ã‚’æŠ½å‡º: {all_mentioned_dates[:10]}...")

            except Exception as e:
                logger.error(f"æ—¥ä»˜æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

            document_data = {
                "source_type": source_type,  # å¼•æ•°ã¾ãŸã¯file_metaã‹ã‚‰å–å¾—ã—ãŸå€¤ã‚’ä½¿ç”¨
                "source_id": file_id,
                "source_url": f"https://drive.google.com/file/d/{file_id}/view",
                "file_name": file_name,
                "file_type": self._get_file_type(mime_type),
                "doc_type": workspace,  # doc_typeã¯å…¥åŠ›å…ƒã§æ±ºå®šï¼ˆworkspaceã¨åŒã˜å€¤ã‚’ä½¿ç”¨ï¼‰
                "workspace": workspace,  # å¼•æ•°ã§æ¸¡ã•ã‚ŒãŸå€¤ã‚’ä½¿ç”¨ï¼ˆå…¥åŠ›å…ƒã§æ±ºå®šï¼‰
                "attachment_text": extracted_text,  # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆï¼ˆsource_documentsãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚«ãƒ©ãƒ ã«å¯¾å¿œï¼‰
                "summary": summary,
                "metadata": metadata,
                # extracted_tables ã¨ event_dates ã¯ metadata å†…ã«å«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã‚«ãƒ©ãƒ ã‹ã‚‰å‰Šé™¤
                "all_mentioned_dates": all_mentioned_dates,  # æ­£è¦è¡¨ç¾+AIçµ±åˆã«ã‚ˆã‚‹å…¨æ—¥ä»˜é…åˆ—ï¼ˆæ¤œç´¢æœ€å„ªå…ˆï¼‰
                "content_hash": content_hash,
                "processing_status": PROCESSING_STATUS["COMPLETED"],
                "processing_stage": processing_stage,
                "stagea_classifier_model": ModelTier.STAGE1_CLASSIFIER["model"],  # B1æ›´æ–°ï¼ˆå°æ–‡å­—ï¼‰
                "stagec_extractor_model": stagec_model,  # Stage CæŠ½å‡ºãƒ¢ãƒ‡ãƒ«
                "stageb_vision_model": vision_model,  # B1æ›´æ–°ï¼ˆå°æ–‡å­—ï¼‰
                "relevant_date": relevant_date,
            }

            try:
                # upsertã‚’ä½¿ç”¨
                # force_reprocess=Trueæ™‚ã¯å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ›´æ–°ã™ã‚‹ãŒã€GASç”±æ¥ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ä¿æŒ
                preserve_fields = ['doc_type', 'workspace', 'source_type', 'display_sender', 'classroom_sender_email', 'display_sent_at', 'display_subject', 'classroom_course_id', 'classroom_course_name'] if force_reprocess else []
                result = await self.db.upsert_document(
                    'source_documents',  # 3å±¤ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒ†ãƒ¼ãƒ–ãƒ«åã«ä¿®æ­£
                    document_data,
                    conflict_column='source_id',
                    force_update=force_reprocess,
                    preserve_fields=preserve_fields
                )
                document_id = result.get('id')
                logger.info(f"Documentä¿å­˜å®Œäº†ï¼ˆupsert, force_update={force_reprocess}ï¼‰: {document_id}")

                # ============================================
                # å†å‡¦ç†æ™‚ã®æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤
                # ============================================
                if force_reprocess and document_id:
                    logger.info(f"  ğŸ”„ å†å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤ã—ã¾ã™")
                    try:
                        delete_result = self.db.client.table('search_index').delete().eq('document_id', document_id).execute()
                        deleted_count = len(delete_result.data) if delete_result.data else 0
                        logger.info(f"  æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤å®Œäº†: {deleted_count}å€‹")
                    except Exception as e:
                        logger.warning(f"  æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {e}")

                # ============================================
                # ãƒãƒ£ãƒ³ã‚¯åŒ–å‡¦ç†ï¼ˆB2: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ¥ãƒ™ã‚¯ãƒˆãƒ«åŒ–æˆ¦ç•¥å¯¾å¿œï¼‰
                # - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€ã‚µãƒãƒªãƒ¼ã€æ—¥ä»˜ã€ã‚¿ã‚°ï¼‰
                # - å°ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ç”¨
                # - å¤§ãƒãƒ£ãƒ³ã‚¯å›ç­”ç”¨
                # - åˆæˆãƒãƒ£ãƒ³ã‚¯
                # ============================================
                if extracted_text and document_id:
                    logger.info(f"  ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒ£ãƒ³ã‚¯åŒ–é–‹å§‹ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ + å° + åˆæˆï¼‰")
                    try:
                        # ClassroomæŠ•ç¨¿æœ¬æ–‡ã‚’å–å¾—
                        display_subject = None
                        if existing and existing.get('display_subject'):
                            display_subject = existing.get('display_subject')
                        elif 'display_subject' in file_meta:
                            display_subject = file_meta.get('display_subject')

                        # ãƒãƒ£ãƒ³ã‚¯åŒ–å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆï¼šClassroomæŠ•ç¨¿æœ¬æ–‡ + æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
                        chunk_target_text = ""
                        if extracted_text and display_subject:
                            # ä¸¡æ–¹ã‚ã‚‹å ´åˆï¼šæŠ•ç¨¿æœ¬æ–‡ + æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«
                            chunk_target_text = f"ã€æŠ•ç¨¿æœ¬æ–‡ã€‘\n{display_subject}\n\nã€æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã€‘\n{extracted_text}"
                            logger.info(f"  ClassroomæŠ•ç¨¿æœ¬æ–‡ã‚’è¿½åŠ : {len(display_subject)}æ–‡å­—")
                        elif display_subject:
                            # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®æŠ•ç¨¿ï¼ˆæ·»ä»˜ãªã—ï¼‰
                            chunk_target_text = f"ã€æŠ•ç¨¿æœ¬æ–‡ã€‘\n{display_subject}"
                            logger.info(f"  ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ•ç¨¿: {len(display_subject)}æ–‡å­—")
                        elif extracted_text:
                            # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
                            chunk_target_text = extracted_text
                        elif summary:
                            # ã‚µãƒãƒªãƒ¼ã®ã¿ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                            chunk_target_text = summary
                            logger.info(f"  ã‚µãƒãƒªãƒ¼ã‚’ãƒãƒ£ãƒ³ã‚¯åŒ–: {len(summary)}æ–‡å­—")

                        # ãƒãƒ£ãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
                        current_chunk_index = 0
                        metadata_chunk_success_count = 0

                        # ============================================
                        # ã‚¹ãƒ†ãƒƒãƒ—0: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ï¼ˆB2: é‡ã¿ä»˜ããƒãƒ£ãƒ³ã‚¯ï¼‰
                        # ============================================
                        logger.info(f"  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ã®ç”Ÿæˆé–‹å§‹")
                        metadata_chunker = MetadataChunker()

                        # Classroomæƒ…å ±ã‚’å–å¾—ï¼ˆexisting ã¾ãŸã¯ file_meta ã‹ã‚‰ï¼‰
                        classroom_fields = {}
                        for field in ['display_subject', 'display_post_text', 'display_type',
                                     'display_sender', 'display_sent_at', 'classroom_sender_email']:
                            value = None
                            if existing and existing.get(field):
                                value = existing.get(field)
                            elif field in file_meta:
                                value = file_meta.get(field)
                            if value:
                                classroom_fields[field] = value

                        document_data = {
                            'file_name': file_name,
                            'summary': summary,
                            'document_date': document_date,
                            'tags': tags,
                            'event_dates': event_dates if 'event_dates' in dir() else [],
                            **classroom_fields  # Classroomãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å±•é–‹
                        }
                        metadata_chunks = metadata_chunker.create_metadata_chunks(document_data)

                        for meta_chunk in metadata_chunks:
                            try:
                                meta_text = meta_chunk.get('chunk_text', '')
                                meta_type = meta_chunk.get('chunk_type', 'metadata')
                                meta_weight = meta_chunk.get('search_weight', 1.0)

                                if not meta_text:
                                    continue

                                meta_embedding = self.llm_client.generate_embedding(meta_text)

                                meta_doc = {
                                    'document_id': document_id,
                                    'chunk_index': current_chunk_index,
                                    'chunk_content': meta_text,
                                    'chunk_size': len(meta_text),
                                    'chunk_type': meta_type,
                                    'search_weight': meta_weight,
                                    'embedding': meta_embedding
                                }

                                chunk_result = await self.db.insert_document('search_index', meta_doc)
                                if chunk_result:
                                    metadata_chunk_success_count += 1
                                    current_chunk_index += 1
                                    logger.debug(f"    âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ä¿å­˜: {meta_type} (weight={meta_weight})")
                            except Exception as meta_chunk_error:
                                logger.error(f"  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ä¿å­˜ã‚¨ãƒ©ãƒ¼: {meta_chunk_error}")

                        logger.info(f"  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å®Œäº†: {metadata_chunk_success_count}å€‹")

                        # å°ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆæ¤œç´¢ç”¨ï¼‰
                        chunker = TextChunker(chunk_size=150, chunk_overlap=30)
                        small_chunks = chunker.split_text(chunk_target_text)

                        logger.info(f"  å°ãƒãƒ£ãƒ³ã‚¯ä½œæˆå®Œäº†: {len(small_chunks)}å€‹")

                        # ã‚¹ãƒ†ãƒƒãƒ—1: å°ãƒãƒ£ãƒ³ã‚¯ï¼ˆæ¤œç´¢ç”¨ï¼‰ã‚’ä¿å­˜
                        logger.info(f"  å°ãƒãƒ£ãƒ³ã‚¯ï¼ˆæ¤œç´¢ç”¨ï¼‰ã®ä¿å­˜é–‹å§‹: {len(small_chunks)}å€‹")
                        small_chunk_success_count = 0
                        for small_chunk in small_chunks:
                            try:
                                small_text = small_chunk.get('chunk_text', '')
                                small_embedding = self.llm_client.generate_embedding(small_text)

                                small_doc = {
                                    'document_id': document_id,
                                    'chunk_index': current_chunk_index,  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ã®å¾Œã‹ã‚‰
                                    'chunk_content': small_text,
                                    'chunk_size': small_chunk.get('chunk_size', len(small_text)),
                                    'chunk_type': 'content_small',  # B2: ãƒãƒ£ãƒ³ã‚¯ç¨®åˆ¥
                                    'search_weight': 1.0,  # B2: æ¤œç´¢é‡ã¿
                                    'embedding': small_embedding
                                }

                                chunk_result = await self.db.insert_document('search_index', small_doc)
                                if chunk_result:
                                    small_chunk_success_count += 1
                                    current_chunk_index += 1
                            except Exception as chunk_insert_error:
                                logger.error(f"  å°ãƒãƒ£ãƒ³ã‚¯ä¿å­˜ã‚¨ãƒ©ãƒ¼: {type(chunk_insert_error).__name__}: {chunk_insert_error}")
                                logger.debug(f"  ã‚¨ãƒ©ãƒ¼è©³ç´°: {repr(chunk_insert_error)}", exc_info=True)

                        logger.info(f"  å°ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å®Œäº†: {small_chunk_success_count}/{len(small_chunks)}å€‹")

                        # ã‚¹ãƒ†ãƒƒãƒ—2: åˆæˆãƒãƒ£ãƒ³ã‚¯ï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»è­°é¡Œç­‰ï¼‰ã‚’ç”Ÿæˆãƒ»ä¿å­˜
                        logger.info(f"  åˆæˆãƒãƒ£ãƒ³ã‚¯ï¼ˆæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å°‚ç”¨ï¼‰ã®ç”Ÿæˆé–‹å§‹")
                        synthetic_chunk_success_count = 0
                        synthetic_chunks = create_all_synthetic_chunks(metadata, file_name)

                        if synthetic_chunks:
                            logger.info(f"  åˆæˆãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆå®Œäº†: {len(synthetic_chunks)}å€‹")
                            for synthetic in synthetic_chunks:
                                try:
                                    synthetic_text = synthetic.get('content', '')
                                    synthetic_type = synthetic.get('type', 'unknown')

                                    if not synthetic_text:
                                        continue

                                    # åˆæˆãƒãƒ£ãƒ³ã‚¯ç”¨ã®embeddingã‚’ç”Ÿæˆ
                                    synthetic_embedding = self.llm_client.generate_embedding(synthetic_text)

                                    synthetic_doc = {
                                        'document_id': document_id,
                                        'chunk_index': current_chunk_index,
                                        'chunk_content': synthetic_text,
                                        'chunk_size': len(synthetic_text),
                                        'chunk_type': 'synthetic',  # B2: ãƒãƒ£ãƒ³ã‚¯ç¨®åˆ¥
                                        'search_weight': 1.0,  # B2: æ¤œç´¢é‡ã¿
                                        'embedding': synthetic_embedding,
                                        'section_title': f'[åˆæˆãƒãƒ£ãƒ³ã‚¯: {synthetic_type}]'  # è­˜åˆ¥ç”¨
                                    }

                                    chunk_result = await self.db.insert_document('search_index', synthetic_doc)
                                    if chunk_result:
                                        synthetic_chunk_success_count += 1
                                        current_chunk_index += 1
                                        logger.info(f"  âœ… åˆæˆãƒãƒ£ãƒ³ã‚¯ä¿å­˜æˆåŠŸ: {synthetic_type} ({len(synthetic_text)}æ–‡å­—)")
                                except Exception as chunk_insert_error:
                                    logger.error(f"  åˆæˆãƒãƒ£ãƒ³ã‚¯ä¿å­˜ã‚¨ãƒ©ãƒ¼: {type(chunk_insert_error).__name__}: {chunk_insert_error}")
                                    logger.debug(f"  ã‚¨ãƒ©ãƒ¼è©³ç´°: {repr(chunk_insert_error)}", exc_info=True)
                        else:
                            logger.info(f"  åˆæˆãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ: å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")

                        # B2: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ã‚’å«ã‚ãŸåˆè¨ˆ
                        total_chunks = metadata_chunk_success_count + small_chunk_success_count + synthetic_chunk_success_count
                        logger.info(f"  ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å®Œäº†ï¼ˆåˆè¨ˆï¼‰: {total_chunks}å€‹ï¼ˆãƒ¡ã‚¿{metadata_chunk_success_count}å€‹ + å°{small_chunk_success_count}å€‹ + åˆæˆ{synthetic_chunk_success_count}å€‹ï¼‰")

                        # ã‚¹ãƒ†ãƒƒãƒ—4: document ã® chunk_count ã‚’æ›´æ–°
                        try:
                            update_data = {
                                'chunk_count': total_chunks,
                                'chunking_strategy': 'metadata_small_synthetic'  # B2: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ + å° + åˆæˆ
                            }
                            response = (
                                self.db.client.table('source_documents')
                                .update(update_data)
                                .eq('id', document_id)
                                .execute()
                            )
                            logger.info(f"  Document chunk_countæ›´æ–°å®Œäº†: {total_chunks}å€‹")
                        except Exception as update_error:
                            logger.error(f"  Document chunk_countæ›´æ–°ã‚¨ãƒ©ãƒ¼: {update_error}")

                    except Exception as chunk_error:
                        logger.error(f"  ãƒãƒ£ãƒ³ã‚¯åŒ–ã‚¨ãƒ©ãƒ¼: {chunk_error}", exc_info=True)
                else:
                    logger.warning(f"  ãƒãƒ£ãƒ³ã‚¯åŒ–ã‚¹ã‚­ãƒƒãƒ—: extracted_text={bool(extracted_text)}, document_id={bool(document_id)}")

                logger.info(f"=== å‡¦ç†å®Œäº†: {file_name} ({doc_type}, {processing_stage}) ===")
                return {"success": True, "document_id": document_id, "doc_type": doc_type}
            except Exception as db_error:
                # é‡è¤‡ã‚¨ãƒ©ãƒ¼ï¼ˆ23505ï¼‰ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                error_str = str(db_error)
                if '23505' in error_str or 'duplicate' in error_str.lower():
                    logger.warning(f"é‡è¤‡ã‚¨ãƒ©ãƒ¼æ¤œå‡ºï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {file_name} - {error_str}")
                    return {"status": "skipped", "reason": "duplicate"}
                else:
                    # ãã®ä»–ã®DBã‚¨ãƒ©ãƒ¼ã¯å†ã‚¹ãƒ­ãƒ¼
                    raise
            
        except Exception as e:
            error_msg = str(e)
            error_traceback = traceback.format_exc()
            # KeyErrorå›é¿: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®‰å…¨ã«æ–‡å­—åˆ—åŒ–
            safe_error_msg = error_msg.replace('{', '{{').replace('}', '}}')
            safe_traceback = error_traceback.replace('{', '{{').replace('}', '}}')
            logger.error(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_name} - {safe_error_msg}\n{safe_traceback}")

            error_data = {
                "source_type": source_type,  # å¼•æ•°ã¾ãŸã¯file_metaã‹ã‚‰å–å¾—ã—ãŸå€¤ã‚’ä½¿ç”¨
                "source_id": file_id,
                "file_name": file_name,
                "workspace": workspace,
                "processing_status": PROCESSING_STATUS["FAILED"],
                "error_message": str(e),
                "file_type": self._get_file_type(mime_type),
            }

            try:
                # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã¯ä¸Šæ›¸ãï¼ˆforce_reprocessãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰
                await self.db.upsert_document('source_documents', error_data, conflict_column='source_id', force_update=True)
            except Exception as db_error:
                db_error_traceback = traceback.format_exc()
                # KeyErrorå›é¿: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®‰å…¨ã«æ–‡å­—åˆ—åŒ–
                safe_db_error = str(db_error).replace('{', '{{').replace('}', '}}')
                safe_db_traceback = db_error_traceback.replace('{', '{{').replace('}', '}}')
                logger.critical(f"DBä¿å­˜å¤±æ•—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰: {file_name} - DB Error: {safe_db_error}\n{safe_db_traceback}")

                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                fallback_dir = Path('_runtime/logs/db_errors')
                fallback_dir.mkdir(parents=True, exist_ok=True)

                fallback_file = fallback_dir / f"db_error_{datetime.now():%Y%m%d_%H%M%S}_{file_id}.json"

                try:
                    with open(fallback_file, 'w', encoding='utf-8') as f:
                        json.dump({
                            'error_data': error_data,
                            'db_error': str(db_error),
                            'db_error_traceback': traceback.format_exc(),
                            'timestamp': datetime.now().isoformat()
                        }, f, ensure_ascii=False, indent=2)
                    logger.warning(f"ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜: {fallback_file}")
                except Exception as file_error:
                    # KeyErrorå›é¿: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®‰å…¨ã«æ–‡å­—åˆ—åŒ–
                    safe_file_error = str(file_error).replace('{', '{{').replace('}', '}}')
                    logger.critical(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ä¿å­˜ã‚‚å¤±æ•—: {safe_file_error}") 
                
            return None
            
        finally:
            if local_path and Path(local_path).exists():
                os.remove(local_path)
                logger.debug(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {local_path}")