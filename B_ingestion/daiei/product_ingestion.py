"""
ãƒ€ã‚¤ã‚¨ãƒ¼ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼ å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦Supabaseã«ä¿å­˜ã—ã¾ã™ã€‚

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
1. ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦é…é”æ—¥æ™‚ã‚’é¸æŠ
2. ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸ã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
3. JANã‚³ãƒ¼ãƒ‰ã§æ—¢å­˜å•†å“ã‚’ãƒã‚§ãƒƒã‚¯
4. Supabaseã«ä¿å­˜ï¼ˆæ–°è¦ or æ›´æ–°ï¼‰
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime, date

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
root_dir = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(root_dir))

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from A_common.database.client import DatabaseClient
from B_ingestion.daiei.daiei_scraper_playwright import DaieiScraperPlaywright

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class DaieiProductIngestionPipeline:
    """ãƒ€ã‚¤ã‚¨ãƒ¼å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self, login_id: str, password: str, headless: bool = True):
        """
        Args:
            login_id: ãƒ€ã‚¤ã‚¨ãƒ¼ãƒ­ã‚°ã‚¤ãƒ³ID
            password: ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
            headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹ã‹
        """
        self.login_id = login_id
        self.password = password
        self.headless = headless
        self.scraper: Optional[DaieiScraperPlaywright] = None
        # Service Role Keyã‚’ä½¿ç”¨ï¼ˆRLSã‚’ãƒã‚¤ãƒ‘ã‚¹ï¼‰
        self.db = DatabaseClient(use_service_role=True)

        logger.info("DaieiProductIngestionPipelineåˆæœŸåŒ–å®Œäº†ï¼ˆService Roleä½¿ç”¨ï¼‰")

    async def start(self) -> bool:
        """
        ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’èµ·å‹•ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³

        Returns:
            æˆåŠŸã—ãŸã‚‰True
        """
        try:
            self.scraper = DaieiScraperPlaywright()
            await self.scraper.start(headless=self.headless)

            # ãƒ­ã‚°ã‚¤ãƒ³
            success = await self.scraper.login(self.login_id, self.password)
            if not success:
                logger.error("âŒ ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—")
                await self.scraper.close()
                return False

            # é…é”æ—¥æ™‚é¸æŠ
            success = await self.scraper.select_delivery_slot()
            if not success:
                logger.error("âŒ é…é”æ—¥æ™‚é¸æŠå¤±æ•—")
                await self.scraper.close()
                return False

            logger.info("âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•ãƒ»ãƒ­ã‚°ã‚¤ãƒ³ãƒ»é…é”æ—¥æ™‚é¸æŠå®Œäº†")
            return True

        except Exception as e:
            logger.error(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False

    async def close(self):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’çµ‚äº†"""
        if self.scraper:
            await self.scraper.close()

    async def check_existing_products(self, jan_codes: List[str]) -> set:
        """
        Supabaseã§æ—¢å­˜ã®å•†å“ï¼ˆJANã‚³ãƒ¼ãƒ‰ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯

        Args:
            jan_codes: ãƒã‚§ãƒƒã‚¯ã™ã‚‹JANã‚³ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ

        Returns:
            æ—¢ã«å­˜åœ¨ã™ã‚‹JANã‚³ãƒ¼ãƒ‰ã®ã‚»ãƒƒãƒˆ
        """
        try:
            # Noneã‚„ç©ºæ–‡å­—åˆ—ã‚’é™¤å¤–
            valid_jan_codes = [jan for jan in jan_codes if jan]

            if not valid_jan_codes:
                return set()

            # 80_rd_products ãƒ†ãƒ¼ãƒ–ãƒ«ã§æ—¢å­˜ã®JANã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
            result = self.db.client.table('80_rd_products').select(
                'jan_code'
            ).in_('jan_code', valid_jan_codes).execute()

            # JANã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            existing_codes = set()
            if result.data:
                for doc in result.data:
                    jan_code = doc.get('jan_code')
                    if jan_code:
                        existing_codes.add(jan_code)

            logger.info(f"æ—¢å­˜ã®å•†å“: {len(existing_codes)}ä»¶")
            return existing_codes

        except Exception as e:
            logger.error(f"Supabaseæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return set()

    def _prepare_product_data(self, product: Dict[str, Any]) -> Dict[str, Any]:
        """
        å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’Supabaseä¿å­˜ç”¨ã«æ•´å½¢

        Args:
            product: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‹ã‚‰å–å¾—ã—ãŸå•†å“ãƒ‡ãƒ¼ã‚¿

        Returns:
            Supabaseä¿å­˜ç”¨ã®ãƒ‡ãƒ¼ã‚¿
        """
        today = date.today()

        # ä¾¡æ ¼ã®å‡¦ç†
        price = product.get("price")
        price_float = None
        if price is not None:
            try:
                price_float = float(price)
            except (ValueError, TypeError):
                logger.warning(f"ä¾¡æ ¼ã®å¤‰æ›å¤±æ•—: {price}")

        # å•†å“åã®æ­£è¦åŒ–ï¼ˆå…¨è§’ã‚¹ãƒšãƒ¼ã‚¹â†’åŠè§’ã€é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤ï¼‰
        product_name = product.get("product_name", "")
        product_name_normalized = " ".join(product_name.replace("ã€€", " ").split())

        data = {
            # åŸºæœ¬æƒ…å ±
            "source_type": "online_shop",
            "workspace": "shopping",
            "doc_type": "online shop",
            "organization": "ãƒ€ã‚¤ã‚¨ãƒ¼ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼",

            # å•†å“åŸºæœ¬æƒ…å ±
            "product_name": product_name,
            "product_name_normalized": product_name_normalized,
            "jan_code": product.get("jan_code"),

            # ä¾¡æ ¼
            "current_price": price_float,
            "current_price_tax_included": price_float,  # ç¨è¾¼ã¿ä¾¡æ ¼
            "price_text": str(price) if price else None,

            # åˆ†é¡
            "category": product.get("category"),
            "manufacturer": product.get("manufacturer"),

            # å•†å“è©³ç´°
            "image_url": product.get("image_url"),

            # åœ¨åº«ãƒ»è²©å£²çŠ¶æ³
            "in_stock": product.get("in_stock", True),
            "is_available": product.get("is_available", True),

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            "metadata": json.dumps(product.get("raw_data", {}), ensure_ascii=False),

            # æ—¥ä»˜
            "document_date": today.isoformat(),
            "last_scraped_at": datetime.now().isoformat(),

            # è¡¨ç¤ºç”¨
            "display_subject": f"{product_name} - ãƒ€ã‚¤ã‚¨ãƒ¼",
            "display_sender": "ãƒ€ã‚¤ã‚¨ãƒ¼ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼",

            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            "updated_at": datetime.now().isoformat()
        }

        return data

    async def process_category_page(
        self,
        category_url: str,
        page: int = 1
    ) -> Dict[str, Any]:
        """
        1ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®1ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†

        Args:
            category_url: ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®å®Œå…¨URL
            page: ãƒšãƒ¼ã‚¸ç•ªå·

        Returns:
            å‡¦ç†çµæœã®è¾æ›¸
        """
        logger.info(f"ãƒšãƒ¼ã‚¸ {page} ã‚’å‡¦ç†ä¸­...")

        # å•†å“ãƒ‡ãƒ¼ã‚¿ã¨ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
        products, pagination_info = await self.scraper.fetch_products_page(category_url, page)

        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        if page == 1 and pagination_info:
            logger.info(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±: {pagination_info}")

        if not products:
            logger.warning("å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return {
                "success": True,
                "total_products": 0,
                "new_products": 0,
                "updated_products": 0,
                "pagination_info": pagination_info
            }

        # JANã‚³ãƒ¼ãƒ‰ã§æ—¢å­˜å•†å“ã‚’ãƒã‚§ãƒƒã‚¯
        jan_codes = [p.get("jan_code") for p in products if p.get("jan_code")]
        existing_jan_codes = await self.check_existing_products(jan_codes)

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        new_count = 0
        updated_count = 0

        for product in products:
            try:
                product_data = self._prepare_product_data(product)
                jan_code = product.get("jan_code")

                if jan_code and jan_code in existing_jan_codes:
                    # æ—¢å­˜å•†å“ã®æ›´æ–°
                    result = self.db.client.table('80_rd_products').update(
                        product_data
                    ).eq('jan_code', jan_code).execute()

                    if result.data:
                        updated_count += 1
                        logger.debug(f"å•†å“æ›´æ–°: {product.get('product_name')}")
                else:
                    # æ–°è¦å•†å“ã®è¿½åŠ 
                    product_data["created_at"] = datetime.now().isoformat()
                    result = self.db.client.table('80_rd_products').insert(
                        product_data
                    ).execute()

                    if result.data:
                        new_count += 1
                        logger.debug(f"å•†å“è¿½åŠ : {product.get('product_name')}")

            except Exception as e:
                logger.error(f"å•†å“ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                continue

        logger.info(f"âœ… å‡¦ç†å®Œäº†: åˆè¨ˆ{len(products)}ä»¶ï¼ˆæ–°è¦{new_count}ä»¶ã€æ›´æ–°{updated_count}ä»¶ï¼‰")

        return {
            "success": True,
            "total_products": len(products),
            "new_products": new_count,
            "updated_products": updated_count,
            "pagination_info": pagination_info
        }

    async def process_category_all_pages(
        self,
        category_url: str,
        category_name: str,
        max_pages: int = 100
    ) -> Dict[str, Any]:
        """
        1ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®å…¨ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†

        Args:
            category_url: ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®å®Œå…¨URL
            category_name: ã‚«ãƒ†ã‚´ãƒªãƒ¼å
            max_pages: æœ€å¤§ãƒšãƒ¼ã‚¸æ•°

        Returns:
            å‡¦ç†çµæœã®è¾æ›¸
        """
        logger.info(f"ã‚«ãƒ†ã‚´ãƒªãƒ¼ '{category_name}' ã®å…¨ãƒšãƒ¼ã‚¸å‡¦ç†é–‹å§‹")

        total_new = 0
        total_updated = 0
        total_products = 0
        page = 1
        total_pages_from_pagination = None

        while page <= max_pages:
            result = await self.process_category_page(category_url, page)

            # åˆå›ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
            if page == 1 and result.get("pagination_info"):
                pagination = result["pagination_info"]
                total_pages_from_pagination = pagination.get("totalPages")
                if total_pages_from_pagination:
                    logger.info(f"ğŸ“„ æ¤œå‡ºã•ã‚ŒãŸç·ãƒšãƒ¼ã‚¸æ•°: {total_pages_from_pagination}")
                    # ç·ãƒšãƒ¼ã‚¸æ•°ãŒmax_pagesã‚ˆã‚Šå°‘ãªã„å ´åˆã€max_pagesã‚’æ›´æ–°
                    if total_pages_from_pagination < max_pages:
                        max_pages = total_pages_from_pagination
                        logger.info(f"âœ… å‡¦ç†ãƒšãƒ¼ã‚¸æ•°ã‚’ {total_pages_from_pagination} ã«åˆ¶é™")

            if not result["success"] or result["total_products"] == 0:
                logger.info(f"ãƒšãƒ¼ã‚¸ {page} ã§å•†å“ãªã—ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼å‡¦ç†çµ‚äº†")
                break

            total_products += result["total_products"]
            total_new += result["new_products"]
            total_updated += result["updated_products"]

            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã«åŸºã¥ãçµ‚äº†åˆ¤å®š
            if total_pages_from_pagination and page >= total_pages_from_pagination:
                logger.info(f"âœ… å…¨{total_pages_from_pagination}ãƒšãƒ¼ã‚¸ã®å‡¦ç†å®Œäº†")
                break

            page += 1

        logger.info(f"âœ… ã‚«ãƒ†ã‚´ãƒªãƒ¼ '{category_name}' å®Œäº†")
        logger.info(f"   åˆè¨ˆ: {total_products}ä»¶ï¼ˆæ–°è¦{total_new}ä»¶ã€æ›´æ–°{total_updated}ä»¶ï¼‰")

        return {
            "success": True,
            "category_url": category_url,
            "category_name": category_name,
            "total_products": total_products,
            "new_products": total_new,
            "updated_products": total_updated,
            "pages_processed": page - 1,
            "total_pages": total_pages_from_pagination
        }


async def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    logger.info("ãƒ€ã‚¤ã‚¨ãƒ¼å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")

    login_id = os.getenv("DAIEI_LOGIN_ID")
    password = os.getenv("DAIEI_PASSWORD")

    if not login_id or not password:
        logger.error("âŒ ç’°å¢ƒå¤‰æ•° DAIEI_LOGIN_ID ã¨ DAIEI_PASSWORD ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return

    async with DaieiScraperPlaywright() as scraper:
        # ãƒ­ã‚°ã‚¤ãƒ³
        success = await scraper.login(login_id, password)
        if not success:
            logger.error("âŒ ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—")
            return

        # é…é”æ—¥æ™‚é¸æŠ
        success = await scraper.select_delivery_slot()
        if not success:
            logger.error("âŒ é…é”æ—¥æ™‚é¸æŠå¤±æ•—")
            return

        # ãƒ†ã‚¹ãƒˆ: ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
        # TODO: å®Ÿéš›ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼URLã‚’æŒ‡å®š
        test_category_url = "https://netsuper.daiei.co.jp/0000/category/test"
        products, pagination = await scraper.fetch_products_page(test_category_url, 1)

        logger.info(f"âœ… ãƒ†ã‚¹ãƒˆå®Œäº†: {len(products)}ä»¶ã®å•†å“ã‚’å–å¾—")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
