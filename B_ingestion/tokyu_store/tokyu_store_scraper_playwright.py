"""
æ±æ€¥ã‚¹ãƒˆã‚¢ ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (Playwrightç‰ˆ)

Playwrightã‚’ä½¿ç”¨ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ä¿æŒã—ãŸã¾ã¾å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ã€‚
"""

import json
import re
import time
import random
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from playwright.async_api import async_playwright, Browser, Page, BrowserContext

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger(__name__)


class TokyuStoreScraperPlaywright:
    """æ±æ€¥ã‚¹ãƒˆã‚¢ ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¹ (Playwrightç‰ˆ)"""

    def __init__(self):
        self.base_url = "https://ns.tokyu-bell.jp"
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None

    async def __aenter__(self):
        """async withæ§‹æ–‡ã§ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé–‹å§‹"""
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """async withæ§‹æ–‡ã§ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµ‚äº†"""
        await self.close()

    async def start(self, headless: bool = True):
        """
        ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•

        Args:
            headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•ã™ã‚‹ã‹
        """
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(headless=headless)
            self.context = await self.browser.new_context(
                viewport={'width': 1280, 'height': 800},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            self.page = await self.context.new_page()
            logger.info("âœ… Playwrightãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•å®Œäº†")

        except Exception as e:
            logger.error(f"ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            raise

    async def close(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã‚‹"""
        try:
            if self.page:
                await self.page.close()
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
            logger.info("âœ… ãƒ–ãƒ©ã‚¦ã‚¶çµ‚äº†")

        except Exception as e:
            logger.error(f"ãƒ–ãƒ©ã‚¦ã‚¶çµ‚äº†ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

    async def _is_logged_in(self) -> bool:
        """ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            # ãƒ­ã‚°ã‚¤ãƒ³ãƒ•ã‚©ãƒ¼ãƒ ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            login_form = await self.page.query_selector('input[name="LoginID"]')
            if login_form:
                # ãƒ­ã‚°ã‚¤ãƒ³ãƒ•ã‚©ãƒ¼ãƒ ãŒè¦‹ã¤ã‹ã£ãŸ = ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ãªã„
                return False

            # ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒªãƒ³ã‚¯ã¾ãŸã¯ãƒã‚¤ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿
            logout_link = await self.page.query_selector('a[href*="logout"]')
            mypage_link = await self.page.query_selector('a[href*="mypage"]')

            return bool(logout_link or mypage_link)
        except:
            return False

    async def login(self, login_id: str, password: str) -> bool:
        """
        æ±æ€¥ã‚¹ãƒˆã‚¢ ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼ã«ãƒ­ã‚°ã‚¤ãƒ³

        Args:
            login_id: ãƒ­ã‚°ã‚¤ãƒ³IDï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼‰
            password: ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰

        Returns:
            æˆåŠŸã—ãŸã‚‰True
        """
        import asyncio

        try:
            logger.info("ğŸ” æ±æ€¥ã‚¹ãƒˆã‚¢ ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼ã«ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")

            # ç›´æ¥ãƒ­ã‚°ã‚¤ãƒ³/ä¼šå“¡ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            logger.info("ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«é·ç§»ä¸­...")
            await self.page.goto(f"{self.base_url}/shop/customer/menu.aspx", wait_until="domcontentloaded", timeout=30000)
            await self.page.wait_for_timeout(2000)

            # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã‚’ç¢ºèª
            current_url = self.page.url
            logger.info(f"ğŸ“ ç¾åœ¨ã®URL: {current_url}")

            # ãƒ‡ãƒãƒƒã‚°: ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®HTML/ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜
            logger.info("ğŸ“¸ ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®HTML/ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜ä¸­...")
            await self.page.screenshot(path="tokyu_store_login_page.png")
            login_page_html = await self.page.content()
            with open("tokyu_store_login_page.html", "w", encoding="utf-8") as f:
                f.write(login_page_html)
            logger.info("âœ… ä¿å­˜å®Œäº†: tokyu_store_login_page.png, tokyu_store_login_page.html")

            # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å…¥åŠ›
            logger.info("ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å…¥åŠ›ä¸­...")
            login_id_input = await self.page.wait_for_selector(
                'input[name="uid"], input[id="login_uid"]',
                timeout=10000,
                state="visible"
            )
            await login_id_input.click()
            await login_id_input.fill(login_id)
            logger.info("âœ… ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹å…¥åŠ›å®Œäº†")

            # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›
            logger.info("ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ä¸­...")
            password_input = await self.page.wait_for_selector(
                'input[name="pwd"], input[id="login_pwd"]',
                timeout=5000,
                state="visible"
            )
            await password_input.click()
            await password_input.fill(password)
            logger.info("âœ… ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›å®Œäº†")

            # ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
            await asyncio.sleep(0.5)
            login_button = await self.page.wait_for_selector(
                'input[type="submit"][name="order"]',
                timeout=5000,
                state="visible"
            )
            await login_button.click()
            logger.info("ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯")

            # ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ã‚’å¾…æ©Ÿ
            await asyncio.sleep(3)
            await self.page.wait_for_load_state("domcontentloaded")

            current_url = self.page.url
            logger.info(f"ãƒ­ã‚°ã‚¤ãƒ³å¾Œã®URL: {current_url}")

            # ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸç¢ºèª
            if await self._is_logged_in():
                logger.info("âœ… ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ")
                return True
            else:
                logger.error("âŒ ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—")
                return False

        except Exception as e:
            logger.error(f"ãƒ­ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False

    async def select_delivery_area(self, zip_code: str = "158-0094") -> bool:
        """
        é…é”ã‚¨ãƒªã‚¢ï¼ˆéƒµä¾¿ç•ªå·ï¼‰ã‚’é¸æŠ

        Args:
            zip_code: éƒµä¾¿ç•ªå·ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 158-0094 ä¸–ç”°è°·åŒºï¼‰

        Returns:
            æˆåŠŸã—ãŸã‚‰True
        """
        import asyncio

        try:
            logger.info("ğŸ“ é…é”ã‚¨ãƒªã‚¢ã‚’é¸æŠä¸­...")

            # éƒµä¾¿ç•ªå·å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã‚’æ¢ã™
            zip_input = await self.page.query_selector('input[name*="zip"], input[id*="txtZip"]')

            if zip_input:
                await zip_input.click()
                await zip_input.fill(zip_code.replace("-", ""))
                logger.info(f"âœ… éƒµä¾¿ç•ªå·å…¥åŠ›: {zip_code}")

                # æ¤œç´¢ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
                search_button = await self.page.query_selector('input[type="submit"], button[type="submit"]')
                if search_button:
                    await search_button.click()
                    await asyncio.sleep(2)
                    await self.page.wait_for_load_state("domcontentloaded")
                    logger.info("âœ… é…é”ã‚¨ãƒªã‚¢é¸æŠå®Œäº†")
                    return True
            else:
                # éƒµä¾¿ç•ªå·é¸æŠãŒä¸è¦ãªå ´åˆ
                logger.info("âœ… é…é”ã‚¨ãƒªã‚¢é¸æŠã¯ä¸è¦ã§ã™")
                return True

        except Exception as e:
            logger.error(f"é…é”ã‚¨ãƒªã‚¢é¸æŠã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False

    async def fetch_products_page(
        self,
        category_url: str,
        page: int = 1
    ) -> tuple[List[Dict[str, Any]], Optional[dict]]:
        """
        ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸ã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Args:
            category_url: ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®å®Œå…¨URL
            page: ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆ1å§‹ã¾ã‚Šï¼‰

        Returns:
            (å•†å“ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ, ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±)
        """
        try:
            # URLã«ãƒšãƒ¼ã‚¸ç•ªå·ã‚’è¿½åŠ ï¼ˆæ±æ€¥ã‚¹ãƒˆã‚¢ã¯ _p2/ å½¢å¼ï¼‰
            if page == 1:
                url = category_url
            else:
                # æœ«å°¾ã® / ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰ _p{page}/ ã‚’è¿½åŠ 
                base_url = category_url.rstrip('/')
                url = f"{base_url}_p{page}/"

            logger.info(f"å•†å“ãƒšãƒ¼ã‚¸å–å¾—ä¸­ (page={page}): {url}")

            await self.page.goto(url, wait_until="networkidle", timeout=60000)
            await self.page.wait_for_timeout(3000)

            # å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆHTMLãƒ™ãƒ¼ã‚¹ï¼‰
            products, pagination_info = await self._extract_products_from_html()

            # ã‚¢ã‚¯ã‚»ã‚¹é–“éš”åˆ¶å¾¡
            await self.page.wait_for_timeout(random.randint(1000, 2000))

            return products, pagination_info

        except Exception as e:
            logger.error(f"å•†å“ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return [], None

    async def _extract_products_from_html(self) -> tuple[List[Dict[str, Any]], Optional[dict]]:
        """
        HTMLã‹ã‚‰å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º

        Returns:
            (å•†å“ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ, ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±)
        """
        try:
            logger.info("âœ… HTMLè§£æé–‹å§‹")

            # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ»HTMLä¿å­˜
            await self.page.screenshot(path="tokyu_store_product_page.png")
            logger.info("ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜: tokyu_store_product_page.png")

            html_content = await self.page.content()
            with open("tokyu_store_product_page.html", "w", encoding="utf-8") as f:
                f.write(html_content)
            logger.info("HTMLä¿å­˜: tokyu_store_product_page.html")

            # å•†å“ã‚³ãƒ³ãƒ†ãƒŠã‚’å–å¾—ï¼ˆæ±æ€¥ã‚¹ãƒˆã‚¢ã®å®Ÿéš›ã®HTMLæ§‹é€ ï¼‰
            product_containers = await self.page.query_selector_all(
                'div.block-pickup-list-p--item-body, li:has(div.block-pickup-list-p--item-body)'
            )
            logger.info(f"å•†å“ã‚³ãƒ³ãƒ†ãƒŠæ•°: {len(product_containers)}")

            products = []

            for container in product_containers:
                try:
                    # å•†å“åã‚’å–å¾—
                    name_elem = await container.query_selector('.block-pickup-list-p--goods-name a')
                    product_name = await name_elem.inner_text() if name_elem else None

                    # å•†å“ç”»åƒURLã‚’å–å¾—
                    img_elem = await container.query_selector('.block-pickup-list-p--image img')
                    if img_elem:
                        img_src = await img_elem.get_attribute('data-src')  # lazyloadç”¨
                        if not img_src:
                            img_src = await img_elem.get_attribute('src')
                        if img_src and not img_src.startswith('http'):
                            img_src = f"{self.base_url}{img_src}"
                    else:
                        img_src = None

                    # ä¾¡æ ¼ã‚’å–å¾—ï¼ˆç¨æŠœä¾¡æ ¼ï¼‰
                    price_elem = await container.query_selector('.block-pickup-list-p--net-price')
                    price_text = await price_elem.inner_text() if price_elem else None
                    price = None
                    if price_text:
                        # ä¾¡æ ¼ã‹ã‚‰æ•°å­—ã‚’æŠ½å‡ºï¼ˆï¿¥ã‚’é™¤å»ï¼‰
                        price_cleaned = price_text.replace('ï¿¥', '').replace(',', '').strip()
                        match = re.search(r'(\d+\.?\d*)', price_cleaned)
                        if match:
                            price = float(match.group(1))

                    # ç¨è¾¼ä¾¡æ ¼ã‚’å–å¾—
                    price_tax_elem = await container.query_selector('.block-pickup-list-p--price.reference-price')
                    price_tax_text = await price_tax_elem.inner_text() if price_tax_elem else None
                    price_tax_included = None
                    if price_tax_text:
                        price_tax_cleaned = price_tax_text.replace('å‚è€ƒç¨è¾¼', '').replace('ï¿¥', '').replace(',', '').strip()
                        match = re.search(r'(\d+\.?\d*)', price_tax_cleaned)
                        if match:
                            price_tax_included = float(match.group(1))

                    # å•†å“IDã‚’å–å¾—ï¼ˆãƒªãƒ³ã‚¯ã‹ã‚‰ï¼‰
                    product_id = None
                    link = await container.query_selector('.block-pickup-list-p--goods-name a')
                    if link:
                        href = await link.get_attribute('href')
                        if href:
                            # /shop/g/g01087086/ ã®ã‚ˆã†ãªå½¢å¼ã‹ã‚‰IDã‚’æŠ½å‡º
                            id_match = re.search(r'/g/g(\d+)', href)
                            if id_match:
                                product_id = id_match.group(1)

                    if product_name:  # å•†å“åãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ 
                        product = {
                            "product_id": product_id,
                            "product_name": product_name.strip() if product_name else None,
                            "price": price,
                            "price_tax_included": price_tax_included if price_tax_included else price,
                            "image_url": img_src,
                            "in_stock": True,  # ãƒšãƒ¼ã‚¸ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ = åœ¨åº«ã‚ã‚Š
                            "is_available": True,
                            "raw_data": {
                                "product_id": product_id,
                                "price_text": price_text,
                                "price_tax_text": price_tax_text
                            }
                        }

                        products.append(product)

                except Exception as e:
                    logger.warning(f"å•†å“ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {e}")
                    continue

            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
            pagination_info = None
            try:
                # ãƒšãƒ¼ã‚¸ãƒ³ã‚°æƒ…å ±ã‚’æ¢ã™ï¼ˆä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
                pager = await self.page.query_selector('div.pager, div.pagination, ul.pagination')
                if pager:
                    page_links = await pager.query_selector_all('a, span')
                    total_pages = len([p for p in page_links if (await p.inner_text()).strip().isdigit()])

                    pagination_info = {
                        "totalItems": len(products) * total_pages,  # æ¨å®š
                        "currentPage": 1,  # URLã‹ã‚‰å–å¾—ãŒå¿…è¦
                        "totalPages": total_pages,
                        "itemsPerPage": len(products),
                        "source": "html:pagination"
                    }
            except Exception as e:
                logger.warning(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

            logger.info(f"âœ… å•†å“æŠ½å‡ºå®Œäº†: {len(products)}ä»¶")

            return products, pagination_info

        except Exception as e:
            logger.error(f"å•†å“æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return [], None


async def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import os
    from dotenv import load_dotenv
    load_dotenv()

    login_id = os.getenv("TOKYU_STORE_LOGIN_ID")
    password = os.getenv("TOKYU_STORE_PASSWORD")
    zip_code = os.getenv("DELIVERY_ZIP_CODE", "158-0094")

    if not login_id or not password:
        logger.error("âŒ ç’°å¢ƒå¤‰æ•° TOKYU_STORE_LOGIN_ID ã¨ TOKYU_STORE_PASSWORD ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return

    scraper = TokyuStoreScraperPlaywright()

    try:
        # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚ªãƒ•ã«ã—ã¦ãƒ–ãƒ©ã‚¦ã‚¶ã‚’è¡¨ç¤º
        await scraper.start(headless=False)

        # ãƒ­ã‚°ã‚¤ãƒ³
        success = await scraper.login(login_id, password)
        if not success:
            logger.error("âŒ ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—")
            return

        # é…é”ã‚¨ãƒªã‚¢é¸æŠ
        success = await scraper.select_delivery_area(zip_code)
        if not success:
            logger.warning("âš ï¸ é…é”ã‚¨ãƒªã‚¢é¸æŠã«å¤±æ•—ã—ã¾ã—ãŸãŒç¶šè¡Œã—ã¾ã™")

        # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ»HTMLä¿å­˜
        await scraper.page.screenshot(path="tokyu_store_top.png")
        html_content = await scraper.page.content()
        with open("tokyu_store_top.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        logger.info("ğŸ“¸ ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ»HTMLä¿å­˜å®Œäº†")

        # ãƒ†ã‚¹ãƒˆ: ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
        # å®Ÿéš›ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼URLã‚’ç¢ºèªã—ã¦ã‹ã‚‰è¨­å®š
        test_category_url = f"{scraper.base_url}/shop/default.aspx"
        logger.info(f"ğŸ“¦ ãƒ†ã‚¹ãƒˆãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹: {test_category_url}")

        products, pagination = await scraper.fetch_products_page(test_category_url, page=1)
        logger.info(f"âœ… å•†å“å–å¾—å®Œäº†: {len(products)}ä»¶")
        if pagination:
            logger.info(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±: {pagination}")

        logger.info("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†")

        # ç”»é¢ã‚’ç¢ºèªã§ãã‚‹ã‚ˆã†ã«5ç§’å¾…ã¤
        await scraper.page.wait_for_timeout(5000)

    finally:
        await scraper.close()


if __name__ == "__main__":
    import asyncio
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    asyncio.run(main())
