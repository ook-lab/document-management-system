"""
å…±é€šå•†å“å–ã‚Šè¾¼ã¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŸºç›¤ã‚¯ãƒ©ã‚¹
å…¨ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å…±æœ‰ã™ã‚‹å‡¦ç†ã‚’å®šç¾©
"""

from abc import ABC, abstractmethod
from datetime import date, datetime
from typing import Dict, List, Optional, Set
from uuid import UUID
import uuid

from A_common.database.client import DatabaseClient
from C_ai_common.llm_client.llm_client import LLMClient
from loguru import logger


class BaseProductIngestionPipeline(ABC):
    """å•†å“å–ã‚Šè¾¼ã¿ã®å…±é€šåŸºç›¤ã‚¯ãƒ©ã‚¹"""

    def __init__(self, organization_name: str, headless: bool = True):
        """
        åˆæœŸåŒ–

        Args:
            organization_name: çµ„ç¹”åï¼ˆæ±æ€¥ã‚¹ãƒˆã‚¢ã€æ¥½å¤©è¥¿å‹ã€ãƒ€ã‚¤ã‚¨ãƒ¼ï¼‰
            headless: ãƒ–ãƒ©ã‚¦ã‚¶ã®ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰
        """
        self.organization_name = organization_name
        self.headless = headless
        self.db = DatabaseClient(use_service_role=True)
        self.llm_client = LLMClient()
        self.scraper = None

    @abstractmethod
    async def start(self) -> bool:
        """
        ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®èµ·å‹•ã¨ãƒ­ã‚°ã‚¤ãƒ³
        å„ã‚¹ãƒˆã‚¢å›ºæœ‰ã®å®Ÿè£…ãŒå¿…è¦
        """
        pass

    @abstractmethod
    async def close(self):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        pass

    async def check_existing_products(self, jan_codes: List[str]) -> Set[str]:
        """
        æ—¢å­˜å•†å“ã®ãƒã‚§ãƒƒã‚¯ï¼ˆJANã‚³ãƒ¼ãƒ‰ã§é‡è¤‡æ’é™¤ï¼‰

        Args:
            jan_codes: ãƒã‚§ãƒƒã‚¯ã™ã‚‹JANã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ

        Returns:
            æ—¢å­˜ã®JANã‚³ãƒ¼ãƒ‰ã®ã‚»ãƒƒãƒˆ
        """
        if not jan_codes:
            return set()

        # ç©ºæ–‡å­—ãƒ»Noneã‚’é™¤å¤–
        valid_jan_codes = [code for code in jan_codes if code]

        if not valid_jan_codes:
            return set()

        result = self.db.client.table('80_rd_products').select(
            'jan_code'
        ).in_('jan_code', valid_jan_codes).execute()

        return {row['jan_code'] for row in result.data if row.get('jan_code')}

    def _prepare_product_data(
        self,
        product: Dict,
        category_name: Optional[str] = None,
        general_name: Optional[str] = None,
        category_id: Optional[UUID] = None,
        confidence: Optional[float] = None
    ) -> Dict:
        """
        å•†å“ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ã¨æº–å‚™

        Args:
            product: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‹ã‚‰å–å¾—ã—ãŸç”Ÿãƒ‡ãƒ¼ã‚¿
            category_name: ã‚«ãƒ†ã‚´ãƒªåï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å–å¾—å€¤ï¼‰
            general_name: ä¸€èˆ¬åè©ï¼ˆåˆ†é¡æ¸ˆã¿ã®å ´åˆï¼‰
            category_id: ã‚«ãƒ†ã‚´ãƒªIDï¼ˆåˆ†é¡æ¸ˆã¿ã®å ´åˆï¼‰
            confidence: åˆ†é¡ä¿¡é ¼åº¦

        Returns:
            ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŒ¿å…¥ç”¨ã®æ­£è¦åŒ–æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
        """
        today = date.today()

        # å•†å“åã®æ­£è¦åŒ–
        product_name = product.get("product_name", "")
        product_name_normalized = " ".join(product_name.replace("ã€€", " ").split())

        # ä¾¡æ ¼ã®ãƒ‘ãƒ¼ã‚¹
        price_text = product.get("price", "")
        try:
            current_price = float(price_text.replace(",", "").replace("å††", "").replace("Â¥", "").strip())
        except (ValueError, AttributeError):
            current_price = None

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = {
            "raw_data": product,
            "scraping_timestamp": datetime.now().isoformat()
        }

        # ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
        data = {
            # åŸºæœ¬æƒ…å ±
            "source_type": "online_shop",
            "workspace": "shopping",
            "doc_type": "online shop",
            "organization": self.organization_name,

            # å•†å“æƒ…å ±
            "product_name": product_name,
            "product_name_normalized": product_name_normalized,
            "jan_code": product.get("jan_code"),

            # ä¾¡æ ¼æƒ…å ±
            "current_price": current_price,
            "current_price_tax_included": current_price,
            "price_text": price_text,

            # åˆ†é¡æƒ…å ±
            "category": category_name,
            "general_name": general_name,
            "category_id": str(category_id) if category_id else None,
            "classification_confidence": confidence,
            "needs_approval": general_name is None,  # æœªåˆ†é¡ã®å ´åˆã¯æ‰¿èªå¾…ã¡

            # ãã®ä»–
            "manufacturer": product.get("manufacturer"),
            "image_url": product.get("image_url"),
            "in_stock": product.get("in_stock", True),
            "is_available": product.get("is_available", True),

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            "metadata": metadata,
            "document_date": today.isoformat(),
            "last_scraped_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }

        return data

    async def process_category_page(
        self,
        category_url: str,
        page: int = 1,
        category_name: Optional[str] = None
    ) -> Dict:
        """
        ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ã®å‡¦ç†ï¼ˆå…±é€šãƒ­ã‚¸ãƒƒã‚¯ï¼‰

        Args:
            category_url: ã‚«ãƒ†ã‚´ãƒªURL
            page: ãƒšãƒ¼ã‚¸ç•ªå·
            category_name: ã‚«ãƒ†ã‚´ãƒªå

        Returns:
            å‡¦ç†çµæœ
        """
        if not self.scraper:
            raise RuntimeError("Scraper not initialized. Call start() first.")

        logger.info(f"ãƒšãƒ¼ã‚¸ {page} ã‚’å‡¦ç†ä¸­...")

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å›ºæœ‰ã®å®Ÿè£…ã‚’å‘¼ã³å‡ºã—ï¼ˆã‚¿ãƒ—ãƒ«è¿”å´ï¼‰
        products, pagination_info = await self.scraper.fetch_products_page(category_url, page)

        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        if page == 1 and pagination_info:
            logger.info(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±: {pagination_info}")

        if not products:
            logger.warning("å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return {
                "success": True,
                "total_products": 0,
                "new_products": 0,
                "updated_products": 0,
                "pagination_info": pagination_info
            }

        # å•†å“ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ã¨ä¿å­˜
        jan_codes = [p.get("jan_code") for p in products if p.get("jan_code")]
        existing_jan_codes = await self.check_existing_products(jan_codes)

        insert_count = 0
        update_count = 0

        for product in products:
            # å•†å“ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆåˆ†é¡ã¯å¾Œã§å®Ÿæ–½ï¼‰
            product_data = self._prepare_product_data(product, category_name)
            jan_code = product.get("jan_code")

            try:
                if jan_code and jan_code in existing_jan_codes:
                    # æ—¢å­˜å•†å“ã‚’æ›´æ–°
                    self.db.client.table('80_rd_products').update(
                        product_data
                    ).eq('jan_code', jan_code).execute()
                    update_count += 1
                else:
                    # æ–°è¦å•†å“ã‚’æŒ¿å…¥
                    product_data["created_at"] = datetime.now().isoformat()
                    self.db.client.table('80_rd_products').insert(
                        product_data
                    ).execute()
                    insert_count += 1

            except Exception as e:
                logger.error(f"Failed to save product {product_name}: {e}")

        logger.info(f"âœ… å‡¦ç†å®Œäº†: åˆè¨ˆ{len(products)}ä»¶ï¼ˆæ–°è¦{insert_count}ä»¶ã€æ›´æ–°{update_count}ä»¶ï¼‰")

        return {
            "success": True,
            "total_products": len(products),
            "new_products": insert_count,
            "updated_products": update_count,
            "pagination_info": pagination_info
        }

    async def process_category_all_pages(
        self,
        category_url: str,
        category_name: Optional[str] = None,
        max_pages: int = 100
    ) -> Dict:
        """
        ã‚«ãƒ†ã‚´ãƒªã®å…¨ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†

        Args:
            category_url: ã‚«ãƒ†ã‚´ãƒªURL
            category_name: ã‚«ãƒ†ã‚´ãƒªå
            max_pages: æœ€å¤§ãƒšãƒ¼ã‚¸æ•°

        Returns:
            å‡¦ç†çµæœ
        """
        logger.info(f"ã‚«ãƒ†ã‚´ãƒªãƒ¼ '{category_name}' ã®å…¨ãƒšãƒ¼ã‚¸å‡¦ç†é–‹å§‹")

        page = 1
        total_products = 0
        total_new = 0
        total_updated = 0
        total_pages_from_pagination = None

        while page <= max_pages:
            result = await self.process_category_page(category_url, page, category_name)

            # åˆå›ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
            if page == 1 and result.get("pagination_info"):
                pagination = result["pagination_info"]
                total_pages_from_pagination = pagination.get("totalPages")
                if total_pages_from_pagination:
                    logger.info(f"ğŸ“„ æ¤œå‡ºã•ã‚ŒãŸç·ãƒšãƒ¼ã‚¸æ•°: {total_pages_from_pagination}")
                    # ç·ãƒšãƒ¼ã‚¸æ•°ãŒmax_pagesã‚ˆã‚Šå°‘ãªã„å ´åˆã€max_pagesã‚’æ›´æ–°
                    if total_pages_from_pagination < max_pages:
                        max_pages = total_pages_from_pagination
                        logger.info(f"âœ… å‡¦ç†ãƒšãƒ¼ã‚¸æ•°ã‚’ {total_pages_from_pagination} ã«åˆ¶é™")

            if not result.get("success") or result.get("total_products", 0) == 0:
                logger.info(f"ãƒšãƒ¼ã‚¸ {page} ã§å•†å“ãªã—ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼å‡¦ç†çµ‚äº†")
                break

            total_products += result.get("total_products", 0)
            total_new += result.get("new_products", 0)
            total_updated += result.get("updated_products", 0)

            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã«åŸºã¥ãçµ‚äº†åˆ¤å®š
            if total_pages_from_pagination and page >= total_pages_from_pagination:
                logger.info(f"âœ… å…¨{total_pages_from_pagination}ãƒšãƒ¼ã‚¸ã®å‡¦ç†å®Œäº†")
                break

            page += 1

        logger.info(f"âœ… ã‚«ãƒ†ã‚´ãƒªãƒ¼ '{category_name}' å®Œäº†")
        logger.info(f"   åˆè¨ˆ: {total_products}ä»¶ï¼ˆæ–°è¦{total_new}ä»¶ã€æ›´æ–°{total_updated}ä»¶ï¼‰")

        return {
            "success": True,
            "category_url": category_url,
            "category_name": category_name,
            "total_products": total_products,
            "new_products": total_new,
            "updated_products": total_updated,
            "pages_processed": page - 1,
            "total_pages": total_pages_from_pagination
        }
