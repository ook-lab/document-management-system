"""
æ—©ç¨²ç”°ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼OnlineãŠçŸ¥ã‚‰ã›å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

HTML â†’ PDFæŠ½å‡ºãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ â†’ Google Drive â†’ Supabase (pending)

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
1. HTMLãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆwindow.appPropsã®JSONï¼‰ã‹ã‚‰ãŠçŸ¥ã‚‰ã›ä¸€è¦§ã‚’å–å¾—
2. Supabaseã§æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æ–°ç€ãŠçŸ¥ã‚‰ã›ã‚’æŠ½å‡º
3. PDFãƒªãƒ³ã‚¯ã‹ã‚‰PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦Google Driveã«ä¿å­˜
4. Supabaseã«åŸºæœ¬æƒ…å ±ã‚’ç™»éŒ²ï¼ˆprocessing_status='pending'ï¼‰
5. åˆ¥é€” process_queued_documents.py ã§å‡¦ç†ï¼ˆPDFæŠ½å‡ºã€Stage A/B/Cï¼‰
"""
import os
import sys
import re
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from loguru import logger

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from A_common.connectors.google_drive import GoogleDriveConnector
from A_common.database.client import DatabaseClient
from B_ingestion.waseda_academy.browser_automation import WasedaAcademyBrowser


class WasedaNoticeIngestionPipeline:
    """æ—©ç¨²ç”°ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼OnlineãŠçŸ¥ã‚‰ã›å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(
        self,
        pdf_folder_id: Optional[str] = None,
        session_cookies: Optional[Dict[str, str]] = None
    ):
        """
        Args:
            pdf_folder_id: PDFä¿å­˜å…ˆã®Driveãƒ•ã‚©ãƒ«ãƒ€IDï¼ˆNoneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            session_cookies: æ—©ç¨²ç”°ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼Onlineã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒƒã‚­ãƒ¼ï¼ˆPDFå–å¾—ç”¨ï¼‰
        """
        self.pdf_folder_id = pdf_folder_id or os.getenv("WASEDA_PDF_FOLDER_ID")
        self.session_cookies = session_cookies or {}
        self.base_url = "https://online.waseda-ac.co.jp"

        # ã‚³ãƒã‚¯ã‚¿ã®åˆæœŸåŒ–
        self.drive = GoogleDriveConnector()
        self.db = DatabaseClient()

        logger.info(f"WasedaNoticeIngestionPipelineåˆæœŸåŒ–å®Œäº†")
        logger.info(f"  - PDF folder: {self.pdf_folder_id}")

    async def fetch_html_with_browser(self) -> Optional[str]:
        """
        ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã‚’ä½¿ç”¨ã—ã¦HTMLã‚’å–å¾—

        Returns:
            HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€å¤±æ•—æ™‚ã¯None
        """
        try:
            browser = WasedaAcademyBrowser(headless=True)
            html_content, _ = await browser.run_automated_session()
            return html_content
        except Exception as e:
            logger.error(f"ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None

    def extract_notice_data(self, html_content: str) -> List[Dict[str, Any]]:
        """
        HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãŠçŸ¥ã‚‰ã›ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹
        ãƒ‡ãƒ¼ã‚¿ã¯window.appPropsã¨ã„ã†JavaScriptå¤‰æ•°å†…ã®JSONã¨ã—ã¦åŸ‹ã‚è¾¼ã¾ã‚Œã¦ã„ã‚‹

        Args:
            html_content: HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„å…¨ä½“

        Returns:
            ãŠçŸ¥ã‚‰ã›ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        match = re.search(r'window\.appProps\s*=\s*(\{.*?\});', html_content, re.DOTALL)

        if not match:
            logger.warning("HTMLã‹ã‚‰window.appPropsãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return []

        json_string = match.group(1)

        try:
            app_props = json.loads(json_string)
            notices = app_props['page']['noticeList']['_0']['notices']
            logger.info(f"ãŠçŸ¥ã‚‰ã›ã‚’{len(notices)}ä»¶æŠ½å‡ºã—ã¾ã—ãŸ")
            return notices
        except (json.JSONDecodeError, KeyError) as e:
            logger.error(f"JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    async def check_existing_notices(self, notice_ids: List[str]) -> set:
        """
        Supabaseã§æ—¢å­˜ã®ãŠçŸ¥ã‚‰ã›IDã‚’ãƒã‚§ãƒƒã‚¯

        Args:
            notice_ids: ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãŠçŸ¥ã‚‰ã›IDã®ãƒªã‚¹ãƒˆ

        Returns:
            æ—¢ã«å­˜åœ¨ã™ã‚‹ãŠçŸ¥ã‚‰ã›IDã®ã‚»ãƒƒãƒˆ
        """
        try:
            # source_documents ãƒ†ãƒ¼ãƒ–ãƒ«ã§ source_type='waseda_academy_online' ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
            result = self.db.client.table('Rawdata_FILE_AND_MAIL').select('metadata').eq(
                'source_type', 'waseda_academy_online'
            ).execute()

            # metadata->notice_id ã‚’æŠ½å‡º
            existing_ids = set()
            if result.data:
                for doc in result.data:
                    metadata = doc.get('metadata', {})
                    if isinstance(metadata, dict):
                        notice_id = metadata.get('notice_id')
                        if notice_id:
                            existing_ids.add(notice_id)

            logger.info(f"æ—¢å­˜ã®ãŠçŸ¥ã‚‰ã›: {len(existing_ids)}ä»¶")
            return existing_ids

        except Exception as e:
            logger.error(f"Supabaseæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return set()

    async def download_pdfs_with_browser(
        self,
        pdf_info_list: List[Dict[str, str]]
    ) -> Dict[str, bytes]:
        """
        ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã‚’ä½¿ç”¨ã—ã¦è¤‡æ•°ã®PDFã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

        Args:
            pdf_info_list: [{'notice_id': 'xxx', 'pdf_url': '/notice/xxx/pdf/0', 'pdf_title': 'ã‚¿ã‚¤ãƒˆãƒ«'}, ...]

        Returns:
            {notice_id: pdf_data}ã®è¾æ›¸
        """
        try:
            browser = WasedaAcademyBrowser(headless=True)
            pdfs = await browser.download_pdfs_batch(pdf_info_list)
            return pdfs
        except Exception as e:
            logger.error(f"PDFãƒãƒƒãƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {}

    def save_pdf_to_drive(
        self,
        pdf_data: bytes,
        pdf_title: str,
        notice_id: str
    ) -> Optional[str]:
        """
        PDFã‚’Google Driveã«ä¿å­˜

        Args:
            pdf_data: PDFã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            pdf_title: PDFã®ã‚¿ã‚¤ãƒˆãƒ«
            notice_id: ãŠçŸ¥ã‚‰ã›ID

        Returns:
            Driveã®ãƒ•ã‚¡ã‚¤ãƒ«IDã€å¤±æ•—æ™‚ã¯None
        """
        # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        safe_title = "".join(c for c in pdf_title if c.isalnum() or c in (' ', '-', '_', 'ã€€')).strip()
        if not safe_title:
            safe_title = "waseda_notice"

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_name = f"{timestamp}_{safe_title}_{notice_id[:8]}.pdf"

        # Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        file_id = self.drive.upload_file(
            file_content=pdf_data,
            file_name=file_name,
            mime_type='application/pdf',
            folder_id=self.pdf_folder_id
        )

        if file_id:
            logger.info(f"PDFã‚’Driveã«ä¿å­˜: {file_name}")
        else:
            logger.error(f"PDFã®ä¿å­˜ã«å¤±æ•—: {file_name}")

        return file_id, file_name

    async def process_single_notice(
        self,
        notice: Dict[str, Any],
        pdf_data_dict: Dict[str, bytes]
    ) -> Dict[str, Any]:
        """
        1ä»¶ã®ãŠçŸ¥ã‚‰ã›ã‚’å‡¦ç†ï¼ˆPDFã®ã¿ï¼‰

        Args:
            notice: ãŠçŸ¥ã‚‰ã›ãƒ‡ãƒ¼ã‚¿
            pdf_data_dict: {notice_id: pdf_data}ã®è¾æ›¸ï¼ˆäº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ï¼‰

        Returns:
            å‡¦ç†çµæœã®è¾æ›¸
        """
        result = {
            'notice_id': notice.get('id'),
            'success': False,
            'pdf_file_ids': [],
            'document_ids': [],
            'error': None
        }

        try:
            notice_id = notice.get('id')
            title = notice.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')
            date = notice.get('date', '')
            message = notice.get('message', '')
            source = notice.get('source', {})
            category = notice.get('category', {})

            logger.info(f"ãŠçŸ¥ã‚‰ã›å‡¦ç†é–‹å§‹: {title}")

            # PDFãƒªãƒ³ã‚¯ãŒã‚ã‚‹ã‹ç¢ºèª
            pdfs = notice.get('pdfs', [])
            if not pdfs:
                logger.info(f"PDFãƒªãƒ³ã‚¯ãªã—ã€ã‚¹ã‚­ãƒƒãƒ—: {title}")
                result['success'] = True
                return result

            # å„PDFã‚’å‡¦ç†
            for pdf in pdfs:
                pdf_title = pdf.get('title', 'untitled')
                pdf_url = pdf.get('url', '')

                if not pdf_url:
                    logger.warning(f"PDFã®URLãŒç©º: {pdf_title}")
                    continue

                # 1. äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®PDFãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                pdf_data = pdf_data_dict.get(notice_id)
                if not pdf_data:
                    logger.warning(f"PDFãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {pdf_title}")
                    continue

                # 2. PDFã‚’Google Driveã«ä¿å­˜
                file_id, actual_file_name = self.save_pdf_to_drive(pdf_data, pdf_title, notice_id)
                if not file_id:
                    logger.error(f"PDFã®ä¿å­˜ã«å¤±æ•—: {pdf_title}")
                    continue

                result['pdf_file_ids'].append(file_id)

                # 3. æ—¥ä»˜ã‚’ datetime ã«å¤‰æ›ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: 2025.12.16ï¼‰
                sent_at = None
                if date:
                    try:
                        sent_at = datetime.strptime(date, '%Y.%m.%d').isoformat()
                    except ValueError:
                        logger.warning(f"æ—¥ä»˜ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {date}")

                # 4. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æº–å‚™
                # å®Œå…¨ãªPDF URLã‚’æ§‹ç¯‰
                if pdf_url.startswith('http'):
                    full_pdf_url = pdf_url
                elif pdf_url.startswith('/'):
                    full_pdf_url = f"{self.base_url}{pdf_url}"
                else:
                    full_pdf_url = f"{self.base_url}/{pdf_url}"

                metadata = {
                    'notice_id': notice_id,
                    'notice_title': title,
                    'notice_date': date,
                    'notice_source': source.get('label', 'ä¸æ˜'),
                    'notice_category': category.get('label', 'ãã®ä»–'),
                    'notice_message': message,
                    'pdf_url': full_pdf_url,
                    'pdf_title': pdf_title
                }

                # 5. Supabaseã«åŸºæœ¬æƒ…å ±ã®ã¿ä¿å­˜
                doc_data = {
                    'source_type': 'waseda_academy_online',
                    'source_id': file_id,
                    'source_url': f"https://drive.google.com/file/d/{file_id}/view",
                    'file_name': actual_file_name,  # æ‹¡å¼µå­ä»˜ãã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨
                    'file_type': 'pdf',
                    'doc_type': 'æ—©ç¨²ã‚¢ã‚«ã‚ªãƒ³ãƒ©ã‚¤ãƒ³',  # å›ºå®šå€¤
                    'workspace': 'waseda_academy',
                    'person': 'è‚²å“‰',  # æ‹…å½“è€…
                    'organization': 'æ—©ç¨²ç”°ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼',  # çµ„ç¹”
                    'attachment_text': '',  # ç©ºï¼ˆprocess_queued_documents.py ã§æŠ½å‡ºï¼‰
                    'summary': '',  # ç©ºï¼ˆprocess_queued_documents.py ã§ç”Ÿæˆï¼‰
                    'tags': [category.get('label', 'ãã®ä»–')],
                    'document_date': date,
                    'metadata': metadata,
                    'content_hash': hashlib.sha256(pdf_data).hexdigest(),
                    'processing_status': 'pending',  # PDFå‡¦ç†å¾…ã¡
                    'processing_stage': 'waseda_notice_downloaded',
                    # è¡¨ç¤ºç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                    'display_subject': title,
                    'display_sent_at': sent_at,
                    'display_sender': source.get('label', 'ä¸æ˜'),
                    'display_post_text': message
                }

                try:
                    # Supabaseã«ä¿å­˜
                    doc_result = await self.db.insert_document('Rawdata_FILE_AND_MAIL', doc_data)
                    if doc_result:
                        doc_id = doc_result.get('id')
                        result['document_ids'].append(doc_id)
                        logger.info(f"Supabaseä¿å­˜å®Œäº†ï¼ˆpendingçŠ¶æ…‹ï¼‰: {doc_id}")
                        logger.info(f"  â†’ process_queued_documents.py ã§å‡¦ç†ã—ã¦ãã ã•ã„")

                except Exception as db_error:
                    logger.error(f"Supabaseä¿å­˜ã‚¨ãƒ©ãƒ¼: {db_error}")
                    result['error'] = str(db_error)

            result['success'] = True
            logger.info(f"ãŠçŸ¥ã‚‰ã›å‡¦ç†å®Œäº†: {title} ({len(result['pdf_file_ids'])} PDFs)")

        except Exception as e:
            logger.error(f"ãŠçŸ¥ã‚‰ã›å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            result['error'] = str(e)

        return result



async def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    import sys

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
    pipeline = WasedaNoticeIngestionPipeline()

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ
    use_browser = "--browser" in sys.argv or "--auto" in sys.argv

    html_content = None

    if use_browser:
        # ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã§HTMLã‚’å–å¾—
        logger.info("ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ãƒ¢ãƒ¼ãƒ‰: ãƒ­ã‚°ã‚¤ãƒ³ â†’ HTMLå–å¾—")
        html_content = await pipeline.fetch_html_with_browser()

        if not html_content:
            logger.error("HTMLã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return

        # HTMLã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        temp_html_file = Path(__file__).parent.parent.parent / "waseda_notice_page.html"
        with open(temp_html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.info(f"å–å¾—ã—ãŸHTMLã‚’ä¿å­˜: {temp_html_file}")
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        html_file = Path(__file__).parent.parent.parent / "pasted_content.txt"

        if not html_file.exists():
            logger.error(f"HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {html_file}")
            logger.info("ãƒ’ãƒ³ãƒˆ: --browser ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã‚’ä½¿ç”¨ã§ãã¾ã™")
            logger.info("  python -m B_ingestion.waseda_academy.notice_ingestion --browser")
            return

        with open(html_file, 'r', encoding='utf-8') as f:
            html_content = f.read()

    # ãŠçŸ¥ã‚‰ã›ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    current_notices = pipeline.extract_notice_data(html_content)
    if not current_notices:
        logger.warning("ãŠçŸ¥ã‚‰ã›ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return

    # æ—¢å­˜ã®ãŠçŸ¥ã‚‰ã›IDã‚’Supabaseã‹ã‚‰å–å¾—
    notice_ids = [n.get('id') for n in current_notices if n.get('id')]
    existing_ids = await pipeline.check_existing_notices(notice_ids)

    # æ–°ç€ãŠçŸ¥ã‚‰ã›ã‚’æŠ½å‡º
    new_notices = [n for n in current_notices if n.get('id') not in existing_ids]

    logger.info(f"ç¾åœ¨ã®ãŠçŸ¥ã‚‰ã›: {len(current_notices)}ä»¶")
    logger.info(f"æ—¢å­˜ã®ãŠçŸ¥ã‚‰ã›: {len(existing_ids)}ä»¶")
    logger.info(f"æ–°ç€ãŠçŸ¥ã‚‰ã›: {len(new_notices)}ä»¶")

    if not new_notices:
        logger.info("æ–°ç€ãŠçŸ¥ã‚‰ã›ã¯ã‚ã‚Šã¾ã›ã‚“")
        return

    # æ–°ç€ãŠçŸ¥ã‚‰ã›ã‹ã‚‰PDFæƒ…å ±ã‚’åé›†
    pdf_info_list = []
    for notice in new_notices:
        notice_id = notice.get('id')
        pdfs = notice.get('pdfs', [])
        for pdf in pdfs:
            pdf_title = pdf.get('title', 'untitled')
            pdf_url = pdf.get('url', '')
            if pdf_url:
                pdf_info_list.append({
                    'notice_id': notice_id,
                    'pdf_url': pdf_url,
                    'pdf_title': pdf_title
                })

    logger.info(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®PDF: {len(pdf_info_list)}ä»¶")

    # PDFã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ï¼‰
    pdf_data_dict = {}
    if pdf_info_list:
        logger.info("ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã§PDFã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        pdf_data_dict = await pipeline.download_pdfs_with_browser(pdf_info_list)
        logger.info(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(pdf_data_dict)}/{len(pdf_info_list)}ä»¶")

    # æ–°ç€ãŠçŸ¥ã‚‰ã›ã‚’å‡¦ç†ï¼ˆPDFãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ï¼‰
    results = []
    for i, notice in enumerate(new_notices, 1):
        logger.info(f"[{i}/{len(new_notices)}] å‡¦ç†ä¸­...")
        result = await pipeline.process_single_notice(notice, pdf_data_dict)
        results.append(result)

    # ã‚µãƒãƒªãƒ¼
    success_count = sum(1 for r in results if r['success'])
    total_pdfs = sum(len(r['pdf_file_ids']) for r in results)
    total_docs = sum(len(r['document_ids']) for r in results)

    logger.info("=" * 60)
    logger.info("å‡¦ç†å®Œäº†")
    logger.info(f"  æˆåŠŸ: {success_count}/{len(results)}")
    logger.info(f"  å¤±æ•—: {len(results) - success_count}/{len(results)}")
    logger.info(f"  å‡¦ç†ã—ãŸPDF: {total_pdfs}ä»¶")
    logger.info(f"  ç™»éŒ²ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {total_docs}ä»¶ï¼ˆpendingçŠ¶æ…‹ï¼‰")
    logger.info("=" * 60)
    logger.info("")
    logger.info("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    logger.info("  python process_queued_documents.py --workspace=waseda_academy")
    logger.info("=" * 60)

    # çµæœã‚’è¡¨ç¤º
    print("\n" + "=" * 80)
    print("ğŸ“¢ æ—©ç¨²ç”°ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼ãŠçŸ¥ã‚‰ã›å–å¾—çµæœ")
    print("=" * 80)

    for result in results:
        print(f"\nNotice ID: {result['notice_id']}")
        print(f"  Success: {result['success']}")
        print(f"  PDFs: {len(result['pdf_file_ids'])}")
        for file_id in result['pdf_file_ids']:
            print(f"    - https://drive.google.com/file/d/{file_id}/view")
        print(f"  Documents: {len(result['document_ids'])} (pending)")
        if result['error']:
            print(f"  âŒ Error: {result['error']}")

    print("\n" + "=" * 80)
    print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  python process_queued_documents.py --workspace=waseda_academy")
    print("=" * 80)


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
