"""
æ¥½å¤©è¥¿å‹ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (Playwrightç‰ˆ)

Playwrightã‚’ä½¿ç”¨ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ä¿æŒã—ãŸã¾ã¾å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ã€‚
"""

import json
import re
import time
import random
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from playwright.async_api import async_playwright, Browser, Page, BrowserContext

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger(__name__)


class RakutenSeiyuScraperPlaywright:
    """æ¥½å¤©è¥¿å‹ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¹ (Playwrightç‰ˆ)"""

    def __init__(self):
        self.base_url = "https://netsuper.rakuten.co.jp/seiyu"
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None

    async def __aenter__(self):
        """async withæ§‹æ–‡ã§ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé–‹å§‹"""
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """async withæ§‹æ–‡ã§ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµ‚äº†"""
        await self.close()

    async def start(self, headless: bool = True):
        """
        ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•

        Args:
            headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•ã™ã‚‹ã‹
        """
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(headless=headless)
            self.context = await self.browser.new_context(
                viewport={'width': 1280, 'height': 720},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            self.page = await self.context.new_page()
            logger.info("âœ… Playwrightãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•å®Œäº†")

        except Exception as e:
            logger.error(f"ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            raise

    async def close(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã‚‹"""
        try:
            if self.page:
                await self.page.close()
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
            logger.info("âœ… ãƒ–ãƒ©ã‚¦ã‚¶çµ‚äº†")

        except Exception as e:
            logger.error(f"ãƒ–ãƒ©ã‚¦ã‚¶çµ‚äº†ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

    async def _is_logged_in(self) -> bool:
        """ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            page_content = await self.page.content()
            current_url = self.page.url

            # URLãŒæ¥½å¤©è¥¿å‹ã®ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã§ã‚ã‚Œã°ã€ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸã¨ã¿ãªã™
            if "netsuper.rakuten.co.jp/seiyu" in current_url and "id.rakuten.co.jp" not in current_url:
                # ã•ã‚‰ã«ã€ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ç‰¹æœ‰ã®è¦ç´ ãŒãªã„ã“ã¨ã‚’ç¢ºèª
                if "ãƒ­ã‚°ã‚¤ãƒ³" not in page_content or "shopping-cart" in page_content or "minicart" in page_content:
                    return True

            # å¾“æ¥ã®æ–¹æ³•ã‚‚ãƒã‚§ãƒƒã‚¯
            return "ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ" in page_content or "ãƒã‚¤ãƒšãƒ¼ã‚¸" in page_content
        except:
            return False

    async def login(self, rakuten_id: str, password: str) -> bool:
        """
        æ¥½å¤©è¥¿å‹ã«ãƒ­ã‚°ã‚¤ãƒ³ (auth_manager.pyã‹ã‚‰ç§»æ¤)

        Args:
            rakuten_id: æ¥½å¤©ID
            password: ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰

        Returns:
            æˆåŠŸã—ãŸã‚‰True
        """
        import asyncio

        try:
            logger.info("ğŸ” æ¥½å¤©è¥¿å‹ã«ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
            await self.page.goto(self.base_url, wait_until="domcontentloaded", timeout=60000)

            # ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ã‚’æ¢ã—ã¦ã‚¯ãƒªãƒƒã‚¯
            login_selectors = [
                'a:has-text("ãƒ­ã‚°ã‚¤ãƒ³")',
                'button:has-text("ãƒ­ã‚°ã‚¤ãƒ³")',
                '[data-test="login-button"]',
                '.login-button',
                '#login-link'
            ]

            login_clicked = False
            for selector in login_selectors:
                try:
                    login_button = await self.page.wait_for_selector(
                        selector,
                        timeout=5000,
                        state="visible"
                    )
                    if login_button:
                        await login_button.click()
                        login_clicked = True
                        logger.info(f"ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯: {selector}")
                        break
                except Exception:
                    continue

            if not login_clicked:
                if await self._is_logged_in():
                    logger.info("âœ… æ—¢ã«ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã§ã™")
                    return True
                else:
                    logger.error("ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã«é·ç§»ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    return False

            await self.page.wait_for_load_state("domcontentloaded")

            # ã‚¹ãƒ†ãƒƒãƒ—1: æ¥½å¤©IDå…¥åŠ›
            logger.info("ã‚¹ãƒ†ãƒƒãƒ—1: æ¥½å¤©IDã‚’å…¥åŠ›ä¸­...")
            username_selectors = [
                'input[name="username"]',
                '#user_id',
                'input[autocomplete="username"]',
                'input[name="u"]',
                '#loginInner_u',
                'input[type="email"]',
                'input[placeholder*="æ¥½å¤©ä¼šå“¡ID"]',
                'input[placeholder*="ãƒ¦ãƒ¼ã‚¶ID"]',
                'input[placeholder*="ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹"]'
            ]

            username_filled = False
            for selector in username_selectors:
                try:
                    username_input = await self.page.wait_for_selector(
                        selector,
                        timeout=5000,
                        state="visible"
                    )
                    if username_input:
                        await username_input.click()
                        await username_input.fill(rakuten_id)
                        logger.info(f"âœ… æ¥½å¤©IDå…¥åŠ›å®Œäº†: {selector}")
                        username_filled = True
                        break
                except Exception:
                    continue

            if not username_filled:
                logger.error("æ¥½å¤©IDå…¥åŠ›æ¬„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return False

            # ã€Œæ¬¡ã¸ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
            next_button_selectors = [
                '#cta001',
                'div[role="button"]:has-text("æ¬¡ã¸")',
                'button:has-text("æ¬¡ã¸")',
                '[id*="cta"]',
                'button[type="submit"]',
                'input[type="submit"]',
                '.submit-button'
            ]

            next_clicked = False
            for selector in next_button_selectors:
                try:
                    next_button = await self.page.wait_for_selector(
                        selector,
                        timeout=5000,
                        state="visible"
                    )
                    if next_button:
                        await next_button.click()
                        logger.info(f"ã€Œæ¬¡ã¸ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯: {selector}")
                        next_clicked = True
                        await asyncio.sleep(0.5)
                        await self.page.wait_for_load_state("domcontentloaded")
                        break
                except Exception:
                    continue

            # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
            logger.info("ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ä¸­...")
            password_selectors = [
                '#password_current',
                'input[name="password"]',
                'input[autocomplete="current-password"]'
            ]

            password_filled = False
            for selector in password_selectors:
                try:
                    password_input = await self.page.wait_for_selector(
                        selector,
                        timeout=3000,
                        state="visible"
                    )
                    if password_input:
                        await password_input.click()
                        await password_input.fill(password)
                        logger.info(f"âœ… ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›å®Œäº†: {selector}")
                        password_filled = True
                        break
                except Exception:
                    continue

            if not password_filled:
                logger.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›æ¬„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return False

            # ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
            await asyncio.sleep(0.5)
            login_button_selectors = [
                '#cta011',
                '#cta001',
                'div[role="button"]:has-text("æ¬¡ã¸")',
                '[id*="cta"]'
            ]

            login_clicked = False
            for selector in login_button_selectors:
                try:
                    login_button = await self.page.wait_for_selector(
                        selector,
                        timeout=5000,
                        state="visible"
                    )
                    if login_button:
                        await login_button.click()
                        logger.info(f"ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯: {selector}")
                        login_clicked = True
                        break
                except Exception:
                    continue

            if not login_clicked:
                logger.error("ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return False

            # ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ã‚’å¾…æ©Ÿ
            await asyncio.sleep(2)
            await self.page.wait_for_load_state("domcontentloaded")

            # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šãƒ­ã‚°ã‚¤ãƒ³å¾Œã®ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜
            await self.page.screenshot(path="after_login.png")
            logger.info("ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜: after_login.png")

            current_url = self.page.url
            logger.info(f"ãƒ­ã‚°ã‚¤ãƒ³å¾Œã®URL: {current_url}")

            page_content = await self.page.content()
            with open("after_login.html", "w", encoding="utf-8") as f:
                f.write(page_content)
            logger.info("HTMLä¿å­˜: after_login.html")

            # ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸç¢ºèª
            is_logged_in = await self._is_logged_in()

            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
            has_logout = "ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ" in page_content
            has_mypage = "ãƒã‚¤ãƒšãƒ¼ã‚¸" in page_content
            logger.info(f"ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèª: ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ={has_logout}, ãƒã‚¤ãƒšãƒ¼ã‚¸={has_mypage}")

            if is_logged_in:
                logger.info("âœ… ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ")
                return True
            else:
                logger.error("âŒ ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—")
                return False

        except Exception as e:
            logger.error(f"ãƒ­ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False

    async def fetch_products_page(
        self,
        category_url: str,
        page: int = 1
    ) -> tuple[List[Dict[str, Any]], Optional[dict]]:
        """
        ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸ã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Args:
            category_url: ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®å®Œå…¨URLï¼ˆä¾‹: "https://netsuper.rakuten.co.jp/seiyu/search/110001/"ï¼‰
            page: ãƒšãƒ¼ã‚¸ç•ªå·

        Returns:
            (å•†å“ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ, ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±)ã€å¤±æ•—æ™‚ã¯(ç©ºãƒªã‚¹ãƒˆ, None)
        """
        try:
            # URLã«ãƒšãƒ¼ã‚¸ç•ªå·ã‚’è¿½åŠ ï¼ˆæ—¢ã«ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯&ã§è¿½åŠ ï¼‰
            if '?' in category_url:
                url = f"{category_url}&page={page}"
            else:
                url = f"{category_url}?page={page}"
            logger.info(f"å•†å“ãƒšãƒ¼ã‚¸å–å¾—ä¸­: {url}")

            await self.page.goto(url, wait_until="networkidle", timeout=30000)
            await self.page.wait_for_timeout(3000)

            # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šå•†å“ãƒšãƒ¼ã‚¸ã®HTMLã‚’ä¿å­˜
            if page == 1:
                await self.page.screenshot(path="product_page.png")
                logger.info("å•†å“ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜: product_page.png")

                html_debug = await self.page.content()
                with open("product_page.html", "w", encoding="utf-8") as f:
                    f.write(html_debug)
                logger.info("å•†å“ãƒšãƒ¼ã‚¸ã®HTMLä¿å­˜: product_page.html")

            logger.info(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: OK (Playwright)")

            # å•†å“ãƒ‡ãƒ¼ã‚¿ã¨ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’æŠ½å‡º
            products, pagination_info = await self.extract_products_from_page()

            # ã‚¢ã‚¯ã‚»ã‚¹é–“éš”åˆ¶å¾¡
            await self.page.wait_for_timeout(random.randint(1000, 2000))

            return products, pagination_info

        except Exception as e:
            logger.error(f"å•†å“ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return [], None

    async def extract_products_from_page(self) -> tuple[List[Dict[str, Any]], Optional[dict]]:
        """
        Playwrightãƒšãƒ¼ã‚¸ã‹ã‚‰HTMLã‚’è§£æã—ã¦å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º

        Returns:
            (å•†å“ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ, ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±)
        """
        try:
            # HTMLã‹ã‚‰ç›´æ¥å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            extraction_result = await self.page.evaluate("""
                () => {
                    const result = {
                        products: [],
                        pagination: null
                    };

                    // ç·å•†å“æ•°ã‚’å–å¾—ï¼ˆtotal-hitå±æ€§ã‹ã‚‰ï¼‰
                    const itemListContainer = document.querySelector('[id="item-list"]');
                    if (itemListContainer) {
                        const totalHit = itemListContainer.getAttribute('total-hit');
                        if (totalHit) {
                            const totalItems = parseInt(totalHit);
                            // 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Š48ä»¶ã¨ã—ã¦è¨ˆç®—
                            const itemsPerPage = 48;
                            const totalPages = Math.ceil(totalItems / itemsPerPage);

                            result.pagination = {
                                totalItems: totalItems,
                                itemsPerPage: itemsPerPage,
                                totalPages: totalPages,
                                source: 'html:total-hit'
                            };
                        }
                    }

                    // å•†å“è¦ç´ ã‚’å–å¾—ï¼ˆdata-ratidå±æ€§ã‚’æŒã¤.product-itemè¦ç´ ï¼‰
                    const productElements = document.querySelectorAll('.product-item[data-ratid][data-ratunit="item"]');

                    productElements.forEach((element) => {
                        try {
                            // å•†å“IDï¼ˆdata-ratidï¼‰
                            const itemId = element.getAttribute('data-ratid');

                            // ãƒ¡ãƒ¼ã‚«ãƒ¼/ç”£åœ°
                            const makerElement = element.querySelector('.product-item-info-maker');
                            const manufacturer = makerElement ? makerElement.textContent.trim() : null;

                            // å•†å“å
                            const nameElement = element.querySelector('.product-item-info-name');
                            const productName = nameElement ? nameElement.textContent.trim() : null;

                            // æ•°é‡
                            const amountElement = element.querySelector('.product-item-info-amount');
                            const amount = amountElement ? amountElement.textContent.trim() : null;

                            // ä¾¡æ ¼ï¼ˆç¨æŠœï¼‰
                            const priceElement = element.querySelector('.product-item-info-price');
                            let price = null;
                            if (priceElement) {
                                const priceText = priceElement.childNodes[0].textContent.trim();
                                price = parseInt(priceText.replace(/[^0-9]/g, ''));
                            }

                            // ä¾¡æ ¼ï¼ˆç¨è¾¼ï¼‰
                            const taxPriceElement = element.querySelector('.product-item-info-tax');
                            let priceTaxIncluded = null;
                            if (taxPriceElement) {
                                const taxPriceText = taxPriceElement.textContent.trim();
                                const match = taxPriceText.match(/(\d+)å††/);
                                if (match) {
                                    priceTaxIncluded = parseInt(match[1]);
                                }
                            }

                            // ç”»åƒURL
                            const imgElement = element.querySelector('.product-item-img img');
                            let imageUrl = null;
                            if (imgElement) {
                                imageUrl = imgElement.getAttribute('data-src') || imgElement.getAttribute('src');
                                if (imageUrl && imageUrl.startsWith('//')) {
                                    imageUrl = 'https:' + imageUrl;
                                }
                            }

                            // å•†å“ãƒšãƒ¼ã‚¸URL
                            const linkElement = element.querySelector('a[href*="/item/"]');
                            const productUrl = linkElement ? linkElement.getAttribute('href') : null;

                            const product = {
                                itemId: itemId,
                                productName: productName,
                                manufacturer: manufacturer,
                                amount: amount,
                                price: price,
                                priceTaxIncluded: priceTaxIncluded,
                                imageUrl: imageUrl,
                                productUrl: productUrl
                            };

                            result.products.push(product);
                        } catch (err) {
                            console.error('Error parsing product element:', err);
                        }
                    });

                    return result;
                }
            """)

            products_data = extraction_result.get('products', [])
            pagination_info = extraction_result.get('pagination')

            logger.info(f"âœ… HTMLè§£æçµæœ:")
            logger.info(f"  - å•†å“æ•°: {len(products_data)}ä»¶")

            if pagination_info:
                logger.info(f"  - ç·å•†å“æ•°: {pagination_info.get('totalItems')}ä»¶")
                logger.info(f"  - ç·ãƒšãƒ¼ã‚¸æ•°: {pagination_info.get('totalPages')}ãƒšãƒ¼ã‚¸")
                logger.info(f"  - 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Š: {pagination_info.get('itemsPerPage')}ä»¶")
            else:
                logger.warning("  - âš ï¸ ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

            if not products_data:
                logger.warning("å•†å“ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return [], pagination_info

            # å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
            products = []
            for item in products_data:
                try:
                    # å•†å“URLã‚’å®Œå…¨ãªURLã«å¤‰æ›
                    product_url = item.get('productUrl')
                    if product_url:
                        if product_url.startswith('http'):
                            pass  # æ—¢ã«å®Œå…¨ãªURL
                        elif product_url.startswith('/'):
                            product_url = f"https://netsuper.rakuten.co.jp{product_url}"
                        else:
                            product_url = f"https://netsuper.rakuten.co.jp/{product_url}"

                    product = {
                        "product_name": f"{item.get('manufacturer', '')} {item.get('productName', '')} {item.get('amount', '')}".strip(),
                        "price": item.get('price'),
                        "price_tax_included": item.get('priceTaxIncluded'),
                        "jan_code": item.get('itemId'),  # å•†å“IDã‚’JANã‚³ãƒ¼ãƒ‰ã¨ã—ã¦ä½¿ç”¨
                        "image_url": item.get('imageUrl'),
                        "url": product_url,  # URLã‚’è¿½åŠ 
                        "manufacturer": item.get('manufacturer'),
                        "category": None,  # HTMLã‹ã‚‰ã¯å–å¾—ã§ããªã„ãŸã‚
                        "in_stock": True,  # ãƒªã‚¹ãƒˆã«ã‚ã‚‹ã‚‚ã®ã¯åœ¨åº«ã‚ã‚Šã¨ä»®å®š
                        "raw_data": item
                    }
                    products.append(product)
                except Exception as e:
                    logger.error(f"å•†å“ãƒ‡ãƒ¼ã‚¿æ•´å½¢ã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            logger.info(f"âœ… {len(products)}ä»¶ã®å•†å“ã‚’æŠ½å‡ºå®Œäº†")
            return products, pagination_info

        except Exception as e:
            logger.error(f"å•†å“æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return [], None

    def _find_products_in_nuxt(self, nuxt_data: dict) -> Optional[list]:
        """
        NUXT ãƒ‡ãƒ¼ã‚¿å†…ã‹ã‚‰å•†å“ãƒªã‚¹ãƒˆã‚’æ¢ã™

        Args:
            nuxt_data: window.__NUXT__ ã®JSONãƒ‡ãƒ¼ã‚¿

        Returns:
            å•†å“ãƒªã‚¹ãƒˆã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯None
        """
        possible_paths = [
            ["data", 0, "itemList"],
            ["data", 0, "products"],
            ["data", "items"],
            ["data", "products"],
            ["state", "items"],
            ["state", "products"],
        ]

        for path in possible_paths:
            try:
                current = nuxt_data
                for key in path:
                    current = current[key]

                if isinstance(current, list) and len(current) > 0:
                    logger.info(f"å•†å“ãƒ‡ãƒ¼ã‚¿ç™ºè¦‹: {'.'.join(map(str, path))}")
                    return current
            except (KeyError, IndexError, TypeError):
                continue

        logger.debug(f"NUXT ãƒ‡ãƒ¼ã‚¿æ§‹é€ : {json.dumps(nuxt_data, indent=2, ensure_ascii=False)[:1000]}")
        return None

    def _parse_product_item(self, item: dict) -> Optional[Dict[str, Any]]:
        """
        å•†å“ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›

        Args:
            item: å•†å“ãƒ‡ãƒ¼ã‚¿ï¼ˆè¾æ›¸ï¼‰

        Returns:
            æ•´å½¢ã•ã‚ŒãŸå•†å“ãƒ‡ãƒ¼ã‚¿
        """
        try:
            product_name = item.get("name") or item.get("productName") or item.get("title")
            if not product_name:
                logger.warning("å•†å“åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None

            # ä¾¡æ ¼
            price = None
            price_data = item.get("price") or item.get("priceInfo") or {}
            if isinstance(price_data, dict):
                price = price_data.get("value") or price_data.get("price")
            elif isinstance(price_data, (int, float)):
                price = price_data

            # JANã‚³ãƒ¼ãƒ‰
            jan_code = item.get("janCode") or item.get("jan") or item.get("barcode")

            # ç”»åƒURL
            image_url = item.get("imageUrl") or item.get("image") or item.get("thumbnailUrl")
            if image_url:
                image_url = self._fix_image_url(image_url)

            # ãƒ¡ãƒ¼ã‚«ãƒ¼
            manufacturer = item.get("manufacturer") or item.get("brand") or item.get("maker")

            # ã‚«ãƒ†ã‚´ãƒªãƒ¼
            category = item.get("category") or item.get("categoryName")

            # åœ¨åº«çŠ¶æ³
            in_stock = item.get("inStock", True)
            is_available = item.get("isAvailable", True)

            # å•†å“URLï¼ˆè¤‡æ•°ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’ãƒã‚§ãƒƒã‚¯ï¼‰
            product_url = item.get("url") or item.get("productUrl") or item.get("itemUrl") or item.get("link")

            # URLãŒãªã„å ´åˆã€å•†å“IDã‹ã‚‰æ§‹ç¯‰ã‚’è©¦ã¿ã‚‹
            if not product_url:
                item_id = item.get("itemId") or item.get("id") or item.get("productId")
                if item_id:
                    product_url = f"https://netsuper.rakuten.co.jp/seiyu/product/{item_id}/"

            product = {
                "product_name": product_name,
                "price": price,
                "jan_code": jan_code,
                "image_url": image_url,
                "manufacturer": manufacturer,
                "category": category,
                "in_stock": in_stock,
                "is_available": is_available,
                "url": product_url,  # URLã‚’è¿½åŠ 
                "raw_data": item
            }

            return product

        except Exception as e:
            logger.error(f"å•†å“ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None

    def _fix_image_url(self, url: str) -> str:
        """
        ç”»åƒURLã‚’ä¿®æ­£ï¼ˆãƒ—ãƒ­ãƒˆã‚³ãƒ«è£œå®Œãªã©ï¼‰

        Args:
            url: å…ƒã®URL

        Returns:
            ä¿®æ­£ã•ã‚ŒãŸURL
        """
        if url.startswith("//"):
            return f"https:{url}"
        elif not url.startswith("http"):
            return f"{self.base_url}{url}"
        return url

    async def scrape_category_all_pages(
        self,
        category_url: str,
        category_name: str,
        max_pages: int = 100
    ) -> List[Dict[str, Any]]:
        """
        ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®å…¨ãƒšãƒ¼ã‚¸ã‹ã‚‰å•†å“ã‚’å–å¾—

        Args:
            category_url: ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®å®Œå…¨URL
            category_name: ã‚«ãƒ†ã‚´ãƒªãƒ¼å
            max_pages: æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ï¼ˆå®‰å…¨è£…ç½®ï¼‰

        Returns:
            å…¨å•†å“ã®ãƒªã‚¹ãƒˆ
        """
        all_products = []
        page = 1
        total_pages = None

        logger.info(f"ã‚«ãƒ†ã‚´ãƒªãƒ¼ '{category_name}' ã®å…¨ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")

        while page <= max_pages:
            logger.info(f"ãƒšãƒ¼ã‚¸ {page} ã‚’å‡¦ç†ä¸­..." + (f"ï¼ˆå…¨{total_pages}ãƒšãƒ¼ã‚¸ï¼‰" if total_pages else ""))

            products, pagination_info = await self.fetch_products_page(category_url, page)

            # åˆå›ã®ã¿ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
            if page == 1 and pagination_info:
                total_pages = pagination_info.get('totalPages')
                total_items = pagination_info.get('totalItems')
                items_per_page = pagination_info.get('itemsPerPage')

                if total_pages:
                    logger.info(f"ğŸ“„ ç·ãƒšãƒ¼ã‚¸æ•°: {total_pages}ãƒšãƒ¼ã‚¸")
                if total_items:
                    logger.info(f"ğŸ“¦ ç·å•†å“æ•°: {total_items}ä»¶")
                if items_per_page:
                    logger.info(f"ğŸ“ 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Š: {items_per_page}ä»¶")

                # ç·ãƒšãƒ¼ã‚¸æ•°ãŒã‚ã‹ã£ã¦ã„ã‚‹å ´åˆã¯max_pagesã‚’æ›´æ–°
                if total_pages and total_pages < max_pages:
                    max_pages = total_pages
                    logger.info(f"âœ… æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ã‚’ {total_pages} ã«è¨­å®š")

            if not products:
                logger.info(f"ãƒšãƒ¼ã‚¸ {page} ã«å•†å“ãªã—ã€çµ‚äº†ã—ã¾ã™")
                break

            all_products.extend(products)
            logger.info(f"ãƒšãƒ¼ã‚¸ {page}: {len(products)}ä»¶å–å¾—ï¼ˆç´¯è¨ˆ: {len(all_products)}ä»¶ï¼‰")

            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‹ã‚‰ç·ãƒšãƒ¼ã‚¸æ•°ãŒã‚ã‹ã£ã¦ã„ã‚‹å ´åˆã€ãã‚Œã‚’è¶…ãˆãŸã‚‰çµ‚äº†
            if total_pages and page >= total_pages:
                logger.info(f"âœ… å…¨{total_pages}ãƒšãƒ¼ã‚¸ã®å‡¦ç†å®Œäº†")
                break

            page += 1

            # ãƒšãƒ¼ã‚¸é–“å¾…æ©Ÿ
            await self.page.wait_for_timeout(random.randint(1000, 2000))

        logger.info(f"âœ… ã‚«ãƒ†ã‚´ãƒªãƒ¼ '{category_name}' å®Œäº†: åˆè¨ˆ {len(all_products)}ä»¶")
        return all_products


async def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import os
    from dotenv import load_dotenv
    load_dotenv()

    rakuten_id = os.getenv("RAKUTEN_ID")
    password = os.getenv("RAKUTEN_PASSWORD")

    if not rakuten_id or not password:
        logger.error("âŒ ç’°å¢ƒå¤‰æ•° RAKUTEN_ID ã¨ RAKUTEN_PASSWORD ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return

    async with RakutenSeiyuScraperPlaywright() as scraper:
        # ãƒ­ã‚°ã‚¤ãƒ³
        success = await scraper.login(rakuten_id, password)
        if not success:
            logger.error("âŒ ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—")
            return

        # ãƒ†ã‚¹ãƒˆ: 1ãƒšãƒ¼ã‚¸å–å¾—
        html = await scraper.fetch_products_page(category_id="110001", page=1)
        if html:
            products = scraper.extract_products_from_html(html)
            logger.info(f"å–å¾—ã—ãŸå•†å“æ•°: {len(products)}")

            if products:
                logger.info(f"æœ€åˆã®å•†å“: {json.dumps(products[0], indent=2, ensure_ascii=False)}")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
