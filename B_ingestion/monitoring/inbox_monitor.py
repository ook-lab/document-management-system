"""
InBoxè‡ªå‹•ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (v1.0)

ç›®çš„: Google Driveã®ç‰¹å®šã®InBoxãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒãƒ¼ãƒªãƒ³ã‚°ã—ã€
     æ–°è¦è¿½åŠ ã•ã‚ŒãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã€æ—¢å­˜ã®2æ®µéšAIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«æ¸¡ã™ã€‚

è¨­è¨ˆ: AUTO_INBOX_COMPLETE_v3.0.md ã® Phase 2 (Track 3) ã«æº–æ‹ 
     ã€Œå—ä¿¡ç®±è‡ªå‹•ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã€ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å®šç¾©ã«åŸºã¥ã

å®Ÿè¡Œé »åº¦: GitHub Actions (æ¯æ™‚å®Ÿè¡Œ)
"""

import os
import sys
import asyncio
import tempfile
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from loguru import logger
import traceback

# ãƒ‘ã‚¹è¨­å®š
sys.path.insert(0, str(Path(__file__).parent.parent))

from A_common.connectors.google_drive import GoogleDriveConnector
from A_common.database.client import DatabaseClient
from B_ingestion.two_stage_ingestion import TwoStageIngestionPipeline
from A_common.processors.pdf import calculate_content_hash

# ãƒ­ã‚°è¨­å®š
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)

logger.add(log_dir / f'inbox_monitor_{datetime.now():%Y%m%d_%H%M%S}.log', rotation="10 MB", level="INFO")
logger.add(sys.stdout, level="INFO")


class InBoxMonitor:
    """InBoxè‡ªå‹•ç›£è¦–ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.drive = GoogleDriveConnector()
        self.db = DatabaseClient()
        self.pipeline = TwoStageIngestionPipeline()

        # InBoxãƒ•ã‚©ãƒ«ãƒ€IDã¨Archiveãƒ•ã‚©ãƒ«ãƒ€IDã‚’å–å¾—
        self.inbox_folder_id = self.drive.get_inbox_folder_id()
        self.archive_folder_id = self.drive.get_archive_folder_id()

        if not self.inbox_folder_id:
            raise ValueError("INBOX_FOLDER_ID ãŒç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        logger.info(f"InBoxç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        logger.info(f"InBox Folder ID: {self.inbox_folder_id}")
        logger.info(f"Archive Folder ID: {self.archive_folder_id if self.archive_folder_id else 'Not Set'}")

    def get_processed_file_ids(self) -> List[str]:
        """
        æ—¢ã«å‡¦ç†æ¸ˆã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—

        Returns:
            å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«IDã®ãƒªã‚¹ãƒˆ
        """
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’å–å¾—ä¸­...")
        processed_ids = self.db.get_processed_file_ids()
        logger.info(f"âœ… {len(processed_ids)} ä»¶ã®å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’å–å¾—")
        return processed_ids

    def scan_inbox_for_new_files(self, processed_file_ids: List[str]) -> List[Dict[str, Any]]:
        """
        InBoxãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³

        Args:
            processed_file_ids: æ—¢ã«å‡¦ç†æ¸ˆã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«IDãƒªã‚¹ãƒˆ

        Returns:
            æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
        """
        logger.info(f"ğŸ“ InBoxãƒ•ã‚©ãƒ«ãƒ€ [{self.inbox_folder_id}] ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­...")

        new_files = self.drive.list_inbox_files(
            folder_id=self.inbox_folder_id,
            processed_file_ids=processed_file_ids
        )

        if new_files:
            logger.info(f"ğŸ†• {len(new_files)} ä»¶ã®æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º:")
            for file in new_files:
                logger.info(f"  - {file['name']} (ID: {file['id'][:8]}...)")
        else:
            logger.info("æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

        return new_files

    def check_duplicate_by_hash(self, file_meta: Dict[str, Any]) -> Optional[str]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã®content_hashã‚’è¨ˆç®—ã—ã€é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯

        Args:
            file_meta: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

        Returns:
            content_hash: é‡è¤‡ã—ã¦ã„ãªã„å ´åˆã¯ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¿”ã™
            None: é‡è¤‡ã—ã¦ã„ã‚‹å ´åˆã¯Noneã‚’è¿”ã™
        """
        file_id = file_meta['id']
        file_name = file_meta['name']

        try:
            # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            temp_dir = tempfile.gettempdir()
            logger.info(f"ğŸ” é‡è¤‡ãƒã‚§ãƒƒã‚¯: {file_name} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")

            file_path = self.drive.download_file(file_id, file_name, temp_dir)

            # content_hashã‚’è¨ˆç®—
            content_hash = calculate_content_hash(file_path)
            logger.info(f"   è¨ˆç®—ã•ã‚ŒãŸãƒãƒƒã‚·ãƒ¥: {content_hash[:16]}...")

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            is_duplicate = self.db.check_duplicate_hash(content_hash)

            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            try:
                Path(file_path).unlink()
            except Exception:
                pass

            if is_duplicate:
                logger.warning(f"âš ï¸  é‡è¤‡æ¤œçŸ¥: {file_name} ã¯æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™ï¼ˆAIå‡¦ç†ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                return None

            logger.info(f"âœ… é‡è¤‡ãªã—: {file_name} ã¯æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™")
            return content_hash

        except Exception as e:
            logger.error(f"âŒ é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {file_name} - {e}")
            logger.error(traceback.format_exc())
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å‡¦ç†ã‚’ç¶šè¡Œï¼ˆå®‰å…¨å´ã«å€’ã™ï¼‰
            return "error_skip_hash_check"

    async def process_file(self, file_meta: Dict[str, Any]) -> bool:
        """
        æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’2æ®µéšAIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å‡¦ç†

        Args:
            file_meta: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

        Returns:
            å‡¦ç†ãŒæˆåŠŸã—ãŸå ´åˆTrue
        """
        file_name = file_meta['name']
        file_id = file_meta['id']

        logger.info(f"âš™ï¸  ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file_name}")

        try:
            # TwoStageIngestionPipelineã§å‡¦ç†
            # workspaceã¯'inbox'ã¨ã—ã¦æ‰±ã†
            result = await self.pipeline.process_file(file_meta, workspace='inbox')

            if result:
                logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æˆåŠŸ: {file_name}")
                return True
            else:
                logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—: {file_name}")
                return False

        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {file_name}")
            logger.error(traceback.format_exc())
            return False

    def move_to_archive(self, file_id: str, file_name: str) -> bool:
        """
        å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Archiveãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•

        Args:
            file_id: ãƒ•ã‚¡ã‚¤ãƒ«ID
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«å

        Returns:
            ç§»å‹•ãŒæˆåŠŸã—ãŸå ´åˆTrue
        """
        if not self.archive_folder_id:
            logger.warning(f"âš ï¸  ARCHIVE_FOLDER_ID ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€{file_name} ã®ç§»å‹•ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return False

        logger.info(f"ğŸ“¦ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Archiveã«ç§»å‹•ä¸­: {file_name}")

        success = self.drive.move_file(file_id, self.archive_folder_id)

        if success:
            logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•æˆåŠŸ: {file_name} -> Archive")
        else:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•å¤±æ•—: {file_name}")

        return success

    async def run_monitoring_cycle(self):
        """ç›£è¦–ã‚µã‚¤ã‚¯ãƒ«ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        logger.info("=" * 70)
        logger.info("ğŸ” InBoxè‡ªå‹•ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ  é–‹å§‹")
        logger.info(f"å®Ÿè¡Œæ™‚åˆ»: {datetime.now():%Y-%m-%d %H:%M:%S}")
        logger.info("=" * 70)

        stats = {
            'new_files_detected': 0,
            'duplicates_skipped': 0,
            'processed_success': 0,
            'processed_failed': 0,
            'archived_success': 0,
            'archived_failed': 0
        }

        try:
            # Step 1: å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’å–å¾—
            processed_file_ids = self.get_processed_file_ids()

            # Step 2: InBoxã‹ã‚‰æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º
            new_files = self.scan_inbox_for_new_files(processed_file_ids)
            stats['new_files_detected'] = len(new_files)

            # Step 3: å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
            for file_meta in new_files:
                file_id = file_meta['id']
                file_name = file_meta['name']

                # Step 3-1: é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆcontent_hashï¼‰
                content_hash = self.check_duplicate_by_hash(file_meta)

                if content_hash is None:
                    # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ï¼šAIå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    stats['duplicates_skipped'] += 1
                    logger.info(f"ğŸ’° ã‚³ã‚¹ãƒˆå‰Šæ¸›: {file_name} ã®AIå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")

                    # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚Archiveã«ç§»å‹•
                    if self.archive_folder_id:
                        archive_success = self.move_to_archive(file_id, file_name)
                        if archive_success:
                            stats['archived_success'] += 1
                        else:
                            stats['archived_failed'] += 1
                    continue

                # Step 3-2: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ï¼ˆé‡è¤‡ãªã—ã®å ´åˆï¼‰
                success = await self.process_file(file_meta)

                if success:
                    stats['processed_success'] += 1

                    # å‡¦ç†æˆåŠŸå¾Œã€Archiveã«ç§»å‹•
                    if self.archive_folder_id:
                        archive_success = self.move_to_archive(file_id, file_name)
                        if archive_success:
                            stats['archived_success'] += 1
                        else:
                            stats['archived_failed'] += 1
                else:
                    stats['processed_failed'] += 1
                    logger.warning(f"âš ï¸  å‡¦ç†å¤±æ•—ã®ãŸã‚ã€{file_name} ã¯InBoxã«æ®‹ã—ã¾ã™")

        except Exception as e:
            logger.error(f"âŒ ç›£è¦–ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            logger.error(traceback.format_exc())

        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        logger.info("=" * 70)
        logger.info("ğŸ“Š InBoxè‡ªå‹•ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ  å®Œäº†ã‚µãƒãƒªãƒ¼")
        logger.info(f"æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡ºæ•°: {stats['new_files_detected']}")
        logger.info(f"é‡è¤‡ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—: {stats['duplicates_skipped']} ä»¶ ğŸ’°")
        logger.info(f"AIå‡¦ç†æˆåŠŸæ•°: {stats['processed_success']}")
        logger.info(f"AIå‡¦ç†å¤±æ•—æ•°: {stats['processed_failed']}")
        logger.info(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æˆåŠŸæ•°: {stats['archived_success']}")
        logger.info(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¤±æ•—æ•°: {stats['archived_failed']}")
        logger.info("=" * 70)

        return stats


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        monitor = InBoxMonitor()
        stats = await monitor.run_monitoring_cycle()

        # çµ‚äº†ã‚³ãƒ¼ãƒ‰æ±ºå®š
        if stats['new_files_detected'] == 0:
            logger.info("âœ¨ æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆæ­£å¸¸çµ‚äº†ï¼‰")
            sys.exit(0)

        if stats['processed_failed'] == 0:
            logger.info("âœ… ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«å‡¦ç†ã•ã‚Œã¾ã—ãŸ")
            sys.exit(0)

        failure_rate = stats['processed_failed'] / stats['new_files_detected']

        if failure_rate >= 0.5:
            logger.error(f"âŒ å¤±æ•—ç‡ãŒé«˜ã™ãã¾ã™ ({failure_rate:.1%})ã€‚ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã®å•é¡Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            sys.exit(1)
        else:
            logger.warning(f"âš ï¸  {stats['processed_failed']}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
            sys.exit(2)

    except Exception as e:
        logger.error(f"âŒ ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Ÿè¡Œä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
