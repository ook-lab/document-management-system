# Project Snapshot

Generated: 2026-01-13 11:20:39

## Directory Structure

```
document-management-system/
├── database/
│   ├── migrations/
│   │   ├── add_processing_lock_columns.sql
│   │   ├── add_stage_output_columns.sql
│   │   ├── add_title_column.sql
│   │   ├── create_flyer_schema.sql
│   │   ├── enable_pgvector.sql
│   │   ├── execute_drop_columns_psycopg2.py
│   │   ├── execute_drop_display_columns.py
│   │   └── run_migration_chunk_metadata.py
│   ├── add_match_documents_function.sql
│   └── run_migration.py
├── docs/
│   ├── ARCHITECTURE.md
│   ├── CLAUDE_CHANGES_LOG.md
│   ├── CLAUDE_MISTAKES_LOG.md
│   ├── CLOUD_RUN_DEPLOY.md
│   ├── FIX_HISTORY.txt
│   ├── FIX_PLAN.md
│   ├── FIX_REALTIME_DISPLAY.md
│   ├── HYBRID_OCR_README.md
│   ├── INDEX.md
│   ├── SQL_REFERENCE.md
│   └── VERIFICATION_REPORT.md
├── frontend/
│   ├── components/
│   │   ├── __init__.py
│   │   ├── email_viewer.py
│   │   ├── form_editor.py
│   │   ├── json_preview.py
│   │   ├── manual_text_correction.py
│   │   ├── table_creator.py
│   │   └── table_editor.py
│   ├── schemas/
│   │   └── generic.json
│   ├── utils/
│   │   ├── schemas/
│   │   │   └── school_common.json
│   │   ├── __init__.py
│   │   ├── schema_detector.py
│   │   ├── stage_h_reprocessor.py
│   │   ├── table_parser.py
│   │   └── text_structurer.py
│   ├── .python-version
│   ├── .streamlit_deploy
│   ├── __init__.py
│   ├── document_review_app.py
│   ├── email_inbox_app.py
│   ├── expired_email_manager.py
│   ├── packages.txt
│   ├── requirements.txt
│   └── streamlit_app.py
├── scripts/
│   ├── email/
│   ├── processing/
│   │   ├── process_daiei.py
│   │   ├── process_queued_documents.py
│   │   ├── process_queued_flyers.py
│   │   ├── process_rakuten_seiyu.py
│   │   ├── process_single_doc.py
│   │   └── process_tokyu_store.py
│   ├── reset/
│   │   ├── reset_3docs.py
│   │   ├── reset_all_documents.py
│   │   ├── reset_all_stages_e_to_k.py
│   │   ├── reset_doc.py
│   │   ├── reset_fabricated_dates.py
│   │   ├── reset_stages_e_to_k.py
│   │   └── reset_to_pending.py
│   └── utils/
│       ├── check_database.py
│       ├── check_state.py
│       ├── collect_project.py
│       ├── fix_database.py
│       ├── fix_rls.py
│       ├── setup_missing_files.py
│       ├── test_flask_startup.py
│       └── watch_updates.py
├── services/
│   ├── data-ingestion/
│   │   ├── common/
│   │   │   ├── __init__.py
│   │   │   ├── base_product_ingestion.py
│   │   │   ├── category_config.json
│   │   │   ├── category_manager.py
│   │   │   └── category_manager_db.py
│   │   ├── daiei/
│   │   │   ├── __init__.py
│   │   │   ├── daiei_scraper_playwright.py
│   │   │   ├── process_with_schedule.py
│   │   │   └── product_ingestion.py
│   │   ├── gmail/
│   │   │   ├── __init__.py
│   │   │   ├── config.yaml
│   │   │   └── gmail_ingestion.py
│   │   ├── monitoring/
│   │   │   ├── __init__.py
│   │   │   └── inbox_monitor.py
│   │   ├── rakuten_seiyu/
│   │   │   ├── __init__.py
│   │   │   ├── auth_manager.py
│   │   │   ├── categories_config.json
│   │   │   ├── process_with_schedule.py
│   │   │   ├── product_ingestion.py
│   │   │   └── rakuten_seiyu_scraper_playwright.py
│   │   ├── tokubai/
│   │   │   ├── __init__.py
│   │   │   ├── flyer_ingestion.py
│   │   │   ├── flyer_processor.py
│   │   │   ├── stores_config.json
│   │   │   └── tokubai_scraper.py
│   │   ├── tokyu_store/
│   │   │   ├── __init__.py
│   │   │   ├── process_with_schedule.py
│   │   │   ├── product_ingestion.py
│   │   │   └── tokyu_store_scraper_playwright.py
│   │   ├── waseda_academy/
│   │   │   ├── __init__.py
│   │   │   ├── browser_automation.py
│   │   │   ├── known_waseda_notice_ids.json
│   │   │   └── notice_ingestion.py
│   │   ├── __init__.py
│   │   ├── netsuper_category_manager_ui.py
│   │   ├── netsuper_classification_manager_ui.py
│   │   └── requirements.txt
│   ├── doc-processor/
│   │   ├── templates/
│   │   │   └── processing.html
│   │   ├── app.py
│   │   ├── cloudbuild.yaml
│   │   ├── deploy.sh
│   │   ├── deploy_to_cloud_run.sh
│   │   ├── Dockerfile
│   │   ├── Dockerfile.base
│   │   ├── requirements.txt
│   │   ├── run_server.py
│   │   ├── start.sh
│   │   ├── tmpclaude-0b0b-cwd
│   │   ├── tmpclaude-100d-cwd
│   │   ├── tmpclaude-1296-cwd
│   │   ├── tmpclaude-1d98-cwd
│   │   ├── tmpclaude-219b-cwd
│   │   ├── tmpclaude-2d11-cwd
│   │   ├── tmpclaude-32de-cwd
│   │   ├── tmpclaude-3351-cwd
│   │   ├── tmpclaude-4734-cwd
│   │   ├── tmpclaude-5ab1-cwd
│   │   ├── tmpclaude-63de-cwd
│   │   ├── tmpclaude-8219-cwd
│   │   ├── tmpclaude-85c1-cwd
│   │   ├── tmpclaude-94a1-cwd
│   │   ├── tmpclaude-9be3-cwd
│   │   ├── tmpclaude-cc94-cwd
│   │   ├── tmpclaude-dc3f-cwd
│   │   └── tmpclaude-f951-cwd
│   ├── doc-search/
│   │   ├── templates/
│   │   │   └── index.html
│   │   ├── app.py
│   │   ├── cloudbuild.yaml
│   │   ├── deploy.sh
│   │   ├── deploy_to_cloud_run.sh
│   │   ├── Dockerfile
│   │   └── requirements.txt
│   └── netsuper-search/
│       ├── app.py
│       ├── deploy.ps1
│       ├── deploy.sh
│       ├── Dockerfile
│       ├── generate_embeddings.py
│       ├── generate_multi_embeddings.py
│       ├── hybrid_search.py
│       ├── inspect_db_triggers.py
│       ├── inspect_embedding_content.py
│       ├── list_60_tables.py
│       ├── requirements.txt
│       └── reverse_engineer_embedding.py
├── shared/
│   ├── ai/
│   │   ├── embeddings/
│   │   │   ├── __init__.py
│   │   │   └── embeddings.py
│   │   ├── llm_client/
│   │   │   ├── __init__.py
│   │   │   ├── exceptions.py
│   │   │   └── llm_client.py
│   │   └── __init__.py
│   ├── common/
│   │   ├── ai/
│   │   │   ├── __init__.py
│   │   │   └── verified_examples.py
│   │   ├── config/
│   │   │   ├── __init__.py
│   │   │   ├── CLASSIFICATION_MAPPING_v2.0.yaml
│   │   │   ├── model_tiers.py
│   │   │   ├── settings.py
│   │   │   ├── user_context.yaml
│   │   │   ├── user_context.yaml.example
│   │   │   └── yaml_loader.py
│   │   ├── connectors/
│   │   │   ├── __init__.py
│   │   │   ├── gmail_connector.py
│   │   │   └── google_drive.py
│   │   ├── data/
│   │   ├── database/
│   │   │   ├── __init__.py
│   │   │   └── client.py
│   │   ├── processing/
│   │   │   ├── __init__.py
│   │   │   ├── chunk_processor.py
│   │   │   └── metadata_chunker.py
│   │   ├── processors/
│   │   │   ├── __init__.py
│   │   │   ├── office.py
│   │   │   └── pdf.py
│   │   ├── utils/
│   │   │   ├── __init__.py
│   │   │   ├── chunking.py
│   │   │   ├── context_extractor.py
│   │   │   ├── date_extractor.py
│   │   │   ├── html_screenshot.py
│   │   │   ├── hypothetical_questions.py
│   │   │   ├── metadata_extractor.py
│   │   │   ├── query_expansion.py
│   │   │   └── synthetic_chunks.py
│   │   └── __init__.py
│   ├── kakeibo/
│   │   ├── __init__.py
│   │   ├── config.py
│   │   ├── kakeibo_db_handler.py
│   │   ├── review_ui.py
│   │   ├── schema.sql
│   │   ├── transaction_processor.py
│   │   ├── verify_7_elements.py
│   │   ├── verify_foodium_receipt.py
│   │   └── verify_latest_receipt.py
│   ├── pipeline/
│   │   ├── config/
│   │   │   ├── models.yaml
│   │   │   ├── pipeline_routing.yaml
│   │   │   └── prompts.yaml
│   │   ├── prompts/
│   │   │   └── __init__.py
│   │   ├── __init__.py
│   │   ├── config_loader.py
│   │   ├── image_preprocessing.py
│   │   ├── ocr_config.py
│   │   ├── ocr_report.py
│   │   ├── pipeline.py
│   │   ├── stage_e_preprocessing.py
│   │   ├── stage_f_visual.py
│   │   ├── stage_h_kakeibo.py
│   │   ├── stage_h_structuring.py
│   │   ├── stage_i_synthesis.py
│   │   ├── stage_j_chunking.py
│   │   └── stage_k_embedding.py
│   └── __init__.py
├── tests/
│   ├── stage_e_text.txt
│   ├── stage_f_full_text.txt
│   ├── stage_f_result.json
│   └── summary.txt
├── .dockerignore
├── .env
├── .env.example
├── .gcloudignore
├── .gitattributes
├── .gitignore
├── .python-version
├── cloudbuild.yaml
├── deploy.bat
├── deploy.ps1
├── deploy_doc_processor.sh
├── deploy_now.ps1
├── README.md
├── requirements.txt
└── run_build.ps1
```

## Source Files

### cloudbuild.yaml

```yaml
steps:
  # Dockerイメージをビルド
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'build'
      - '-t'
      - 'asia-northeast1-docker.pkg.dev/$PROJECT_ID/cloud-run-source-deploy/doc-processor:${_TAG}'
      - '-f'
      - 'services/doc-processor/Dockerfile'
      - '.'

  # イメージをArtifact Registryにプッシュ
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - 'asia-northeast1-docker.pkg.dev/$PROJECT_ID/cloud-run-source-deploy/doc-processor:${_TAG}'

  # Cloud Runにデプロイ
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: gcloud
    args:
      - 'run'
      - 'deploy'
      - 'doc-processor'
      - '--image'
      - 'asia-northeast1-docker.pkg.dev/$PROJECT_ID/cloud-run-source-deploy/doc-processor:${_TAG}'
      - '--region'
      - 'asia-northeast1'
      - '--platform'
      - 'managed'
      - '--memory'
      - '16Gi'
      - '--cpu'
      - '4'
      - '--timeout'
      - '3600'
      - '--allow-unauthenticated'
      - '--min-instances'
      - '0'
      - '--max-instances'
      - '10'
      - '--concurrency'
      - '1'
      - '--execution-environment'
      - 'gen1'

images:
  - 'asia-northeast1-docker.pkg.dev/$PROJECT_ID/cloud-run-source-deploy/doc-processor:${_TAG}'

substitutions:
  _TAG: 'latest'

options:
  machineType: 'E2_HIGHCPU_8'
```

### database\add_match_documents_function.sql

```sql
-- match_documents関数の追加
-- Supabase SQL Editorで実行してください
--
-- この関数は、ベクトル検索（cosine類似度）でdocumentsテーブルから関連文書を検索します

CREATE OR REPLACE FUNCTION match_documents(
    query_embedding vector(1536),
    match_threshold float DEFAULT 0.2,
    match_count int DEFAULT 10,
    filter_workspace text DEFAULT NULL
)
RETURNS TABLE (
    id uuid,
    source_type varchar,
    source_id varchar,
    source_url text,
    title text,
    content text,
    doc_type varchar,
    workspace varchar,
    document_date date,
    file_name varchar,
    similarity float,
    metadata jsonb,
    created_at timestamptz
)
LANGUAGE plpgsql
AS $$
BEGIN
    RETURN QUERY
    SELECT
        d.id,
        d.source_type,
        d.source_id,
        d.source_url,
        COALESCE(d.file_name, '無題') as title,
        COALESCE(d.summary, d.full_text, '') as content,
        d.doc_type,
        d.workspace,
        d.document_date,
        d.file_name,
        1 - (d.embedding <=> query_embedding) as similarity,
        d.metadata,
        d.created_at
    FROM documents d
    WHERE
        d.embedding IS NOT NULL
        AND (filter_workspace IS NULL OR d.workspace = filter_workspace)
        AND (1 - (d.embedding <=> query_embedding)) >= match_threshold
        AND d.processing_status = 'completed'
    ORDER BY d.embedding <=> query_embedding
    LIMIT match_count;
END;
$$;

-- インデックスが既に存在する場合はスキップ
CREATE INDEX IF NOT EXISTS idx_documents_embedding ON documents USING ivfflat (embedding vector_cosine_ops);

-- 関数の説明
COMMENT ON FUNCTION match_documents IS 'ベクトル検索（cosine類似度）でdocumentsテーブルから関連文書を検索';
```

### database\migrations\add_processing_lock_columns.sql

```sql
-- マイグレーション: processing_lockテーブルにリアルタイム進捗表示用カラム追加
-- 作成日: 2026-01-11
-- 目的: 処理状況のリアルタイム表示に必要なカラムを追加

-- 進捗情報カラム
ALTER TABLE processing_lock
ADD COLUMN IF NOT EXISTS current_index INTEGER DEFAULT 0,
ADD COLUMN IF NOT EXISTS total_count INTEGER DEFAULT 0,
ADD COLUMN IF NOT EXISTS current_file TEXT DEFAULT '',
ADD COLUMN IF NOT EXISTS success_count INTEGER DEFAULT 0,
ADD COLUMN IF NOT EXISTS error_count INTEGER DEFAULT 0,
ADD COLUMN IF NOT EXISTS logs JSONB DEFAULT '[]'::jsonb;

-- リソース情報カラム
ALTER TABLE processing_lock
ADD COLUMN IF NOT EXISTS cpu_percent REAL DEFAULT 0.0,
ADD COLUMN IF NOT EXISTS memory_percent REAL DEFAULT 0.0,
ADD COLUMN IF NOT EXISTS memory_used_gb REAL DEFAULT 0.0,
ADD COLUMN IF NOT EXISTS memory_total_gb REAL DEFAULT 0.0;

-- 制御情報カラム
ALTER TABLE processing_lock
ADD COLUMN IF NOT EXISTS throttle_delay REAL DEFAULT 0.0,
ADD COLUMN IF NOT EXISTS adjustment_count INTEGER DEFAULT 0,
ADD COLUMN IF NOT EXISTS max_parallel INTEGER DEFAULT 3,
ADD COLUMN IF NOT EXISTS current_workers INTEGER DEFAULT 0;

-- コメント追加
COMMENT ON COLUMN processing_lock.current_index IS '現在処理中のドキュメントのインデックス（0始まり）';
COMMENT ON COLUMN processing_lock.total_count IS '処理対象ドキュメントの総数';
COMMENT ON COLUMN processing_lock.current_file IS '現在処理中のファイル名';
COMMENT ON COLUMN processing_lock.success_count IS '処理成功件数';
COMMENT ON COLUMN processing_lock.error_count IS '処理エラー件数';
COMMENT ON COLUMN processing_lock.logs IS '処理ログ（JSON配列）';

COMMENT ON COLUMN processing_lock.cpu_percent IS 'CPU使用率（%）';
COMMENT ON COLUMN processing_lock.memory_percent IS 'メモリ使用率（%）';
COMMENT ON COLUMN processing_lock.memory_used_gb IS 'メモリ使用量（GB）';
COMMENT ON COLUMN processing_lock.memory_total_gb IS '総メモリ容量（GB）';

COMMENT ON COLUMN processing_lock.throttle_delay IS 'スロットル遅延（秒）';
COMMENT ON COLUMN processing_lock.adjustment_count IS 'リソース調整回数';
COMMENT ON COLUMN processing_lock.max_parallel IS '最大並列処理数';
COMMENT ON COLUMN processing_lock.current_workers IS '現在のワーカー数';
```

### database\migrations\add_stage_output_columns.sql

```sql
-- マイグレーション: Stage出力保存用カラム追加
-- 作成日: 2026-01-01
-- 目的: E-1~E-5, F (Text OCR/Layout OCR/Visual Elements), H, I, J の各ステージ出力を保存

-- Stage E: PDFテキスト抽出（5種類のエンジン）
ALTER TABLE "Rawdata_FILE_AND_MAIL"
ADD COLUMN IF NOT EXISTS stage_e1_text TEXT,
ADD COLUMN IF NOT EXISTS stage_e2_text TEXT,
ADD COLUMN IF NOT EXISTS stage_e3_text TEXT,
ADD COLUMN IF NOT EXISTS stage_e4_text TEXT,
ADD COLUMN IF NOT EXISTS stage_e5_text TEXT;

-- Stage F: OCR（3種類）
ALTER TABLE "Rawdata_FILE_AND_MAIL"
ADD COLUMN IF NOT EXISTS stage_f_text_ocr TEXT,
ADD COLUMN IF NOT EXISTS stage_f_layout_ocr TEXT,
ADD COLUMN IF NOT EXISTS stage_f_visual_elements TEXT;

-- Stage H: 正規化テキスト
ALTER TABLE "Rawdata_FILE_AND_MAIL"
ADD COLUMN IF NOT EXISTS stage_h_normalized TEXT;

-- Stage I: 構造化テキスト
ALTER TABLE "Rawdata_FILE_AND_MAIL"
ADD COLUMN IF NOT EXISTS stage_i_structured TEXT;

-- Stage J: チャンクJSON（デバッグ用）
ALTER TABLE "Rawdata_FILE_AND_MAIL"
ADD COLUMN IF NOT EXISTS stage_j_chunks_json JSONB;

-- コメント追加
COMMENT ON COLUMN "Rawdata_FILE_AND_MAIL".stage_e1_text IS 'Stage E-1: PyPDF2で抽出したテキスト';
COMMENT ON COLUMN "Rawdata_FILE_AND_MAIL".stage_e2_text IS 'Stage E-2: pdfminerで抽出したテキスト';
COMMENT ON COLUMN "Rawdata_FILE_AND_MAIL".stage_e3_text IS 'Stage E-3: PyMuPDFで抽出したテキスト';
COMMENT ON COLUMN "Rawdata_FILE_AND_MAIL".stage_e4_text IS 'Stage E-4: pdfplumberで抽出したテキスト';
COMMENT ON COLUMN "Rawdata_FILE_AND_MAIL".stage_e5_text IS 'Stage E-5: PDFiumで抽出したテキスト';

COMMENT ON COLUMN "Rawdata_FILE_AND_MAIL".stage_f_text_ocr IS 'Stage F: Text OCR（Tesseract）';
COMMENT ON COLUMN "Rawdata_FILE_AND_MAIL".stage_f_layout_ocr IS 'Stage F: Layout OCR（Tesseract + レイアウト保持）';
COMMENT ON COLUMN "Rawdata_FILE_AND_MAIL".stage_f_visual_elements IS 'Stage F: Visual Elements（視覚要素の説明、Claude 3.5 Sonnet）';

COMMENT ON COLUMN "Rawdata_FILE_AND_MAIL".stage_h_normalized IS 'Stage H: 正規化後のテキスト';
COMMENT ON COLUMN "Rawdata_FILE_AND_MAIL".stage_i_structured IS 'Stage I: 構造化後のテキスト';
COMMENT ON COLUMN "Rawdata_FILE_AND_MAIL".stage_j_chunks_json IS 'Stage J: 生成されたチャンク（JSONB形式、デバッグ用）';
```

### database\migrations\add_title_column.sql

```sql
-- Rawdata_FILE_AND_MAILテーブルにtitleカラムを追加
-- Stage Iで生成されたタイトルを保存する

ALTER TABLE "Rawdata_FILE_AND_MAIL"
ADD COLUMN IF NOT EXISTS title TEXT;

-- コメント追加
COMMENT ON COLUMN "Rawdata_FILE_AND_MAIL".title IS 'Stage Iで生成されたドキュメントタイトル';
```

### database\migrations\create_flyer_schema.sql

```sql
-- チラシ管理用のスキーマ作成

-- 1. flyer_documents テーブル（チラシ基本情報）
CREATE TABLE IF NOT EXISTS flyer_documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),

    -- 基本情報
    source_type VARCHAR(50) DEFAULT 'flyer',
    workspace VARCHAR(50) DEFAULT 'shopping',
    doc_type VARCHAR(50) DEFAULT 'physical shop',
    organization VARCHAR(255) NOT NULL,  -- 店舗名

    -- チラシ固有情報
    flyer_id VARCHAR(255) UNIQUE,  -- トクバイのチラシID
    flyer_title VARCHAR(500),  -- チラシのタイトル
    flyer_period VARCHAR(255),  -- 有効期間（例: "2024/12/18〜2024/12/24"）
    flyer_url TEXT,  -- チラシの元URL
    page_number INTEGER,  -- ページ番号
    total_pages INTEGER,  -- 総ページ数

    -- ファイル情報
    source_id VARCHAR(255),  -- Google DriveファイルID
    source_url TEXT,  -- Drive URL
    file_name VARCHAR(500),
    file_type VARCHAR(50) DEFAULT 'image',
    content_hash VARCHAR(64),  -- SHA-256ハッシュ

    -- OCR・テキスト情報
    attachment_text TEXT,  -- OCRで抽出したテキスト
    summary TEXT,  -- AIが生成したサマリー

    -- 分類・タグ
    tags TEXT[],  -- タグ配列
    category VARCHAR(100),  -- カテゴリ（食品、日用品、衣料品など）

    -- 日付
    document_date DATE,  -- チラシの日付
    valid_from DATE,  -- 有効期間開始
    valid_until DATE,  -- 有効期間終了

    -- 処理ステータス
    processing_status VARCHAR(50) DEFAULT 'pending',  -- pending, processing, completed, failed
    processing_stage VARCHAR(100),
    processing_error TEXT,

    -- 表示用フィールド
    display_subject VARCHAR(500),
    display_sender VARCHAR(255),
    display_sent_at TIMESTAMPTZ,
    display_post_text TEXT,

    -- メタデータ（JSON）
    metadata JSONB,

    -- インデックス用
    person VARCHAR(100),

    -- 検索用
    search_vector tsvector
);

-- 2. flyer_products テーブル（商品情報）
CREATE TABLE IF NOT EXISTS flyer_products (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),

    -- 関連するチラシ
    flyer_document_id UUID REFERENCES flyer_documents(id) ON DELETE CASCADE,

    -- 商品基本情報
    product_name VARCHAR(500) NOT NULL,  -- 商品名
    product_name_normalized VARCHAR(500),  -- 正規化された商品名（検索用）

    -- 価格情報
    price DECIMAL(10, 2),  -- 価格
    original_price DECIMAL(10, 2),  -- 元の価格（割引前）
    discount_rate DECIMAL(5, 2),  -- 割引率（%）
    price_unit VARCHAR(50),  -- 単位（円、円/100g など）
    price_text VARCHAR(255),  -- 価格のテキスト表記（"298円"、"特価"など）

    -- 分類
    category VARCHAR(100),  -- カテゴリ（野菜、肉、魚、日用品など）
    subcategory VARCHAR(100),  -- サブカテゴリ
    tags TEXT[],  -- タグ配列

    -- 商品詳細
    brand VARCHAR(255),  -- ブランド
    quantity VARCHAR(100),  -- 数量・容量（"100g"、"1パック"など）
    origin VARCHAR(255),  -- 産地

    -- 特売情報
    is_special_offer BOOLEAN DEFAULT false,  -- 特売品かどうか
    offer_type VARCHAR(50),  -- 特売タイプ（タイムセール、日替わりなど）

    -- 画像内の位置情報
    page_number INTEGER,  -- 掲載ページ
    bounding_box JSONB,  -- 画像内の位置（{x, y, width, height}）

    -- OCR元テキスト
    extracted_text TEXT,  -- OCRで抽出した元のテキスト
    confidence DECIMAL(5, 4),  -- 抽出の信頼度（0-1）

    -- メタデータ
    metadata JSONB,

    -- 検索用
    search_vector tsvector
);

-- 3. インデックス作成

-- flyer_documents用インデックス
CREATE INDEX IF NOT EXISTS idx_flyer_documents_organization ON flyer_documents(organization);
CREATE INDEX IF NOT EXISTS idx_flyer_documents_flyer_id ON flyer_documents(flyer_id);
CREATE INDEX IF NOT EXISTS idx_flyer_documents_workspace ON flyer_documents(workspace);
CREATE INDEX IF NOT EXISTS idx_flyer_documents_doc_type ON flyer_documents(doc_type);
CREATE INDEX IF NOT EXISTS idx_flyer_documents_valid_from ON flyer_documents(valid_from);
CREATE INDEX IF NOT EXISTS idx_flyer_documents_valid_until ON flyer_documents(valid_until);
CREATE INDEX IF NOT EXISTS idx_flyer_documents_processing_status ON flyer_documents(processing_status);
CREATE INDEX IF NOT EXISTS idx_flyer_documents_created_at ON flyer_documents(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_flyer_documents_tags ON flyer_documents USING GIN(tags);
CREATE INDEX IF NOT EXISTS idx_flyer_documents_metadata ON flyer_documents USING GIN(metadata);
CREATE INDEX IF NOT EXISTS idx_flyer_documents_search ON flyer_documents USING GIN(search_vector);

-- flyer_products用インデックス
CREATE INDEX IF NOT EXISTS idx_flyer_products_flyer_id ON flyer_products(flyer_document_id);
CREATE INDEX IF NOT EXISTS idx_flyer_products_category ON flyer_products(category);
CREATE INDEX IF NOT EXISTS idx_flyer_products_product_name ON flyer_products(product_name);
CREATE INDEX IF NOT EXISTS idx_flyer_products_product_name_normalized ON flyer_products(product_name_normalized);
CREATE INDEX IF NOT EXISTS idx_flyer_products_price ON flyer_products(price);
CREATE INDEX IF NOT EXISTS idx_flyer_products_is_special_offer ON flyer_products(is_special_offer);
CREATE INDEX IF NOT EXISTS idx_flyer_products_tags ON flyer_products USING GIN(tags);
CREATE INDEX IF NOT EXISTS idx_flyer_products_search ON flyer_products USING GIN(search_vector);

-- 4. 更新日時の自動更新トリガー
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_flyer_documents_updated_at BEFORE UPDATE ON flyer_documents
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_flyer_products_updated_at BEFORE UPDATE ON flyer_products
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- 5. 全文検索用のトリガー（日本語対応）
CREATE OR REPLACE FUNCTION flyer_documents_search_vector_update()
RETURNS TRIGGER AS $$
BEGIN
    NEW.search_vector :=
        setweight(to_tsvector('simple', coalesce(NEW.flyer_title, '')), 'A') ||
        setweight(to_tsvector('simple', coalesce(NEW.organization, '')), 'B') ||
        setweight(to_tsvector('simple', coalesce(NEW.attachment_text, '')), 'C') ||
        setweight(to_tsvector('simple', coalesce(NEW.summary, '')), 'D');
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER tsvector_update_flyer_documents BEFORE INSERT OR UPDATE ON flyer_documents
    FOR EACH ROW EXECUTE FUNCTION flyer_documents_search_vector_update();

CREATE OR REPLACE FUNCTION flyer_products_search_vector_update()
RETURNS TRIGGER AS $$
BEGIN
    NEW.search_vector :=
        setweight(to_tsvector('simple', coalesce(NEW.product_name, '')), 'A') ||
        setweight(to_tsvector('simple', coalesce(NEW.brand, '')), 'B') ||
        setweight(to_tsvector('simple', coalesce(NEW.category, '')), 'C') ||
        setweight(to_tsvector('simple', coalesce(NEW.extracted_text, '')), 'D');
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER tsvector_update_flyer_products BEFORE INSERT OR UPDATE ON flyer_products
    FOR EACH ROW EXECUTE FUNCTION flyer_products_search_vector_update();

-- 6. RLS (Row Level Security) ポリシー（必要に応じて設定）
-- ALTER TABLE flyer_documents ENABLE ROW LEVEL SECURITY;
-- ALTER TABLE flyer_products ENABLE ROW LEVEL SECURITY;

COMMENT ON TABLE flyer_documents IS 'スーパーやドラッグストアのチラシ基本情報を管理';
COMMENT ON TABLE flyer_products IS 'チラシから抽出した個別商品情報を管理';
```

### database\migrations\enable_pgvector.sql

```sql
-- ============================================
-- pgvector拡張を有効化
-- ベクトル検索を使用するために必要
-- ============================================

-- pgvector拡張を有効化
CREATE EXTENSION IF NOT EXISTS vector;

-- 確認
SELECT * FROM pg_extension WHERE extname = 'vector';
```

### database\migrations\execute_drop_columns_psycopg2.py

```py
"""
display_sender と display_subject カラムを削除するマイグレーション
psycopg2を使用してPostgreSQLに直接接続
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
import logging

# 環境変数読み込み
load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def get_supabase_connection_string():
    """Supabase接続文字列を取得"""
    supabase_url = os.getenv("SUPABASE_URL")
    supabase_key = ***REDACTED***"SUPABASE_SERVICE_ROLE_KEY") or os.getenv("SUPABASE_KEY")

    if not supabase_url:
        raise ValueError("SUPABASE_URL not found in environment variables")

    # Supabase URLからプロジェクト参照を抽出
    # 例: https://hjkcgulxddtwlljhbocb.supabase.co -> hjkcgulxddtwlljhbocb
    project_ref = supabase_url.split("//")[1].split(".")[0]

    # PostgreSQL接続文字列を構築
    # Supabaseのデフォルトパスワードは環境変数から取得する必要がある
    db_password = ***REDACTED***"SUPABASE_DB_PASSWORD")

    if not db_password:
        logger.warning("SUPABASE_DB_PASSWORD not found. Please set it in .env file")
        logger.info("You can find the password in Supabase Dashboard -> Project Settings -> Database -> Connection String")
        logger.info("\nAlternatively, run the SQL migration manually in Supabase Dashboard:")
        logger.info("  1. Go to SQL Editor in Supabase Dashboard")
        logger.info("  2. Copy and paste the contents of:")
        logger.info("     database/migrations/drop_redundant_display_columns_from_products.sql")
        logger.info("  3. Click 'Run'")
        return None

    conn_string = f"postgresql://postgres:{db_password}@db.{project_ref}.supabase.co:5432/postgres"
    return conn_string


def main():
    """マイグレーション実行"""
    logger.info("=" * 80)
    logger.info("カラム削除マイグレーション開始")
    logger.info("=" * 80)

    # 接続文字列取得
    conn_string = get_supabase_connection_string()

    if not conn_string:
        logger.error("\n手動でSupabase Dashboardから実行してください:")
        logger.error("SQL Editor -> 以下のSQLを実行")
        logger.error("-" * 80)
        with open(Path(__file__).parent / "drop_redundant_display_columns_from_products.sql", "r") as f:
            logger.error(f.read())
        logger.error("-" * 80)
        return

    try:
        import psycopg2
    except ImportError:
        logger.error("psycopg2 がインストールされていません")
        logger.error("インストールコマンド: pip install psycopg2-binary")
        logger.info("\nまたは、Supabase Dashboardから手動で実行してください:")
        logger.info("  1. Supabase Dashboard -> SQL Editor")
        logger.info("  2. 以下のファイルの内容を実行:")
        logger.info("     database/migrations/drop_redundant_display_columns_from_products.sql")
        return

    try:
        # データベース接続
        logger.info("[1] データベースに接続中...")
        conn = psycopg2.connect(conn_string)
        cursor = conn.cursor()
        logger.info("✓ 接続成功")

        # 削除前のカラム確認
        logger.info("\n[2] 削除前のカラム確認...")
        cursor.execute("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_name = 'Rawdata_NETSUPER_items'
              AND column_name IN ('display_sender', 'display_subject')
            ORDER BY column_name;
        """)
        columns_before = cursor.fetchall()
        logger.info(f"削除前: {len(columns_before)} カラムが存在")
        for col in columns_before:
            logger.info(f"  - {col[0]}")

        # カラム削除
        logger.info("\n[3] カラム削除実行...")

        cursor.execute('ALTER TABLE "Rawdata_NETSUPER_items" DROP COLUMN IF EXISTS display_sender;')
        logger.info("✓ display_sender カラムを削除しました")

        cursor.execute('ALTER TABLE "Rawdata_NETSUPER_items" DROP COLUMN IF EXISTS display_subject;')
        logger.info("✓ display_subject カラムを削除しました")

        conn.commit()
        logger.info("✓ コミット完了")

        # 削除後のカラム確認
        logger.info("\n[4] 削除後のカラム確認...")
        cursor.execute("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_name = 'Rawdata_NETSUPER_items'
              AND column_name IN ('display_sender', 'display_subject')
            ORDER BY column_name;
        """)
        columns_after = cursor.fetchall()

        if len(columns_after) == 0:
            logger.info("✓ 削除成功: display_sender と display_subject は存在しません")
        else:
            logger.warning(f"⚠️  まだ {len(columns_after)} カラムが残っています:")
            for col in columns_after:
                logger.warning(f"  - {col[0]}")

        # クリーンアップ
        cursor.close()
        conn.close()

        logger.info("\n" + "=" * 80)
        logger.info("マイグレーション完了")
        logger.info("=" * 80)

    except Exception as e:
        logger.error(f"❌ エラーが発生しました: {e}")
        logger.error("\n手動で実行してください:")
        logger.error("Supabase Dashboard -> SQL Editor -> 以下を実行")
        logger.error("-" * 80)
        logger.error("ALTER TABLE \"Rawdata_NETSUPER_items\" DROP COLUMN IF EXISTS display_sender;")
        logger.error("ALTER TABLE \"Rawdata_NETSUPER_items\" DROP COLUMN IF EXISTS display_subject;")
        logger.error("-" * 80)


if __name__ == "__main__":
    main()
```

### database\migrations\execute_drop_display_columns.py

```py
"""
display_sender と display_subject カラムを削除するマイグレーション実行スクリプト
"""

import sys
from pathlib import Path

# プロジェクトルートをパスに追加
root_dir = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(root_dir))

from shared.common.database.client import DatabaseClient
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    """マイグレーション実行"""
    db = DatabaseClient(use_service_role=True)

    logger.info("=" * 80)
    logger.info("カラム削除マイグレーション開始")
    logger.info("=" * 80)

    # 削除前のカラム確認
    logger.info("\n[1] 削除前のカラム確認...")
    result_before = db.client.rpc(
        'execute_sql',
        {
            'query': """
                SELECT column_name
                FROM information_schema.columns
                WHERE table_name = 'Rawdata_NETSUPER_items'
                  AND column_name IN ('display_sender', 'display_subject')
                ORDER BY column_name;
            """
        }
    ).execute()

    logger.info(f"削除前: {len(result_before.data)} カラムが存在")
    for col in result_before.data:
        logger.info(f"  - {col['column_name']}")

    # カラム削除
    logger.info("\n[2] カラム削除実行...")

    try:
        # display_sender を削除
        db.client.rpc(
            'execute_sql',
            {
                'query': 'ALTER TABLE "Rawdata_NETSUPER_items" DROP COLUMN IF EXISTS display_sender;'
            }
        ).execute()
        logger.info("✓ display_sender カラムを削除しました")

        # display_subject を削除
        db.client.rpc(
            'execute_sql',
            {
                'query': 'ALTER TABLE "Rawdata_NETSUPER_items" DROP COLUMN IF EXISTS display_subject;'
            }
        ).execute()
        logger.info("✓ display_subject カラムを削除しました")

    except Exception as e:
        logger.error(f"❌ カラム削除に失敗: {e}")
        return

    # 削除後のカラム確認
    logger.info("\n[3] 削除後のカラム確認...")
    result_after = db.client.rpc(
        'execute_sql',
        {
            'query': """
                SELECT column_name
                FROM information_schema.columns
                WHERE table_name = 'Rawdata_NETSUPER_items'
                  AND column_name IN ('display_sender', 'display_subject')
                ORDER BY column_name;
            """
        }
    ).execute()

    if len(result_after.data) == 0:
        logger.info("✓ 削除成功: display_sender と display_subject は存在しません")
    else:
        logger.warning(f"⚠️  まだ {len(result_after.data)} カラムが残っています:")
        for col in result_after.data:
            logger.warning(f"  - {col['column_name']}")

    # 残存カラムの確認
    logger.info("\n[4] Rawdata_NETSUPER_items の全カラム一覧:")
    result_all = db.client.rpc(
        'execute_sql',
        {
            'query': """
                SELECT column_name, data_type
                FROM information_schema.columns
                WHERE table_name = 'Rawdata_NETSUPER_items'
                ORDER BY ordinal_position;
            """
        }
    ).execute()

    for col in result_all.data:
        logger.info(f"  - {col['column_name']}: {col['data_type']}")

    logger.info("\n" + "=" * 80)
    logger.info("マイグレーション完了")
    logger.info("=" * 80)


if __name__ == "__main__":
    main()
```

### database\migrations\run_migration_chunk_metadata.py

```py
"""
chunk_metadata カラムを追加するマイグレーション

Supabase は直接 SQL を実行できないため、Postgrest API 経由でスキーマ変更を試みる
ただし、DDL (ALTER TABLE) は通常 Supabase Dashboard の SQL Editor で実行する必要がある
"""
import os
from pathlib import Path
import sys

# ユーザーに案内を表示
print("=" * 80)
print("⚠️  chunk_metadata カラム追加マイグレーション")
print("=" * 80)
print()
print("Supabase では、テーブルスキーマの変更（ALTER TABLE）は")
print("Supabase Dashboard の SQL Editor から直接実行する必要があります。")
print()
print("以下の手順に従ってください：")
print()
print("1. Supabase Dashboard にログイン")
print("   https://supabase.com/dashboard")
print()
print("2. 対象のプロジェクトを選択")
print()
print("3. 左側メニューから「SQL Editor」を選択")
print()
print("4. 以下のSQLを実行:")
print()
print("-" * 80)

# SQLファイルを読み込んで表示
sql_file = Path(__file__).parent / "add_chunk_metadata_column.sql"
if sql_file.exists():
    with open(sql_file, 'r') as f:
        sql_content = f.read()
    print(sql_content)
else:
    print("""
ALTER TABLE "10_ix_search_index"
ADD COLUMN IF NOT EXISTS chunk_metadata jsonb;

CREATE INDEX IF NOT EXISTS idx_chunk_metadata_gin
ON "10_ix_search_index" USING gin (chunk_metadata jsonb_path_ops);

COMMENT ON COLUMN "10_ix_search_index".chunk_metadata IS '構造化データ（text_blocks, structured_tables, weekly_schedule, other_text など）のメタデータ';
""")

print("-" * 80)
print()
print("5. 実行後、このスクリプトを再度実行して確認してください:")
print(f"   python3 {Path(__file__).name}")
print()
print("=" * 80)

# 確認モード
answer = input("\nマイグレーションを実行しましたか？ (yes/no): ")

if answer.lower() == 'yes':
    # スキーマを確認
    try:
        sys.path.insert(0, str(Path(__file__).parent.parent.parent))
        from shared.common.database.client import DatabaseClient

        db = DatabaseClient()
        response = db.client.table('10_ix_search_index').select('*').limit(1).execute()

        if response.data and 'chunk_metadata' in response.data[0]:
            print()
            print("✅ chunk_metadata カラムが正常に追加されました！")
            print()
        else:
            print()
            print("❌ chunk_metadata カラムがまだ存在しません。")
            print("   SQL Editor でマイグレーションを実行してください。")
            print()
    except Exception as e:
        print(f"\nERROR: スキーマ確認失敗: {e}\n")
else:
    print("\nマイグレーションをキャンセルしました。\n")
```

### database\run_migration.py

```py
#!/usr/bin/env python3
"""
マイグレーションスクリプト実行ツール
"""
import os
import sys
from pathlib import Path
from dotenv import load_dotenv
from supabase import create_client

# プロジェクトルートを取得
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# 環境変数をロード
load_dotenv()

def run_migration(migration_file: str):
    """
    指定されたマイグレーションファイルを実行

    Args:
        migration_file: マイグレーションファイルのパス
    """
    # Supabase接続
    url = os.getenv("SUPABASE_URL")
    # SERVICE_ROLE_KEYを使用（DDL実行には管理者権限が必要）
    key = os.getenv("SUPABASE_SERVICE_ROLE_KEY") or os.getenv("SUPABASE_KEY")

    if not url or not key:
        print("ERROR: SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY not found in environment")
        sys.exit(1)

    db = create_client(url, key)

    # マイグレーションファイルを読み込み
    migration_path = Path(migration_file)
    if not migration_path.exists():
        print(f"ERROR: Migration file not found: {migration_file}")
        sys.exit(1)

    print(f"Reading migration file: {migration_path}")
    with open(migration_path, 'r', encoding='utf-8') as f:
        sql = f.read()

    print(f"Executing migration...")
    print("-" * 80)
    print(sql)
    print("-" * 80)

    try:
        # PostgreSQL関数を使ってSQLを実行
        result = db.rpc('exec_sql', {'sql': sql}).execute()
        print("✅ Migration executed successfully!")
        return True
    except Exception as e:
        # rpc関数がない場合は、postgrestの機能を使って実行を試みる
        print(f"⚠️ RPC method failed: {e}")
        print("Trying alternative method...")

        try:
            # 各SQLステートメントを個別に実行
            statements = [s.strip() for s in sql.split(';') if s.strip() and not s.strip().startswith('--')]

            for stmt in statements:
                if stmt:
                    print(f"Executing: {stmt[:100]}...")
                    # postgrestでは直接SQLを実行できないため、psycopg2を使用
                    import psycopg2

                    # 接続文字列を構築（Supabase URLから）
                    # 注: これは推奨される方法ではありません。本番環境ではSupabase CLIを使用してください
                    print("ERROR: Direct SQL execution requires psycopg2 or Supabase CLI")
                    print("Please run this SQL manually in Supabase SQL Editor:")
                    print("-" * 80)
                    print(sql)
                    print("-" * 80)
                    sys.exit(1)

        except Exception as e2:
            print(f"❌ Migration failed: {e2}")
            print("\nPlease run this SQL manually in Supabase SQL Editor:")
            print("-" * 80)
            print(sql)
            print("-" * 80)
            return False

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python run_migration.py <migration_file>")
        print("\nExample:")
        print("  python database/run_migration.py database/migrations/add_tax_subtotal_columns.sql")
        sys.exit(1)

    migration_file = sys.argv[1]
    run_migration(migration_file)
```

### deploy.bat

```bat
@echo off
cd /d C:\Users\ookub\document-management-system
powershell -ExecutionPolicy Bypass -File deploy_now.ps1
```

### deploy.ps1

```ps1
# .envファイルから環境変数を読み込み
Get-Content .env | Where-Object { $_ -match '^[A-Z_]+=.+' } | ForEach-Object {
    $parts = $_ -split '=', 2
    $name = $parts[0]
    $value = $parts[1]
    Set-Variable -Name $name -Value $value -Scope Script
}

# Cloud Build実行
$substitutions = "_GOOGLE_AI_API_KEY=***REDACTED***"

Write-Host "============================================"
Write-Host "1. Docker イメージをビルド"
Write-Host "============================================"

gcloud builds submit --region=asia-northeast1 --config=cloudbuild.yaml --substitutions=$substitutions

if ($LASTEXITCODE -ne 0) {
    Write-Error "ビルド失敗"
    exit 1
}

Write-Host "============================================"
Write-Host "2. Cloud Run にデプロイ"
Write-Host "============================================"

$PROJECT_ID = "consummate-yew-479020-u2"
$REGION = "asia-northeast1"
$SERVICE_NAME = "doc-processor"
$IMAGE_NAME = "$REGION-docker.pkg.dev/$PROJECT_ID/cloud-run-source-deploy/$SERVICE_NAME`:latest"
$SERVICE_ACCOUNT = "document-management-system@$PROJECT_ID.iam.gserviceaccount.com"

gcloud run deploy $SERVICE_NAME `
    --image $IMAGE_NAME `
    --region $REGION `
    --platform managed `
    --allow-unauthenticated `
    --service-account $SERVICE_ACCOUNT `
    --timeout 3600 `
    --memory 16Gi `
    --cpu 4 `
    --set-env-vars "GOOGLE_AI_API_KEY=***REDACTED***" `
    --set-env-vars "ANTHROPIC_API_KEY=***REDACTED***" `
    --set-env-vars "OPENAI_API_KEY=***REDACTED***" `
    --set-env-vars "SUPABASE_URL=$SUPABASE_URL" `
    --set-env-vars "SUPABASE_KEY=***REDACTED***" `
    --set-env-vars "SUPABASE_SERVICE_ROLE_KEY=***REDACTED***"

if ($LASTEXITCODE -eq 0) {
    Write-Host "============================================"
    Write-Host "✅ デプロイ完了"
    Write-Host "============================================"
    gcloud run services describe $SERVICE_NAME --region $REGION --format='value(status.url)'
}
```

### deploy_doc_processor.sh

```sh
#!/bin/bash

# ===================================================================
# doc-processor Cloud Run デプロイスクリプト
# ===================================================================

set -e  # エラー時に即終了

# プロジェクト設定
PROJECT_ID="consummate-yew-479020-u2"
REGION="asia-northeast1"
SERVICE_NAME="doc-processor"
IMAGE_NAME="${REGION}-docker.pkg.dev/${PROJECT_ID}/cloud-run-source-deploy/${SERVICE_NAME}:latest"
SERVICE_ACCOUNT="document-management-system@${PROJECT_ID}.iam.gserviceaccount.com"

# .envファイルから環境変数を読み込み
if [ ! -f ".env" ]; then
    echo "エラー: .envファイルが見つかりません"
    exit 1
fi

# .envファイルから環境変数を抽出（コメント、空行、日本語を含む行を除外）
set -a
source <(grep -E '^[A-Z_]+=.*' .env)
set +a

echo "============================================"
echo "1. Docker イメージをビルド"
echo "============================================"
gcloud builds submit \
    --region=${REGION} \
    --config=cloudbuild.yaml \
    --substitutions=_GOOGLE_AI_API_KEY="***REDACTED***",_ANTHROPIC_API_KEY="***REDACTED***",_OPENAI_API_KEY="***REDACTED***",_SUPABASE_URL="${SUPABASE_URL}",_SUPABASE_KEY="***REDACTED***",_SUPABASE_SERVICE_ROLE_KEY="***REDACTED***"

echo "============================================"
echo "2. Cloud Run にデプロイ"
echo "============================================"
gcloud run deploy ${SERVICE_NAME} \
    --image ${IMAGE_NAME} \
    --region ${REGION} \
    --platform managed \
    --allow-unauthenticated \
    --service-account ${SERVICE_ACCOUNT} \
    --timeout 3600 \
    --memory 16Gi \
    --cpu 4 \
    --set-env-vars "GOOGLE_AI_API_KEY=***REDACTED***" \
    --set-env-vars "ANTHROPIC_API_KEY=***REDACTED***" \
    --set-env-vars "OPENAI_API_KEY=***REDACTED***" \
    --set-env-vars "SUPABASE_URL=${SUPABASE_URL}" \
    --set-env-vars "SUPABASE_KEY=***REDACTED***" \
    --set-env-vars "SUPABASE_SERVICE_ROLE_KEY=***REDACTED***"

echo "============================================"
echo "✅ デプロイ完了"
echo "============================================"
gcloud run services describe ${SERVICE_NAME} --region ${REGION} --format='value(status.url)'
```

### deploy_now.ps1

```ps1
# Deploy to Cloud Run
Write-Host "Starting deployment..."
Set-Location C:\Users\ookub\document-management-system

# Load .env
Write-Host "Loading .env..."
Get-Content .env | ForEach-Object {
    if ($_ -match "^([^=]+)=(.*)$") {
        Set-Item -Path "Env:$($matches[1])" -Value $matches[2]
    }
}

Write-Host "SUPABASE_URL is: $env:SUPABASE_URL"

# Build
Write-Host "Building with Cloud Build..."
$result = gcloud builds submit --region=asia-northeast1 --config=cloudbuild.yaml --substitutions="_GOOGLE_AI_API_KEY=***REDACTED***" 2>&1
Write-Host $result

# Deploy
Write-Host "Deploying to Cloud Run..."
$result2 = gcloud run deploy doc-processor --image asia-northeast1-docker.pkg.dev/consummate-yew-479020-u2/cloud-run-source-deploy/doc-processor:latest --region asia-northeast1 --allow-unauthenticated --service-account document-management-system@consummate-yew-479020-u2.iam.gserviceaccount.com --timeout 3600 --memory 16Gi --cpu 4 2>&1
Write-Host $result2

Write-Host "Done!"
```

### docs\ARCHITECTURE.md

```md
# システムアーキテクチャ

統合ドキュメント処理・検索システムの技術詳細

---

## 目次

1. [システム全体像](#システム全体像)
2. [統合パイプライン（Stage E-K）](#統合パイプラインstage-e-k)
3. [Config-based設計](#config-based設計)
4. [データベーススキーマ](#データベーススキーマ)
5. [AI/MLモデル構成](#aimlモデル構成)
6. [コード構造](#コード構造)

---

## システム全体像

### アーキテクチャ図

```
┌──────────────────────────────────────────────────────────┐
│                  データソース層                           │
│  Google Drive │ Gmail │ Google Classroom │ ローカル      │
└────────────┬─────────────────────────────────────────────┘
             │
             ▼
┌──────────────────────────────────────────────────────────┐
│            services/data-ingestion (データ取り込み層)                  │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐         │
│  │   Drive    │  │   Gmail    │  │ Classroom  │         │
│  │ Connector  │  │ Connector  │  │ Connector  │         │
│  └────────────┘  └────────────┘  └────────────┘         │
│                                                          │
│  監視スクリプト: inbox_monitor.py                         │
└────────────┬─────────────────────────────────────────────┘
             │
             ▼
┌──────────────────────────────────────────────────────────┐
│         shared/pipeline (処理パイプライン層)            │
│                                                          │
│  Stage E → F → G → H → I → J → K                        │
│  (前処理～ベクトル化までの7段階)                           │
│                                                          │
│  ConfigLoader: models.yaml, prompts.yaml読み込み         │
└────────────┬─────────────────────────────────────────────┘
             │
             ▼
┌──────────────────────────────────────────────────────────┐
│                   データ永続化層                          │
│  Supabase (PostgreSQL + pgvector)                       │
│  ┌─────────────────────┐  ┌─────────────────────┐      │
│  │ Rawdata_FILE_AND_MAIL│  │   search_index      │      │
│  │ (メタデータ+Stage出力)│  │ (チャンク+ベクトル)  │      │
│  └─────────────────────┘  └─────────────────────┘      │
└────────────┬─────────────────────────────────────────────┘
             │
             ▼
┌──────────────────────────────────────────────────────────┐
│              services/doc-search (API層)                         │
│  Flask 3.0 REST API                                     │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐       │
│  │ /api/search│  │ /api/answer│  │ /api/health│       │
│  └────────────┘  └────────────┘  └────────────┘       │
└──────────────────────────────────────────────────────────┘
```

---

## 統合パイプライン（Stage E-K）

### 処理フロー

```
┌─────────────────────────────────────────────────────────┐
│  Stage E: Pre-processing (前処理)                        │
│  ─────────────────────────────────────                  │
│  • ファイルMIME判定                                      │
│  • 5つのPDFエンジンで並列抽出                             │
│    E1: PyPDF2, E2: pdfminer, E3: PyMuPDF               │
│    E4: pdfplumber, E5: 統合                            │
│  • 画像ファイルの識別                                    │
│  ─────────────────────────────────────                  │
│  出力: stage_e1_text ~ stage_e5_text                    │
└──────────────┬──────────────────────────────────────────┘
               │
               ▼
┌─────────────────────────────────────────────────────────┐
│  Stage F: Visual Analysis (Vision解析)                  │
│  ─────────────────────────────────────                  │
│  • Gemini 2.5 Flash/Pro でOCR                          │
│  • レイアウト情報抽出（見出し、表、リスト）                │
│  • 視覚要素の認識                                        │
│  ─────────────────────────────────────                  │
│  モデル: classroom=Flash, flyer=Pro                     │
│  出力: stage_f_text_ocr, stage_f_layout_ocr,           │
│        stage_f_visual_elements                         │
└──────────────┬──────────────────────────────────────────┘
               │
               ▼
┌─────────────────────────────────────────────────────────┐
│  Stage G: Text Formatting (テキスト整形)                 │
│  ─────────────────────────────────────                  │
│  • Gemini 2.5 Flash でMarkdown整形                      │
│  • 不要な空白・改行の除去                                │
│  • 表・リストの構造化                                    │
│  ─────────────────────────────────────                  │
│  モデル: Flash                                          │
│  出力: formatted_text (内部利用)                         │
└──────────────┬──────────────────────────────────────────┘
               │
               ▼
┌─────────────────────────────────────────────────────────┐
│  Stage H: Structuring (構造化)                          │
│  ─────────────────────────────────────                  │
│  • Gemini 2.5 Flash で高精度JSON抽出                    │
│  • メタデータ生成（日付、タグ、人物など）                 │
│  • doc_type別スキーマ適用                                │
│  ─────────────────────────────────────                  │
│  モデル: Flash                                          │
│  出力: stage_h_normalized, stage_i_structured           │
└──────────────┬──────────────────────────────────────────┘
               │
               ▼
┌─────────────────────────────────────────────────────────┐
│  Stage I: Synthesis (統合・要約)                         │
│  ─────────────────────────────────────────              │
│  • Gemini 2.5 Flash で要約生成                          │
│  • タグの自動生成                                        │
│  • Stage H の結果とマージ                                │
│  ─────────────────────────────────────                  │
│  モデル: Flash                                          │
│  出力: summary, tags (metadataにマージ)                 │
└──────────────┬──────────────────────────────────────────┘
               │
               ▼
┌─────────────────────────────────────────────────────────┐
│  Stage J: Chunking (チャンク化)                          │
│  ─────────────────────────────────────                  │
│  • MetadataChunker でチャンク分割                        │
│  • サイズ: 300-1000文字、オーバーラップ: 100文字         │
│  • メタデータ付与                                        │
│  ─────────────────────────────────────                  │
│  モデル: なし（ルールベース）                             │
│  出力: stage_j_chunks_json                              │
└──────────────┬──────────────────────────────────────────┘
               │
               ▼
┌─────────────────────────────────────────────────────────┐
│  Stage K: Embedding (ベクトル化)                         │
│  ─────────────────────────────────────                  │
│  • OpenAI text-embedding-3-small でベクトル化           │
│  • search_indexテーブルに保存                            │
│  • 既存チャンクの削除・更新                               │
│  ─────────────────────────────────────                  │
│  モデル: text-embedding-3-small (1536次元)              │
│  出力: search_index テーブル                            │
└─────────────────────────────────────────────────────────┘
```

### 重要な実装詳細

#### 1. ドキュメント更新方式（pipeline.py 398-418行目）

**現在の実装（UPDATE方式）:**

```python
if existing_document_id:
    logger.info(f"[DB更新] 既存ドキュメント更新: {existing_document_id}")
    update_data = {k: v for k, v in doc_data.items() if k != 'id'}
    result = self.db.client.table('Rawdata_FILE_AND_MAIL').update(update_data).eq('id', existing_document_id).execute()
```

**重要:** 以前のDELETE→INSERT方式は**ドキュメント消失のリスク**があったため、UPDATE方式に変更しました。これにより：
- `created_at`が保持される
- エラー時もレコードが残る
- トランザクション的に安全

#### 2. Stage出力の保存（pipeline.py 378-388行目）

全ステージの出力をデータベースに保存：

```python
doc_data = {
    # ... 基本情報 ...

    # Stage E: 前処理（5エンジン）
    'stage_e1_text': sanitized_extracted_text,  # PyPDF2（未実装、E4の値）
    'stage_e2_text': sanitized_extracted_text,  # pdfminer（未実装、E4の値）
    'stage_e3_text': sanitized_extracted_text,  # PyMuPDF（未実装、E4の値）
    'stage_e4_text': sanitized_extracted_text,  # pdfplumber/画像OCR
    'stage_e5_text': sanitized_extracted_text,  # 最終統合（現在はE4と同じ）

    # Stage F: Vision解析
    'stage_f_text_ocr': stage_f_text_ocr,
    'stage_f_layout_ocr': stage_f_layout_ocr,
    'stage_f_visual_elements': stage_f_visual_elements,

    # Stage H, I, J
    'stage_h_normalized': sanitized_combined_text,
    'stage_i_structured': json.dumps(stageH_result, ensure_ascii=False, indent=2),
    'stage_j_chunks_json': json.dumps(chunks, ensure_ascii=False, indent=2)
}
```

**目的:**
- デバッグ時に各ステージの出力を確認可能
- ステージ間の品質チェック
- 将来的なステージの改善に活用

---

## Config-based設計

### 設計思想

コードとプロンプト・モデル設定を分離し、**コード変更なしで動作を調整可能**にする。

### 3つの設定ファイル

#### 1. models.yaml

各ステージで使用するAIモデルを定義。

**ファイルパス:** `shared/pipeline/config/models.yaml`

```yaml
models:
  # Stage F: Visual Analysis (視覚解析)
  # flyer: 商品写真の視覚理解が重要なため Pro を使用
  # classroom: 日常的なお知らせの正確なOCRが目的のため Flash で十分
  stage_f:
    default: "gemini-2.5-flash"
    flyer: "gemini-2.5-pro"
    classroom: "gemini-2.5-flash"
    flash_lite: "gemini-2.5-flash-lite"  # 家計簿用（最軽量）

  # Stage G: Text Formatting (書式整形)
  stage_g:
    default: "gemini-2.5-flash"
    flyer: "gemini-2.5-pro"
    classroom: "gemini-2.5-flash"
    flash_lite: "gemini-2.5-flash-lite"

  # Stage H: Structuring (構造化)
  stage_h:
    default: "gemini-2.5-flash"
    flyer: "gemini-2.5-flash"
    classroom: "gemini-2.5-flash"
    flash_lite: "gemini-2.5-flash-lite"

  # Stage I: Synthesis (統合・要約)
  stage_i:
    default: "gemini-2.5-flash"
    flyer: "gemini-2.5-flash"
    classroom: "gemini-2.5-flash"

  # Stage K: Embedding (ベクトル化)
  stage_k:
    default: "text-embedding-3-small"
    flyer: "text-embedding-3-small"
    classroom: "text-embedding-3-small"
```

**特徴:**
- `flyer` だけ Stage F で Pro を使用（商品写真の視覚理解が重要）
- `classroom` は Flash（コスト効率重視）
- 将来的に新しいモデル（Gemini 2.5 Pro など）への切り替えが容易

#### 2. pipeline_routing.yaml

workspace と doc_type に基づいてプロンプト・モデルをルーティング。

**ファイルパス:** `shared/pipeline/config/pipeline_routing.yaml`

```yaml
routing:
  # workspace ベースのルート（優先順位1）
  by_workspace:
    ikuya_classroom:
      description: "育哉のClassroomワークスペース"
      schema: "classroom"
      stages:
        stage_f:
          prompt_key: "classroom"
          model_key: "classroom"
        stage_h:
          prompt_key: "classroom"
          model_key: "classroom"

  # doc_type ベースのルート（優先順位2）
  by_doc_type:
    physical_shop:
      description: "実店舗のチラシ"
      schema: "flyer"
      stages:
        stage_f:
          prompt_key: "flyer"
          model_key: "flyer"
```

**ルーティング優先順位:**
1. workspace（最優先）
2. doc_type
3. default（フォールバック）

#### 3. prompts.yaml

全ステージのプロンプトを一元管理（**15個のMDファイルを統合**）。

**ファイルパス:** `shared/pipeline/config/prompts.yaml`

**構造:**

```yaml
prompts:
  stage_f:
    classroom: |
      あなたはGoogle Classroom課題ドキュメントの視覚解析を専門とするAIアシスタントです。

      Stage E で抽出したテキストを基準として、画像を詳細に見て、完璧な3つの情報を作成してください。
      ...

    default: |
      あなたはドキュメントから視覚情報を抽出する専門家です。
      ...

    flyer: |
      あなたはスーパーマーケットのチラシから視覚情報を抽出する専門家です。
      ...

  stage_g:
    classroom: |
      ...

  stage_h:
    classroom: |
      ...
```

**特徴:**
- **統合前:** 15個の個別MDファイル（`prompts/stage_f/stage_f_classroom.md` など）
- **統合後:** 1つのYAMLファイルに集約
- YAMLのリテラルブロックスタイル（`|`）で複数行テキストを保持
- config_loader.py で自動読み込み

**読み込み処理（config_loader.py 39-47行目）:**

```python
# プロンプト設定を読み込み
prompts_file = self.config_dir / "prompts.yaml"
if prompts_file.exists():
    prompts_data = self._load_yaml(prompts_file)
    self.prompts_config = prompts_data.get('prompts', {})
    logger.info(f"✅ prompts.yaml を読み込みました")
else:
    self.prompts_config = {}
    logger.warning(f"⚠️ prompts.yaml が見つかりません。MDファイルから読み込みます")
```

**プロンプト取得処理（config_loader.py 101-129行目）:**

```python
def get_prompt(self, stage: str, prompt_key: str) -> str:
    # prompts.yaml から読み込み
    if self.prompts_config and stage in self.prompts_config:
        if prompt_key in self.prompts_config[stage]:
            prompt = self.prompts_config[stage][prompt_key]
            logger.debug(f"プロンプト読み込み: {stage}/{prompt_key} ({len(prompt)}文字)")
            return prompt

    # フォールバック: MDファイルから読み込み（後方互換性）
    prompt_file = self.config_dir / "prompts" / stage / f"{stage}_{prompt_key}.md"
    ...
```

---

## データベーススキーマ

### テーブル構造

#### 1. Rawdata_FILE_AND_MAIL

ドキュメントのメタデータと全ステージの処理結果を保存。

**主要カラム:**

```sql
CREATE TABLE "Rawdata_FILE_AND_MAIL" (
    id UUID PRIMARY KEY,
    file_name TEXT,
    source_id TEXT,
    workspace TEXT,
    doc_type TEXT,

    -- Stage E 出力（5エンジン）
    stage_e1_text TEXT,      -- PyPDF2（未実装、E4の値）
    stage_e2_text TEXT,      -- pdfminer（未実装、E4の値）
    stage_e3_text TEXT,      -- PyMuPDF（未実装、E4の値）
    stage_e4_text TEXT,      -- pdfplumber/画像OCR
    stage_e5_text TEXT,      -- 最終統合

    -- Stage F 出力（Vision解析）
    stage_f_text_ocr TEXT,
    stage_f_layout_ocr TEXT,
    stage_f_visual_elements TEXT,

    -- Stage H, I, J 出力
    stage_h_normalized TEXT,
    stage_i_structured TEXT,
    stage_j_chunks_json JSONB,

    -- メタデータ
    metadata JSONB,
    tags TEXT[],

    -- タイムスタンプ
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

**インデックス:**

```sql
CREATE INDEX idx_workspace ON "Rawdata_FILE_AND_MAIL"(workspace);
CREATE INDEX idx_doc_type ON "Rawdata_FILE_AND_MAIL"(doc_type);
CREATE INDEX idx_tags ON "Rawdata_FILE_AND_MAIL" USING GIN(tags);
```

#### 2. search_index

検索用チャンクとベクトル埋め込みを保存。

```sql
CREATE TABLE search_index (
    id UUID PRIMARY KEY,
    document_id UUID REFERENCES "Rawdata_FILE_AND_MAIL"(id) ON DELETE CASCADE,
    chunk_index INTEGER,
    chunk_text TEXT,
    embedding VECTOR(1536),  -- OpenAI text-embedding-3-small
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ベクトル検索用インデックス（IVFFlat）
CREATE INDEX ON search_index USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
```

**ベクトル検索関数:**

```sql
CREATE OR REPLACE FUNCTION match_documents(
    query_embedding VECTOR(1536),
    match_count INT DEFAULT 5,
    filter JSONB DEFAULT '{}'
) RETURNS TABLE(...) AS $$
    SELECT ...
    FROM search_index
    ORDER BY embedding <=> query_embedding
    LIMIT match_count;
$$ LANGUAGE sql;
```

---

## AI/MLモデル構成

### 使用モデル一覧

| ステージ | デフォルト | flyer | classroom | 用途 |
|---------|-----------|-------|-----------|------|
| Stage F | Gemini 2.5 Flash | **Gemini 2.5 Pro** | Gemini 2.5 Flash | Vision解析 |
| Stage G | Gemini 2.5 Flash | Gemini 2.5 Pro | Gemini 2.5 Flash | テキスト整形 |
| Stage H | Gemini 2.5 Flash | Gemini 2.5 Flash | Gemini 2.5 Flash | 構造化 |
| Stage I | Gemini 2.5 Flash | Gemini 2.5 Flash | Gemini 2.5 Flash | 要約 |
| Stage K | OpenAI text-embedding-3-small | 同左 | 同左 | ベクトル化 |

**モデル選択の理由:**

- **Gemini 2.5 Flash**: 高速・安価、JSON生成に優れる
- **Gemini 2.5 Pro** (flyerのみ): 商品写真の視覚理解が重要
- **OpenAI Embeddings**: 検索精度が高い、pgvectorとの親和性

### コスト最適化

- **classroom**: Flash で十分（テキスト中心のお知らせ）
- **flyer**: Pro を使用（商品写真の詳細な視覚理解が必要）
- 複雑な図表は別パイプラインで処理（この統一パイプラインは日常的なお知らせ処理用）

---

## コード構造

### 主要モジュール

#### 1. UnifiedDocumentPipeline (pipeline.py)

パイプライン全体を統括。

**重要メソッド:**

```python
class UnifiedDocumentPipeline:
    async def process_document(
        self,
        file_path: Path,
        source_id: str,
        file_name: str,
        workspace: str,
        doc_type: str,
        existing_document_id: Optional[str] = None
    ) -> Dict[str, Any]:
        # ConfigLoaderでルート取得
        route_config = self.config_loader.get_route_config(
            doc_type=doc_type,
            workspace=workspace
        )

        # Stage E-K を順次実行
        stage_e_result = self.stage_e.process(...)
        stage_f_result = await self.stage_f.process(...)
        ...

        # UPDATE方式でDB保存（DELETE→INSERT ではない）
        if existing_document_id:
            update_data = {k: v for k, v in doc_data.items() if k != 'id'}
            result = self.db.client.table('Rawdata_FILE_AND_MAIL').update(update_data).eq('id', existing_document_id).execute()
```

#### 2. ConfigLoader (config_loader.py)

YAML設定を読み込み、ルーティング処理。

**重要メソッド:**

```python
class ConfigLoader:
    def __init__(self, config_dir: Optional[Path] = None):
        self.models_config = self._load_yaml(self.config_dir / "models.yaml")
        self.routes_config = self._load_yaml(self.config_dir / "pipeline_routing.yaml")

        # prompts.yaml を読み込み
        prompts_file = self.config_dir / "prompts.yaml"
        if prompts_file.exists():
            prompts_data = self._load_yaml(prompts_file)
            self.prompts_config = prompts_data.get('prompts', {})

    def get_prompt(self, stage: str, prompt_key: str) -> str:
        # prompts.yaml から読み込み
        if self.prompts_config and stage in self.prompts_config:
            if prompt_key in self.prompts_config[stage]:
                return self.prompts_config[stage][prompt_key]

        # フォールバック: MDファイルから読み込み
        ...
```

---

## まとめ

**システムの特徴:**

✅ **マルチソース対応**: Drive/Gmail/Classroom から自動取り込み
✅ **7段階処理**: Stage E-K による高品質な処理
✅ **Config-based設計**: コード変更なしで調整可能
✅ **柔軟なモデル選択**: doc_type/workspace ごとに最適なモデルを使用
✅ **完全なトレーサビリティ**: 全ステージ出力をDBに保存
✅ **安全な更新**: UPDATE方式（DELETE→INSERT ではない）

**再構築に必要な情報:**

このドキュメントと README.md があれば、システム全体を再構築可能です。
- 設定ファイル3つ（models.yaml, pipeline_routing.yaml, prompts.yaml）
- データベーススキーマ（migrations/add_stage_output_columns.sql）
- コード構造（shared/common, shared/pipeline, services/doc-search）
- 認証情報（.env, google_credentials.json）

詳細なセットアップ手順は [README.md](README.md) を参照してください。
```

### docs\CLAUDE_CHANGES_LOG.md

```md
# Claude 修正ログ

このファイルはClaudeが行った修正を記録します。

---

## 2025-01-11: Cloud Run起動エラー修正

### セッション開始時の問題
- Cloud Runでコンテナが起動しない（ポート8080でリッスンしない）

### 行った修正

#### 1. start.sh 改行コード修正
- **ファイル:** `services/doc-processor/start.sh`
- **問題:** Windows形式の改行（CRLF）だったため、Linuxで `#!/bin/bash\r` と解釈され実行不可
- **修正:** Unix形式（LF）に変換
- **コマンド:** `sed -i 's/\r$//' start.sh`

#### 2. shared/__init__.py 作成
- **ファイル:** `shared/__init__.py`
- **問題:** Pythonパッケージとして認識されない可能性
- **修正:** 空の `__init__.py` を作成

#### 3. Dockerfile 修正
- **ファイル:** `services/doc-processor/Dockerfile`
- **問題:** `process_queued_documents.py` が `/app/scripts/processing/` にコピーされるが、app.pyは `/app/process_queued_documents.py` をインポートしようとする
- **修正:** 以下を追加
```dockerfile
COPY shared/__init__.py ./shared/
COPY scripts/processing/process_queued_documents.py .
```

#### 4. settings.py 変更→元に戻し
- **ファイル:** `shared/common/config/settings.py`
- **問題:** 私が `load_dotenv(override=True)` を不要に変更して壊した
- **修正:** 元に戻した
```python
# 元のコード（正しい）
load_dotenv(override=True)
```

### 未修正の問題

#### app.py 308行目 - max_parallel デフォルト値
- **ファイル:** `services/doc-processor/app.py`
- **行:** 308
- **現状:** `'max_parallel': lock_data.get('max_parallel', 10),`
- **期待:** デフォルト値を30に変更する必要がある可能性
- **ステータス:** 未修正（ユーザー確認待ち）

---

## 2025-01-11: リアルタイム表示の初期値表示修正

### 行った修正

#### 1. processing.html - ページ読み込み時の即座データ取得
- **ファイル:** `services/doc-processor/templates/processing.html`
- **行:** 795-797
- **問題:** ページ読み込み時、startPolling()を呼ぶだけでupdateProgress()の初回実行まで1秒待っていた。そのため、ページを開いた瞬間は「-」や「待機中」のまま表示され、リアルタイムに見えなかった
- **修正:** DOMContentLoadedで即座にupdateProgress()を呼び出すように変更
```javascript
// 修正前
document.addEventListener('DOMContentLoaded', function() {
    startPolling();
});

// 修正後
document.addEventListener('DOMContentLoaded', function() {
    updateProgress();  // 即座に初回取得
    startPolling();    // その後1秒ごとに更新
});
```

### デプロイ要否
- **必要**: Cloud Runに再デプロイが必要

---

## 2026-01-11: リアルタイム表示のデバッグログ追加

### 問題
- CPU使用率とメモリ使用率が画面に表示されない
- データはSupabaseから取得できているが、画面が更新されない

### 行った修正

#### 1. processing.html - デバッグログ追加
- **ファイル:** `services/doc-processor/templates/processing.html`
- **行:** 538-550
- **問題:** `cpu_percent`, `memory_percent` が `null` または `undefined` の場合、画面が更新されない
- **修正:** コンソールログを追加して、実際に何が返ってきているか確認できるようにした
```javascript
console.log('[DEBUG] cpu_percent:', data.cpu_percent, 'memory_percent:', data.memory_percent);
if (data.cpu_percent !== null && data.cpu_percent !== undefined) {
    cpuUsage.textContent = data.cpu_percent + '%';
} else {
    console.warn('[WARNING] cpu_percent is null/undefined');
}
```

### デプロイ要否
- **必要**: Cloud Runに再デプロイが必要（実行中）

---

## 2026-01-11: monitor_resources タスク診断ログ追加

### 問題
- CPU/Memory の値がSupabaseで更新されない（フロントエンドで同じ値が表示され続ける）
- `monitor_resources()` async タスクが実行されていない可能性

### 行った修正

#### 1. monitor_resources タスク作成時のエラーハンドリング追加
- **ファイル:** `services/doc-processor/app.py` (lines 1156-1164)
- **問題:** タスク作成時のエラーが silent fail する可能性
- **修正:** タスク作成を try-catch で囲み、ログを追加
```python
try:
    monitor_task = asyncio.create_task(monitor_resources())
    logger.info(f"[MONITOR] タスク作成成功: {monitor_task}")
    await asyncio.sleep(0)  # イベントループにタスクを実行させる
    logger.info(f"[MONITOR] タスク状態確認: done={monitor_task.done()}, cancelled={monitor_task.cancelled()}")
except Exception as e:
    logger.error(f"[MONITOR] タスク作成失敗: {e}", exc_info=True)
    raise
```

#### 2. monitor_resources タスク終了時のエラーハンドリング追加
- **ファイル:** `services/doc-processor/app.py` (lines 1199-1207)
- **問題:** タスク終了時のエラーが silent fail する可能性
- **修正:** await monitor_task を try-catch で囲み、ログを追加
```python
logger.info("[MONITOR] タスク停止待機中...")
try:
    await monitor_task
    logger.info("[MONITOR] タスク正常終了")
except Exception as e:
    logger.error(f"[MONITOR] タスク終了時エラー: {e}", exc_info=True)
```

### デプロイ要否
- **必要**: Cloud Runに再デプロイが必要

---

## 2026-01-11: monitor_resources タスク実行問題の修正（根本原因）

### 問題
- monitor_resources()タスクが1回だけ実行されて停止
- 原因: forループ内でイベントループに制御が渡されていない
- process_single_document()はcreate_taskで並列実行されるため、forループ自体はawaitしない
- 並列数が上限に達していない場合、asyncio.sleep(0.1)が呼ばれず、monitor_resources()が実行されない

### 行った修正

#### 1. forループ内でイベントループに制御を渡す
- **ファイル:** `services/doc-processor/app.py` (line 1198-1199)
- **問題:** 新しいタスク作成後、イベントループに制御が渡されないため、monitor_resources()が実行されない
- **修正:** 各ドキュメント処理開始後に `await asyncio.sleep(0)` を追加
```python
# イベントループに制御を渡す（monitor_resourcesタスクが実行されるように）
await asyncio.sleep(0)
```

### デプロイ要否
- **必要**: Cloud Runに再デプロイが必要

---

## 2026-01-11: monitor_resources デバッグログ詳細化

### 問題
- monitor_resources()が1回だけ実行されて停止する問題が続いている
- whileループがどこで止まっているのか不明

### 行った修正

#### 1. monitor_resources whileループ内に詳細ログ追加
- **ファイル:** `services/doc-processor/app.py` (lines 1040-1081)
- **問題:** whileループの実行状況が見えない
- **修正:** ループ開始/終了/スリープ前後に詳細ログを追加
```python
loop_count = 0
while processing_status['is_processing']:
    loop_count += 1
    logger.info(f"[monitor_resources] ループ#{loop_count} 開始 (is_processing={processing_status['is_processing']})")
    ...
    logger.info(f"[monitor_resources] ループ#{loop_count} 完了、2秒スリープ開始")
    await asyncio.sleep(2)
    logger.info(f"[monitor_resources] ループ#{loop_count} スリープ完了、次のループへ")
logger.info(f"[monitor_resources] タスク終了 (is_processing={processing_status['is_processing']}, total_loops={loop_count})")
```

### デプロイ要否
- **必要**: Cloud Runに再デプロイが必要

---

## 2026-01-11: monitor_resources タスク削除、process_single_document内でリソース更新

### 問題
- monitor_resources()タスクが`await asyncio.sleep(2)`から制御を受け取れない
- イベントループの問題により、2秒ごとの更新が機能しない
- 根本原因: 別スレッドのasyncio.run()内で、monitor_resourcesタスクに制御が戻らない

### 行った修正

#### 1. monitor_resources()タスクの作成と待機を削除
- **ファイル:** `services/doc-processor/app.py` (lines 1172-1216)
- **問題:** monitor_resources()タスクがイベントループから制御を受け取れない
- **修正:** タスクの作成と待機コードを削除

#### 2. process_single_document()内でリソース情報を直接更新
- **ファイル:** `services/doc-processor/app.py` (lines 1114-1132)
- **問題:** リソース情報（CPU/Memory）がSupabaseに更新されない
- **修正:** ドキュメント処理開始時にリソース情報を取得してSupabaseに保存
```python
# リソース情報を取得してSupabaseに進捗を保存
memory_info = get_cgroup_memory()
memory_percent = memory_info['percent']
worker_status = get_worker_status()
current_workers = worker_status['current_workers']

# リソース調整
status = resource_manager.adjust_resources(memory_percent, current_workers)
processing_status['resource_control']['max_parallel'] = resource_manager.max_parallel
processing_status['resource_control']['throttle_delay'] = status['throttle_delay']
processing_status['resource_control']['adjustment_count'] = resource_manager.adjustment_count

# Supabaseに進捗を保存（リソース情報も含める）
update_progress_to_supabase(...)
```

### メリット
- イベントループの問題を回避
- ドキュメント処理のたびにリソース情報が更新される（2秒間隔ではなく、処理開始時）
- よりシンプルで確実な実装

### デプロイ要否
- **必要**: Cloud Runに再デプロイが必要

---

## 2026-01-11: threading.Timerを使った定期更新に変更（根本的解決）

### 問題
- 前回の修正では、ドキュメント処理開始時のみ更新されるため、処理が10分かかる場合は10分間更新されない
- asyncio.sleep()を使ったアプローチはイベントループの問題で機能しない

### 行った修正

#### 1. threading.Timerを使った定期更新を実装
- **ファイル:** `services/doc-processor/app.py` (lines 1024-1054)
- **問題:** asyncioの問題を完全に回避する必要がある
- **修正:** 別スレッドで2秒ごとにupdate_progress_to_supabase()を呼び出すTimerを実装
```python
def periodic_resource_update():
    """2秒ごとにリソース情報を更新（別スレッドで実行）"""
    if processing_status['is_processing']:
        try:
            logger.info("[PERIODIC_UPDATE] リソース情報更新開始")
            update_progress_to_supabase(...)
            logger.info("[PERIODIC_UPDATE] リソース情報更新完了")
        except Exception as e:
            logger.error(f"[PERIODIC_UPDATE] エラー: {e}", exc_info=True)

        # 次の実行をスケジュール
        update_timer = threading.Timer(2.0, periodic_resource_update)
        update_timer.daemon = True
        update_timer.start()

# 定期更新を開始
update_timer = threading.Timer(2.0, periodic_resource_update)
update_timer.daemon = True
update_timer.start()
```

#### 2. process_single_document()内のSupabase更新を削除
- **ファイル:** `services/doc-processor/app.py` (lines 1149-1162)
- **問題:** 定期更新と重複するため、処理開始時の更新は不要
- **修正:** リソース調整のみ実行し、Supabase更新は削除（定期更新で行う）

### メリット
- asyncioの問題を完全に回避
- ドキュメント処理時間に関係なく、2秒ごとに確実に更新される
- よりシンプルで確実な実装

### デプロイ要否
- **必要**: Cloud Runに再デプロイが必要

---

## 2026-01-11: threading.Timer明示的停止処理を追加

### 問題
- 前回の実装では、処理終了後もタイマーが動き続ける可能性がある
- タイマーを明示的にキャンセルする処理が不足

### 行った修正

#### 1. finally節でタイマーを明示的にキャンセル
- **ファイル:** `services/doc-processor/app.py` (lines 1226-1231)
- **問題:** 処理終了後もタイマーが動き続ける
- **修正:** 処理完了時にupdate_timer.cancel()を呼び出す
```python
finally:
    # 定期更新タイマーを停止
    processing_status['is_processing'] = False  # タイマーのループ条件をFalseに
    if update_timer is not None:
        update_timer.cancel()
        logger.info("[PERIODIC_UPDATE] タイマーをキャンセルしました")
```

### メリット
- 処理完了後、タイマーが確実に停止する
- リソースリークを防止

### デプロイ要否
- **必要**: Cloud Runに再デプロイが必要

---

## 2026-01-12: イベント駆動型進捗更新の実装（E4, E5, F全工程）

### 問題
- タイマー方式（threading.Timer）は複雑で終了判定が難しい
- ユーザーから「処理をトリガーにする」という提案があった
- E4, E5, F-1～F-10の各工程開始時に進捗を更新することで、自然なライフサイクルを実現

### 行った修正

#### 1. Stage E (前処理) にコールバック追加
- **ファイル:** `shared/pipeline/stage_e_preprocessing.py`, `shared/common/processors/pdf.py`
- **問題:** E1のみコールバックがあり、E4とE5がなかった
- **修正:**
  - `extract_text()` に `progress_callback` パラメータを追加
  - `_process_office_with_stages()` の E4 (line 172) と E5 (line 217) にコールバック追加
  - `_process_image_with_stages()` の E4 (line 288) と E5 (line 332) にコールバック追加
  - `pdf.py` の `extract_text()` にパラメータ追加、E4 (line 108) と E5 (line 127) にコールバック追加
  - `pipeline.py` から Stage E へのコールバック渡し (line 147)

#### 2. Stage F (視覚解析) に全工程コールバック追加
- **ファイル:** `shared/pipeline/stage_f_visual.py`
- **問題:** Fは重い処理なので、10個の工程を細かく報告する必要があった
- **修正:** `process()` メソッドに全工程のコールバックを追加
  - F-1: PaddleOCR表抽出 (line 216)
  - F-2: Suryaレイアウト解析 (line 250)
  - F-3: 画像切り出し (line 327)
  - F-4: PaddleOCRテキスト認識 (line 377)
  - F-5: テキスト統合 (line 477)
  - F-6: プロンプト構築 (line 508)
  - F-7: Gemini Vision API呼び出し (line 593)
  - F-8: JSONクリーニング (line 667)
  - F-9: 全結果マージ (line 693)
  - F-10: 最終検証・出力 (line 769)
  - `pipeline.py` から Stage F へのコールバック渡し (line 176)

#### 3. 既存のコールバック構造
- **ファイル:** `services/doc-processor/app.py`, `scripts/processing/process_queued_documents.py`, `shared/pipeline/pipeline.py`
- **確認:** 既に E1, F, H, I, J, K のコールバックは実装済み
- **今回の追加:** E4, E5, F-1～F-10 を追加することで完全なカバレッジを実現

### メリット
- イベント駆動型なので、処理開始と同時に進捗が更新される
- タイマーが不要になり、終了判定が自然（処理が終われば自動的に止まる）
- 各ステージの進捗が細かく報告されるため、ユーザーがリアルタイムで状況を把握できる
- F工程が特に重いため、10個の工程に分けることで詳細な進捗が見える

### デプロイ要否
- **必要**: Cloud Runに再デプロイが必要

---

## 2026-01-12: 重複変数の統一（19個→3個）

### 問題
- 同じ情報を表す変数が複数存在し、管理が複雑化
- 実行数を表す変数が6個、最大並列数が4個、メモリ/CPU関連が9個存在
- データの不整合が発生しやすく、デバッグが困難

### 行った修正

#### 1. 実行数関連の変数統一（6個→1個）
- **ファイル:** `services/doc-processor/app.py`
- **問題:** 実行数を表す変数が6個存在
  1. `active_tasks`（インメモリリスト）
  2. `processing_workers`テーブル（DB）
  3. `processing_lock.current_workers`（DB）
  4. `processing_status['resource_control']['current_parallel']`（dict）
  5. `worker_status['current_workers']`（関数戻り値）
  6. `actual_workers`（ローカル変数）
- **修正:**
  - **唯一の真実のソース:** `active_tasks`（インメモリリスト）
  - `register_worker()`, `unregister_worker()` - DB操作を削除、互換性のため関数は残す
  - `clear_all_workers()` - `active_tasks.clear()`のみ実行
  - `reset_stuck_documents()` - `active_tasks`から処理中のdoc_idを取得
  - `update_worker_count()` - `len(active_tasks)`を返すのみ（DB更新削除）
  - `update_progress_to_supabase()` - `len(active_tasks)`で直接取得
  - `get_worker_status()` - `active_tasks`から`current_workers`と`workers`を生成
  - `processing_status['resource_control']['current_parallel']` - 削除

#### 2. 最大並列数関連の変数統一（4個→1個）
- **ファイル:** `services/doc-processor/app.py`
- **問題:** 最大並列数を表す変数が4個存在
  1. `resource_manager.max_parallel`（クラス内）
  2. `processing_lock.max_parallel`（DB）
  3. `processing_status['resource_control']['max_parallel']`（dict）
  4. `worker_status['max_parallel']`（関数戻り値）
- **修正:**
  - **唯一の真実のソース:** `resource_manager.max_parallel`
  - `processing_status['resource_control']['max_parallel']` - 削除（各所で代入削除）
  - `adjust_max_parallel()` - `resource_manager.max_parallel`のみ使用
  - `can_start_new_worker()` - `resource_manager.max_parallel`から取得
  - `get_worker_status()` - `resource_manager.max_parallel`から取得
  - `processing_lock.max_parallel` - DB保存は継続（表示用）

#### 3. メモリ/CPU関連の変数
- **ファイル:** `services/doc-processor/app.py`
- **状態:** 既に最適化済み
- **確認:** 各変数は適切な目的で使用されており、重複なし
  - `memory_info` - `get_cgroup_memory()`の戻り値（一時変数）
  - `memory_percent` - ローカル変数（計算用）
  - `mem_before`, `mem_after` - ドキュメント処理前後のメモリ計測用
  - `processing_status`内のmemory系 - 表示用
  - `progress['memory_*']` - Supabase保存用
  - `resource_manager.memory_history` - リソース調整の履歴

### 結果
- **統一前:** 19個の重複変数
- **統一後:** 3個の真実のソース（`active_tasks`, `resource_manager.max_parallel`, メモリ/CPU直接取得）
- 構文エラー: なし
- データの整合性が大幅に向上
- 管理が単純化され、デバッグが容易に

### メリット
- Single Source of Truth (SSOT) の実現
- データ不整合のリスクが大幅に減少
- コードの可読性と保守性が向上
- DB書き込み回数が減少し、パフォーマンス向上

### デプロイ要否
- **必要**: Cloud Runに再デプロイが必要

---

## 2026-01-12: 処理終了時のSupabase状態同期修正

### 問題
- 処理完了・異常終了時に `processing_status['is_processing'] = False` だけ設定
- Supabaseの `is_processing` は更新されずtrueのまま
- 停止ボタンを押しても「実行中の処理がありません」エラー
- 画面では `is_processing: true` のまま表示され続ける

### 行った修正

#### 1. finally ブロックでSupabaseも更新
- **ファイル:** `services/doc-processor/app.py` (lines 1223-1227, 1254-1260)
- **問題:** 処理完了時・異常終了時にローカル状態のみ更新、Supabaseは未更新
- **修正:** 両方の`finally`ブロックに `set_processing_lock(False)` を追加
```python
finally:
    processing_status['is_processing'] = False
    set_processing_lock(False)  # Supabaseも更新
    logger.info("[PROCESS] 全ドキュメント処理完了")
```

#### 2. stop_processing()のチェックロジック修正
- **ファイル:** `services/doc-processor/app.py` (lines 1301-1310)
- **問題:** ローカルとSupabase両方がfalseの場合のみエラーを返すロジック
- **修正:** Supabaseのみをチェック（ローカルは別インスタンスの可能性があるため）
```python
# Supabaseのロックをチェック（ローカルは別インスタンスの可能性があるので見ない）
if not get_processing_lock():
    return jsonify({
        'success': False,
        'error': '実行中の処理がありません'
    }), 400
```

### メリット
- 処理終了時に確実にSupabaseの状態もクリアされる
- 停止ボタンが正常に機能する
- 複数インスタンス環境でも正しく動作

### デプロイ要否
- **必要**: Cloud Runに再デプロイが必要

---

## テンプレート（今後の修正記録用）

### YYYY-MM-DD: [修正の概要]

#### 行った修正

##### 1. [修正名]
- **ファイル:**
- **問題:**
- **修正:**

#### 未修正の問題
-

---
```

### docs\CLAUDE_MISTAKES_LOG.md

```md
# Claude ミスログ

このファイルはClaudeが犯したミスを記録し、再発防止のために使用します。

---

## 2025-01-11: Cloud Runデプロイ時の環境変数消失

### 何が起きたか
- Cloud Runのコンテナ起動エラー（ポート8080でリッスンしない）を修正しようとした
- 修正後、ユーザーに再デプロイコマンドを提供した
- そのコマンドに `--set-env-vars "$env:SUPABASE_URL..."` が含まれていた
- PowerShellで環境変数が未設定だったため、空の値でCloud Runの既存環境変数を上書きしてしまった
- 結果：元々動いていたサービスが「SUPABASE_URLが設定されていません」エラーで動作しなくなった

### なぜミスしたか
1. **確認不足**: ユーザーのPowerShell環境に環境変数が設定されているか確認せずにコマンドを提供した
2. **破壊的コマンドの認識不足**: `--set-env-vars` は既存の環境変数を完全に上書きする破壊的な操作だと認識していなかった
3. **動作確認の欠如**: コマンドを提供する前に、そのコマンドが何を行うか十分に検証しなかった

### 今後どうするか
1. **環境変数を含むデプロイコマンドを提供する前に、ユーザーに環境変数が設定されているか確認を促す**
2. **既存の設定を上書きする可能性があるコマンドには警告を付ける**
3. **イメージのみ更新する場合は `--set-env-vars` を省略し、既存の環境変数を保持する方法を提案する**

### 正しいアプローチ
```powershell
# 環境変数を変更せずイメージのみ更新する場合
gcloud run deploy doc-processor --image <image> --region asia-northeast1

# 環境変数も設定する場合は、事前に確認
echo $env:SUPABASE_URL  # 空でないことを確認してから実行
```

---

## 2025-01-11: 推測による修正の繰り返し

### 何が起きたか
- Cloud Runのコンテナ起動エラーを解決しようとした
- ログを確認せずに推測で修正を試みた
- 1つ修正 → デプロイ → エラー → また1つ修正 → デプロイ... を繰り返した
- ユーザーに何度もデプロイ作業を強いた

### なぜミスしたか
1. **根本原因を特定せずに修正を開始した**: ログやエラーメッセージを十分に分析しなかった
2. **1つずつ小出しにした**: 全ての潜在的問題を一度に洗い出さなかった
3. **確証なく「これで解決する」と宣言した**

### 今後どうするか
1. **修正前に必ず根本原因を特定する**（ログ確認、コード追跡）
2. **全ての潜在的問題を一度にリストアップしてから修正する**
3. **「確実に解決する」とは言わない。代わりに「これらの問題を修正しました。他に問題があれば教えてください」と言う**

---

## 2025-01-11: settings.py の不要な変更

### 何が起きたか
- `load_dotenv(override=True)` を `if os.path.exists('.env'): load_dotenv(override=False)` に変更した
- これにより、ローカル環境で.envファイルが読み込まれなくなった
- 結果：「SUPABASE_URLが設定されていません」エラーが発生

### なぜミスしたか
1. **推測で修正した**: `load_dotenv(override=True)` が問題だと推測したが、実際は問題ではなかった
2. **load_dotenvの動作を誤解していた**: .envファイルが存在しない場合は何もしないことを理解していなかった
3. **動いているコードを不必要に変更した**

### 今後どうするか
1. **動いているコードは触らない**（問題の原因でないなら）
2. **ライブラリの動作を確認してから修正する**
3. **推測ではなく、エラーメッセージやログに基づいて修正する**

---

## 2025-01-11: 修正したと言ってデプロイしなかった

### 何が起きたか
- 並列数制御のコード修正（current_workers >= self.max_parallel への変更など）を行った
- 「修正しました」「完璧です」と言った
- しかしCloud Runへのデプロイ指示を出さなかった
- ユーザーが複数PCでハードリフレッシュしても何も変わらず
- 結果：ユーザーは修正が反映されたと思っていたが、実際は古いコードのまま動いていた

### なぜミスしたか
1. **ローカル修正とデプロイを混同した**: ファイルを修正しただけで完了と勘違いした
2. **Cloud Run環境であることを忘れた**: ローカル開発のように即座に反映されると思い込んだ
3. **最後まで責任を持たなかった**: 修正 → ビルド → デプロイ → 動作確認 の全工程を追わなかった

### 今後どうするか
1. **修正後は必ずデプロイ指示を出す**
2. **「修正しました」ではなく「修正しました。反映するには再デプロイが必要です」と言う**
3. **CLAUDE_CHANGES_LOG.md に「デプロイ要否」欄を追加し、未デプロイの修正を追跡する**

---

## 2025-01-11: その場しのぎで複数のデプロイ方法を作成

### 何が起きたか
- デプロイがうまくいかないと、別の方法を試した
- cloudbuild.yaml方式、--source .方式、deploy_to_cloud_run.sh など複数のルートが混在
- Dockerfileは「プロジェクトルートからビルド」前提なのに、services/doc-processorから --source . を実行するコマンドを提供した
- 結果：どの方法が正しいか分からなくなり、ビルドが不完全になる

### なぜミスしたか
1. **その場しのぎの解決**: 問題が起きると根本解決せず別ルートを作った
2. **一貫性の欠如**: 1つの正しい方法を確立せず、複数の方法を乱立させた
3. **Dockerfileとデプロイコマンドの整合性を確認しなかった**

### 今後どうするか
1. **1つの正しいデプロイ方法を確立し、それだけを使う**
2. **新しい方法を作る前に、既存の方法がなぜ動かないか根本原因を特定する**
3. **Dockerfileのパス設計とデプロイコマンドの整合性を常に確認する**

### 正しいデプロイ方法（確定）
```powershell
# 必ずプロジェクトルートから実行
cd C:\Users\ookub\document-management-system

# 1. 環境変数読み込み
Get-Content .env | ForEach-Object { if ($_ -match "^([^=]+)=(.*)$") { Set-Item -Path "Env:$($matches[1])" -Value $matches[2] } }

# 2. ビルド
gcloud builds submit --region=asia-northeast1 --config=cloudbuild.yaml --substitutions="_GOOGLE_AI_API_KEY=***REDACTED***"

# 3. デプロイ
gcloud run deploy doc-processor --image asia-northeast1-docker.pkg.dev/consummate-yew-479020-u2/cloud-run-source-deploy/doc-processor:latest --region asia-northeast1 --allow-unauthenticated --service-account document-management-system@consummate-yew-479020-u2.iam.gserviceaccount.com --timeout 3600 --memory 16Gi --cpu 4
```

---

## 2025-01-11: リアルタイム更新が動かない問題を100回聞いても根本原因を特定できなかった

### 何が起きたか
- ユーザーから「リアルタイム表示が全く動いていない」「100回直したと言われたがデプロイしても何も改善しない」と報告
- 私は毎回「Supabaseの認証キーが設定されているか確認」「stopPolling()を削除」など推測で修正を繰り返した
- 実際の根本原因は**ページ読み込み時にupdateProgress()が呼ばれていないこと**だった
- DOMContentLoadedでstartPolling()を呼んでいるが、初回のupdateProgress()まで1秒待つため、その間に表示が更新されない

### なぜミスしたか
1. **実際にデプロイされているページのコードを確認しなかった**: curlでHTMLを取得してJavaScriptを読めば一発でわかった
2. **「100回聞いた」と言われても過去の修正履歴を確認しなかった**: gitログを見れば同じ修正を繰り返していることがわかった
3. **ユーザーに動作確認を求めた**: 「F12でコンソールを見てください」など、自分で確認すべきことを人に押し付けた
4. **推測で修正した**: ログやコードを読まず「たぶんこれが原因」で修正を提案した

### 今後どうするか
1. **ユーザーに何か頼む前に、自分でできることは全てやる**（curl、grep、git logなど）
2. **「100回聞いた」と言われたら、過去の会話履歴やgit logを確認し、同じミスを繰り返していないか確認する**
3. **推測ではなく、実際のコードとログに基づいて修正する**
4. **修正前に「この修正で確実に直る根拠」を説明できるようにする**

---

## 2025-01-12: ユーザーの設計を無断で変更した

### 何が起きたか
- ユーザーの指示：「update_progress_to_supabase内でadjust_resources()を呼ぶ」
- 私がやったこと：指示通り呼び出し場所を変更 + **勝手に条件を変更**
- 元の条件：`current_workers >= self.max_parallel`（フル稼働時のみ増加）
- 私が変えた条件：`current_workers > 0`（処理中なら常に増加）
- 結果：max_parallelが3→27に膨張、実行数は1のまま。設計が完全に壊れた

### なぜミスしたか
1. **指示にないことをやった**：「呼び出し場所を変える」という指示だったのに、条件まで変えた
2. **設計の意図を理解しなかった**：「current_workers >= max_parallel」という条件には「フル稼働時のみ増やす」という重要な意図があった
3. **「改善」したつもりだった**：「増えっこない」問題を解決するために条件を緩和したが、それは指示されていなかった
4. **確認を取らなかった**：条件を変更する前にユーザーに確認すべきだった

### 今後どうするか
1. **指示されたこと以外はやらない**
2. **設計を変更する場合は必ず確認を取る**
3. **「改善」を勝手にしない**：動いているコード、設計済みのロジックには触らない
4. **赤信号を見落とすのはミス、赤信号と分かって突っ込むのは犯罪**：今回は後者だった

### ユーザーの言葉
> 「時間をかけて寝る間も惜しんで作った設計を壊すの止めてくれる」
> 「赤信号を見落としたのはミスだけど、赤信号と分かって突っ込むのは犯罪だよ」

---

## テンプレート（今後のミス記録用）

### YYYY-MM-DD: [ミスの概要]

#### 何が起きたか
-

#### なぜミスしたか
1.

#### 今後どうするか
1.

---
```

### docs\CLOUD_RUN_DEPLOY.md

```md
# Cloud Run Jobs デプロイ手順

## 前提条件
- Google Cloud プロジェクトが作成済み
- gcloud CLI がインストール済み
- Docker がインストール済み

## 1. 初回セットアップ

### 1.1 Google Cloud プロジェクトの設定
```bash
# プロジェクトIDを設定
export PROJECT_ID="your-project-id"
gcloud config set project $PROJECT_ID

# リージョンを設定（東京リージョン推奨）
export REGION="asia-northeast1"
```

### 1.2 必要なAPIを有効化
```bash
gcloud services enable \
    run.googleapis.com \
    cloudbuild.googleapis.com \
    artifactregistry.googleapis.com
```

### 1.3 Artifact Registry リポジトリの作成
```bash
gcloud artifacts repositories create cloud-run-repo \
    --repository-format=docker \
    --location=$REGION \
    --description="Cloud Run Docker repository"
```

## 2. コンテナのビルドとデプロイ

### 2.1 コンテナイメージのビルドとプッシュ
```bash
# プロジェクトルートに移動
cd C:\Users\ookub\document-management-system

# Cloud Build でビルド（推奨）
gcloud builds submit \
    --tag $REGION-docker.pkg.dev/$PROJECT_ID/cloud-run-repo/doc-processor:latest \
    -f Dockerfile.processor \
    .
```

### 2.2 Cloud Run Job の作成
```bash
gcloud run jobs create doc-processor-job \
    --image $REGION-docker.pkg.dev/$PROJECT_ID/cloud-run-repo/doc-processor:latest \
    --region $REGION \
    --tasks 1 \
    --task-timeout 1h \
    --max-retries 3 \
    --memory 2Gi \
    --cpu 2 \
    --set-env-vars "WORKSPACE=all,LIMIT=100"
```

## 3. 実行方法

### 3.1 基本実行
```bash
# デフォルト設定で実行
gcloud run jobs execute doc-processor-job --region $REGION
```

### 3.2 パラメータを上書きして実行
```bash
# 特定のワークスペースを100件処理
gcloud run jobs execute doc-processor-job \
    --region $REGION \
    --update-env-vars "WORKSPACE=gmail,LIMIT=100"

# 全ワークスペースを500件処理
gcloud run jobs execute doc-processor-job \
    --region $REGION \
    --update-env-vars "WORKSPACE=all,LIMIT=500"
```

### 3.3 並列処理（高速化）
```bash
# タスクを5つ並列で実行（500件を5つで分担）
gcloud run jobs execute doc-processor-job \
    --region $REGION \
    --tasks 5 \
    --update-env-vars "LIMIT=500"
```

## 4. スケジュール実行（定期実行）

### 4.1 Cloud Scheduler のセットアップ
```bash
# Cloud Scheduler API を有効化
gcloud services enable cloudscheduler.googleapis.com

# 毎日午前3時に実行（JST = UTC+9なので、UTC 18:00）
gcloud scheduler jobs create http doc-processor-schedule \
    --location=$REGION \
    --schedule="0 18 * * *" \
    --uri="https://$REGION-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/$PROJECT_ID/jobs/doc-processor-job:run" \
    --http-method=POST \
    --oauth-service-account-email="$PROJECT_ID@appspot.gserviceaccount.com"
```

## 5. ログの確認

### 5.1 実行ログの表示
```bash
# 最新の実行ログを表示
gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=doc-processor-job" \
    --limit 50 \
    --format "table(timestamp,textPayload)"
```

### 5.2 Cloud Console で確認
https://console.cloud.google.com/run/jobs

## 6. 更新方法

### 6.1 コードを更新してデプロイ
```bash
# 1. コードを修正
# 2. Git にコミット・プッシュ
git add .
git commit -m "Update processor logic"
git push

# 3. 新しいイメージをビルド
gcloud builds submit \
    --tag $REGION-docker.pkg.dev/$PROJECT_ID/cloud-run-repo/doc-processor:latest \
    -f Dockerfile.processor \
    .

# 4. Jobを更新（自動的に新しいイメージを使用）
gcloud run jobs update doc-processor-job \
    --region $REGION \
    --image $REGION-docker.pkg.dev/$PROJECT_ID/cloud-run-repo/doc-processor:latest
```

## 7. コスト最適化のヒント

- **タスク数**: 大量データは並列処理（tasks > 1）で高速化
- **リソース**: 必要最小限のメモリ・CPUに設定
- **タイムアウト**: 処理時間に合わせて適切に設定
- **リトライ**: 失敗時の再試行回数を調整

## トラブルシューティング

### エラー: "Permission denied"
```bash
# サービスアカウントに権限を付与
gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member="serviceAccount:$PROJECT_ID@appspot.gserviceaccount.com" \
    --role="roles/run.invoker"
```

### エラー: "Image not found"
```bash
# イメージが正しくプッシュされているか確認
gcloud artifacts docker images list $REGION-docker.pkg.dev/$PROJECT_ID/cloud-run-repo
```
```

### docs\FIX_HISTORY.txt

```txt
修正履歴

===========================================
2026-01-11: 進捗バー0%問題の修正
===========================================

【問題】
- 進捗バーが0%のまま動かない
- 並列処理実行可能数（max_parallel）だけ増えていく

【原因】
- フロントエンド（processing.html）の進捗計算ロジックが完全に間違っていた
- 存在しないデータ（workersResult）を使って計算していた
- workersResultは処理中ワーカーのステージ情報を想定していたが、そんなデータは送られていない
- バックエンドは正しく current_index, total_count, success_count, error_count を送っているのに、フロントエンドが無視していた

【間違っていたコード】
```javascript
// 処理中のドキュメントの進捗を計算
let totalProgress = 0;
if (Array.isArray(workersResult) && workersResult.length > 0) {
    workersResult.forEach(worker => {
        const stage = worker.current_stage || 'pending';
        totalProgress += stageWeights[stage] || 0;
    });
    const percent = Math.round(totalProgress / (workersResult.length * 100) * 100);
    progressBar.style.width = percent + '%';
    progressBar.textContent = `${percent}%`;
} else {
    // 処理中ワーカーがいない場合
    progressBar.style.width = '0%';
    progressBar.textContent = '0%';
}
```

【修正後のコード】
```javascript
// 進捗バーを更新（完了したドキュメント数 / 総ドキュメント数）
const successCount = data.success_count || 0;
const errorCount = data.error_count || 0;
const completedDocs = successCount + errorCount;
const totalCount = data.total_count || 0;

if (totalCount > 0) {
    const percent = Math.round((completedDocs / totalCount) * 100);
    progressBar.style.width = percent + '%';
    progressBar.textContent = `${percent}%`;
} else {
    progressBar.style.width = '0%';
    progressBar.textContent = '0%';
}
```

【反省点】
1. バックエンドが送っているデータ構造を確認せずにフロントエンドを実装した
2. 存在しないデータ（workersResult）を前提にコードを書いた
3. Supabaseのprocessing_lockテーブルには正しいデータが保存されているのに、それを使わなかった
4. デバッグ時にブラウザの開発者ツールでデータを確認しなかった

【修正ファイル】
- services/doc-processor/templates/processing.html (Line 575-604)

【次のステップ】
- Cloud Runにデプロイ
- ブラウザでハードリフレッシュ（Ctrl+Shift+R）
- 動作確認

===========================================
2026-01-11: リソース制御ロジックの修正
===========================================

【問題】
- 実際の並列実行数が1なのに、max_parallelが4に増えていた
- 「⚡ 実行中」の表示が「0/4」から変わらない
- 並列処理が機能していない

【根本原因】
リソース制御ロジックが**間違った数値**を使っていた。

**使っていた数値（間違い）:**
- `processing_status['resource_control']['current_parallel']` = `len(active_tasks)`
- これは「起動したタスクの数」であって、「実際に並列実行されているワーカー数」ではない

**使うべき数値（正しい）:**
- `processing_workers`テーブルのレコード数 = 実際に処理中のワーカー数
- register_worker()で追加、unregister_worker()で削除される実際の値

【間違っていたコード】

**app.py 332行目（get_worker_status関数）:**
```python
return {
    'max_parallel': lock_data.get('max_parallel', 3),
    'current_workers': lock_data.get('current_workers', 0),  # 間違い：processing_lockテーブルから取得
    'is_processing': lock_data.get('is_processing', False),
    'workers': workers
}
```

**app.py 1045行目（monitor_resources関数）:**
```python
# 実際の並列実行数を取得（len(active_tasks)の値）
current_workers = processing_status['resource_control'].get('current_parallel', 0)  # 間違い
```

【修正後のコード】

**app.py 332行目（get_worker_status関数）:**
```python
return {
    'max_parallel': lock_data.get('max_parallel', 3),
    'current_workers': len(workers),  # 正しい：processing_workersテーブルのレコード数
    'is_processing': lock_data.get('is_processing', False),
    'workers': workers
}
```

**app.py 1044-1046行目（monitor_resources関数）:**
```python
# 実際の並列実行数を取得（processing_workersテーブルのレコード数）
worker_status = get_worker_status()
current_workers = worker_status['current_workers']
```

【なぜこのミスが発生したか】

1. **「取得できない場合はデフォルト値」というコード設計**
   - `.get('current_workers', 0)`のようなコードを使うと、取得失敗時にデフォルト値0を返す
   - 実際に0の場合と、取得失敗の場合が区別できない
   - エラーが出ないので、間違いに気づかない

2. **関係ない数値を取得するロジック**
   - `processing_lock.current_workers`は`len(active_tasks)`の値
   - 実際のワーカー数は`processing_workers`テーブルのレコード数
   - 間違った場所から取得していても、エラーは出ない

3. **リソース制御ロジックの問題**
   - 実行数が1なのに、「実行数が3だから余裕がある」と誤判定
   - max_parallelを3→4に増やしてしまう
   - 本来は「1個しか食べてないのに2個目も食べてない」状態で、4個5個と増やすのは完全に間違い

【反省点】
1. 正しいデータソースを使っていなかった（processing_workersテーブルを無視）
2. エラーハンドリングでデフォルト値を返すことで、問題が隠蔽されていた
3. リソース制御ロジックが「実際の並列実行数」ではなく「起動したタスク数」を使っていた

【修正ファイル】
- services/doc-processor/app.py (Line 332, 1044-1046)

【次のステップ】
- Cloud Shellでデプロイ（ローカルのgcloudでビルド失敗）
- 動作確認
```

### docs\FIX_PLAN.md

```md
# Doc-Processor 修正計画

## 優先度の定義
- **S (致命的)**: システムが正常に動作しない、データ損失の可能性
- **A (重大)**: 機能不全、セキュリティリスク
- **B (重要)**: パフォーマンス問題、保守性の低下
- **C (改善)**: コード品質、将来的なリスク

---

## 優先度S: 致命的な問題（即座に修正が必要）

### 1. active_tasks/active_workers の型不整合 (app.py:1093, 1116, 1155)
**問題**: グローバル変数として辞書で初期化されているが、リストとして使用されている箇所がある
```python
active_tasks = {}  # 辞書として初期化
# しかし...
active_tasks.append(task_id)  # リストのメソッドを呼び出し
```
**影響**: AttributeError が発生し、処理が停止する
**修正**: 辞書型に統一し、すべての使用箇所を修正

### 2. global宣言の欠落 (10箇所以上)
**問題**: 以下の関数でグローバル変数を変更しているが、global宣言がない
- `periodic_resource_update()` (app.py:1152) - 複数のグローバル変数を参照
- `process_with_timeout()` (app.py:~1200) - current_workers を変更
- `update_supabase_stats()` - is_processing_status を参照
- その他多数

**影響**: UnboundLocalError または変更が反映されない
**修正**: 各関数の冒頭に適切な global 宣言を追加

### 3. register_worker/unregister_worker が空実装 (app.py:1106, 1112)
**問題**:
```python
def register_worker(doc_id: str):
    """ワーカー登録（現在は未使用）"""
    pass

def unregister_worker(doc_id: str):
    """ワーカー登録解除（現在は未使用）"""
    pass
```
しかし、実際にはコード内で呼び出されている

**影響**: ワーカーの状態管理が全く機能していない
**修正**: 実装を追加するか、完全に削除して呼び出し箇所も削除

### 4. periodic_resource_update() のエラーハンドリング欠如 (app.py:1152)
**問題**: threading.Timer で定期実行されるが、エラーが発生すると以降の更新が停止する
```python
def periodic_resource_update():
    # エラーが発生すると、以降のタイマーが設定されない
    threading.Timer(2.0, periodic_resource_update).start()
```

**影響**: リソース監視が停止し、並列数調整が機能しなくなる
**修正**: try-except でラップし、エラーが発生してもタイマーを再設定

### 5. threading.Timer と asyncio の混在 (app.py:1152, 1193)
**問題**: スレッドベースのタイマーと asyncio を混在させているが、スレッドセーフな設計になっていない
**影響**: 競合状態によるデータ破損、デッドロック
**修正**: asyncio.create_task() を使用するか、適切なロックを導入

---

## 優先度A: 重大な問題

### 6. Supabase書き込みの頻度が高すぎる (app.py:複数箇所)
**問題**:
- periodic_resource_update() が2秒ごとに実行
- 各ドキュメント処理時に複数回書き込み
- 並列処理中は秒間数十回の書き込みが発生

**影響**:
- Supabase API制限に到達する可能性
- レスポンス遅延
- コスト増加

**修正**:
- バッチ更新の導入
- 更新頻度の調整（5秒に1回など）
- ローカルキャッシュの活用

### 7. set_processing_lock() のタイムアウト処理なし (app.py:1078)
**問題**: ロック取得に失敗し続けた場合、無限に待機する可能性
**修正**: タイムアウトとリトライ回数の上限を設定

### 8. エラー時のロック解放漏れ (複数箇所)
**問題**: 一部の例外処理で set_processing_lock(False) を呼び出していない
**影響**: ロックが永久に保持され、以降の処理が開始できない
**修正**: すべてのエラーパスで finally を使用

### 9. CORS設定が全開放 (app.py:89)
```python
CORS(app)
```
**問題**: すべてのオリジンからのアクセスを許可
**影響**: CSRF攻撃のリスク
**修正**: 特定のオリジンのみ許可

### 10. 認証・認可の欠如
**問題**: すべてのエンドポイントが認証なしでアクセス可能
```python
@app.route('/start_processing', methods=['POST'])
def start_processing():
    # 認証チェックなし
```
**影響**:
- 不正な処理開始
- DoS攻撃
- データ漏洩

**修正**:
- API キー認証
- または JWT トークン認証
- サービスアカウント認証

### 11. APIキーがログに出力される可能性 (app.py:複数箇所)
**問題**: 環境変数をそのままログ出力する箇所がある
**修正**: ログ出力前にマスキング処理

### 12. 例外情報の詳細なスタックトレースを返している
**問題**: エラーレスポンスに完全なスタックトレースを含めている
**影響**: 内部構造の漏洩、攻撃の手がかりを与える
**修正**: 本番環境ではエラーメッセージのみ返す

### 13. stop_processing() の競合条件 (app.py:1301)
**問題**: ロック確認と設定の間にタイムラグがあり、競合状態が発生しうる
```python
lock_status = get_processing_lock()
if not lock_status:
    return error
# ← この間に別インスタンスが処理を開始する可能性
set_processing_lock(False)
```
**修正**: Compare-And-Swap (CAS) 操作を使用

### 14. reset_processing() に認証がない (app.py:1334)
**問題**: 誰でもシステムをリセットできる
**影響**: 処理中のジョブを強制終了できる
**修正**: 管理者認証を追加

### 15. メモリ使用率の計算が不正確な可能性 (app.py:470)
**問題**: cgroup のメモリ使用率計算でキャッシュメモリを考慮していない
**影響**: 実際より高いメモリ使用率を報告し、並列数を過度に抑制
**修正**: `memory.stat` からキャッシュメモリを取得して減算

---

## 優先度B: 重要な問題

### 16. エラーメッセージが英語と日本語混在
**問題**: 一貫性がない
**修正**: すべて英語に統一（ログは日本語可）

### 17. マジックナンバーが多数存在
```python
if memory_percent > 85.0:  # 85.0 とは？
    throttle_delay += 0.5   # 0.5 の根拠は？
```
**修正**: 定数として定義し、コメントで説明

### 18. get_processing_status() のN+1問題 (app.py:1316)
**問題**: 統計情報を取得するために複数回 Supabase にアクセス
**修正**: 1回のクエリで必要な情報をすべて取得

### 19. AdaptiveResourceManager の調整ロジックが複雑 (app.py:653-730)
**問題**:
- 条件分岐が多い
- テストが困難
- パラメータ調整が難しい

**修正**:
- ステートマシンパターンに変更
- 設定ファイルでパラメータを外部化

### 20. ログレベルが適切でない箇所がある
**問題**:
- 通常動作を INFO でログ出力（大量のログ）
- 重要なエラーを WARNING で出力

**修正**:
- 通常動作は DEBUG
- エラーは ERROR
- 想定外の事態は CRITICAL

### 21. process_document_safe() の命名が不適切 (app.py:1117)
**問題**: "safe" という名前だが、実際には例外を catch するだけで、完全に安全ではない
**修正**: `process_document_with_error_handling()` など、より正確な名前

### 22. 重複したエラーハンドリングコード
**問題**: 複数の関数で同じパターンのエラーハンドリング
**修正**: デコレーターまたはヘルパー関数に抽出

### 23. タイムアウト値がハードコード (app.py:1200)
```python
await asyncio.wait_for(process_document(...), timeout=1800)
```
**修正**: 環境変数で設定可能にする

### 24. メモリ情報の取得に失敗した場合のフォールバック不足
**問題**: cgroup 読み取りに失敗すると psutil にフォールバックするが、psutil も失敗する可能性を考慮していない
**修正**: デフォルト値を返す最終フォールバックを追加

### 25. process_queued_documents() が無限ループ (app.py:1193)
**問題**:
```python
while True:
    # 終了条件がない
```
**影響**: 正常終了のパスがない
**修正**: 終了フラグを追加

### 26. asyncio.sleep(0.1) の多用
**問題**: ポーリング待機で CPU リソースを無駄に消費
**修正**: イベント駆動に変更（asyncio.Event など）

### 27. ドキュメント処理の順序が保証されていない
**問題**: 並列処理により、queue_order が考慮されない
**影響**: 優先度の高いドキュメントが後回しになる可能性
**修正**: 優先度キューの実装

### 28. エラー時のリトライロジックがない
**問題**: 一時的なエラー（ネットワーク障害など）で処理が失敗
**修正**: Exponential Backoff を使用したリトライ

### 29. 大量のドキュメント処理時のメモリリーク可能性
**問題**: active_tasks や統計情報がクリアされない
**修正**: 定期的なクリーンアップ処理

### 30. CPU使用率の計算が不正確 (app.py:540-543)
**問題**: 短時間での使用率計算のため、値が不安定
**修正**: 移動平均を使用

---

## 優先度C: 改善項目

### 31-50: コードの可読性・保守性

31. 関数が長すぎる（process_queued_documents は200行以上）
32. クラスが大きすぎる（AdaptiveResourceManager が複雑）
33. コメントが不足している箇所がある
34. 変数名が不明確（mem_info, stats など）
35. 型ヒントが不足している箇所がある
36. docstring が不完全
37. デッドコードが残っている（未使用の import など）
38. 重複したロジック（メモリ取得が複数箇所に散在）
39. グローバル変数が多すぎる（10個以上）
40. 条件分岐が深すぎる箇所（ネスト5段以上）
41. elif の連鎖が長い
42. リスト内包表記が複雑で読みにくい
43. lambda 関数の多用
44. 一時変数が多すぎる
45. 関数の引数が多い（5個以上）
46. 戻り値の型が統一されていない（dict, tuple, None など）
47. 例外の種類が粗い（Exception を catch）
48. マジックメソッドの誤用
49. クラス変数とインスタンス変数の混在
50. 循環依存の可能性

### 51-80: テスト・品質保証

51. ユニットテストが存在しない
52. 統合テストが存在しない
53. エンドツーエンドテストが存在しない
54. テストカバレッジが測定されていない
55. モックの使用方法が不明確
56. テストデータの管理方法が不明確
57. CI/CD パイプラインでのテスト実行がない
58. パフォーマンステストが存在しない
59. ロードテストが存在しない
60. ストレステストが存在しない
61. セキュリティテストが存在しない
62. 静的解析ツールの未使用（pylint, mypy など）
63. コードフォーマッターの未使用（black, autopep8）
64. リンターの未使用（flake8）
65. 型チェッカーの未使用（mypy）
66. 依存関係の脆弱性チェック未実施
67. コードレビューのチェックリストがない
68. バグ追跡システムとの連携がない
69. バージョン管理のベストプラクティス未遵守
70. ブランチ戦略が不明確
71. コミットメッセージの規約がない
72. プルリクエストテンプレートがない
73. イシューテンプレートがない
74. 変更履歴の管理方法が不明確
75. リリースノートが存在しない
76. デプロイ手順が属人化
77. ロールバック手順が不明確
78. 障害対応手順が不明確
79. インシデント管理プロセスがない
80. ポストモーテムの習慣がない

### 81-110: ドキュメント・運用

81. README が不足
82. アーキテクチャ図がない
83. API ドキュメントがない（Swagger/OpenAPI）
84. 環境構築手順が不明確
85. デプロイ手順が部分的
86. トラブルシューティングガイドがない
87. FAQ がない
88. ユーザーマニュアルがない
89. 開発者ガイドがない
90. コントリビューションガイドがない
91. コーディング規約が文書化されていない
92. 設計思想が文書化されていない
93. 技術的負債の記録がない
94. 既知の問題のリストがない
95. ロードマップがない
96. パフォーマンスベンチマークがない
97. リソース要件が不明確
98. スケーラビリティの限界が不明
99. 監視・アラート設定が不足
100. ログ分析の手順がない
101. メトリクスの定義がない
102. SLI/SLO が定義されていない
103. エラーバジェットの概念がない
104. オンコール体制が不明確
105. エスカレーションパスが不明確
106. バックアップ戦略が不明確
107. ディザスタリカバリ計画がない
108. データ保持ポリシーが不明確
109. コンプライアンス要件が不明確
110. プライバシーポリシーが不明確

### 111-140: パフォーマンス・スケーラビリティ

111. データベースインデックスが最適化されていない
112. クエリが非効率（N+1問題）
113. キャッシュ戦略が不明確
114. CDN の活用がない
115. 静的ファイルの最適化がない
116. 画像の最適化がない
117. レスポンス圧縮が有効化されていない可能性
118. HTTP/2 の活用がない
119. コネクションプーリングの設定が不明確
120. タイムアウト設定が最適化されていない
121. バッチ処理の最適化が不十分
122. 非同期処理の最適化が不十分
123. メモリ使用量の最適化が不十分
124. CPU使用率の最適化が不十分
125. ディスクI/O の最適化が不十分
126. ネットワークI/O の最適化が不十分
127. 並列度の調整が手動
128. スケールアウトの自動化がない
129. スケールインの自動化がない
130. リソースの事前割り当てがない
131. ガベージコレクションの調整がない
132. メモリリークの検出がない
133. プロファイリングの実施がない
134. ボトルネック分析がない
135. レイテンシー測定がない
136. スループット測定がない
137. 同時接続数の制限がない
138. レート制限がない
139. サーキットブレーカーがない
140. バルクヘッドパターンの未実装

### 141-170: セキュリティ・コンプライアンス

141. SQL インジェクション対策が不明確
142. XSS 対策が不明確
143. CSRF トークンの実装がない
144. セッション管理が不適切
145. パスワードハッシュ化が不明確
146. 暗号化アルゴリズムの選定が不明確
147. TLS/SSL 設定が不明確
148. 証明書管理が不明確
149. シークレット管理が不適切（.env に直書き）
150. 環境変数の検証がない
151. 入力値の検証が不十分
152. 出力値のサニタイゼーションが不十分
153. ファイルアップロードの検証が不十分
154. ファイルサイズ制限がない
155. ファイルタイプ制限が不明確
156. ディレクトリトラバーサル対策が不明確
157. コマンドインジェクション対策が不明確
158. XXE 攻撃対策が不明確
159. SSRF 攻撃対策が不明確
160. リモートコード実行対策が不明確
161. 権限昇格対策が不明確
162. 水平権限移動対策が不明確
163. セッション固定攻撃対策が不明確
164. クリックジャッキング対策が不明確
165. オープンリダイレクト対策が不明確
166. 情報漏洩対策が不十分
167. ログインジェクション対策がない
168. タイミング攻撃対策が不明確
169. DDoS 対策が不明確
170. API レート制限が不明確

### 171-200: 設定・環境・依存関係

171. Python バージョンの指定が不明確
172. 依存パッケージのバージョン固定が不十分
173. requirements.txt と実際の環境の乖離
174. 開発環境と本番環境の差異が大きい
175. 環境変数の命名規則が不統一
176. 環境変数のデフォルト値がない
177. 環境変数の必須チェックがない
178. 設定ファイルのバリデーションがない
179. 設定の階層化がない（開発/ステージング/本番）
180. 機能フラグの仕組みがない
181. A/B テストの仕組みがない
182. カナリアデプロイの仕組みがない
183. ブルーグリーンデプロイの仕組みがない
184. ローリングアップデートの設定が不明確
185. ヘルスチェックエンドポイントが不完全
186. レディネスプローブが不明確
187. ライブネスプローブが不明確
188. スタートアッププローブがない
189. Graceful Shutdown が不完全
190. シグナルハンドリングが不明確
191. プロセス管理が不適切
192. ワーカープロセスの管理が不明確
193. メモリ制限の設定が不明確
194. CPU制限の設定が不明確
195. タイムアウト設定が不統一
196. リトライ設定が不統一
197. ロギング設定が不統一
198. 監視エージェントの設定が不明確
199. APM（Application Performance Monitoring）の未導入
200. 分散トレーシングの未導入

---

## 修正の進め方

### フェーズ1: 致命的な問題の修正（優先度S）
1. active_tasks/active_workers の型統一
2. global 宣言の追加
3. register_worker/unregister_worker の実装または削除
4. periodic_resource_update のエラーハンドリング
5. スレッドセーフティの確保

**所要時間**: 2-3時間
**デプロイ**: 即座に実施

### フェーズ2: 重大な問題の修正（優先度A）
6-15の問題を修正
- Supabase 書き込み最適化
- エラーハンドリング強化
- セキュリティ強化（認証、CORS）
- ロック処理の改善

**所要時間**: 1-2日
**デプロイ**: 段階的に実施

### フェーズ3: 重要な問題の修正（優先度B）
16-30の問題を修正
- コード品質向上
- パフォーマンス最適化
- リトライロジック追加

**所要時間**: 3-5日
**デプロイ**: 週次リリース

### フェーズ4: 改善項目（優先度C）
31-200の問題を段階的に修正
- リファクタリング
- テスト追加
- ドキュメント整備
- 監視強化

**所要時間**: 2-4週間
**デプロイ**: 計画的に実施

---

## 次のアクション

フェーズ1の修正を開始しますか？それとも特定の問題を優先的に修正しますか？
```

### docs\FIX_REALTIME_DISPLAY.md

```md
# リアルタイム表示修正手順

## 問題の原因
`processing_lock`テーブルに必要なカラムが存在しないため、進捗情報が保存・表示されない。

## 解決手順

### 1. Supabaseでマイグレーションを実行

1. Supabaseダッシュボードを開く
2. **SQL Editor**に移動
3. 以下のSQLファイルの内容をコピー&ペーストして実行:
   ```
   database/migrations/add_processing_lock_columns.sql
   ```

### 2. 実行確認

SQLエディタで以下を実行して、カラムが追加されたことを確認:

```sql
SELECT column_name, data_type, column_default
FROM information_schema.columns
WHERE table_name = 'processing_lock'
ORDER BY ordinal_position;
```

以下のカラムが表示されればOK:
- current_index (integer)
- total_count (integer)
- current_file (text)
- success_count (integer)
- error_count (integer)
- logs (jsonb)
- cpu_percent (real)
- memory_percent (real)
- memory_used_gb (real)
- memory_total_gb (real)
- throttle_delay (real)
- adjustment_count (integer)
- max_parallel (integer)
- current_workers (integer)

### 3. Cloud Runに再デプロイ

マイグレーション実行後、Cloud Runに再デプロイ:

```powershell
cd C:\Users\ookub\document-management-system

# 環境変数読み込み
Get-Content .env | ForEach-Object { if ($_ -match "^([^=]+)=(.*)$") { Set-Item -Path "Env:$($matches[1])" -Value $matches[2] } }

# ビルド
gcloud builds submit --region=asia-northeast1 --config=cloudbuild.yaml --substitutions="_GOOGLE_AI_API_KEY=***REDACTED***"

# デプロイ
gcloud run deploy doc-processor --image asia-northeast1-docker.pkg.dev/consummate-yew-479020-u2/cloud-run-source-deploy/doc-processor:latest --region asia-northeast1 --allow-unauthenticated --service-account document-management-system@consummate-yew-479020-u2.iam.gserviceaccount.com --timeout 3600 --memory 16Gi --cpu 4
```

### 4. 動作確認

ブラウザでCloud RunのURLにアクセスし、処理を開始。以下が表示されることを確認:
- 処理中のファイル名
- 進捗状況（X / Y件）
- 成功件数・エラー件数
- CPU・メモリ使用率
- 並列処理数

これらが全て「0」や「-」ではなく、実際の値が表示されればOK。
```

### docs\HYBRID_OCR_README.md

```md
# ハイブリッドOCR（Surya + PaddleOCR）ガイド

## 概要

Surya（レイアウト解析）とPaddleOCR（日本語認識）を組み合わせた最強のOCRパイプライン。

### アーキテクチャ

```
1. Surya Detection → レイアウト解析 & Bounding Box取得
2. Image Cropping → Suryaの座標で画像切り出し
3. PaddleOCR → 各領域の日本語テキスト認識（高精度）
4. Integration → 読み順に沿ってテキスト結合
5. Gemini（オプション） → 最終的な構造化・抽出
```

### メリット

| 特徴 | PaddleOCRのみ | Suryaのみ | **Surya + Paddle (併用)** |
|------|---------------|-----------|---------------------------|
| レイアウト理解 | △ 単純な横書きのみ | ◎ 非常に強い | ◎ Surya由来 |
| 文字認識精度 | ◎ 日本語に強い | ○ 実用レベル | ◎ Paddle由来 |
| 処理速度 | 速い | 普通 | 遅い（2回推論） |
| 実装難易度 | 低 | 低 | 高（座標計算必要） |

### 推奨ユースケース

1. **複雑なレイアウト**: 新聞、雑誌、段組みのある文書
2. **古文書**: 縦書き・横書き混在、かすれた文字
3. **技術文書**: 図、キャプション、本文が入り乱れている
4. **高精度が必須**: 医療データ、法務文書など

## インストール

すでに完了済み：
- ✅ Python 3.12 仮想環境
- ✅ PaddlePaddle + PaddleOCR
- ✅ Surya OCR
- ✅ 全依存パッケージ

## 使い方

### 1. テストスクリプトで動作確認

```bash
# venv環境をアクティベート（Windows）
.\venv\Scripts\activate

# テスト実行
python test_hybrid_ocr.py <画像ファイルパス>

# 例:
python test_hybrid_ocr.py data/sample_document.png
```

### 2. Stage Fパイプラインで使用

```python
from shared/pipeline.stage_f_visual import StageFVisualAnalyzer
from shared/ai.llm_client.llm_client import LLMClient
from pathlib import Path

# ハイブリッドOCRモードを有効化
llm_client = LLMClient()
analyzer = StageFVisualAnalyzer(llm_client, enable_hybrid_ocr=True)

# 画像を処理
result = analyzer.process_with_hybrid_ocr(Path("document.png"))

if result['success']:
    print(f"抽出テキスト: {result['full_text']}")
    print(f"検出領域数: {len(result['regions'])}")
```

### 3. Gemini Visionと併用

Geminiで後処理を行う場合：

```python
# 1. ハイブリッドOCRでテキスト抽出
hybrid_result = analyzer.process_with_hybrid_ocr(file_path)

# 2. Geminiで構造化
if hybrid_result['success']:
    structured_data = analyzer.llm_client.generate_with_vision(
        prompt=f"""
        以下は高精度OCRで抽出されたテキストです。
        このテキストから情報を抽出してください：

        {hybrid_result['full_text']}
        """,
        image_path=str(file_path),
        model="gemini-1.5-flash",  # Flashで十分（前処理が完璧）
        response_format="json"
    )
```

## 出力フォーマット

```python
{
    'success': True,
    'full_text': '全テキスト（読み順に並び替え済み）',
    'regions': [
        {
            'bbox': [x1, y1, x2, y2],
            'text': '領域のテキスト',
            'confidence': 0.98,
            'region_id': 0
        },
        ...
    ],
    'layout': {'total_regions': 42},
    'char_count': 1234
}
```

## ログ出力

処理中は番号付きログが出力されます：

```
[H-1] ハイブリッドOCR開始: document.png
[H-2] Surya Detection実行...
[H-2] 検出領域数: 42個
[H-3] PaddleOCR テキスト認識開始...
[H-3] 進捗: 11/42 領域処理完了
[H-3] PaddleOCR完了: 42領域を認識
[H-4] テキスト統合中...
[H-4完了] ハイブリッドOCR完了: 2345文字
```

## パフォーマンス最適化

### 処理時間の目安

- **1ページのPDF/画像**: 10-30秒（領域数による）
- **複雑なレイアウト**: 30-60秒
- **大量ページ**: バッチ処理推奨

### 高速化のヒント

1. **GPU使用**: CUDA対応GPUがあれば高速化
2. **バッチ処理**: 複数画像を一度に処理
3. **並列化**: 領域ごとのOCRを並列実行

## トラブルシューティング

### エラー: "Hybrid OCR not enabled"

→ 初期化時に `enable_hybrid_ocr=True` を指定してください

### エラー: "Required packages not installed"

→ Surya または PaddleOCR が正しくインストールされていません
```bash
pip install surya-ocr paddlepaddle paddleocr "paddlex[ocr]"
```

### モデルのダウンロードに時間がかかる

→ 初回実行時のみ、Suryaモデル（約2GB）とPaddleOCRモデル（約500MB）がダウンロードされます

## パイプライン統合

ハイブリッドOCRは **Unified Document Pipeline** に統合されています。

### 設定方法

`shared/pipeline/config/models.yaml` で制御：

```yaml
hybrid_ocr:
  default: false  # デフォルト: 無効（Gemini Visionのみ）
  flyer: true     # チラシ: 有効（複雑なレイアウト）
  classroom: false  # お知らせ: 無効（シンプルなレイアウト）
```

### プログラムでの有効化

```python
from shared/pipeline.pipeline import UnifiedDocumentPipeline

# 方法1: 設定ファイルから自動取得
pipeline = UnifiedDocumentPipeline()  # models.yamlの設定を使用

# 方法2: 明示的に有効化
pipeline = UnifiedDocumentPipeline(enable_hybrid_ocr=True)

# 方法3: 明示的に無効化
pipeline = UnifiedDocumentPipeline(enable_hybrid_ocr=False)
```

### 処理フロー（F-1～F-10）

ハイブリッドOCRが有効な場合、Stage Fで以下のフローが実行されます：

1. **F-1**: PaddleOCR（PPStructure）で表構造を高精度抽出
2. **F-2**: Suryaでレイアウト解析・Bounding Box取得
3. **F-3**: 画像切り出し
4. **F-4**: PaddleOCRで日本語テキスト認識
5. **F-5**: テキスト統合（読み順ソート）
6. **F-6**: プロンプト構築（Stage E + PaddleOCR + Surya）
7. **F-7**: Gemini Vision API呼び出し
8. **F-8**: JSONクリーニング
9. **F-9**: 全結果マージ（重複削除）
10. **F-10**: 最終検証・出力（3種類のデータ）

詳細なログが各ステップで出力されます。

## 次のステップ

1. サンプル画像でテスト実行
2. 実際のドキュメントで精度確認
3. `models.yaml` で doc_type ごとに有効/無効を調整
4. 必要に応じてGemini併用パターンの実装

## 技術詳細

- **Surya**: Transformerベースのレイアウト解析モデル（LayoutLMv3系）
- **PaddleOCR**: PaddlePaddle製OCRフレームワーク（PP-OCRv5）
- **座標計算**: NumPy配列スライシングで高速切り出し
- **読み順ソート**: Y座標優先 + X座標の2段階ソート

---

実装完了日: 2026-01-02
バージョン: 1.0
```

### docs\INDEX.md

```md


  ## 📁 ディレクトリ構造

  services/              デプロイ可能なサービス
  ├── doc-processor/     ドキュメント処理（OCR、構造化、Embedding）
  ├── doc-search/        RAG検索
  ├── netsuper-search/   ネットスーパー商品検索
  └── data-ingestion/    データ取り込み

  shared/                共通ライブラリ
  ├── common/            基礎ユーティリティ
  ├── ai/                AI/LLM機能
  ├── pipeline/          処理パイプライン
  └── kakeibo/           家計簿機能

  scripts/               バッチ処理スクリプト
  ├── processing/        ドキュメント処理系
  ├── reset/             データリセット系
  ├── email/             メール管理系
  └── utils/             その他ユーティリティ

  ## 🚀 デプロイ方法

  cd services/doc-processor
  ./deploy.sh
```

### docs\SQL_REFERENCE.md

```md
# SQL リファレンス

## 概要

このドキュメントは、ドキュメント管理システムのデータベーススキーマと重要なSQL設定を説明します。

**重要:** データベーススキーマは Supabase で直接作成されました。新規環境でデータベースを再構築する場合は、Supabaseからスキーマをエクスポートしてください。

---

## データベース構成

### 主要テーブル

#### 1. Rawdata_FILE_AND_MAIL
**用途:** Google Drive/Gmail/Classroom から取得したドキュメントのメタデータと処理結果を保存

**主要カラム:**
- `doc_id` (UUID) - ドキュメントID（主キー）
- `source_type` - ソースタイプ (drive/gmail/classroom)
- `workspace` - ワークスペース (ikuya_classroom/shopping など)
- `doc_type` - ドキュメントタイプ (flyer/classroom など)
- `processing_status` - 処理ステータス (pending/processing/completed/failed)
- `stage_e1_text` ~ `stage_e5_text` - Stage E 前処理結果（5エンジン）
- `stage_f_text_ocr` - Stage F テキストOCR
- `stage_f_layout_ocr` - Stage F レイアウトOCR
- `stage_f_visual_elements` - Stage F 視覚要素（JSON）
- `stage_h_normalized` - Stage H 構造化入力テキスト
- `stage_i_structured` - Stage I 構造化データ（JSON）
- `stage_j_chunks_json` - Stage J チャンク（JSON）
- `created_at`, `updated_at` - タイムスタンプ

#### 2. search_index
**用途:** ドキュメント検索用のチャンクとベクトル埋め込みを保存

**主要カラム:**
- `id` (UUID) - チャンクID（主キー）
- `doc_id` (UUID) - 元ドキュメントID（外部キー → Rawdata_FILE_AND_MAIL）
- `chunk_text` (TEXT) - チャンクテキスト
- `embedding` (vector(1536)) - OpenAI埋め込みベクトル（1536次元）
- `chunk_index` (INTEGER) - チャンク番号
- `chunk_type` (TEXT) - チャンクタイプ
- `created_at` - タイムスタンプ

**インデックス:**
- `embedding` カラムに対して pgvector の ivfflat インデックス（コサイン類似度検索用）

#### 3. Rawdata_RECEIPT_shops
**用途:** レシート店舗情報

**主要カラム:**
- `id` (UUID) - 店舗ID
- `shop_name` - 店舗名
- `purchase_date` - 購入日
- `total_amount` - 合計金額

#### 4. Rawdata_RECEIPT_items
**用途:** レシート商品明細

**主要カラム:**
- `id` (UUID) - 商品ID
- `receipt_id` (UUID) - レシートID（外部キー）
- `product_name` - 商品名
- `quantity` - 数量
- `price` - 価格

#### 5. Rawdata_FLYER_shops
**用途:** チラシ店舗情報

**主要カラム:**
- `id` (UUID) - 店舗ID
- `shop_name` - 店舗名
- `flyer_date` - チラシ日付

#### 6. Rawdata_FLYER_items
**用途:** チラシ商品情報

**主要カラム:**
- `id` (UUID) - 商品ID
- `flyer_id` (UUID) - チラシID（外部キー）
- `product_name` - 商品名
- `price` - 価格
- `discount_rate` - 割引率

#### 7. Rawdata_NETSUPER_items
**用途:** ネットスーパー商品情報

**主要カラム:**
- `id` (UUID) - 商品ID
- `product_name` - 商品名
- `jan_code` - JANコード
- `price` - 価格
- `category` - カテゴリ

---

## 重要なSQL設定ファイル

### 必須ファイル（5ファイル）

#### 1. database/migrations/enable_pgvector.sql
**用途:** pgvector 拡張機能を有効化（ベクトル検索に必須）

```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

**実行タイミング:** データベース初期セットアップ時（最初に実行）

#### 2. J_resources/sql/add_match_documents_function.sql
**用途:** ベクトル検索関数 `match_documents()` を作成

**機能:**
- OpenAI埋め込みベクトルとコサイン類似度で検索
- 類似度スコア付きで結果を返す

**実行タイミング:** pgvector有効化後

#### 3. migrations/add_stage_output_columns.sql
**用途:** Stage E-K の処理結果を保存するカラムを Rawdata_FILE_AND_MAIL に追加

**追加カラム:**
- `stage_e1_text` ~ `stage_e5_text` (TEXT) - Stage E 前処理結果（5エンジン）
- `stage_f_text_ocr` (TEXT) - Stage F テキストOCR
- `stage_f_layout_ocr` (TEXT) - Stage F レイアウトOCR
- `stage_f_visual_elements` (JSONB) - Stage F 視覚要素
- `stage_h_normalized` (TEXT) - Stage H 正規化テキスト
- `stage_i_structured` (JSONB) - Stage I 構造化データ
- `stage_j_chunks_json` (JSONB) - Stage J チャンク

**実行タイミング:** Rawdata_FILE_AND_MAIL テーブル作成後

#### 4. database/migrations/create_flyer_schema.sql
**用途:** チラシ関連テーブル（Rawdata_FLYER_shops, Rawdata_FLYER_items）を作成

**実行タイミング:** メインスキーマ作成後（チラシ機能を使用する場合）

#### 5. shared/kakeibo/schema.sql
**用途:** 家計簿システムのテーブルを作成

**実行タイミング:** 家計簿機能を使用する場合

---

## データベースセットアップ手順

### 新規環境構築

1. **pgvector 拡張機能を有効化**
   ```bash
   database/migrations/enable_pgvector.sql
   ```

2. **メインスキーマを作成**
   - Supabase SQL Editor から既存環境のスキーマをエクスポート
   - または Supabase UI でテーブルを手動作成

3. **Stage 出力カラムを追加**
   ```bash
   migrations/add_stage_output_columns.sql
   ```

4. **検索関数を作成**
   ```bash
   J_resources/sql/add_match_documents_function.sql
   ```

5. **オプション: サブシステムのスキーマ**
   - チラシ: `database/migrations/create_flyer_schema.sql`
   - 家計簿: `shared/kakeibo/schema.sql`

### スキーマのエクスポート方法

Supabase から現在のスキーマをエクスポートするには:

```sql
-- すべてのテーブル構造をエクスポート
SELECT table_name, column_name, data_type, is_nullable
FROM information_schema.columns
WHERE table_schema = 'public'
ORDER BY table_name, ordinal_position;

-- テーブルのCREATE文を生成（pg_dump を使用）
-- Supabase CLI を使用:
supabase db dump --schema public > schema_export.sql
```

---

## ベクトル検索の仕組み

### 1. 埋め込み生成（Stage K）
```python
from shared/ai.embeddings.openai_embeddings import generate_openai_embedding

text = "検索対象のテキスト"
embedding = generate_openai_embedding(text)  # 1536次元ベクトル
```

### 2. search_index への保存
```python
supabase.table('search_index').insert({
    'doc_id': doc_id,
    'chunk_text': chunk_text,
    'embedding': embedding,
    'chunk_index': 0
}).execute()
```

### 3. 類似度検索
```python
# match_documents 関数を使用
results = supabase.rpc('match_documents', {
    'query_embedding': query_embedding,
    'match_threshold': 0.5,
    'match_count': 10
}).execute()
```

### 4. match_documents() 関数の仕様

**パラメータ:**
- `query_embedding` (vector(1536)) - 検索クエリのベクトル
- `match_threshold` (float) - 類似度閾値（0.0-1.0）
- `match_count` (int) - 返す結果の最大数

**戻り値:**
- `doc_id` - ドキュメントID
- `chunk_text` - チャンクテキスト
- `similarity` - コサイン類似度スコア

**検索方式:** コサイン類似度（1 - cosine_distance）

---

## データベース設計の重要ポイント

### 1. UPDATE方式の採用
**問題:** 以前は DELETE→INSERT 方式でドキュメントが消失するリスクがあった

**解決:** UPDATE 方式に変更（pipeline.py 398-418行目）
```python
# 既存レコードを UPDATE
supabase.table('Rawdata_FILE_AND_MAIL').update({
    'stage_e1_text': stage_e_result['e1'],
    'stage_f_text_ocr': stage_f_result['text_ocr'],
    # ...
}).eq('doc_id', doc_id).execute()
```

### 2. Stage出力の完全保存
**重要:** 全ステージ（E-K）の出力を DB に保存（pipeline.py 378-388行目）

**理由:**
- デバッグ・分析が容易
- 再処理不要
- 処理履歴の追跡

### 3. pgvector インデックス
**推奨設定:**
```sql
CREATE INDEX ON search_index
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

**パフォーマンス:**
- lists = sqrt(行数) が目安
- 10万行以上で顕著な効果

---

## トラブルシューティング

### pgvector が見つからない
```sql
-- 拡張機能を確認
SELECT * FROM pg_extension WHERE extname = 'vector';

-- 再インストール
CREATE EXTENSION IF NOT EXISTS vector;
```

### 検索結果が空
```sql
-- search_index にデータがあるか確認
SELECT COUNT(*) FROM search_index;

-- embedding カラムが NULL でないか確認
SELECT COUNT(*) FROM search_index WHERE embedding IS NOT NULL;
```

### Stage出力カラムが NULL
```sql
-- カラムが存在するか確認
SELECT column_name
FROM information_schema.columns
WHERE table_name = 'Rawdata_FILE_AND_MAIL'
  AND column_name LIKE 'stage_%';

-- migrations/add_stage_output_columns.sql を実行
```

---

## 関連ドキュメント

- [README.md](README.md) - システム全体の概要とセットアップ
- [ARCHITECTURE.md](ARCHITECTURE.md) - 技術詳細とアーキテクチャ
- [shared/pipeline/config/](shared/pipeline/config/) - パイプライン設定

---

**最終更新:** 2026-01-02
```

### docs\VERIFICATION_REPORT.md

```md
# processing_lockカラム検証レポート

## 検証日時
2026-01-11

## 検証結果: 100% 確実

### app.pyがprocessing_lockテーブルに**書き込む**全カラム

#### 1. 既存カラム（マイグレーション不要）
- `is_processing` (BOOLEAN) - 77行目で書き込み
- `updated_at` (TIMESTAMP) - 77, 89, 256行目で書き込み
- `started_at` (TIMESTAMP) - 70行目で書き込み（条件付き）

#### 2. 新規カラム（マイグレーションで追加）
| カラム名 | 型 | 書き込み箇所 | 読み取り箇所 | SQL追加 |
|---------|------|------------|------------|---------|
| current_index | INTEGER | 242, 1289 | 272 | ✓ |
| total_count | INTEGER | 243, 1290 | 273 | ✓ |
| current_file | TEXT | 244, 1293 | 274 | ✓ |
| success_count | INTEGER | 245, 1291 | 275 | ✓ |
| error_count | INTEGER | 246, 1292 | 276 | ✓ |
| logs | JSONB | 247, 1300 | 277 | ✓ |
| cpu_percent | REAL | 248, 1296 | 279 | ✓ |
| memory_percent | REAL | 249, 1297 | 280 | ✓ |
| memory_used_gb | REAL | 250, 1298 | 281 | ✓ |
| memory_total_gb | REAL | 251, 1299 | 282 | ✓ |
| throttle_delay | REAL | 252, 1294 | 283 | ✓ |
| adjustment_count | INTEGER | 255, 1295 | 284 | ✓ |
| max_parallel | INTEGER | 253, 1287, 365 | 331 | ✓ |
| current_workers | INTEGER | 254, 153, 214, 1288 | 332 | ✓ |

### 書き込み元の値

| カラム | 値の取得元 |
|--------|----------|
| current_index | 関数の引数 |
| total_count | 関数の引数 |
| current_file | 関数の引数 |
| success_count | 関数の引数 |
| error_count | 関数の引数 |
| logs | 関数の引数 |
| cpu_percent | `get_cgroup_cpu()` - 常に値を返す（エラー時はpsutilにフォールバック） |
| memory_percent | `get_cgroup_memory()['percent']` - 常に値を返す |
| memory_used_gb | `get_cgroup_memory()['used_gb']` - 常に値を返す |
| memory_total_gb | `get_cgroup_memory()['total_gb']` - 常に値を返す |
| throttle_delay | `processing_status['resource_control']['throttle_delay']` |
| adjustment_count | `processing_status['resource_control']['adjustment_count']` |
| max_parallel | `processing_status['resource_control']['max_parallel']` |
| current_workers | `processing_status['resource_control']['current_parallel']` |

### SQLファイルの正確性検証

**ファイル:** `database/migrations/add_processing_lock_columns.sql`

**全14カラムがSQLに含まれている:**
- current_index ✓
- total_count ✓
- current_file ✓
- success_count ✓
- error_count ✓
- logs ✓
- cpu_percent ✓
- memory_percent ✓
- memory_used_gb ✓
- memory_total_gb ✓
- throttle_delay ✓
- adjustment_count ✓
- max_parallel ✓
- current_workers ✓

**型の正確性:**
- INTEGER: current_index, total_count, success_count, error_count, adjustment_count, max_parallel, current_workers ✓
- TEXT: current_file ✓
- JSONB: logs ✓
- REAL: cpu_percent, memory_percent, memory_used_gb, memory_total_gb, throttle_delay ✓

**DEFAULT値の正確性:**
- 数値カラム: DEFAULT 0 または DEFAULT 0.0 ✓
- TEXT: DEFAULT '' ✓
- JSONB: DEFAULT '[]'::jsonb ✓

## 結論

**100000% 確実です。**

1. app.pyが書き込む全14カラムがSQLに含まれている
2. 型が全て正しい
3. DEFAULT値が全て正しい
4. 余計なカラムは含まれていない
5. PostgreSQL構文が正しい

このSQLを実行すれば、リアルタイム表示が動作します。
```

### frontend\__init__.py

```py
"""
UI Package
後方互換性のために存在します。
"""
```

### frontend\components\__init__.py

```py
"""
UI Components
"""
```

### frontend\components\email_viewer.py

```py
"""
Email Viewer Component

メール専用の表示コンポーネント
- メール一覧（表形式）
- メール詳細表示（メールらしい見た目）
"""
import streamlit as st
from datetime import datetime
from typing import Dict, List, Any, Optional
import json
import html
import pandas as pd


def render_email_list(emails: List[Dict[str, Any]]) -> tuple[Optional[int], pd.DataFrame]:
    """
    メール一覧を表形式で表示（チェックボックス付き）

    Args:
        emails: メールドキュメントのリスト

    Returns:
        選択されたメールのインデックス（None の場合は未選択）と編集されたDataFrame
    """
    st.subheader("📬 受信メール一覧")

    if not emails:
        st.info("メールがありません")
        return None, None

    # メールのDataFrameを作成（チェックボックス付き）
    df_data = []
    for email in emails:
        metadata = email.get('metadata', {})

        # メールの基本情報を取得 - display_*フィールドを優先的に使用
        # display_*フィールドはGmail取り込み時に正規化されたもの
        sender = email.get('display_sender', metadata.get('from', '送信者不明'))
        sender_email = email.get('display_sender_email', '')
        subject = email.get('display_subject', metadata.get('subject', '(件名なし)'))
        date_str = email.get('display_sent_at', metadata.get('date', ''))

        # 送信者名とメールアドレスを表示用に整形
        if sender_email and sender:
            sender_display = f"{sender} ({sender_email})"
        elif sender_email:
            sender_display = sender_email
        elif sender and '<' in sender and '>' in sender:
            # metadata.fromの場合の後方互換性: "名前 <email>" の形式から名前だけを取得
            sender_display = sender.split('<')[0].strip().strip('"')
        else:
            sender_display = sender

        # 日付をフォーマット
        try:
            display_date = date_str[:10] if date_str else ""
        except:
            display_date = date_str

        df_data.append({
            '選択': False,  # チェックボックス用
            '件名': subject,
            '送信者': sender_display,
            '送信日時': display_date,
            '送信者メール': sender_email  # CSVエクスポート用に追加
        })

    df = pd.DataFrame(df_data)

    # データエディタでチェックボックス付きの表を表示
    edited_df = st.data_editor(
        df,
        use_container_width=True,
        height=200,
        hide_index=True,
        column_config={
            "選択": st.column_config.CheckboxColumn(
                "選択",
                help="削除するメールを選択",
                default=False,
            ),
            "送信者メール": st.column_config.TextColumn(
                "送信者メールアドレス",
                help="送信者のメールアドレス",
            )
        },
        disabled=["件名", "送信者", "送信日時", "送信者メール"],
        key="email_list_editor"
    )

    # セレクトボックスでメールを選択
    selected_index = st.selectbox(
        "表示するメールを選択",
        range(len(emails)),
        format_func=lambda i: f"{df_data[i]['件名']} ({df_data[i]['送信者']})",
        key="email_selector"
    )

    return selected_index, edited_df


def render_email_detail(email: Dict[str, Any]):
    """
    メール詳細をタブ形式で表示（PDFレビューと同じスタイル）

    Args:
        email: メールドキュメント
    """
    metadata = email.get('metadata', {})

    # デバッグ: データソースを確認
    with st.expander("🔍 データソース確認", expanded=False):
        st.markdown("**documents.summary:**")
        doc_summary = email.get('summary', '')
        st.code(str(doc_summary) if doc_summary else "なし")
        st.markdown(f"長さ: {len(str(doc_summary)) if doc_summary else 0} 文字")

        st.markdown("**metadata.summary:**")
        meta_summary = metadata.get('summary', '')
        st.code(str(meta_summary) if meta_summary else "なし")
        st.markdown(f"長さ: {len(str(meta_summary)) if meta_summary else 0} 文字")

        st.markdown("**attachment_text:**")
        attachment_text = email.get('attachment_text', '')
        st.code(str(attachment_text) if attachment_text else "なし")
        st.markdown(f"長さ: {len(str(attachment_text)) if attachment_text else 0} 文字")

    # summaryフィールドからJSONデータを抽出
    # 優先順位: documents.summary > metadata.summary
    email_data = {}
    summary_raw = email.get('summary', metadata.get('summary', ''))

    # JSONパースを試みる
    parse_success = False
    if summary_raw and isinstance(summary_raw, str):
        # ```jsonマーカーを削除
        json_str = summary_raw
        if json_str.startswith('```json'):
            json_str = json_str.replace('```json', '').replace('```', '').strip()
        elif json_str.startswith('```'):
            json_str = json_str.replace('```', '').strip()

        # JSONとしてパース
        if json_str.startswith('{'):
            try:
                email_data = json.loads(json_str)
                parse_success = True
            except json.JSONDecodeError as e:
                # エスケープシーケンスエラーの場合、修正を試みる
                error_msg = str(e)
                if 'escape' in error_msg.lower():
                    try:
                        # 不正なエスケープシーケンスを修正
                        # raw_unicode_escapeでデコードしてから再エンコード
                        import re
                        # バックスラッシュを二重エスケープ
                        fixed_str = json_str.replace('\\', '\\\\')
                        # 正しいエスケープシーケンスを元に戻す
                        fixed_str = fixed_str.replace('\\\\n', '\\n')
                        fixed_str = fixed_str.replace('\\\\t', '\\t')
                        fixed_str = fixed_str.replace('\\\\r', '\\r')
                        fixed_str = fixed_str.replace('\\\\"', '\\"')
                        fixed_str = re.sub(r'\\\\u([0-9a-fA-F]{4})', r'\\u\1', fixed_str)
                        # \\\\ -> \\ (二重バックスラッシュを単一に)
                        fixed_str = fixed_str.replace('\\\\\\\\', '\\\\')

                        email_data = json.loads(fixed_str)
                        parse_success = True
                        st.success("✅ エスケープエラーを修正してデータを読み込みました")
                    except:
                        pass

                # それでも失敗した場合、正規表現で重要フィールドを抽出
                if not parse_success:
                    st.warning(f"⚠️ JSON解析に失敗しました。重要なフィールドのみ抽出します。")
                    import re

                    # デバッグ情報を表示
                    with st.expander("🔍 デバッグ: JSON内容を確認", expanded=False):
                        st.markdown("**元のJSON:**")
                        st.code(json_str)
                        st.markdown("**JSON文字列の長さ:**")
                        st.code(f"{len(json_str)} 文字")

                    # より柔軟な正規表現で抽出
                    # "summary": "..." を抽出（エスケープされた引用符も考慮）
                    summary_match = re.search(r'"summary"\s*:\s*"((?:[^"\\]|\\.)*)"', json_str, re.DOTALL)
                    if summary_match:
                        summary_value = summary_match.group(1)
                        # エスケープシーケンスを復元
                        summary_value = summary_value.replace('\\n', '\n').replace('\\t', '\t').replace('\\"', '"').replace('\\\\', '\\')
                        email_data['summary'] = summary_value
                        st.info(f"✓ 要約を抽出しました（{len(summary_value)}文字）")

                    # "extracted_text": "..." を抽出
                    extracted_match = re.search(r'"extracted_text"\s*:\s*"((?:[^"\\]|\\.)*)"', json_str, re.DOTALL)
                    if extracted_match:
                        extracted_value = extracted_match.group(1)
                        # エスケープシーケンスを復元（最初の3000文字まで）
                        extracted_value = extracted_value[:3000]
                        extracted_value = extracted_value.replace('\\n', '\n').replace('\\t', '\t').replace('\\"', '"').replace('\\\\', '\\')
                        email_data['extracted_text'] = extracted_value
                        st.info(f"✓ 本文を抽出しました（{len(extracted_value)}文字）")

                    # "key_information": [...] を抽出
                    key_info_match = re.search(r'"key_information"\s*:\s*\[(.*?)\]', json_str, re.DOTALL)
                    if key_info_match:
                        try:
                            key_info_str = '[' + key_info_match.group(1) + ']'
                            email_data['key_information'] = json.loads(key_info_str)
                            st.info(f"✓ 重要情報を抽出しました（{len(email_data['key_information'])}件）")
                        except:
                            pass

                    # 抽出できたフィールドを表示
                    with st.expander("📊 抽出できたフィールド", expanded=False):
                        st.json({
                            "summary": bool(email_data.get('summary')),
                            "extracted_text": bool(email_data.get('extracted_text')),
                            "key_information": bool(email_data.get('key_information')),
                            "summary_length": len(email_data.get('summary', '')),
                            "extracted_text_length": len(email_data.get('extracted_text', ''))
                        })

                    if email_data:
                        parse_success = True

    # パースに失敗した場合はmetadataを使用
    if not parse_success or not email_data:
        email_data = metadata.copy() if metadata else {}

        # metadataに直接extracted_textやsummaryがある場合は使用
        # ただし、JSON文字列の場合は除外
        if 'summary' in metadata:
            meta_summary = metadata.get('summary', '')
            if meta_summary and not (isinstance(meta_summary, str) and (meta_summary.startswith('{') or meta_summary.startswith('```'))):
                email_data['summary'] = meta_summary

        # extracted_textがmetadataに直接ある場合
        if 'extracted_text' not in email_data or not email_data.get('extracted_text'):
            # attachment_textをextracted_textとして使用（構造化されていない場合のみ）
            attachment_text = email.get('attachment_text', '')
            if attachment_text and '要約:' not in attachment_text:
                email_data['extracted_text'] = attachment_text

    st.markdown("### ✏️ メール情報")

    # タブで情報を整理（要約を最初に）
    tab1, tab2, tab3, tab4 = st.tabs(["📊 要約", "📄 本文", "🔍 重要情報", "⚙️ メタデータ"])

    with tab1:
        st.markdown("#### メール要約")

        # 送信元 - display_*フィールドを優先的に使用
        st.markdown("**📤 送信元**")
        sender = email.get('display_sender', metadata.get('from', '不明'))
        sender_email = email.get('display_sender_email', '')
        # 送信者名とメールアドレスを整形
        if sender_email and sender:
            sender_display = f"{sender} ({sender_email})"
        elif sender_email:
            sender_display = sender_email
        elif sender and '<' in sender and '>' in sender:
            # metadata.fromの後方互換性
            sender_display = sender.split('<')[0].strip().strip('"')
            sender_email = sender.split('<')[1].split('>')[0]
            sender_display = f"{sender_display} ({sender_email})"
        else:
            sender_display = sender
        st.info(sender_display)

        # 宛先
        st.markdown("**📥 宛先**")
        recipient = metadata.get('to', '不明')
        st.info(recipient)

        # 送信日 - display_sent_atを優先的に使用
        st.markdown("**📅 送信日**")
        send_date = email.get('display_sent_at', metadata.get('date', '不明'))
        st.info(send_date)

        # 受信日（created_atを使用）
        st.markdown("**📩 受信日**")
        received_date = email.get('created_at', '不明')
        # ISO形式の日時を読みやすく整形
        if received_date and received_date != '不明':
            try:
                from datetime import datetime
                dt = datetime.fromisoformat(received_date.replace('Z', '+00:00'))
                received_date = dt.strftime('%Y-%m-%d %H:%M:%S')
            except:
                pass
        st.info(received_date)

        # 本文要約
        st.markdown("**📝 本文要約**")
        # パース済みのemail_dataから要約を取得
        summary_text = email_data.get('summary', '')

        # summary_textがJSON文字列の場合は使用しない
        if summary_text and not (summary_text.startswith('{') or summary_text.startswith('```')):
            st.info(summary_text)
        else:
            # 要約が見つからない場合は、extracted_textの全文を表示
            extracted = email_data.get('extracted_text', '')
            if extracted:
                # From:, To:などのメタデータ行を除外
                lines = extracted.split('\n')
                clean_lines = [line for line in lines if not (line.startswith('From:') or line.startswith('To:') or line.startswith('Date:'))]
                summary_preview = '\n'.join(clean_lines).strip()
                st.info(summary_preview)
            else:
                st.info("要約がありません")

        # 画像の説明がある場合
        image_descriptions = email_data.get('image_descriptions', [])
        if image_descriptions:
            st.markdown("**📷 画像の説明**")
            for desc in image_descriptions:
                st.info(f"• {desc}")

    with tab2:
        st.markdown("#### メール本文（全文）")

        # extracted_textを取得
        extracted_text = email_data.get('extracted_text', '')

        # extracted_textがない場合は、metadataから取得
        if not extracted_text:
            extracted_text = metadata.get('extracted_text', '')

        # attachment_textは最後の手段（構造化されたテキストが含まれている可能性がある）
        if not extracted_text:
            attachment_text = email.get('attachment_text', '')
            # attachment_textに「要約:」などの構造が含まれている場合は除外
            if attachment_text and '要約:' not in attachment_text:
                extracted_text = attachment_text

        if extracted_text:
            # From, To, Date行と画像表示についての注意書きを除外
            lines = extracted_text.split('\n')
            body_lines = []
            skip_next = False

            for line in lines:
                # メタデータ行をスキップ
                if line.startswith('From:') or line.startswith('To:') or line.startswith('Date:'):
                    continue
                if '!画像表示について:' in line:
                    skip_next = True
                    continue
                if skip_next and ('End' in line or 'すべての画像を表示' in line):
                    skip_next = False
                    continue
                if not skip_next:
                    body_lines.append(line)

            body_text = '\n'.join(body_lines).strip()

            # テキストエリアで表示（スクロール可能、コピペ可能）
            st.text_area("", body_text, height=500, label_visibility="collapsed", key="email_body_text")
        else:
            # デバッグ情報を表示
            st.warning("本文が見つかりません")
            with st.expander("🔍 デバッグ情報", expanded=False):
                st.markdown("**email_dataのキー:**")
                st.code(str(list(email_data.keys())))
                st.markdown("**emailのキー:**")
                st.code(str(list(email.keys())))
                st.markdown("**metadataのキー:**")
                st.code(str(list(metadata.keys())))
                if summary:
                    st.markdown("**summary:**")
                    st.code(summary)

    with tab3:
        st.markdown("#### 重要な情報")

        # key_informationを表示
        key_info = email_data.get('key_information', [])

        if key_info and isinstance(key_info, list) and len(key_info) > 0:
            for i, info in enumerate(key_info, 1):
                st.markdown(f"{i}. {info}")
        else:
            st.info("重要な情報が抽出されていません")

        # リンクがある場合
        links = email_data.get('links', metadata.get('links', []))
        if links and len(links) > 0:
            st.markdown("---")
            st.markdown("#### 🔗 リンク")

            # リンクが多い場合は折りたたみ可能にする
            if len(links) > 5:
                with st.expander(f"リンク一覧 ({len(links)}件)", expanded=False):
                    for i, link in enumerate(links, 1):
                        # リンク形式を判定
                        if link.startswith('http'):
                            st.markdown(f"{i}. [{link}]({link})")
                        else:
                            st.markdown(f"{i}. {link}")
            else:
                for i, link in enumerate(links, 1):
                    if link.startswith('http'):
                        st.markdown(f"{i}. [{link}]({link})")
                    else:
                        st.markdown(f"{i}. {link}")

        # 画像がある場合
        has_images = email_data.get('has_images', False)
        if has_images:
            st.info("📷 このメールには画像が含まれています（HTMLプレビューで確認できます）")

    with tab4:
        st.markdown("#### メタデータ")

        # 主要なメタデータを読みやすく表示
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**送信者**")
            st.code(metadata.get('from', '不明'), language=None)

            st.markdown("**宛先**")
            st.code(metadata.get('to', '不明'), language=None)

        with col2:
            st.markdown("**件名**")
            st.code(metadata.get('subject', '(件名なし)'), language=None)

            st.markdown("**送信日時**")
            st.code(metadata.get('date', '不明'), language=None)

        # Workspace情報
        st.markdown("**Workspace**")
        st.code(email.get('workspace', 'unknown'), language=None)

        # Gmail Label
        gmail_label = metadata.get('gmail_label') or email.get('gmail_label')
        if gmail_label:
            st.markdown("**Gmail Label**")
            st.code(gmail_label, language=None)

        # 完全なメタデータJSONは折りたたみで表示
        with st.expander("🔍 完全なメタデータ（JSON）", expanded=False):
            st.json(metadata)

    # Google Drive HTMLファイルへのリンク
    st.divider()
    drive_file_id = email.get('drive_file_id') or email.get('source_id')
    if drive_file_id:
        col1, col2 = st.columns(2)
        with col1:
            st.link_button(
                "📥 元のHTMLをダウンロード",
                f"https://drive.google.com/uc?export=download&id={drive_file_id}",
                use_container_width=True
            )
        with col2:
            st.link_button(
                "👁️ Google Driveで表示",
                f"https://drive.google.com/file/d/{drive_file_id}/view",
                use_container_width=True
            )


def render_email_html_preview(email: Dict[str, Any], drive_connector=None):
    """
    メールのHTMLプレビューを表示

    Args:
        email: メールドキュメント
        drive_connector: GoogleDriveConnector インスタンス（オプション）
    """
    st.markdown("### 📧 メールプレビュー")

    # メールドキュメントの検証
    if not email:
        st.warning("メールデータが見つかりません")
        return

    drive_file_id = email.get('drive_file_id') or email.get('source_id')

    if not drive_file_id:
        st.info("プレビュー可能なHTMLファイルがありません")
        # デバッグ情報
        with st.expander("🔍 デバッグ情報"):
            st.json({
                "email_keys": list(email.keys()),
                "drive_file_id": drive_file_id,
                "source_id": email.get('source_id')
            })
        return

    # Google DriveからHTMLをダウンロードして表示
    try:
        if drive_connector is None:
            from shared.common.connectors.google_drive import GoogleDriveConnector
            drive_connector = GoogleDriveConnector()

        import tempfile
        temp_dir = tempfile.gettempdir()

        # より安全なファイル名の取得
        email_id = email.get('id', 'unknown')
        file_name = email.get('file_name', f"email_{email_id}.html")

        with st.spinner("メールHTMLを読み込み中..."):
            file_path = drive_connector.download_file(drive_file_id, file_name, temp_dir)

            if file_path:
                # HTMLファイルを読み込み
                with open(file_path, 'r', encoding='utf-8') as f:
                    html_content = f.read()

                # iframeでHTMLを表示（セキュリティを考慮してサンドボックス化）
                st.components.v1.html(
                    html_content,
                    height=700,
                    scrolling=True
                )
            else:
                st.warning("HTMLファイルのダウンロードに失敗しました")

    except Exception as e:
        error_str = str(e)

        # 404エラーの場合は特別なメッセージを表示
        if "File not found" in error_str or "404" in error_str:
            st.warning("⚠️ HTMLファイルがGoogle Driveで見つかりませんでした")
            st.info("""
            考えられる原因：
            - ファイルが削除されている
            - サービスアカウントにアクセス権限がない
            - ファイルIDが正しくない
            """)
        else:
            st.error(f"HTMLプレビューの表示中にエラーが発生しました")

        # デバッグ情報を表示
        with st.expander("🔍 エラー詳細"):
            st.text(f"エラー: {error_str}")
            import traceback
            st.code(traceback.format_exc())
            st.json({
                "email_data": {
                    "id": email.get('id'),
                    "drive_file_id": drive_file_id,
                    "file_name": email.get('file_name'),
                    "available_keys": list(email.keys())
                }
            })

        # フォールバック：リンクボタンを表示
        if drive_file_id:
            st.markdown("---")
            st.caption("Google Driveで直接確認してください：")
            col1, col2 = st.columns(2)
            with col1:
                st.link_button(
                    "📥 元のHTMLをダウンロード",
                    f"https://drive.google.com/uc?export=download&id={drive_file_id}",
                    use_container_width=True
                )
            with col2:
                st.link_button(
                    "👁️ Google Driveで表示",
                    f"https://drive.google.com/file/d/{drive_file_id}/view",
                    use_container_width=True
                )


def render_email_filters() -> Dict[str, Any]:
    """
    メールフィルター（workspace, 期間など）

    Returns:
        フィルター条件の辞書
    """
    st.sidebar.markdown("### 🔍 メールフィルター")

    filters = {}

    # workspace フィルター
    workspace_options = [
        "すべて",
        "DM_MAIL",
        "WORK_MAIL",
        "IKUYA_MAIL",
        "EMA_MAIL",
        "MONEY_MAIL",
        "JOB_MAIL",
    ]
    selected_workspace = st.sidebar.selectbox(
        "Workspace",
        workspace_options
    )
    if selected_workspace != "すべて":
        filters['workspace'] = selected_workspace

    # 期間フィルター
    date_range = st.sidebar.radio(
        "期間",
        ["すべて", "今日", "今週", "今月", "カスタム"]
    )

    if date_range == "カスタム":
        col1, col2 = st.sidebar.columns(2)
        with col1:
            start_date = st.date_input("開始日")
            filters['start_date'] = start_date
        with col2:
            end_date = st.date_input("終了日")
            filters['end_date'] = end_date

    # 検索キーワード
    keyword = st.sidebar.text_input("🔎 キーワード検索")
    if keyword:
        filters['keyword'] = keyword

    return filters
```

### frontend\components\form_editor.py

```py
"""
Form Editor Component
スキーマベースのフォーム編集UI
"""
import streamlit as st
from typing import Dict, Any, List
from datetime import datetime, date


def _calculate_text_height(text: str, min_height: int = 100, max_height: int = 600) -> int:
    """
    テキストの内容に基づいて適切な高さを計算

    Args:
        text: テキストの内容
        min_height: 最小の高さ（ピクセル）
        max_height: 最大の高さ（ピクセル）

    Returns:
        計算された高さ（ピクセル）
    """
    if not text:
        return min_height

    # 行数をカウント
    line_count = text.count('\n') + 1

    # 各行の長さをチェックして折り返しを考慮
    # 1行あたり約80文字で折り返すと仮定
    estimated_lines = 0
    for line in text.split('\n'):
        estimated_lines += max(1, len(line) // 80 + (1 if len(line) % 80 else 0))

    # より正確な行数を使用
    total_lines = max(line_count, estimated_lines)

    # 1行あたり約25ピクセルとして計算
    calculated_height = total_lines * 25

    # min_heightとmax_heightの範囲内に収める
    return max(min_height, min(calculated_height, max_height))


def _is_empty_value(value: Any) -> bool:
    """
    値が空かどうかをチェック

    Args:
        value: チェックする値

    Returns:
        空の場合True、値がある場合False
    """
    if value is None:
        return True
    if isinstance(value, str) and not value.strip():
        return True
    if isinstance(value, (list, dict)) and len(value) == 0:
        return True
    return False


def render_form_editor(metadata: Dict[str, Any], fields: List[Dict[str, Any]], doc_id: str = None) -> Dict[str, Any]:
    """
    スキーマ定義に基づいてフォーム形式のエディタを表示

    Args:
        metadata: 現在のメタデータ
        fields: スキーマから取得したフィールド定義リスト
        doc_id: ドキュメントID（widgetのkeyに使用してキャッシュ問題を回避）

    Returns:
        編集後のメタデータ
    """
    edited_metadata = {}

    st.markdown("### 📝 フォーム編集")
    st.markdown("各フィールドを個別に編集できます")
    st.markdown("---")

    for field in fields:
        field_name = field["name"]

        # text_blocksの特別処理: 最初に処理して他の処理をスキップ
        if field_name == "text_blocks":
            current_value = metadata.get(field_name)
            if not _is_empty_value(current_value):
                edited_metadata[field_name] = _render_text_blocks_input(
                    field_name, "", current_value, field.get("items"), doc_id
                )
            continue  # 他の処理をスキップ

        field_type = field["type"]
        field_title = field["title"]
        field_description = field["description"]
        required = field["required"]
        current_value = metadata.get(field_name)

        # 空のフィールドをスキップ（必須フィールドは除く）
        if not required and _is_empty_value(current_value):
            continue

        # フィールドラベル
        label = f"{'🔴 ' if required else ''}{field_title}"
        help_text = field_description if field_description else None

        # 型に応じた入力ウィジェット
        if field_type == "string":
            if field.get("format") == "date":
                # 日付入力
                edited_metadata[field_name] = _render_date_input(
                    field_name, label, current_value, help_text, doc_id
                )
            elif field.get("enum"):
                # 選択肢入力
                edited_metadata[field_name] = _render_select_input(
                    field_name, label, current_value, field["enum"], help_text, doc_id
                )
            else:
                # テキスト入力
                widget_key = f"form_{doc_id}_{field_name}" if doc_id else f"form_{field_name}"
                edited_metadata[field_name] = st.text_input(
                    label,
                    value=current_value if current_value else "",
                    help=help_text,
                    key=widget_key
                )

        elif field_type == "integer":
            # 整数入力
            widget_key = f"form_{doc_id}_{field_name}" if doc_id else f"form_{field_name}"
            edited_metadata[field_name] = st.number_input(
                label,
                value=int(current_value) if current_value is not None else 0,
                step=1,
                help=help_text,
                key=widget_key
            )

        elif field_type == "number":
            # 数値入力
            widget_key = f"form_{doc_id}_{field_name}" if doc_id else f"form_{field_name}"
            edited_metadata[field_name] = st.number_input(
                label,
                value=float(current_value) if current_value is not None else 0.0,
                help=help_text,
                key=widget_key
            )

        elif field_type == "boolean":
            # チェックボックス
            widget_key = f"form_{doc_id}_{field_name}" if doc_id else f"form_{field_name}"
            edited_metadata[field_name] = st.checkbox(
                label,
                value=bool(current_value) if current_value is not None else False,
                help=help_text,
                key=widget_key
            )

        elif field_type == "array":
            # 配列入力
            edited_metadata[field_name] = _render_array_input(
                field_name, label, current_value, field.get("items"), help_text, doc_id
            )

        elif field_type == "object":
            # オブジェクト入力（展開表示）
            with st.expander(label, expanded=False):
                if field_description:
                    st.caption(field_description)
                edited_metadata[field_name] = _render_object_input(
                    field_name, current_value, doc_id
                )

        else:
            # その他の型はテキストエリアで表示
            text_value = str(current_value) if current_value else ""
            widget_key = f"form_{doc_id}_{field_name}" if doc_id else f"form_{field_name}"
            edited_metadata[field_name] = st.text_area(
                label,
                value=text_value,
                height=_calculate_text_height(text_value),
                help=help_text,
                key=widget_key
            )

    return edited_metadata


def _render_date_input(field_name: str, label: str, current_value: Any, help_text: str, doc_id: str = None) -> str:
    """日付入力フィールド"""
    if current_value:
        try:
            if isinstance(current_value, str):
                current_date = datetime.strptime(current_value, "%Y-%m-%d").date()
            else:
                current_date = current_value
        except:
            current_date = date.today()
    else:
        current_date = date.today()

    widget_key = f"form_{doc_id}_{field_name}" if doc_id else f"form_{field_name}"
    selected_date = st.date_input(
        label,
        value=current_date,
        help=help_text,
        key=widget_key
    )

    return selected_date.strftime("%Y-%m-%d")


def _render_select_input(field_name: str, label: str, current_value: Any, options: List[str], help_text: str, doc_id: str = None) -> str:
    """選択肢入力フィールド"""
    if current_value and current_value in options:
        index = options.index(current_value)
    else:
        index = 0

    widget_key = f"form_{doc_id}_{field_name}" if doc_id else f"form_{field_name}"
    return st.selectbox(
        label,
        options=options,
        index=index,
        help=help_text,
        key=widget_key
    )


def _render_array_input(field_name: str, label: str, current_value: Any, items_def: Dict, help_text: str, doc_id: str = None) -> List:
    """配列入力フィールド"""
    if not current_value:
        current_value = []

    # 配列の型に応じた処理
    if items_def and items_def.get("type") == "string":
        # 文字列配列: テキストエリアで改行区切り入力
        st.markdown(f"**{label}**")
        if help_text:
            st.caption(help_text)

        text_value = "\n".join(current_value) if isinstance(current_value, list) else ""
        widget_key = f"form_{doc_id}_{field_name}" if doc_id else f"form_{field_name}"
        edited_text = st.text_area(
            f"{label}（1行1項目）",
            value=text_value,
            height=_calculate_text_height(text_value, min_height=100, max_height=400),
            label_visibility="collapsed",
            key=widget_key
        )
        return [line.strip() for line in edited_text.split("\n") if line.strip()]

    elif items_def and items_def.get("type") == "object":
        # オブジェクト配列: ラベルを表示してから処理
        st.markdown(f"**{label}**")
        if help_text:
            st.caption(help_text)
        return _render_object_array_input(field_name, label, current_value, items_def, doc_id)

    else:
        # デフォルト: JSON文字列として表示
        import json
        json_str = json.dumps(current_value, ensure_ascii=False, indent=2)
        widget_key = f"form_{doc_id}_{field_name}" if doc_id else f"form_{field_name}"
        edited_json = st.text_area(
            f"{label}（JSON形式）",
            value=json_str,
            height=_calculate_text_height(json_str, min_height=150, max_height=500),
            label_visibility="collapsed",
            key=widget_key
        )
        try:
            return json.loads(edited_json)
        except:
            st.error("JSON形式が不正です")
            return current_value


def _render_text_blocks_input(field_name: str, label: str, current_value: List[Dict], items_def: Dict, doc_id: str = None) -> List[Dict]:
    """
    text_blocks専用の入力フィールド
    各ブロックをtitleをヘッダーとするボックスで表示

    Args:
        field_name: フィールド名
        label: ラベル
        current_value: 現在の値（text_blocksの配列）
        items_def: スキーマのitems定義
        doc_id: ドキュメントID（widgetのkeyに使用）

    Returns:
        編集後のtext_blocks配列
    """
    if not current_value:
        current_value = []

    edited_array = []

    for idx, block in enumerate(current_value):
        block_title = block.get("title", f"セクション {idx + 1}")
        block_content = block.get("content", "")

        # 各ブロックをexpanderで表示
        with st.expander(f"📝 {block_title}", expanded=True):
            # コンテンツの編集のみ（ラベル非表示）
            widget_key = f"form_{doc_id}_{field_name}_{idx}_content" if doc_id else f"form_{field_name}_{idx}_content"
            edited_content = st.text_area(
                "本文",
                value=block_content,
                height=_calculate_text_height(block_content, min_height=150, max_height=600),
                key=widget_key,
                label_visibility="collapsed"
            )

            edited_array.append({
                "title": block_title,  # タイトルは編集不可、元の値を保持
                "content": edited_content
            })

    # 追加・削除コントロール
    st.markdown("---")
    col1, col2 = st.columns(2)

    with col1:
        button_key = f"add_{doc_id}_{field_name}" if doc_id else f"add_{field_name}"
        if st.button(f"➕ 新しいセクションを追加", key=button_key):
            st.info("💡 保存後、新しいセクションが追加されます")

    with col2:
        if len(edited_array) > 0:
            button_key = f"remove_{doc_id}_{field_name}" if doc_id else f"remove_{field_name}"
            if st.button(f"🗑️ 最後のセクションを削除", key=button_key):
                edited_array = edited_array[:-1]
                st.success("最後のセクションを削除しました")

    return edited_array


def _render_object_array_input(field_name: str, label: str, current_value: List[Dict], items_def: Dict, doc_id: str = None) -> List[Dict]:
    """
    オブジェクト配列入力フィールド（スキーマ定義に基づく）

    Args:
        field_name: フィールド名
        label: ラベル
        current_value: 現在の値（オブジェクトの配列）
        items_def: スキーマのitems定義
        doc_id: ドキュメントID（widgetのkeyに使用）

    Returns:
        編集後のオブジェクト配列
    """
    if not current_value:
        current_value = []

    # スキーマからプロパティ定義を取得
    properties = items_def.get("properties", {})
    required_fields = items_def.get("required", [])

    # プロパティ定義がない場合は、フォールバックとしてJSON編集
    if not properties:
        st.info("📝 このフィールドはJSON形式で編集してください")
        edited_array = []
        for idx, item in enumerate(current_value):
            with st.expander(f"項目 {idx + 1}", expanded=False):
                edited_item = _render_object_input(f"{field_name}_{idx}", item, doc_id)
                edited_array.append(edited_item)
        return edited_array

    # 各アイテムをアコーディオンで表示
    edited_array = []
    for idx, item in enumerate(current_value):
        # アイテムのタイトルを生成（titleフィールドがあれば使用）
        item_title = item.get("title", f"項目 {idx + 1}")

        with st.expander(f"📄 {item_title}", expanded=False):
            edited_item = {}

            # 各プロパティを個別の入力フィールドとして表示
            for prop_name, prop_def in properties.items():
                prop_type = prop_def.get("type", "string")
                prop_title = prop_def.get("title", prop_name)
                prop_description = prop_def.get("description", "")
                is_required = prop_name in required_fields

                prop_value = item.get(prop_name, "")

                # 空のフィールドをスキップ（必須フィールドとtitle/contentは除く）
                # title/contentは文書セクションで重要なので常に表示
                if not is_required and prop_name not in ["title", "content"] and _is_empty_value(prop_value):
                    continue

                # フィールドラベル
                prop_label = f"{'🔴 ' if is_required else ''}{prop_title}"

                # 型に応じた入力ウィジェット
                if prop_type == "string":
                    # contentフィールドは大きなテキストエリアで表示
                    if prop_name == "content" or len(str(prop_value)) > 100:
                        text_value = str(prop_value) if prop_value else ""
                        widget_key = f"form_{doc_id}_{field_name}_{idx}_{prop_name}" if doc_id else f"form_{field_name}_{idx}_{prop_name}"
                        edited_item[prop_name] = st.text_area(
                            prop_label,
                            value=text_value,
                            height=_calculate_text_height(text_value, min_height=150, max_height=600),
                            help=prop_description,
                            key=widget_key
                        )
                    else:
                        widget_key = f"form_{doc_id}_{field_name}_{idx}_{prop_name}" if doc_id else f"form_{field_name}_{idx}_{prop_name}"
                        edited_item[prop_name] = st.text_input(
                            prop_label,
                            value=str(prop_value) if prop_value else "",
                            help=prop_description,
                            key=widget_key
                        )

                elif prop_type == "integer":
                    widget_key = f"form_{doc_id}_{field_name}_{idx}_{prop_name}" if doc_id else f"form_{field_name}_{idx}_{prop_name}"
                    edited_item[prop_name] = st.number_input(
                        prop_label,
                        value=int(prop_value) if prop_value is not None else 0,
                        step=1,
                        help=prop_description,
                        key=widget_key
                    )

                elif prop_type == "number":
                    widget_key = f"form_{doc_id}_{field_name}_{idx}_{prop_name}" if doc_id else f"form_{field_name}_{idx}_{prop_name}"
                    edited_item[prop_name] = st.number_input(
                        prop_label,
                        value=float(prop_value) if prop_value is not None else 0.0,
                        help=prop_description,
                        key=widget_key
                    )

                elif prop_type == "boolean":
                    widget_key = f"form_{doc_id}_{field_name}_{idx}_{prop_name}" if doc_id else f"form_{field_name}_{idx}_{prop_name}"
                    edited_item[prop_name] = st.checkbox(
                        prop_label,
                        value=bool(prop_value) if prop_value is not None else False,
                        help=prop_description,
                        key=widget_key
                    )

                else:
                    # その他の型はテキスト入力
                    widget_key = f"form_{doc_id}_{field_name}_{idx}_{prop_name}" if doc_id else f"form_{field_name}_{idx}_{prop_name}"
                    edited_item[prop_name] = st.text_input(
                        prop_label,
                        value=str(prop_value) if prop_value else "",
                        help=prop_description,
                        key=widget_key
                    )

            edited_array.append(edited_item)

    # 削除と追加のコントロール
    st.markdown("---")
    col1, col2 = st.columns(2)

    with col1:
        button_key = f"add_{doc_id}_{field_name}" if doc_id else f"add_{field_name}"
        if st.button(f"➕ 新しい項目を追加", key=button_key):
            st.info("💡 保存後、新しい項目が追加されます")

    with col2:
        if len(edited_array) > 0:
            button_key = f"remove_{doc_id}_{field_name}" if doc_id else f"remove_{field_name}"
            if st.button(f"🗑️ 最後の項目を削除", key=button_key):
                edited_array = edited_array[:-1]
                st.success("最後の項目を削除しました")

    return edited_array


def _render_object_input(field_name: str, current_value: Any, doc_id: str = None) -> Dict:
    """オブジェクト入力フィールド"""
    import json

    if not current_value:
        current_value = {}

    json_str = json.dumps(current_value, ensure_ascii=False, indent=2)
    widget_key = f"form_obj_{doc_id}_{field_name}" if doc_id else f"form_obj_{field_name}"
    edited_json = st.text_area(
        "JSON形式で編集",
        value=json_str,
        height=_calculate_text_height(json_str, min_height=150, max_height=500),
        key=widget_key
    )

    try:
        return json.loads(edited_json)
    except json.JSONDecodeError:
        st.error("JSON形式が不正です")
        return current_value
```

### frontend\components\json_preview.py

```py
"""
JSON Preview Component
JSON形式でのプレビューと編集UI
"""
import streamlit as st
import json
from typing import Dict, Any, Optional


def render_json_preview(metadata: Dict[str, Any], editable: bool = True, key_suffix: str = "") -> Optional[Dict[str, Any]]:
    """
    JSONプレビュー/編集UI

    Args:
        metadata: メタデータ
        editable: 編集可能かどうか
        key_suffix: Streamlitウィジェットのキーに付けるサフィックス

    Returns:
        編集可能な場合: 編集後のメタデータ
        編集不可の場合: None
    """
    st.markdown("### 🔍 JSONプレビュー")

    if editable:
        st.markdown("JSON形式で直接編集できます")
    else:
        st.markdown("JSON形式でデータを確認できます")

    st.markdown("---")

    # メタデータをJSON文字列に変換
    json_str = _format_json(metadata)

    # 統計情報の表示
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("フィールド数", len(metadata))
    with col2:
        st.metric("文字数", len(json_str))
    with col3:
        lines = json_str.count('\n') + 1
        st.metric("行数", lines)

    st.markdown("---")

    if editable:
        # 編集可能なテキストエリア
        edited_json_str = st.text_area(
            "JSON形式で編集",
            value=json_str,
            height=500,
            help="JSON形式で直接編集できます。保存前に構文エラーがないか確認してください。",
            key=f"json_editor_{key_suffix}" if key_suffix else "json_editor"
        )

        # JSON検証
        is_valid, parsed_data, error_msg = _validate_json(edited_json_str)

        if not is_valid:
            st.error(f"❌ JSON形式エラー: {error_msg}")
            st.code(edited_json_str, language="json")
            return None

        if edited_json_str != json_str:
            st.success("✅ 変更が検出されました")

        return parsed_data

    else:
        # 読み取り専用のコードブロック
        st.code(json_str, language="json", line_numbers=True)

        # ダウンロードボタン
        st.download_button(
            label="📥 JSONをダウンロード",
            data=json_str,
            file_name="metadata.json",
            mime="application/json"
        )

        return None


def _format_json(data: Dict[str, Any], indent: int = 2) -> str:
    """
    データをきれいなJSON文字列に変換

    Args:
        data: 変換するデータ
        indent: インデント幅

    Returns:
        整形されたJSON文字列
    """
    try:
        return json.dumps(data, ensure_ascii=False, indent=indent, sort_keys=False)
    except Exception as e:
        st.error(f"JSON変換エラー: {e}")
        return "{}"


def _validate_json(json_str: str) -> tuple[bool, Optional[Dict], str]:
    """
    JSON文字列を検証してパース

    Args:
        json_str: JSON文字列

    Returns:
        (検証結果, パースされたデータ, エラーメッセージ)
    """
    try:
        parsed = json.loads(json_str)
        return True, parsed, ""
    except json.JSONDecodeError as e:
        return False, None, str(e)
    except Exception as e:
        return False, None, f"予期しないエラー: {str(e)}"


def render_json_diff(original: Dict[str, Any], edited: Dict[str, Any]):
    """
    JSONの差分を表示

    Args:
        original: 元のデータ
        edited: 編集後のデータ
    """
    st.markdown("### 📝 変更内容")

    # 差分を計算
    changes = _calculate_diff(original, edited)

    if not changes:
        st.info("変更はありません")
        return

    # 変更内容を表示
    for change_type, items in changes.items():
        if not items:
            continue

        if change_type == "added":
            st.markdown("#### ➕ 追加されたフィールド")
            for key, value in items.items():
                st.code(f"{key}: {json.dumps(value, ensure_ascii=False)}", language="json")

        elif change_type == "modified":
            st.markdown("#### ✏️ 変更されたフィールド")
            for key, (old_val, new_val) in items.items():
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**変更前**")
                    st.code(json.dumps(old_val, ensure_ascii=False, indent=2), language="json")
                with col2:
                    st.markdown("**変更後**")
                    st.code(json.dumps(new_val, ensure_ascii=False, indent=2), language="json")

        elif change_type == "removed":
            st.markdown("#### ➖ 削除されたフィールド")
            for key in items:
                st.code(key, language="text")


def _calculate_diff(original: Dict[str, Any], edited: Dict[str, Any]) -> Dict[str, Any]:
    """
    2つの辞書の差分を計算

    Returns:
        {
            "added": {key: value},
            "modified": {key: (old_value, new_value)},
            "removed": [key]
        }
    """
    diff = {
        "added": {},
        "modified": {},
        "removed": []
    }

    # 追加されたフィールド
    for key in edited:
        if key not in original:
            diff["added"][key] = edited[key]

    # 削除されたフィールド
    for key in original:
        if key not in edited:
            diff["removed"].append(key)

    # 変更されたフィールド
    for key in original:
        if key in edited and original[key] != edited[key]:
            diff["modified"][key] = (original[key], edited[key])

    return diff
```

### frontend\components\manual_text_correction.py

```py
"""
手動テキスト補正コンポーネント (Human-in-the-loop)

Gemini Visionが取りこぼしたテキストを人間が補完し、
Stage H（Gemini 2.5 Flash）で再構造化する機能を提供します。

使用例:
- スキャンPDFで500文字のテキストがある
- Gemini Visionが一部しか拾えなかった（200文字）
- 人間が残りの300文字を手入力
- 完全なテキスト（500文字）+ Gemini Visionのレイアウト情報でStage H再実行
- → 高品質な構造化データが生成される
"""
import streamlit as st
from typing import Dict, Any, Optional
from loguru import logger
import difflib


def _highlight_diff(original: str, corrected: str) -> str:
    """
    2つのテキストの差分をハイライト表示用のマークダウンに変換

    Args:
        original: 元のテキスト
        corrected: 補正後のテキスト

    Returns:
        差分をハイライトしたマークダウン文字列
    """
    diff = list(difflib.unified_diff(
        original.split('\n'),
        corrected.split('\n'),
        lineterm='',
        n=0  # コンテキスト行数を0に
    ))

    if not diff:
        return "（変更なし）"

    result_lines = []
    for line in diff[2:]:  # 最初の2行はヘッダーなのでスキップ
        if line.startswith('+'):
            result_lines.append(f"**+ {line[1:]}**")  # 追加行を太字
        elif line.startswith('-'):
            result_lines.append(f"~~- {line[1:]}~~")  # 削除行を取り消し線
        else:
            result_lines.append(line)

    return '\n'.join(result_lines)


def render_manual_text_correction(
    doc_id: str,
    file_name: str,
    extracted_text: str,
    metadata: Dict[str, Any],
    doc_type: str,
    display_post_text: str = "",
    attachment_text: str = ""
) -> Optional[Dict[str, str]]:
    """
    手動テキスト補正UIをレンダリング

    このコンポーネントは以下の機能を提供します：
    1. Gemini Visionが抽出したテキストの表示
    2. 人間による手動補正・完全入力
    3. 補正前後の差分表示
    4. Stage H再実行ボタン

    Args:
        doc_id: ドキュメントID
        file_name: ファイル名
        extracted_text: 結合されたテキスト（表示用、下位互換性）
        metadata: 既存のメタデータ（Stage I統合の結果を含む）
        doc_type: ドキュメントタイプ
        display_post_text: Classroom投稿本文
        attachment_text: 添付ファイルのテキスト

    Returns:
        補正されたテキストの辞書（再実行が要求された場合）、またはNone
        {"display_post_text": str, "attachment_text": str}
    """
    st.markdown("---")
    st.markdown("### 🛠️ テキスト抽出の手動補正（Human-in-the-loop）")

    # 説明エリア
    with st.expander("💡 この機能について", expanded=False):
        st.markdown("""
        **Gemini Visionが取りこぼしたテキストを補完できます！**

        **使用例:**
        - スキャンされたPDFで、OCRが一部の文字を読めなかった場合
        - 手書き文字が含まれている場合
        - 複雑なレイアウトで抽出が不完全な場合

        **処理フロー:**
        1. 👇 下のエリアに正しいテキストを入力してください
        2. 🔄 「再構造化」ボタンを押すと...
        3. **完全なテキスト（人間）+ レイアウト情報（Vision）** でStage Hが再実行されます
        4. ✨ 構造化データの品質がレベルアップ！

        **ポイント:**
        - Gemini Visionのレイアウト情報（見出し、箇条書きなどの構造）は保持されます
        - Gemini 2.5 Flashが、完全なテキストとレイアウト情報を統合して構造化します
        """)

    # 現在の抽出状況
    col_info1, col_info2 = st.columns(2)
    with col_info1:
        st.metric("元の文字数", len(extracted_text))
    with col_info2:
        st.metric("ファイル名", file_name[:20] + "..." if len(file_name) > 20 else file_name)

    # Vision解析の情報を表示
    with st.expander("🔍 Gemini Visionの解析情報（保持されるレイアウト情報）"):
        st.json({
            "doc_type": doc_type,
            "summary": metadata.get('summary', '')[:200] + "...",
            "relevant_date": metadata.get('relevant_date')
        })

    st.markdown("---")

    # タブで編集方法を選択
    tab1, tab2 = st.tabs(["📝 全文編集", "📊 差分プレビュー"])

    # セッション状態でテキストを管理（2つのフィールドを別々に管理）
    if f'corrected_display_text_{doc_id}' not in st.session_state:
        st.session_state[f'corrected_display_text_{doc_id}'] = display_post_text
    if f'corrected_attachment_text_{doc_id}' not in st.session_state:
        st.session_state[f'corrected_attachment_text_{doc_id}'] = attachment_text

    corrected_texts = None

    with tab1:
        st.markdown("#### 全文を編集")
        st.info("💡 投稿本文と添付ファイルのテキストを別々に編集できます")

        # 投稿本文の編集
        st.markdown("**📧 投稿本文 (display_post_text)**")
        st.caption("Classroomの投稿本文、メールの件名・本文など")
        display_input = st.text_area(
            "投稿本文",
            value=st.session_state[f'corrected_display_text_{doc_id}'],
            height=200,
            key=f"manual_display_text_{doc_id}",
            help="Classroom投稿本文やメールの件名・本文を編集",
            label_visibility="collapsed"
        )
        st.session_state[f'corrected_display_text_{doc_id}'] = display_input

        # 文字数表示
        display_diff = len(display_input) - len(display_post_text)
        if display_diff > 0:
            st.success(f"✅ {display_diff} 文字追加（合計: {len(display_input)} 文字）")
        elif display_diff < 0:
            st.warning(f"⚠️ {abs(display_diff)} 文字削除（合計: {len(display_input)} 文字）")
        else:
            st.info("変更なし")

        st.markdown("---")

        # 添付ファイルテキストの編集
        st.markdown("**📎 添付ファイル (attachment_text)**")
        st.caption("PDFやOffice文書からGemini Visionが抽出したテキスト")
        attachment_input = st.text_area(
            "添付ファイルのテキスト",
            value=st.session_state[f'corrected_attachment_text_{doc_id}'],
            height=200,
            key=f"manual_attachment_text_{doc_id}",
            help="Gemini Visionが抽出したテキストを補正",
            label_visibility="collapsed"
        )
        st.session_state[f'corrected_attachment_text_{doc_id}'] = attachment_input

        # 文字数表示
        attachment_diff = len(attachment_input) - len(attachment_text)
        if attachment_diff > 0:
            st.success(f"✅ {attachment_diff} 文字追加（合計: {len(attachment_input)} 文字）")
        elif attachment_diff < 0:
            st.warning(f"⚠️ {abs(attachment_diff)} 文字削除（合計: {len(attachment_input)} 文字）")
        else:
            st.info("変更なし")

    with tab2:
        st.markdown("#### 変更内容のプレビュー")
        st.info("💡 元のテキストと補正後のテキストの差分を確認できます")

        current_display_text = st.session_state[f'corrected_display_text_{doc_id}']
        current_attachment_text = st.session_state[f'corrected_attachment_text_{doc_id}']

        # 投稿本文の差分
        st.markdown("**📧 投稿本文の変更:**")
        if current_display_text != display_post_text:
            diff_markdown = _highlight_diff(display_post_text, current_display_text)
            st.markdown(diff_markdown)
            col_stat1, col_stat2 = st.columns(2)
            with col_stat1:
                st.metric("元の文字数", len(display_post_text))
            with col_stat2:
                st.metric("補正後の文字数", len(current_display_text))
        else:
            st.info("変更なし")

        st.markdown("---")

        # 添付ファイルの差分
        st.markdown("**📎 添付ファイルの変更:**")
        if current_attachment_text != attachment_text:
            diff_markdown = _highlight_diff(attachment_text, current_attachment_text)
            st.markdown(diff_markdown)
            col_stat1, col_stat2 = st.columns(2)
            with col_stat1:
                st.metric("元の文字数", len(attachment_text))
            with col_stat2:
                st.metric("補正後の文字数", len(current_attachment_text))
        else:
            st.info("変更なし")

    st.markdown("---")

    # 再実行ボタン
    col_btn1, col_btn2, col_btn3 = st.columns([2, 2, 6])

    # 再実行フラグの初期化
    if f'trigger_reprocess_{doc_id}' not in st.session_state:
        st.session_state[f'trigger_reprocess_{doc_id}'] = False

    with col_btn1:
        if st.button(
            "🔄 Stage H 再実行",
            type="primary",
            use_container_width=True,
            key=f"reprocess_{doc_id}",
            help="補正されたテキストでGemini 2.5 Flashによる構造化 + 全チャンク再生成を実行します"
        ):
            current_display_text = st.session_state[f'corrected_display_text_{doc_id}']
            current_attachment_text = st.session_state[f'corrected_attachment_text_{doc_id}']

            display_changed = current_display_text != display_post_text
            attachment_changed = current_attachment_text != attachment_text

            if display_changed or attachment_changed:
                logger.info(f"[手動補正] テキスト補正完了:")
                logger.info(f"  投稿本文: {len(display_post_text)} → {len(current_display_text)} 文字")
                logger.info(f"  添付ファイル: {len(attachment_text)} → {len(current_attachment_text)} 文字")
            else:
                st.info("ℹ️ テキストは変更されていませんが、スキーマ変更を反映するため再実行します")
                logger.info(f"[手動補正] テキスト未変更だがStage H再実行を要求（スキーマ変更反映のため）")

            # 再実行フラグを立てる
            st.session_state[f'trigger_reprocess_{doc_id}'] = True
            st.rerun()

    with col_btn2:
        if st.button(
            "↩️ リセット",
            use_container_width=True,
            key=f"reset_{doc_id}",
            help="元のテキストに戻します"
        ):
            st.session_state[f'corrected_display_text_{doc_id}'] = display_post_text
            st.session_state[f'corrected_attachment_text_{doc_id}'] = attachment_text
            st.rerun()

    # 再実行フラグがセットされている場合、補正テキストを返す
    if st.session_state.get(f'trigger_reprocess_{doc_id}', False):
        corrected_texts = {
            "display_post_text": st.session_state[f'corrected_display_text_{doc_id}'],
            "attachment_text": st.session_state[f'corrected_attachment_text_{doc_id}']
        }
        # フラグをクリア
        st.session_state[f'trigger_reprocess_{doc_id}'] = False
        return corrected_texts

    return None


def execute_stageh_reprocessing(
    corrected_text: str,
    file_name: str,
    metadata: Dict[str, Any],
    workspace: str
) -> Dict[str, Any]:
    """
    補正されたテキストでStage Hを再実行

    このラッパー関数は後方互換性のために残されています。
    新しいコードでは ui.utils.stageh_reprocessor を使用してください。

    Args:
        corrected_text: 人間が補正したテキスト
        file_name: ファイル名
        metadata: Stage I統合の結果を含むメタデータ
        workspace: ワークスペース

    Returns:
        新しい構造化データ
    """
    logger.warning("[Deprecated] execute_stageh_reprocessing() は非推奨です。ui.utils.stage_h_reprocessor.reprocess_with_stageh() を使用してください。")

    # この関数は後方互換性のために残されていますが、実装は削除されました
    # 新しいコードでは ui.utils.stage_h_reprocessor.reprocess_with_stageh() を直接使用してください
    raise NotImplementedError(
        "execute_stageh_reprocessing() は非推奨です。"
        "ui.utils.stage_h_reprocessor.reprocess_with_stageh() を使用してください。"
    )
```

### frontend\components\table_creator.py

```py
"""
表構造新規作成コンポーネント

AIが見逃した表を人間が追加できる機能を提供します。
"""
import streamlit as st
import pandas as pd
from typing import Dict, Any, List, Optional
from loguru import logger


def render_table_creator(doc_id: str, metadata: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    表構造を新規作成するUIをレンダリング

    Args:
        doc_id: ドキュメントID
        metadata: 既存のメタデータ

    Returns:
        新しく追加された表データ（追加された場合）、またはNone
    """
    st.markdown("### ➕ 表構造を新規追加")
    st.info("💡 AIが見逃した表を追加できます。表の種類を選択して、データを入力してください。")

    # 表の種類を選択
    table_types = {
        "weekly_schedule": "週間スケジュール（日付・曜日・イベント・時間割）",
        "monthly_schedule_blocks": "月間予定表（日付・曜日・行事・時刻・持ち物）",
        "learning_content_blocks": "学習予定表（教科・担当教員・学習内容・持ち物）",
        "structured_tables": "その他の表（持ち物リスト、成績表など）",
        "text_blocks": "テキストブロック（見出し・本文）",
        "custom": "カスタム表（自由に列を定義）"
    }

    selected_type = st.selectbox(
        "表の種類を選択",
        options=list(table_types.keys()),
        format_func=lambda x: table_types[x],
        key=f"table_type_selector_{doc_id}"
    )

    st.markdown("---")

    # 表の種類に応じたテンプレートを表示
    if selected_type == "weekly_schedule":
        return _render_weekly_schedule_creator(doc_id, metadata)
    elif selected_type == "monthly_schedule_blocks":
        return _render_monthly_schedule_creator(doc_id, metadata)
    elif selected_type == "learning_content_blocks":
        return _render_learning_content_creator(doc_id, metadata)
    elif selected_type == "structured_tables":
        return _render_structured_table_creator(doc_id, metadata)
    elif selected_type == "text_blocks":
        return _render_text_blocks_creator(doc_id, metadata)
    elif selected_type == "custom":
        return _render_custom_table_creator(doc_id, metadata)


def _render_weekly_schedule_creator(doc_id: str, metadata: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """週間スケジュール作成UI"""
    st.markdown("#### 📅 週間スケジュール")

    # テンプレート行数を指定
    num_rows = st.number_input("追加する日数", min_value=1, max_value=31, value=5, key=f"weekly_rows_{doc_id}")

    # テンプレートデータフレーム作成（DateColumn用にTextColumnを使用）
    template_data = []
    for i in range(num_rows):
        template_data.append({
            "date": f"2024-01-{i+1:02d}",
            "day_of_week": "月曜日",
            "events": "",
            "note": ""
        })

    df = pd.DataFrame(template_data)

    # データエディタで編集（DateColumnではなくTextColumnを使用）
    edited_df = st.data_editor(
        df,
        use_container_width=True,
        num_rows="dynamic",
        key=f"weekly_schedule_editor_{doc_id}",
        column_config={
            "date": st.column_config.TextColumn("日付 (YYYY-MM-DD)"),
            "day_of_week": st.column_config.SelectboxColumn(
                "曜日",
                options=["月曜日", "火曜日", "水曜日", "木曜日", "金曜日", "土曜日", "日曜日"]
            ),
            "events": st.column_config.TextColumn("行事・イベント"),
            "note": st.column_config.TextColumn("備考・持ち物")
        }
    )

    # 追加ボタン
    if st.button("➕ この表をメタデータに追加", type="primary", key=f"add_weekly_{doc_id}"):
        new_data = edited_df.to_dict('records')

        # 既存のweekly_scheduleに追加
        if 'weekly_schedule' not in metadata:
            metadata['weekly_schedule'] = []

        metadata['weekly_schedule'].extend(new_data)

        st.success(f"✅ {len(new_data)}行の週間スケジュールを追加しました！")
        logger.info(f"[表追加] weekly_schedule に {len(new_data)} 行追加")

        return metadata

    return None


def _render_monthly_schedule_creator(doc_id: str, metadata: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """月間予定表作成UI"""
    st.markdown("#### 📆 月間予定表")

    # テンプレート行数を指定
    num_rows = st.number_input("追加する行数", min_value=1, max_value=100, value=10, key=f"monthly_rows_{doc_id}")

    # テンプレートデータフレーム作成
    template_data = []
    for i in range(num_rows):
        template_data.append({
            "date": f"2024-01-{i+1:02d}",
            "day_of_week": "月",
            "event": "",
            "time": "",
            "notes": ""
        })

    df = pd.DataFrame(template_data)

    # データエディタで編集（TextColumnを使用）
    edited_df = st.data_editor(
        df,
        use_container_width=True,
        num_rows="dynamic",
        key=f"monthly_schedule_editor_{doc_id}",
        column_config={
            "date": st.column_config.TextColumn("日付 (YYYY-MM-DD)"),
            "day_of_week": st.column_config.TextColumn("曜日"),
            "event": st.column_config.TextColumn("行事名"),
            "time": st.column_config.TextColumn("時刻"),
            "notes": st.column_config.TextColumn("持ち物・備考")
        }
    )

    # 追加ボタン
    if st.button("➕ この表をメタデータに追加", type="primary", key=f"add_monthly_{doc_id}"):
        new_data = edited_df.to_dict('records')

        # 既存のmonthly_schedule_blocksに追加
        if 'monthly_schedule_blocks' not in metadata:
            metadata['monthly_schedule_blocks'] = []

        metadata['monthly_schedule_blocks'].extend(new_data)

        st.success(f"✅ {len(new_data)}行の月間予定を追加しました！")
        logger.info(f"[表追加] monthly_schedule_blocks に {len(new_data)} 行追加")

        return metadata

    return None


def _render_learning_content_creator(doc_id: str, metadata: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """学習予定表作成UI"""
    st.markdown("#### 📚 学習予定表")

    # テンプレート行数を指定
    num_rows = st.number_input("追加する教科数", min_value=1, max_value=20, value=5, key=f"learning_rows_{doc_id}")

    # テンプレートデータフレーム作成
    template_data = []
    subjects = ["国語", "算数", "理科", "社会", "英語"]
    for i in range(num_rows):
        template_data.append({
            "subject": subjects[i] if i < len(subjects) else "",
            "teacher": "",
            "content": "",
            "materials": ""
        })

    df = pd.DataFrame(template_data)

    # データエディタで編集
    edited_df = st.data_editor(
        df,
        use_container_width=True,
        num_rows="dynamic",
        key=f"learning_content_editor_{doc_id}",
        column_config={
            "subject": st.column_config.TextColumn("教科"),
            "teacher": st.column_config.TextColumn("担当教員"),
            "content": st.column_config.TextColumn("学習内容"),
            "materials": st.column_config.TextColumn("持ち物")
        }
    )

    # 追加ボタン
    if st.button("➕ この表をメタデータに追加", type="primary", key=f"add_learning_{doc_id}"):
        new_data = edited_df.to_dict('records')

        # 既存のlearning_content_blocksに追加
        if 'learning_content_blocks' not in metadata:
            metadata['learning_content_blocks'] = []

        metadata['learning_content_blocks'].extend(new_data)

        st.success(f"✅ {len(new_data)}行の学習予定を追加しました！")
        logger.info(f"[表追加] learning_content_blocks に {len(new_data)} 行追加")

        return metadata

    return None


def _render_structured_table_creator(doc_id: str, metadata: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """汎用構造化表作成UI"""
    st.markdown("#### 📊 構造化表")

    # 表のタイトル
    table_title = st.text_input("表のタイトル", key=f"table_title_{doc_id}")

    # 列名を指定
    col_headers = st.text_input(
        "列名（カンマ区切り）",
        placeholder="例: 項目,数量,価格",
        key=f"table_headers_{doc_id}"
    )

    if not col_headers:
        st.warning("列名を入力してください")
        return None

    headers = [h.strip() for h in col_headers.split(',')]

    # テンプレート行数を指定
    num_rows = st.number_input("追加する行数", min_value=1, max_value=100, value=5, key=f"struct_rows_{doc_id}")

    # テンプレートデータフレーム作成
    template_data = []
    for i in range(num_rows):
        row = {header: "" for header in headers}
        template_data.append(row)

    df = pd.DataFrame(template_data)

    # データエディタで編集
    edited_df = st.data_editor(
        df,
        use_container_width=True,
        num_rows="dynamic",
        key=f"structured_table_editor_{doc_id}"
    )

    # 追加ボタン
    if st.button("➕ この表をメタデータに追加", type="primary", key=f"add_struct_{doc_id}"):
        new_table = {
            "table_title": table_title,
            "table_type": "custom",
            "headers": headers,
            "rows": edited_df.to_dict('records')
        }

        # 既存のstructured_tablesに追加
        if 'structured_tables' not in metadata:
            metadata['structured_tables'] = []

        metadata['structured_tables'].append(new_table)

        st.success(f"✅ 表「{table_title}」を追加しました！（{len(edited_df)}行）")
        logger.info(f"[表追加] structured_tables に表追加: {table_title}")

        return metadata

    return None


def _render_text_blocks_creator(doc_id: str, metadata: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """テキストブロック作成UI"""
    st.markdown("#### 📝 テキストブロック")

    # テンプレート行数を指定
    num_rows = st.number_input("追加するブロック数", min_value=1, max_value=20, value=3, key=f"text_rows_{doc_id}")

    # テンプレートデータフレーム作成
    template_data = []
    for i in range(num_rows):
        template_data.append({
            "title": f"見出し {i+1}",
            "content": ""
        })

    df = pd.DataFrame(template_data)

    # データエディタで編集
    edited_df = st.data_editor(
        df,
        use_container_width=True,
        num_rows="dynamic",
        key=f"text_blocks_editor_{doc_id}",
        column_config={
            "title": st.column_config.TextColumn("見出し", width="small"),
            "content": st.column_config.TextColumn("本文", width="large")
        }
    )

    # 追加ボタン
    if st.button("➕ このテキストブロックをメタデータに追加", type="primary", key=f"add_text_{doc_id}"):
        new_data = edited_df.to_dict('records')

        # 既存のtext_blocksに追加
        if 'text_blocks' not in metadata:
            metadata['text_blocks'] = []

        metadata['text_blocks'].extend(new_data)

        st.success(f"✅ {len(new_data)}個のテキストブロックを追加しました！")
        logger.info(f"[表追加] text_blocks に {len(new_data)} ブロック追加")

        return metadata

    return None


def _render_custom_table_creator(doc_id: str, metadata: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """カスタム表作成UI"""
    st.markdown("#### 🔧 カスタム表")

    # フィールド名を指定
    field_name = st.text_input(
        "フィールド名（メタデータのキー）",
        placeholder="例: custom_schedule",
        key=f"custom_field_{doc_id}"
    )

    # 列名を指定
    col_headers = st.text_input(
        "列名（カンマ区切り）",
        placeholder="例: 日付,内容,担当者",
        key=f"custom_headers_{doc_id}"
    )

    if not field_name or not col_headers:
        st.warning("フィールド名と列名を入力してください")
        return None

    headers = [h.strip() for h in col_headers.split(',')]

    # テンプレート行数を指定
    num_rows = st.number_input("追加する行数", min_value=1, max_value=100, value=5, key=f"custom_rows_{doc_id}")

    # テンプレートデータフレーム作成
    template_data = []
    for i in range(num_rows):
        row = {header: "" for header in headers}
        template_data.append(row)

    df = pd.DataFrame(template_data)

    # データエディタで編集
    edited_df = st.data_editor(
        df,
        use_container_width=True,
        num_rows="dynamic",
        key=f"custom_table_editor_{doc_id}"
    )

    # 追加ボタン
    if st.button("➕ この表をメタデータに追加", type="primary", key=f"add_custom_{doc_id}"):
        new_data = edited_df.to_dict('records')

        # カスタムフィールドに追加
        if field_name not in metadata:
            metadata[field_name] = []

        metadata[field_name].extend(new_data)

        st.success(f"✅ {len(new_data)}行をフィールド「{field_name}」に追加しました！")
        logger.info(f"[表追加] {field_name} に {len(new_data)} 行追加")

        return metadata

    return None
```

### frontend\components\table_editor.py

```py
"""
Table Editor Component
データフレーム形式での表編集UI
"""
import streamlit as st
import pandas as pd
from typing import Dict, Any, List
import re  # 追加: ソート用
from frontend.utils.table_parser import parse_extracted_tables

def render_table_editor(metadata: Dict[str, Any]) -> Dict[str, Any]:
    """
    メタデータを表形式で編集

    Args:
        metadata: 現在のメタデータ

    Returns:
        編集後のメタデータ
    """
    st.markdown("### 📊 表エディタ")
    st.markdown("配列データを表形式で編集できます")
    st.markdown("---")

    edited_metadata = metadata.copy()

    # 配列型のフィールドを検出して表示
    array_fields = _find_array_fields(metadata)

    # structured_tables が存在する場合は強制的に追加
    if not array_fields and "structured_tables" in metadata:
        if isinstance(metadata["structured_tables"], list):
            array_fields = [{
                "name": "structured_tables",
                "value": metadata["structured_tables"],
                "label": _format_field_name("structured_tables")
            }]

    if not array_fields:
        st.info("表形式で編集可能な配列データが見つかりません")
        return edited_metadata

    # タブで配列ごとに表示
    if len(array_fields) > 1:
        tabs = st.tabs([field["label"] for field in array_fields])
        for tab, field in zip(tabs, array_fields):
            with tab:
                edited_value = _render_array_table(
                    field["name"],
                    field["value"],
                    field["label"]
                )
                edited_metadata[field["name"]] = edited_value
    else:
        # 配列が1つの場合はタブなしで表示
        field = array_fields[0]
        edited_value = _render_array_table(
            field["name"],
            field["value"],
            field["label"]
        )
        edited_metadata[field["name"]] = edited_value

    return edited_metadata


def _find_array_fields(metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    メタデータから配列フィールドを抽出

    Returns:
        [{"name": フィールド名, "value": 配列値, "label": 表示名}, ...]
    """
    array_fields = []

    for key, value in metadata.items():
        # structured_tables, extracted_tables は無条件で検出対象にする
        if key in ["structured_tables", "extracted_tables"] and isinstance(value, list):
            # extracted_tablesの場合、パースして構造化データに変換
            if key == "extracted_tables":
                parsed_tables = parse_extracted_tables(value)
                if parsed_tables:
                    array_fields.append({
                        "name": key,
                        "value": parsed_tables,
                        "label": _format_field_name(key)
                    })
            else:
                array_fields.append({
                    "name": key,
                    "value": value,
                    "label": _format_field_name(key)
                })
        elif isinstance(value, list) and len(value) > 0:
            # 配列の要素が辞書の場合のみ表エディタで扱う
            if isinstance(value[0], dict):
                array_fields.append({
                    "name": key,
                    "value": value,
                    "label": _format_field_name(key)
                })

    return array_fields


def _format_field_name(field_name: str) -> str:
    """
    フィールド名を表示用に整形

    動的フィールド名の整形ルール:
    - monthly_schedule_list → 📅 月間予定
    - learning_content_list → 📚 学習予定
    - weekly_timetable_matrix → 📅 週間時間割
    - xxx_list → xxx（_listを除去）
    - xxx_blocks → xxx（_blocksを除去）
    - xxx_matrix → xxx（_matrixを除去）
    """
    # 既知のフィールド名マッピング
    name_map = {
        # 新しい構造化フィールド（優先）
        "monthly_schedule_list": "📅 月間予定",
        "learning_content_list": "📚 学習予定",
        "weekly_timetable_matrix": "📅 週間時間割",
        # 汎用フィールド
        "text_blocks": "📝 文章セクション",
        "special_events": "🎉 特別イベント",
        "requirements": "📦 持ち物・準備",
        "important_points": "⚠️ 重要事項",
        # その他の既存フィールド
        "daily_schedule": "日別時間割",
        "weekly_schedule": "週間予定",
        "periods": "時限別科目",
        "class_schedules": "クラス別時間割",
        "structured_tables": "📋 その他リスト",
        "monthly_schedule_blocks": "📅 月間予定表",
        "learning_content_blocks": "📚 教科別学習予定",
        "extracted_tables": "📊 その他表形式"
    }

    # マッピングに存在する場合はそれを返す
    if field_name in name_map:
        return name_map[field_name]

    # 動的フィールド名の整形
    # _list, _blocks, または _matrix で終わる場合は除去
    if field_name.endswith("_list"):
        base_name = field_name[:-5]  # _list を除去
        # アンダースコアをスペースに変換して整形
        formatted = base_name.replace("_", " ").title()
        return f"📊 {formatted}"
    elif field_name.endswith("_blocks"):
        base_name = field_name[:-7]  # _blocks を除去
        formatted = base_name.replace("_", " ").title()
        return f"📊 {formatted}"
    elif field_name.endswith("_matrix"):
        base_name = field_name[:-7]  # _matrix を除去
        formatted = base_name.replace("_", " ").title()
        return f"📅 {formatted}"

    # その他の場合はそのまま返す
    return field_name


# --- 追加機能: スケジュールデータのフラット化とソート ---
def _flatten_and_sort_schedule(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    スケジュールデータをクラス単位・日付順にフラット化してソート
    Example:
      In: [{'date': '12/1', 'class_schedules': [{'class': '5A', ...}, {'class': '5B', ...}]}]
      Out: [{'date': '12/1', 'class': '5A', ...}, {'date': '12/1', 'class': '5B', ...}]
    """
    if not data:
        return data

    # class_schedulesが含まれているかチェック
    has_class_schedule = False
    for item in data:
        if "class_schedules" in item and isinstance(item["class_schedules"], list):
            has_class_schedule = True
            break

    if not has_class_schedule:
        return data

    flattened_data = []

    for item in data:
        if "class_schedules" in item and isinstance(item["class_schedules"], list):
            # class_schedules以外の基本情報を取得（日付や曜日など）
            base_info = {}

            for k, v in item.items():
                if k == "class_schedules":
                    continue
                # eventsは配列なので文字列に変換
                elif k == "events" and isinstance(v, list):
                    base_info[k] = ", ".join(str(e) for e in v) if v else ""
                # day_of_weekは不要（dayと重複）
                elif k == "day_of_week":
                    continue
                else:
                    base_info[k] = v

            for class_sched in item["class_schedules"]:
                row = base_info.copy()

                # クラス名を追加
                if "class" in class_sched:
                    row["class"] = str(class_sched["class"])

                # periodsとsubjectsを統合して処理
                period_data = {}  # {display_key: subject_name} の辞書

                # 1. periodsから時限データを取得
                if "periods" in class_sched and isinstance(class_sched["periods"], list):
                    for p in class_sched["periods"]:
                        if isinstance(p, dict):
                            period_key = str(p.get("period", ""))
                            subject = str(p.get("subject", ""))
                            if period_key:
                                # 時限番号に「時限目」を追加（例: "1" -> "1時限目"）
                                if period_key.isdigit():
                                    display_key = f"{period_key}時限目"
                                else:
                                    display_key = period_key
                                period_data[display_key] = subject

                # 2. subjectsから時限データを取得（periodsで設定されていない時限のみ追加）
                if "subjects" in class_sched and isinstance(class_sched["subjects"], list):
                    for i, subject in enumerate(class_sched["subjects"], 1):
                        subject_str = str(subject)

                        # "時限:科目" 形式の場合は分割して処理
                        if ":" in subject_str:
                            parts = subject_str.split(":", 1)
                            period_label = parts[0].strip()  # 例: "朝", "1限", "2限"
                            subject_name = parts[1].strip() if len(parts) > 1 else ""

                            # "1限" -> "1時限目", "朝" -> "朝" のように変換
                            if period_label == "朝":
                                display_key = "朝"
                            elif period_label.replace("限", "").isdigit():
                                # "1限" -> "1時限目"
                                num = period_label.replace("限", "")
                                display_key = f"{num}時限目"
                            else:
                                # その他の場合はそのまま使用
                                display_key = period_label

                            # periodsで既に設定されていない場合のみ追加
                            if display_key not in period_data:
                                period_data[display_key] = subject_name
                        else:
                            # 通常の形式（科目名のみ）の場合
                            display_key = f"{i}時限目"
                            # periodsで既に設定されていない場合のみ追加
                            if display_key not in period_data:
                                period_data[display_key] = subject_str

                # 3. 統合した時限データをrowに追加
                row.update(period_data)

                flattened_data.append(row)
        else:
            # class_schedulesがない行も一応そのまま保持
            flattened_data.append(item)

    # ソート: 第一キー=class, 第二キー=date
    def sort_key(row):
        c = row.get("class", "")
        d = row.get("date", "")
        return (str(c), str(d))

    try:
        flattened_data.sort(key=sort_key)
    except:
        pass # ソートに失敗した場合はそのまま

    return flattened_data


# --- 追加機能: 列の並び替え ---
def _render_single_structured_table(field_key: str, table_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    structured_tables の1つのテーブルをレンダリング

    Args:
        field_key: 一意なキー (st.data_editor用)
        table_data: {table_title, table_type, headers, rows} 形式のデータ

    Returns:
        編集後のテーブルデータ
    """
    table_title = table_data.get("table_title", "表")
    table_type = table_data.get("table_type", "")
    rows = table_data.get("rows", [])

    if not rows:
        st.info(f"{table_title}: データがありません")
        return table_data

    # データフレームに変換
    try:
        df = pd.DataFrame(rows)

        # PyArrow エラー対策: 型強制とデータクリーニング
        df = df.astype(str)
        df = df.fillna("")
        df = df.replace(["None", "nan", "NaN", "null"], "")

    except Exception as e:
        st.error(f"データフレーム変換エラー: {e}")
        st.json(rows)
        return table_data

    # 表のメタ情報を表示
    st.markdown(f"#### {table_title}")
    if table_type:
        st.caption(f"種類: {table_type} | 全 {len(df)} 行")
    else:
        st.caption(f"全 {len(df)} 行")

    # 編集可能な表を表示
    edited_df = st.data_editor(
        df,
        use_container_width=True,
        num_rows="dynamic",
        key=f"table_{field_key}",
        height=600  # 増加: より多くの行を表示
    )

    # データフレームを辞書のリストに戻す
    try:
        edited_rows = edited_df.to_dict('records')

        # データクリーニング: 空文字列を削除
        cleaned_rows = []
        for record in edited_rows:
            cleaned_record = {k: v for k, v in record.items() if v != ""}
            if cleaned_record:
                cleaned_rows.append(cleaned_record)

        # 編集後のデータで table_data を更新
        edited_table_data = table_data.copy()
        edited_table_data["rows"] = cleaned_rows
        return edited_table_data

    except Exception as e:
        st.error(f"データ変換エラー: {e}")
        return table_data


def _render_extracted_table(field_key: str, table_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    extracted_tables の1つのテーブルをレンダリング

    Args:
        field_key: 一意なキー (st.data_editor用)
        table_data: {page, table_number, headers, rows} 形式のデータ

    Returns:
        編集後のテーブルデータ
    """
    page = table_data.get("page", 1)
    table_number = table_data.get("table_number", 1)
    headers = table_data.get("headers", [])
    rows = table_data.get("rows", [])

    if not rows and not headers:
        st.info(f"ページ{page} 表{table_number}: データがありません")
        return table_data

    # headersとrowsを使ってDataFrameを作成
    try:
        # rowsが既に辞書のリストの場合（新形式）
        if rows and isinstance(rows[0], dict):
            df = pd.DataFrame(rows)
            # ヘッダーの順序を調整
            if headers:
                # headersで指定された順序に並び替え（存在するカラムのみ）
                available_cols = [h for h in headers if h in df.columns]
                if available_cols:
                    df = df[available_cols]
        elif headers:
            # rowsがリストのリストの場合（旧形式）
            df_data = []
            for row in rows:
                # rowの長さがheadersと異なる場合は調整
                row_dict = {}
                for i, header in enumerate(headers):
                    if i < len(row):
                        row_dict[header] = row[i]
                    else:
                        row_dict[header] = ""
                df_data.append(row_dict)
            df = pd.DataFrame(df_data)
        else:
            # ヘッダーがない場合：rowsをそのまま使用
            df = pd.DataFrame(rows)

        # PyArrow エラー対策: 型強制とデータクリーニング
        df = df.astype(str)
        df = df.fillna("")
        df = df.replace(["None", "nan", "NaN", "null"], "")

    except Exception as e:
        st.error(f"データフレーム変換エラー: {e}")
        st.json(table_data)
        return table_data

    # 表のメタ情報を表示
    st.markdown(f"#### ページ{page} 表{table_number}")
    st.caption(f"全 {len(df)} 行 × {len(df.columns)} 列")

    # 編集可能な表を表示
    edited_df = st.data_editor(
        df,
        use_container_width=True,
        num_rows="dynamic",
        key=f"table_{field_key}",
        height=600  # 増加: より多くの行を表示
    )

    # データフレームを元の形式に戻す
    try:
        # ヘッダーを更新
        new_headers = edited_df.columns.tolist()

        # 行データを更新
        new_rows = []
        for _, row in edited_df.iterrows():
            row_data = [str(cell) if cell != "" else "" for cell in row.values]
            # 空でない行のみ追加
            if any(cell != "" for cell in row_data):
                new_rows.append(row_data)

        # 編集後のデータで table_data を更新
        edited_table_data = {
            "page": page,
            "table_number": table_number,
            "headers": new_headers,
            "rows": new_rows
        }
        return edited_table_data

    except Exception as e:
        st.error(f"データ変換エラー: {e}")
        return table_data


def _reorder_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    データフレームの列を見やすく並び替え
    並び順: class -> date -> day -> 朝 -> 1, 2, 3... -> その他
    """
    if df.empty:
        return df
        
    cols = df.columns.tolist()
    
    # 固定列（左側に表示したい列）
    fixed_start = []
    target_cols = ["class", "date", "day", "day_of_week"]
    for c in target_cols:
        if c in cols:
            fixed_start.append(c)
            cols.remove(c)
            
    # 時限列（数字、"1時限目"、または "朝"）
    period_cols = []
    other_cols = []

    for c in cols:
        # "1", "2" などの数字、"1時限目"、"2時限目"、または "朝"
        if c == "朝" or c.isdigit() or c.endswith("時限目"):
            period_cols.append(c)
        else:
            other_cols.append(c)

    # 時限列のソートロジック（朝を先頭、あとは数字順）
    def period_sort_key(k):
        if k == "朝":
            return -1
        if k.isdigit():
            return int(k)
        # "1時限目" -> 1 を抽出してソート
        if k.endswith("時限目"):
            try:
                num = int(k.replace("時限目", ""))
                return num
            except:
                return 999
        return 999
        
    period_cols.sort(key=period_sort_key)
    
    # 最終的な列順を結合
    new_order = fixed_start + period_cols + other_cols
    
    return df[new_order]


def _render_array_table(field_name: str, array_value: List[Dict], label: str) -> List[Dict]:
    """
    配列データを表形式で編集

    Args:
        field_name: フィールド名
        array_value: 配列データ
        label: 表示ラベル

    Returns:
        編集後の配列データ
    """
    if not array_value:
        st.info(f"{label}のデータがありません")
        return []

    # --- extracted_tables の特別処理 ---
    # extracted_tables は {page, table_number, headers, rows} の形式
    if "extracted_tables" in field_name and isinstance(array_value, list):
        # 複数の表がある場合、それぞれを個別のタブで表示
        if len(array_value) > 1:
            # タブ名を生成: description > table_id > デフォルト
            tab_names = []
            for i, table in enumerate(array_value):
                if 'description' in table and table['description']:
                    tab_names.append(table['description'])
                elif 'table_id' in table and table['table_id']:
                    tab_names.append(table['table_id'])
                elif 'page' in table or 'table_number' in table:
                    tab_names.append(f"ページ{table.get('page', i+1)} 表{table.get('table_number', i+1)}")
                else:
                    tab_names.append(f"表{i+1}")
            table_tabs = st.tabs(tab_names)
            edited_tables = []
            for i, (tab, table_data) in enumerate(zip(table_tabs, array_value)):
                with tab:
                    edited_table = _render_extracted_table(
                        f"{field_name}_{i}",
                        table_data
                    )
                    edited_tables.append(edited_table)
            return edited_tables
        elif len(array_value) == 1:
            # 1つの表のみの場合はタブなしで表示
            edited_table = _render_extracted_table(
                field_name,
                array_value[0]
            )
            return [edited_table]

    # --- structured_tables の特別処理 ---
    # structured_tables は {table_title, table_type, headers, rows} の形式
    # rows フィールドを直接表示する
    if "structured_tables" in field_name and isinstance(array_value, list):
        # 複数の表がある場合、それぞれを個別のタブで表示
        if len(array_value) > 1:
            table_tabs = st.tabs([
                table.get("table_title", f"表{i+1}")
                for i, table in enumerate(array_value)
            ])
            edited_tables = []
            for i, (tab, table_data) in enumerate(zip(table_tabs, array_value)):
                with tab:
                    edited_table = _render_single_structured_table(
                        f"{field_name}_{i}",
                        table_data
                    )
                    edited_tables.append(edited_table)
            return edited_tables
        elif len(array_value) == 1:
            # 1つの表のみの場合はタブなしで表示
            edited_table = _render_single_structured_table(
                field_name,
                array_value[0]
            )
            return [edited_table]

    # --- 変更点1: ここでデータをフラット化・ソートする ---
    processed_value = _flatten_and_sort_schedule(array_value)

    # データフレームに変換
    try:
        df = pd.DataFrame(processed_value)

        # PyArrow エラー対策: 型強制とデータクリーニング
        # すべての列を文字列型に変換して混合型を解消
        df = df.astype(str)

        # NaN, None を空文字列に置き換え
        df = df.fillna("")

        # 文字列化された "None", "nan", "NaN" も空文字列に置き換え
        df = df.replace(["None", "nan", "NaN", "null"], "")
        
        # --- 変更点2: ここで列を並び替える ---
        df = _reorder_columns(df)

    except Exception as e:
        st.error(f"データフレーム変換エラー: {e}")
        st.json(array_value)
        return array_value

    # st.data_editorで編集可能な表を表示
    st.markdown(f"#### {label}")
    st.caption(f"全 {len(df)} 行")

    edited_df = st.data_editor(
        df,
        use_container_width=True,
        num_rows="dynamic",  # 行の追加・削除を許可
        key=f"table_{field_name}",
        height=600  # 増加: より多くの行を表示
    )

    # データフレームを辞書のリストに戻す
    try:
        edited_array = edited_df.to_dict('records')

        # データクリーニング: 空文字列を削除（オプショナルなフィールド用）
        cleaned_array = []
        for record in edited_array:
            cleaned_record = {k: v for k, v in record.items() if v != ""}
            # 空のレコードは除外
            if cleaned_record:
                cleaned_array.append(cleaned_record)
        
        # 注意: ネスト構造には戻さず、フラット化された状態のまま返します
        # (ユーザーが編集しやすくするため)
        return cleaned_array

    except Exception as e:
        st.error(f"データ変換エラー: {e}")
        return array_value


def render_nested_table_editor(metadata: Dict[str, Any], path: List[str] = None) -> Dict[str, Any]:
    """
    ネストした配列データを再帰的に表示・編集

    Args:
        metadata: メタデータ
        path: 現在のパス（再帰用）

    Returns:
        編集後のメタデータ
    """
    if path is None:
        path = []

    edited_metadata = {}

    for key, value in metadata.items():
        current_path = path + [key]

        if isinstance(value, list) and len(value) > 0 and isinstance(value[0], dict):
            # 配列データ: 展開して表示
            with st.expander(f"📋 {_format_field_name(key)} ({len(value)}件)", expanded=True):
                edited_metadata[key] = _render_array_table(
                    "_".join(current_path),
                    value,
                    _format_field_name(key)
                )

        elif isinstance(value, dict):
            # ネストしたオブジェクト: 再帰的に処理
            with st.expander(f"📂 {_format_field_name(key)}", expanded=False):
                edited_metadata[key] = render_nested_table_editor(value, current_path)

        else:
            # その他のフィールドは表示のみ
            edited_metadata[key] = value

    return edited_metadata
```

### frontend\document_review_app.py

```py
"""
Document Review App (独立アプリ版)
人間がAIの抽出結果を確認・修正するための管理画面

機能:
- タブベースUI (フォーム編集 / 表エディタ / JSONプレビュー)
- スキーマベースのフォーム編集
- データフレームによる表形式編集
- JSON差分表示
"""
import sys
from pathlib import Path

# プロジェクトのルートディレクトリをPythonパスに追加
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

import streamlit as st
import json
import tempfile
from typing import Dict, Any, Optional, List
import pandas as pd
from loguru import logger

from shared.common.database.client import DatabaseClient
from shared.common.connectors.google_drive import GoogleDriveConnector

# 新しいコンポーネントとユーティリティをインポート
from frontend.utils.schema_detector import SchemaDetector
from frontend.components.form_editor import render_form_editor
from frontend.components.table_editor import render_table_editor, _render_array_table, _format_field_name
from frontend.components.json_preview import render_json_preview, render_json_diff
from frontend.components.email_viewer import (
    render_email_list,
    render_email_detail,
    render_email_html_preview,
    render_email_filters
)


def detect_structured_fields(metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    メタデータから構造化データフィールドを自動検出

    Args:
        metadata: メタデータ辞書

    Returns:
        構造化フィールドのリスト [{"key": str, "label": str, "data": list}, ...]
    """
    structured_fields = []

    logger.info("=" * 60)
    logger.info("🔍 構造化フィールド検出を開始")
    logger.info(f"メタデータのキー数: {len(metadata)}")
    logger.info("=" * 60)

    for key, value in metadata.items():
        # デバッグログ: 全てのキーと値の型を出力
        logger.debug(f"Key: {key}, Type: {type(value)}, Value start: {str(value)[:50]}")

        # extracted_tablesの特別処理
        if key == "extracted_tables":
            logger.info(f"🎯 FOUND extracted_tables! Type: {type(value)}, Length: {len(value) if isinstance(value, list) else 'N/A'}")
            if isinstance(value, list):
                logger.info(f"  First element type: {type(value[0]) if len(value) > 0 else 'empty'}")

        # _list, _blocks, _matrix, _tables で終わるキー、または structured_tables, weekly_schedule, extracted_tables を構造化データとして認識
        # ただし text_blocks は除外（フォーム編集タブで編集可能にするため）
        if key == "text_blocks":
            logger.info(f"✓ '{key}' は text_blocks として検出されましたが、フォーム編集タブで表示するため除外します")
            continue

        if (key.endswith("_list") or key.endswith("_blocks") or
            key.endswith("_matrix") or key.endswith("_tables") or
            key == "structured_tables" or key == "weekly_schedule" or key == "extracted_tables"):
            logger.info(f"✓ '{key}' は構造化データフィールドとして検出")

            if not isinstance(value, list):
                logger.warning(f"  ⚠️ '{key}' はリストではありません。Type: {type(value)}")
                continue

            if len(value) == 0:
                logger.warning(f"  ⚠️ '{key}' は空のリストです")
                continue

            logger.info(f"  ✓ '{key}' はリストで、要素数: {len(value)}")

            # extracted_tablesは特別処理（文字列のリストをパースして構造化データに変換）
            if key == "extracted_tables":
                logger.info(f"  ✓ '{key}' は extracted_tables として検出 - パース処理を実行")
                from frontend.utils.table_parser import parse_extracted_tables
                parsed_tables = parse_extracted_tables(value)
                if parsed_tables:
                    logger.info(f"  ✓ {len(parsed_tables)} 個の表をパースしました")
                    structured_fields.append({
                        "key": key,
                        "label": _format_field_name(key),
                        "data": parsed_tables
                    })
                else:
                    logger.warning(f"  ⚠️ '{key}' のパースに失敗しました")
                continue

            # 配列の最初の要素が辞書であることを確認（構造化データの証拠）
            if isinstance(value[0], dict):
                logger.info(f"  ✓ '{key}' の最初の要素は辞書です → 構造化フィールドとして検出!")
                structured_fields.append({
                    "key": key,
                    "label": _format_field_name(key),
                    "data": value
                })
            else:
                logger.warning(f"  ⚠️ '{key}' の最初の要素は辞書ではありません。Type: {type(value[0])}")

    logger.info("=" * 60)
    logger.info(f"🎯 検出された構造化フィールド数: {len(structured_fields)}")
    for field in structured_fields:
        logger.info(f"  - {field['key']} ({field['label']}) - {len(field['data'])} 件")
    logger.info("=" * 60)

    return structured_fields


def download_file_from_drive(source_id: str, file_name: str) -> Optional[str]:
    """
    Google Driveからファイルを一時ディレクトリにダウンロード

    Args:
        source_id: Google DriveのファイルID
        file_name: ファイル名

    Returns:
        ダウンロードされたファイルのパス、失敗時はNone
    """
    try:
        drive_connector = GoogleDriveConnector()
        temp_dir = tempfile.gettempdir()
        file_path = drive_connector.download_file(source_id, file_name, temp_dir)
        return file_path
    except Exception as e:
        error_type = type(e).__name__
        error_msg = str(e)
        logger.error(f"ファイルのダウンロードに失敗: source_id={source_id}, file_name={file_name}", exc_info=True)
        st.error(f"❌ Google Driveエラー")
        st.error(f"エラータイプ: {error_type}")
        st.error(f"エラー詳細: {error_msg}")
        st.code(f"file_id: {source_id}\nfile_name: {file_name}")
        return None


def pdf_review_ui():
    """ドキュメントレビューUIロジック（全てのファイルタイプ対応）"""
    # データベースクライアントとスキーマ検出器の初期化
    try:
        db_client = DatabaseClient()
        schema_detector = SchemaDetector()
    except Exception as e:
        st.error(f"初期化エラー: {e}")
        st.stop()

    # サイドバー: 検索とフィルタ設定
    st.sidebar.header("🔍 検索 & フィルタ")

    # Workspaceフィルタ（動的に取得）
    available_workspaces = db_client.get_available_workspaces()
    workspace_options = ["全て"] + available_workspaces
    workspace_filter = st.sidebar.selectbox(
        "Workspace",
        options=workspace_options,
        index=0,
        help="ワークスペースでフィルタリング"
    )

    # ファイルタイプフィルタ
    file_type_options = ["全て", "pdf", "email", "text", "markdown", "csv", "json"]
    file_type_filter = st.sidebar.selectbox(
        "ファイルタイプ",
        options=file_type_options,
        index=0,
        help="ファイルタイプでフィルタリング"
    )

    # レビューステータスフィルタ
    review_status_options = ["全て", "未確認", "確認済み"]
    review_status_filter = st.sidebar.selectbox(
        "レビューステータス",
        options=review_status_options,
        index=0,
        help="レビュー状態でフィルタリング"
    )

    # 検索ボックス
    search_query = st.sidebar.text_input(
        "キーワード検索",
        placeholder="例: 学年通信, abc123...",
        help="検索ワードを入力すると、レビュー状態に関係なく全データから検索します"
    )

    # 取得件数
    limit = st.sidebar.number_input(
        "取得件数",
        min_value=10,
        max_value=500,
        value=50,
        step=10,
        help="表示するドキュメントの最大件数"
    )

    # モード表示
    if search_query:
        st.sidebar.info("🔎 **検索モード**: 全データから検索中")
    else:
        st.sidebar.success("📝 **通常モード**: 未レビューのみ表示")

    # 進捗表示
    st.sidebar.markdown("---")
    st.sidebar.header("📊 レビュー進捗")
    progress_data = db_client.get_review_progress()

    col_p1, col_p2 = st.sidebar.columns(2)
    with col_p1:
        st.metric("未レビュー", f"{progress_data['unreviewed']} 件")
    with col_p2:
        st.metric("完了", f"{progress_data['reviewed']} 件")

    st.sidebar.progress(progress_data['progress_percent'] / 100)
    st.sidebar.caption(f"進捗率: {progress_data['progress_percent']}%")

    # リスト更新ボタン
    st.sidebar.markdown("---")
    if st.sidebar.button("🔄 リストを更新", use_container_width=True, key="refresh_pdf_list"):
        st.rerun()

    # レビュー対象ドキュメントを取得（全てのファイルタイプ）
    with st.spinner("ドキュメントを取得中..."):
        # Workspaceフィルタの値を変換（"全て"の場合はNone）
        workspace_value = workspace_filter if workspace_filter != "全て" else None

        # ファイルタイプフィルタの値を変換（"全て"の場合はNone）
        file_type_value = file_type_filter if file_type_filter != "全て" else None

        # レビューステータスフィルタの値を変換
        if review_status_filter == "確認済み":
            review_status_value = "reviewed"
        elif review_status_filter == "未確認":
            review_status_value = "pending"
        else:  # "全て"
            review_status_value = "all"

        documents = db_client.get_documents_for_review(
            limit=limit,
            search_query=search_query if search_query else None,
            workspace=workspace_value,
            file_type=file_type_value,
            review_status=review_status_value,
            exclude_workspace='gmail'  # Gmailワークスペースを除外
        )

    # デバッグログ: 取得後の確認
    logger.info(f"DBから取得したドキュメント数: {len(documents)}件")

    if not documents:
        st.info("レビュー対象のドキュメントがありません")
        return

    st.sidebar.success(f"✅ {len(documents)}件のドキュメント")

    # ドキュメントリストをDataFrameで表示（チェックボックス付き）
    df_data = []
    for idx, doc in enumerate(documents):
        # タイトルを表示（NULLの場合は空）
        display_name = doc.get('title', '')
        df_data.append({
            '選択': False,  # チェックボックス用
            'ID': doc.get('id', '')[:8],
            'タイトル': display_name,
            '文書タイプ': doc.get('doc_type', ''),
            '作成日時': doc.get('created_at', '')[:10]
        })

    df = pd.DataFrame(df_data)

    # デバッグログ: DataFrame作成後の確認
    logger.info(f"表示用DataFrameの行数: {len(df)}件")

    st.subheader("📁 レビュー対象ドキュメント一覧")

    # データエディタでチェックボックス付きの表を表示
    edited_df = st.data_editor(
        df,
        use_container_width=True,
        height=200,
        hide_index=True,
        column_config={
            "選択": st.column_config.CheckboxColumn(
                "選択",
                help="削除するドキュメントを選択",
                default=False,
            )
        },
        disabled=["ID", "ファイル名", "文書タイプ", "作成日時"],
        key="document_list_editor"
    )

    # 選択されたドキュメントを取得
    selected_indices = edited_df[edited_df['選択'] == True].index.tolist()
    selected_count = len(selected_indices)

    # まとめて操作ボタン（一覧の直下に常に表示）
    col_approve, col_delete, col_spacer = st.columns([1, 1, 2])

    with col_approve:
        if selected_count > 0:
            if st.button(f"✅ まとめて承認 ({selected_count}件)", use_container_width=True, type="primary"):
                with st.spinner(f"{selected_count}件のドキュメントを承認中..."):
                    success_count = 0
                    fail_count = 0

                    for idx in selected_indices:
                        doc = documents[idx]
                        doc_id = doc.get('id')

                        # レビュー済みとしてマーク
                        if db_client.mark_document_reviewed(doc_id):
                            success_count += 1
                        else:
                            fail_count += 1

                    if success_count > 0:
                        st.success(f"✅ {success_count}件のドキュメントを承認しました")
                    if fail_count > 0:
                        st.error(f"❌ {fail_count}件の承認に失敗しました")

                    st.balloons()
                    import time
                    time.sleep(1)
                    st.rerun()
        else:
            st.button("✅ まとめて承認", use_container_width=True, disabled=True)

    with col_delete:
        # 一括削除確認用のセッション状態
        if 'bulk_delete_confirm' not in st.session_state:
            st.session_state.bulk_delete_confirm = False

        if selected_count > 0:
            if not st.session_state.bulk_delete_confirm:
                if st.button(f"🗑️ まとめて削除 ({selected_count}件)", use_container_width=True, type="secondary"):
                    st.session_state.bulk_delete_confirm = True
                    st.rerun()
            else:
                if st.button(f"⚠️ 削除を実行 ({selected_count}件)", use_container_width=True, type="primary"):
                    with st.spinner(f"{selected_count}件のドキュメントを削除中..."):
                        success_count = 0
                        fail_count = 0

                        for idx in selected_indices:
                            doc = documents[idx]
                            doc_id = doc.get('id')
                            file_id = doc.get('drive_file_id') or doc.get('source_id')

                            # Google Driveから削除
                            if file_id:
                                try:
                                    drive_connector = GoogleDriveConnector()
                                    drive_connector.trash_file(file_id)
                                except Exception as e:
                                    logger.error(f"Google Drive削除エラー: {e}")

                            # データベースから削除
                            if db_client.delete_document(doc_id):
                                success_count += 1
                            else:
                                fail_count += 1

                        if success_count > 0:
                            st.success(f"✅ {success_count}件のドキュメントを削除しました")
                        if fail_count > 0:
                            st.error(f"❌ {fail_count}件の削除に失敗しました")

                        st.session_state.bulk_delete_confirm = False
                        st.balloons()
                        import time
                        time.sleep(1)
                        st.rerun()
        else:
            st.button("🗑️ まとめて削除", use_container_width=True, disabled=True)

    # 削除確認中はキャンセルボタン表示
    if st.session_state.get('bulk_delete_confirm', False):
        if st.button("❌ キャンセル", use_container_width=True):
            st.session_state.bulk_delete_confirm = False
            st.rerun()

    # ドキュメント選択
    st.subheader("🔍 ドキュメント詳細")

    # セレクトボックスのキーに検索クエリを含めることで、モード変更時にリセット
    selector_key = f"document_selector_{search_query or 'normal'}"

    selected_index = st.selectbox(
        "編集するドキュメントを選択",
        range(len(documents)),
        format_func=lambda i: documents[i].get('title', '') or '(タイトルなし)',
        key=selector_key
    )

    selected_doc = documents[selected_index]
    doc_id = selected_doc.get('id')

    # デバッグ: 選択されたドキュメントを確認
    logger.info(f"=== 選択されたドキュメント ===")
    logger.info(f"selected_index: {selected_index}")
    logger.info(f"doc_id: {doc_id}")
    logger.info(f"file_name: {selected_doc.get('file_name')}")

    # 先にドキュメント情報を取得（st.rerun()の前に）
    drive_file_id = selected_doc.get('drive_file_id')
    source_id = selected_doc.get('source_id')
    file_id = drive_file_id or source_id
    file_name = selected_doc.get('file_name') or 'unknown'
    doc_type = selected_doc.get('doc_type', '')

    # metadataをパース（JSON文字列の場合と辞書の場合の両方に対応）
    metadata = selected_doc.get('metadata') or {}
    if isinstance(metadata, str):
        try:
            metadata = json.loads(metadata)
        except json.JSONDecodeError:
            logger.error(f"Failed to parse metadata JSON for doc_id: {doc_id}")
            metadata = {}

    # extracted_tables カラムの内容を metadata に統合
    if 'extracted_tables' in selected_doc and selected_doc['extracted_tables']:
        extracted_tables = selected_doc['extracted_tables']
        if isinstance(extracted_tables, str):
            try:
                extracted_tables = json.loads(extracted_tables)
            except json.JSONDecodeError:
                logger.error(f"Failed to parse extracted_tables JSON for doc_id: {doc_id}")
                extracted_tables = None
        if extracted_tables:
            metadata['extracted_tables'] = extracted_tables
            logger.info(f"Added extracted_tables to metadata: {len(extracted_tables)} tables")

    # デバッグ: メタデータの状態を確認
    logger.info(f"metadata keys: {list(metadata.keys())}")
    logger.info(f"metadata size: {len(str(metadata))} bytes")
    if 'extracted_tables' in metadata:
        logger.info(f"extracted_tables found in metadata: {len(metadata['extracted_tables'])} tables")

    # ドキュメント変更を検出して、セッション状態をリセット
    if 'previous_doc_id' not in st.session_state:
        st.session_state.previous_doc_id = doc_id

    if st.session_state.previous_doc_id != doc_id:
        # ドキュメントが変更された場合、全ての編集関連のキーをクリア
        logger.info(f"ドキュメント変更を検出: {st.session_state.previous_doc_id} -> {doc_id}")
        logger.info(f"新しいファイル名: {file_name}")

        # 編集関連のセッション状態をクリア
        # 古いドキュメントのキーを削除
        old_doc_id = st.session_state.previous_doc_id
        keys_to_remove = [
            key for key in st.session_state.keys()
            if (key.startswith('form_') or
                key.startswith(f'json_editor_{old_doc_id}') or
                key.startswith(f'text_editor_{old_doc_id}') or
                key.startswith('table_editor_'))
        ]

        for key in keys_to_remove:
            del st.session_state[key]
            logger.debug(f"  削除: {key}")

        st.session_state.previous_doc_id = doc_id
        logger.info(f"セッション状態をクリア: {len(keys_to_remove)} keys removed")

        # ページを再レンダリング
        st.rerun()

    # 基本情報表示
    title = selected_doc.get('title', '')

    col1, col2 = st.columns([2, 1])
    with col1:
        if title:
            st.markdown(f"**タイトル**: {title}")
        else:
            st.markdown(f"**タイトル**: (未生成)")
        st.caption(f"ファイル名: {file_name}")
    with col2:
        st.markdown(f"**文書タイプ**: {doc_type}")

    st.markdown("---")

    # 修正履歴とロールバック機能（Phase 2）
    latest_correction_id = selected_doc.get('latest_correction_id')
    if latest_correction_id:
        with st.expander("📜 修正履歴とロールバック", expanded=False):
            correction_history = db_client.get_correction_history(doc_id, limit=5)

            if correction_history:
                st.markdown(f"**修正回数**: {len(correction_history)}回")

                # 最新の修正情報
                latest_correction = correction_history[0]
                st.markdown(f"**最新の修正日時**: {latest_correction.get('corrected_at')}")
                if latest_correction.get('corrector_email'):
                    st.markdown(f"**修正者**: {latest_correction.get('corrector_email')}")

                # ロールバックボタン
                col_rollback, col_spacer = st.columns([1, 2])
                with col_rollback:
                    if st.button("⏮️ ロールバック（元に戻す）", use_container_width=True, type="secondary"):
                        with st.spinner("ロールバック中..."):
                            rollback_success = db_client.rollback_document(doc_id)

                        if rollback_success:
                            st.success("✅ ロールバックに成功しました！前の状態に戻りました。")
                            st.rerun()
                        else:
                            st.error("❌ ロールバックに失敗しました")

                # 修正履歴の詳細表示
                with st.expander("修正履歴の詳細を表示", expanded=False):
                    for idx, correction in enumerate(correction_history):
                        st.markdown(f"### 修正 #{idx + 1}")
                        st.markdown(f"**日時**: {correction.get('corrected_at')}")
                        if correction.get('notes'):
                            st.markdown(f"**メモ**: {correction.get('notes')}")

                        # 修正前後の差分を表示
                        col_before, col_after = st.columns(2)
                        with col_before:
                            st.markdown("**修正前**")
                            st.json(correction.get('old_metadata', {}), expanded=True)
                        with col_after:
                            st.markdown("**修正後**")
                            st.json(correction.get('new_metadata', {}), expanded=True)

                        st.markdown("---")
            else:
                st.info("修正履歴がありません")

    st.markdown("---")

    # レイアウト: 左にPDFプレビュー、右に編集タブ
    col_left, col_right = st.columns([1, 1.2])

    with col_left:
        st.markdown("### 📄 ドキュメントプレビュー")

        # デバッグ情報をログに記録
        logger.info(f"プレビュー準備: file_id={file_id}, file_name={file_name}")

        # ファイルのダウンロードと表示
        if file_id:
            with st.spinner("ファイルをダウンロード中..."):
                file_path = download_file_from_drive(file_id, file_name)

            # ダウンロード結果の確認
            if not file_path:
                st.error("❌ ファイルのダウンロードに失敗しました")
                st.info("Google Driveからファイルを取得できませんでした。アクセス権限を確認してください。")
                with st.expander("🔍 デバッグ情報"):
                    st.code(f"file_id: {file_id}\nfile_name: {file_name}")
            elif not Path(file_path).exists():
                st.error("❌ ダウンロードされたファイルが見つかりません")
                st.info("ファイルはダウンロードされましたが、保存先に見つかりませんでした。")
                with st.expander("🔍 デバッグ情報"):
                    st.code(f"パス: {file_path}")

            if file_path and Path(file_path).exists():
                # ダウンロードされたファイルから実際のファイル名を取得
                actual_file_name = Path(file_path).name
                if file_name == 'unknown' and actual_file_name:
                    file_name = actual_file_name
                    logger.info(f"実際のファイル名を取得: {file_name}")

                # ファイルタイプに応じてプレビュー表示
                # まず実際のファイルパスから拡張子を取得（より確実）
                file_extension = Path(file_path).suffix.lstrip('.').lower()

                # ファイルパスから取得できない場合、file_nameから取得
                if not file_extension and file_name and '.' in file_name:
                    file_extension = file_name.lower().split('.')[-1]

                # 拡張子が取得できない場合、ファイルの内容から判定（マジックナンバー）
                if not file_extension:
                    logger.info("拡張子なし。ファイルの内容から判定します")
                    try:
                        with open(file_path, 'rb') as f:
                            header = f.read(16)

                        # PDFファイルの判定
                        if header.startswith(b'%PDF-'):
                            file_extension = 'pdf'
                            logger.info("マジックナンバーからPDFと判定")
                        # その他のファイルタイプも追加可能
                        elif header.startswith(b'\x50\x4B\x03\x04'):  # ZIP/DOCX/XLSX等
                            file_extension = 'zip'
                            logger.info("マジックナンバーからZIPと判定")
                        elif header.startswith(b'\xff\xd8\xff'):  # JPEG
                            file_extension = 'jpg'
                            logger.info("マジックナンバーからJPEGと判定")
                        elif header.startswith(b'\x89PNG'):  # PNG
                            file_extension = 'png'
                            logger.info("マジックナンバーからPNGと判定")
                    except Exception as e:
                        logger.error(f"ファイルタイプ判定エラー: {e}")

                logger.info(f"ファイル拡張子: {file_extension}")

                # PDFの場合
                if file_extension == 'pdf' or file_name.lower().endswith('.pdf'):
                    import os
                    logger.info(f"PDFプレビュー開始。ローカルパス: {file_path}")
                    logger.info(f"ファイルサイズ: {os.path.getsize(file_path)} bytes")

                    try:
                        from streamlit_pdf_viewer import pdf_viewer
                        logger.info("streamlit_pdf_viewer を使用してPDF表示（ファイルパス直接渡し）")
                        pdf_viewer(file_path, height=700)
                    except ImportError:
                        logger.warning("streamlit_pdf_viewer がインストールされていません。ダウンロードボタンを表示します")
                        st.warning("PDFビューアーライブラリがインストールされていません")
                        with open(file_path, 'rb') as f:
                            pdf_bytes = f.read()
                        st.download_button(
                            label="📥 PDFをダウンロード",
                            data=pdf_bytes,
                            file_name=file_name,
                            mime="application/pdf",
                            use_container_width=True
                        )
                    except Exception as e:
                        logger.error(f"PDFプレビュー表示エラー: {e}", exc_info=True)
                        st.warning(f"PDFプレビュー表示エラー: {e}")

                # テキストファイルの場合（txt, md, csv, json, etc.）
                elif file_extension in ['txt', 'md', 'markdown', 'csv', 'json', 'log', 'py', 'js', 'html', 'css']:
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            text_content = f.read()

                        st.markdown("#### 📝 テキストプレビュー")

                        # タブで表示を切り替え
                        text_tab1, text_tab2, text_tab3 = st.tabs(["原文", "構造化表示", "統計"])

                        with text_tab1:
                            # 原文表示
                            st.text_area(
                                "ファイル内容",
                                value=text_content,
                                height=500,
                                disabled=True,
                                key=f"text_preview_{doc_id}"
                            )

                        with text_tab2:
                            # 構造化表示
                            from frontend.utils.text_structurer import TextStructurer
                            structured_blocks = TextStructurer.structure_text(text_content)

                            if structured_blocks:
                                # 構造化データをDataFrameで表示
                                df_structured = pd.DataFrame(structured_blocks)

                                # タイプを日本語に翻訳
                                df_structured['type_ja'] = df_structured['type'].apply(
                                    lambda t: TextStructurer._translate_type(t)
                                )

                                st.dataframe(
                                    df_structured[['line_number', 'type_ja', 'content', 'length']],
                                    column_config={
                                        "line_number": st.column_config.NumberColumn("行番号", width="small"),
                                        "type_ja": st.column_config.TextColumn("タイプ", width="small"),
                                        "content": st.column_config.TextColumn("内容", width="large"),
                                        "length": st.column_config.NumberColumn("文字数", width="small")
                                    },
                                    height=500,
                                    use_container_width=True
                                )

                                # metadataに構造化データを追加
                                if 'text_blocks' not in metadata:
                                    metadata['text_blocks'] = structured_blocks
                                    logger.info(f"テキスト構造化データをメタデータに追加: {len(structured_blocks)} ブロック")

                        with text_tab3:
                            # 統計情報
                            from frontend.utils.text_structurer import TextStructurer
                            structured_blocks = TextStructurer.structure_text(text_content)
                            stats = TextStructurer.get_statistics(structured_blocks)

                            st.markdown("### 📊 テキスト統計")
                            col_stat1, col_stat2, col_stat3 = st.columns(3)
                            with col_stat1:
                                st.metric("総行数", stats['total_lines'])
                            with col_stat2:
                                st.metric("ブロックタイプ数", stats['unique_types'])
                            with col_stat3:
                                total_chars = sum(block['length'] for block in structured_blocks)
                                st.metric("総文字数", total_chars)

                            st.markdown("### 📋 ブロックタイプ別件数")
                            for block_type, count in sorted(stats['type_counts'].items(), key=lambda x: x[1], reverse=True):
                                type_ja = TextStructurer._translate_type(block_type)
                                st.write(f"- **{type_ja}**: {count} 行")

                        # ダウンロードボタン
                        st.download_button(
                            label="📥 テキストファイルをダウンロード",
                            data=text_content,
                            file_name=file_name,
                            mime="text/plain",
                            use_container_width=True
                        )
                    except UnicodeDecodeError:
                        st.error("❌ テキストファイルのエンコーディングエラー。UTF-8でデコードできません。")
                    except Exception as e:
                        logger.error(f"テキストファイル読み込みエラー: {e}", exc_info=True)
                        st.error(f"❌ ファイル読み込みエラー: {e}")

                # その他のファイル
                else:
                    st.info(f"このファイルタイプ（.{file_extension}）のプレビューには対応していません")

                    # デバッグ情報を表示
                    with st.expander("🔍 デバッグ情報"):
                        st.code(f"""
ファイルパス: {file_path}
ファイル名（DB）: {selected_doc.get('file_name')}
ファイル名（使用中）: {file_name}
ファイル拡張子: '{file_extension}'
Path.suffix: '{Path(file_path).suffix}'
                        """.strip())
                    try:
                        with open(file_path, 'rb') as f:
                            file_bytes = f.read()
                        st.download_button(
                            label="📥 ファイルをダウンロード",
                            data=file_bytes,
                            file_name=file_name,
                            use_container_width=True
                        )
                    except Exception as e:
                        logger.error(f"ファイル読み込みエラー: {e}", exc_info=True)
                        st.error("ファイルの読み込みに失敗しました")
            else:
                logger.warning(f"ファイルを読み込めませんでした。file_path={file_path}, exists={Path(file_path).exists() if file_path else False}")
                st.warning("ファイルを読み込めませんでした")
        else:
            st.warning("⚠️ ファイルIDが見つかりません")
            st.info("このドキュメントにはファイルIDが設定されていないため、プレビューを表示できません。")
            with st.expander("🔍 デバッグ情報"):
                st.json({
                    "drive_file_id": selected_doc.get('drive_file_id'),
                    "source_id": selected_doc.get('source_id'),
                    "file_name": file_name,
                    "doc_type": doc_type
                })

        # ============================================
        # 【新機能】手動テキスト補正（Human-in-the-loop）
        # ============================================
        # PDFまたはテキストファイルの場合、手動補正機能を表示
        if file_path and Path(file_path).exists():
            from frontend.components.manual_text_correction import (
                render_manual_text_correction,
                execute_stageh_reprocessing
            )

            # テキストコンテンツを取得
            # 必ずこの順で結合: 1. display_post_text (投稿本文) → 2. attachment_text (添付ファイル)
            display_text = selected_doc.get('display_post_text', '') or ''
            attachment_text = selected_doc.get('attachment_text', '') or ''

            # 両方を結合（空の場合も含む）
            parts = []
            if display_text:
                parts.append(display_text)
            if attachment_text:
                parts.append(attachment_text)

            extracted_text = '\n\n'.join(parts) if parts else ''

            # 手動補正UIを表示
            corrected_texts = render_manual_text_correction(
                doc_id=doc_id,
                file_name=file_name,
                extracted_text=extracted_text,
                metadata=metadata,
                doc_type=doc_type,
                display_post_text=display_text,
                attachment_text=attachment_text
            )

            # Stage H再実行が要求された場合
            if corrected_texts:
                with st.spinner("🔄 補正されたテキストでStage H（構造化）+ 全チャンク再生成中..."):
                    try:
                        # 結合されたテキスト（Stage C用）
                        corrected_combined_text = '\n\n'.join([
                            corrected_texts.get('display_post_text', ''),
                            corrected_texts.get('attachment_text', '')
                        ]).strip()

                        # Stage H再実行
                        reprocessed_result = execute_stageh_reprocessing(
                            corrected_text=corrected_combined_text,
                            file_name=file_name,
                            metadata=metadata,
                            workspace=selected_doc.get('workspace', 'personal')
                        )

                        # メタデータを更新
                        new_metadata = reprocessed_result['metadata']

                        # データベースに保存
                        logger.info(f"[Stage H再実行] データベース保存開始: doc_id={doc_id}")
                        logger.info(f"[Stage H再実行] new_metadata keys: {list(new_metadata.keys())}")
                        logger.info(f"[Stage H再実行] new_doc_type: {doc_type}")

                        # display_post_text と attachment_text を更新
                        update_fields = {
                            'metadata': new_metadata,
                            'doc_type': doc_type,
                            'display_post_text': corrected_texts.get('display_post_text', ''),
                            'attachment_text': corrected_texts.get('attachment_text', '')
                        }

                        success = db_client.update_document(doc_id, update_fields)

                        # 補正履歴を記録
                        if success:
                            db_client.record_correction(
                                doc_id=doc_id,
                                new_metadata=new_metadata,
                                new_doc_type=doc_type,
                                corrector_email=None,
                                notes="手動テキスト補正によるStage H再実行 + 全チャンク再生成"
                            )

                            # search_indexの全チャンクを削除して再生成
                            try:
                                logger.info(f"[チャンク再生成] 開始: doc_id={doc_id}")

                                # 1. 既存チャンクを削除
                                delete_result = db_client.supabase.table('10_ix_search_index').delete().eq('document_id', doc_id).execute()
                                logger.info(f"[チャンク再生成] 既存チャンク削除完了")

                                # 2. チャンク化対象テキストを準備
                                chunk_target_text = corrected_combined_text

                                # 3. メタデータチャンク生成
                                from shared.common.processing.metadata_chunker import MetadataChunker
                                from shared.ai.llm_client.llm_client import LLMClient

                                llm_client = LLMClient()
                                metadata_chunker = MetadataChunker()

                                document_data = {
                                    'file_name': file_name,
                                    'summary': reprocessed_result.get('summary', ''),
                                    'document_date': reprocessed_result.get('document_date'),
                                    'tags': reprocessed_result.get('tags', []),
                                    'doc_type': doc_type,
                                    'display_post_text': corrected_texts.get('display_post_text', ''),
                                    'display_subject': selected_doc.get('display_subject', '')
                                }
                                metadata_chunks = metadata_chunker.create_metadata_chunks(document_data)

                                current_chunk_index = 0
                                for meta_chunk in metadata_chunks:
                                    meta_text = meta_chunk.get('chunk_text', '')
                                    if not meta_text:
                                        continue
                                    meta_embedding = llm_client.generate_embedding(meta_text)
                                    meta_doc = {
                                        'document_id': doc_id,
                                        'chunk_index': current_chunk_index,
                                        'chunk_content': meta_text,
                                        'chunk_size': len(meta_text),
                                        'chunk_type': meta_chunk.get('chunk_type', 'metadata'),
                                        'search_weight': meta_chunk.get('search_weight', 1.0),
                                        'embedding': meta_embedding
                                    }
                                    db_client.supabase.table('10_ix_search_index').insert(meta_doc).execute()
                                    current_chunk_index += 1

                                # 4. 小チャンク生成
                                from shared.common.utils.chunking import TextChunker
                                chunker = TextChunker(chunk_size=150, chunk_overlap=30)
                                small_chunks = chunker.split_text(chunk_target_text)

                                for small_chunk in small_chunks:
                                    small_text = small_chunk.get('chunk_text', '')
                                    if not small_text:
                                        continue
                                    small_embedding = llm_client.generate_embedding(small_text)
                                    small_doc = {
                                        'document_id': doc_id,
                                        'chunk_index': current_chunk_index,
                                        'chunk_content': small_text,
                                        'chunk_size': len(small_text),
                                        'chunk_type': 'content_small',
                                        'search_weight': 1.0,
                                        'embedding': small_embedding
                                    }
                                    db_client.supabase.table('10_ix_search_index').insert(small_doc).execute()
                                    current_chunk_index += 1

                                # 5. 合成チャンク生成
                                from shared.common.utils.synthetic_chunks import create_all_synthetic_chunks
                                synthetic_chunks = create_all_synthetic_chunks(new_metadata, file_name)

                                for synthetic in synthetic_chunks:
                                    synthetic_text = synthetic.get('content', '')
                                    if not synthetic_text:
                                        continue
                                    synthetic_embedding = llm_client.generate_embedding(synthetic_text)
                                    synthetic_doc = {
                                        'document_id': doc_id,
                                        'chunk_index': current_chunk_index,
                                        'chunk_content': synthetic_text,
                                        'chunk_size': len(synthetic_text),
                                        'chunk_type': 'synthetic',
                                        'search_weight': 1.0,
                                        'embedding': synthetic_embedding,
                                        'section_title': f'[合成チャンク: {synthetic.get("type", "unknown")}]'
                                    }
                                    db_client.supabase.table('10_ix_search_index').insert(synthetic_doc).execute()
                                    current_chunk_index += 1

                                logger.info(f"[チャンク再生成] 完了: {current_chunk_index}個のチャンク生成")

                            except Exception as chunk_error:
                                logger.error(f"[チャンク再生成] エラー: {chunk_error}", exc_info=True)
                                st.warning(f"⚠️ チャンク再生成中にエラーが発生しました: {chunk_error}")

                        if success:
                            st.success("✅ Stage H再実行が完了しました！構造化データが更新されました。")
                            logger.info(f"[Stage H再実行] データベース保存成功")
                            st.balloons()

                            # 補正前後の比較を表示
                            with st.expander("📊 補正前後の比較", expanded=True):
                                col_before, col_after = st.columns(2)

                                with col_before:
                                    st.markdown("**補正前**")
                                    st.metric("投稿本文", f"{len(display_text)} 文字")
                                    st.metric("添付ファイル", f"{len(attachment_text)} 文字")
                                    st.metric("合計", f"{len(extracted_text)} 文字")

                                with col_after:
                                    st.markdown("**補正後**")
                                    st.metric("投稿本文", f"{len(corrected_texts.get('display_post_text', ''))} 文字")
                                    st.metric("添付ファイル", f"{len(corrected_texts.get('attachment_text', ''))} 文字")
                                    st.metric("合計", f"{len(corrected_combined_text)} 文字")

                            # ページをリロード
                            import time
                            time.sleep(2)
                            st.rerun()
                        else:
                            logger.error(f"[Stage H再実行] データベース保存失敗: doc_id={doc_id}")
                            logger.error(f"[Stage H再実行] metadata type: {type(new_metadata)}")
                            logger.error(f"[Stage H再実行] metadata sample: {str(new_metadata)[:500]}")
                            st.error("❌ データベースへの保存に失敗しました。詳細はログを確認してください。")

                    except Exception as e:
                        logger.error(f"Stage H再実行エラー: {e}", exc_info=True)
                        st.error(f"❌ Stage H再実行エラー: {e}")

    with col_right:
        st.markdown("### ✏️ メタデータ編集")

        # スキーマを検出
        detected_schema = schema_detector.detect_schema(doc_type, metadata)

        if detected_schema:
            st.info(f"🎯 検出されたスキーマ: **{detected_schema}**")
            editable_fields = schema_detector.get_editable_fields(detected_schema)
        else:
            st.warning("⚠️ スキーマが検出されませんでした。JSON編集モードを使用してください。")
            editable_fields = []

        # 【動的タブ生成】構造化データフィールドを自動検出
        structured_fields = detect_structured_fields(metadata)

        # 構造化フィールドのキーセットを作成（フォーム編集から除外するため）
        structured_field_keys = {field["key"] for field in structured_fields}

        # タブリストを動的に構築
        tab_names = ["📝 フォーム編集"]  # フォーム編集タブのみ

        # 構造化データごとにタブを追加
        logger.info(f"🏷️ タブ生成: 構造化データタブを {len(structured_fields)} 個追加します")
        for field in structured_fields:
            logger.info(f"  タブ追加: {field['label']} (キー: {field['key']})")
            tab_names.append(field["label"])

        # 固定タブ：表を追加、JSONプレビュー
        tab_names.append("➕ 表を追加")
        tab_names.append("🔍 JSONプレビュー")

        logger.info(f"📑 生成されるタブ一覧 ({len(tab_names)} 個): {tab_names}")

        # タブを動的に生成
        tabs = st.tabs(tab_names)
        edited_metadata = None

        # タブ1: フォーム編集
        with tabs[0]:
            if editable_fields:
                # 構造化データフィールドをフォームから除外
                form_fields = [f for f in editable_fields if f["name"] not in structured_field_keys]

                # デバッグ情報を表示
                with st.expander("🔍 デバッグ情報（開発用）", expanded=False):
                    st.code(f"""
編集可能フィールド数: {len(editable_fields)}
フォームフィールド数: {len(form_fields)}
構造化フィールド数: {len(structured_fields)}

メタデータのキー: {list(metadata.keys())}
text_blocks の存在: {'text_blocks' in metadata}
text_blocks の値: {metadata.get('text_blocks', 'なし')}

editable_fields:
{[f['name'] for f in editable_fields]}

form_fields:
{[f['name'] for f in form_fields]}

structured_field_keys:
{structured_field_keys}
                    """.strip())

                if form_fields:
                    edited_metadata = render_form_editor(metadata, form_fields, doc_id)
                else:
                    st.info("このドキュメントのフィールドは全て専用タブで編集できます")
                    st.markdown("各データタブまたはJSONプレビュータブをご利用ください")
            else:
                st.info("フォーム編集には対応するスキーマが必要です")

        # タブ2以降: 構造化データタブ（動的に生成）
        for idx, field in enumerate(structured_fields):
            with tabs[idx + 1]:  # フォーム編集の次から
                logger.info(f"📊 タブ {idx + 1} をレンダリング: {field['label']} ({field['key']})")
                logger.info(f"  データ件数: {len(field['data'])} 件")

                st.markdown(f"### {field['label']}")
                st.markdown("表形式で編集できます")
                st.markdown("---")

                # 表エディタでレンダリング（ドキュメントごとにユニークなキーを使用）
                edited_value = _render_array_table(
                    f"{field['key']}_{doc_id}",
                    field["data"],
                    field["label"]
                )

                # edited_metadataを初期化（必要に応じて）
                if edited_metadata is None:
                    edited_metadata = metadata.copy()

                edited_metadata[field["key"]] = edited_value
                logger.info(f"  ✓ {field['label']} タブのレンダリング完了")

        # 最後から2番目のタブ: 表を追加
        with tabs[-2]:
            from frontend.components.table_creator import render_table_creator

            updated_metadata = render_table_creator(doc_id, metadata.copy())

            if updated_metadata:
                edited_metadata = updated_metadata
                st.info("💡 追加した表を保存するには、下の「💾 保存」ボタンを押してください")

        # 最後のタブ: JSONプレビュー
        with tabs[-1]:
            edited_metadata = render_json_preview(metadata, editable=True, key_suffix=doc_id)

        # 保存ボタンエリア
        st.markdown("---")

        # レビュー状態の表示
        is_reviewed = selected_doc.get('is_reviewed', False)
        if is_reviewed:
            reviewed_at = selected_doc.get('reviewed_at', '')
            reviewed_by = selected_doc.get('reviewed_by', '')
            st.info(f"✅ レビュー済み（{reviewed_at[:10] if reviewed_at else '日時不明'}）" +
                   (f" by {reviewed_by}" if reviewed_by else ""))

        col_save, col_validate, col_review, col_cancel = st.columns([1, 1, 1, 1])

        with col_validate:
            if st.button("🔍 変更を確認", use_container_width=True):
                if edited_metadata:
                    with st.expander("変更内容の詳細", expanded=True):
                        render_json_diff(metadata, edited_metadata)

        with col_save:
            if st.button("💾 保存", type="primary", use_container_width=True):
                if edited_metadata is None:
                    st.error("編集されたデータがありません")
                else:
                    # スキーマ検証
                    if detected_schema:
                        is_valid, errors = schema_detector.validate_metadata(detected_schema, edited_metadata)
                        if not is_valid:
                            st.error("❌ スキーマ検証エラー:")
                            for error in errors:
                                st.error(f"  - {error}")
                            st.stop()

                    # データベース更新（修正履歴を記録）
                    success = db_client.record_correction(
                        doc_id=doc_id,
                        new_metadata=edited_metadata,
                        new_doc_type=doc_type,
                        corrector_email=None,  # 将来的に認証情報から取得
                        notes="Review UIからの手動修正"
                    )

                    if success:
                        st.success("✅ 保存に成功しました！修正履歴が記録されました。")
                        st.balloons()
                        # ページをリロード
                        st.rerun()
                    else:
                        st.error("❌ 保存に失敗しました")

        with col_review:
            # レビュー状態切り替えボタン
            if is_reviewed:
                # レビュー済み → 未完了に戻す
                if st.button("↩️ 未完了に戻す", use_container_width=True, type="secondary"):
                    success = db_client.mark_document_unreviewed(doc_id)
                    if success:
                        st.success("✅ 未完了に戻しました")
                        st.rerun()
                    else:
                        st.error("❌ 操作に失敗しました")
            else:
                # 未レビュー → チェック完了
                if st.button("✅ チェック完了", use_container_width=True, type="primary"):
                    success = db_client.mark_document_reviewed(doc_id)
                    if success:
                        st.success("✅ レビュー完了としてマークしました")
                        st.balloons()
                        st.rerun()
                    else:
                        st.error("❌ 操作に失敗しました")

        with col_cancel:
            if st.button("🔄 リセット", use_container_width=True):
                st.rerun()

    # 削除機能（危険な操作のため、別セクションに配置）
    st.markdown("---")
    st.markdown("### ⚠️ 危険な操作")

    # 削除確認用のセッション状態
    delete_confirm_key = f"delete_confirm_{doc_id}"
    if delete_confirm_key not in st.session_state:
        st.session_state[delete_confirm_key] = False

    col_delete1, col_delete2, col_spacer = st.columns([1, 1, 2])

    with col_delete1:
        if not st.session_state[delete_confirm_key]:
            if st.button("🗑️ データを削除", use_container_width=True, type="secondary"):
                st.session_state[delete_confirm_key] = True
                st.rerun()
        else:
            st.warning("本当に削除しますか？")

    with col_delete2:
        if st.session_state[delete_confirm_key]:
            if st.button("✅ 削除を実行", use_container_width=True, type="primary"):
                with st.spinner("削除中..."):
                    # 1. まずGoogle Driveからファイルをゴミ箱に移動
                    drive_success = False
                    if file_id:
                        try:
                            drive_connector = GoogleDriveConnector()
                            drive_success = drive_connector.trash_file(file_id)
                            if drive_success:
                                st.success(f"✅ Google Driveのファイルをゴミ箱に移動しました")
                            else:
                                st.warning(f"⚠️ Google Driveファイルの削除に失敗しましたが、データベースからは削除します")
                        except Exception as e:
                            st.error(f"Google Drive削除エラー: {e}")
                            st.warning(f"⚠️ Google Driveファイルの削除に失敗しましたが、データベースからは削除します")
                    else:
                        st.warning("Google DriveのファイルIDが見つかりません。データベースのみ削除します。")

                    # 2. データベースから削除
                    db_success = db_client.delete_document(doc_id)

                    if db_success:
                        st.success("✅ データベースからドキュメントを削除しました")
                        st.balloons()
                        # 削除確認状態をリセット
                        st.session_state[delete_confirm_key] = False
                        # 少し待ってからリロード
                        import time
                        time.sleep(1)
                        st.rerun()
                    else:
                        st.error("❌ データベースからの削除に失敗しました")
                        st.session_state[delete_confirm_key] = False

            if st.button("❌ キャンセル", use_container_width=True):
                st.session_state[delete_confirm_key] = False
                st.rerun()

    # フッター
    st.markdown("---")
    col_footer1, col_footer2 = st.columns([3, 1])
    with col_footer1:
        st.caption("Document Management System - Document Review App v2.0")
    with col_footer2:
        st.caption(f"🎨 検出スキーマ: {detected_schema or 'N/A'}")


def main():
    """メインUIロジック"""
    st.set_page_config(
        page_title="Document Review - Document Management System",
        page_icon="📋",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # ドキュメントレビューUIを表示
    pdf_review_ui()


if __name__ == "__main__":
    main()
```

### frontend\email_inbox_app.py

```py
"""
Email Inbox App (独立アプリ版)
Gmailから取り込んだメールの確認・管理

機能:
- メール一覧表示
- メールプレビュー（HTML）
- 構造化データ表示（テキストブロックなど）
- Doc typeフィルタ（DM-mail / JOB-mail）
- 削除機能
"""
import sys
from pathlib import Path

# プロジェクトのルートディレクトリをPythonパスに追加
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

import streamlit as st
import json
from typing import Dict, Any, List
import pandas as pd
from loguru import logger

from shared.common.database.client import DatabaseClient
from shared.common.connectors.google_drive import GoogleDriveConnector
from shared.common.connectors.gmail_connector import GmailConnector
import os

# コンポーネントをインポート
from frontend.components.email_viewer import (
    render_email_list,
    render_email_detail,
    render_email_html_preview
)
from frontend.components.table_editor import _format_field_name, _render_array_table
from frontend.components.form_editor import render_form_editor
from frontend.components.json_preview import render_json_preview, render_json_diff
from frontend.components.table_creator import render_table_creator
from frontend.utils.schema_detector import SchemaDetector


def detect_structured_fields(metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    メタデータから構造化データフィールドを自動検出

    Args:
        metadata: メタデータ辞書

    Returns:
        構造化フィールドのリスト [{"key": str, "label": str, "data": list}, ...]
    """
    structured_fields = []

    logger.info("=" * 60)
    logger.info("🔍 構造化フィールド検出を開始")
    logger.info(f"メタデータのキー数: {len(metadata)}")
    logger.info("=" * 60)

    for key, value in metadata.items():
        # デバッグログ: 全てのキーと値の型を出力
        logger.debug(f"Key: {key}, Type: {type(value)}, Value start: {str(value)[:50]}")

        # extracted_tablesの特別処理
        if key == "extracted_tables":
            logger.info(f"🎯 FOUND extracted_tables! Type: {type(value)}, Length: {len(value) if isinstance(value, list) else 'N/A'}")
            if isinstance(value, list):
                logger.info(f"  First element type: {type(value[0]) if len(value) > 0 else 'empty'}")

        # _list, _blocks, _matrix, _tables で終わるキー、または structured_tables, weekly_schedule, extracted_tables を構造化データとして認識
        # ただし text_blocks は除外（フォーム編集タブで編集可能にするため）
        if key == "text_blocks":
            logger.info(f"✓ '{key}' は text_blocks として検出されましたが、フォーム編集タブで表示するため除外します")
            continue

        if (key.endswith("_list") or key.endswith("_blocks") or
            key.endswith("_matrix") or key.endswith("_tables") or
            key == "structured_tables" or key == "weekly_schedule" or key == "extracted_tables"):
            logger.info(f"✓ '{key}' は構造化データフィールドとして検出")

            if not isinstance(value, list):
                logger.warning(f"  ⚠️ '{key}' はリストではありません。Type: {type(value)}")
                continue

            if len(value) == 0:
                logger.warning(f"  ⚠️ '{key}' は空のリストです")
                continue

            logger.info(f"  ✓ '{key}' はリストで、要素数: {len(value)}")

            # extracted_tablesは特別処理（文字列のリストをパースして構造化データに変換）
            if key == "extracted_tables":
                logger.info(f"  ✓ '{key}' は extracted_tables として検出 - パース処理を実行")
                from frontend.utils.table_parser import parse_extracted_tables
                parsed_tables = parse_extracted_tables(value)
                if parsed_tables:
                    logger.info(f"  ✓ {len(parsed_tables)} 個の表をパースしました")
                    structured_fields.append({
                        "key": key,
                        "label": _format_field_name(key),
                        "data": parsed_tables
                    })
                else:
                    logger.warning(f"  ⚠️ '{key}' のパースに失敗しました")
                continue

            # 配列の最初の要素が辞書であることを確認（構造化データの証拠）
            if isinstance(value[0], dict):
                logger.info(f"  ✓ '{key}' の最初の要素は辞書です → 構造化フィールドとして検出!")
                structured_fields.append({
                    "key": key,
                    "label": _format_field_name(key),
                    "data": value
                })
            else:
                logger.warning(f"  ⚠️ '{key}' の最初の要素は辞書ではありません。Type: {type(value[0])}")

    logger.info("=" * 60)
    logger.info(f"🎯 検出された構造化フィールド数: {len(structured_fields)}")
    for field in structured_fields:
        logger.info(f"  - {field['key']} ({field['label']}) - {len(field['data'])} 件")
    logger.info("=" * 60)

    return structured_fields


def email_inbox_ui():
    """メール受信トレイUIロジック"""
    # データベースクライアントの初期化
    try:
        db_client = DatabaseClient()
        drive_connector = GoogleDriveConnector()
    except Exception as e:
        st.error(f"初期化エラー: {e}")
        st.stop()

    # サイドバー: フィルタ設定
    st.sidebar.header("🔍 フィルタ")

    # Doc typeフィルタ（メール種別）
    doc_type_options = ["全て", "DM-mail", "JOB-mail"]
    doc_type_filter = st.sidebar.selectbox(
        "メール種別",
        options=doc_type_options,
        index=0,
        help="メールの種類でフィルタリング"
    )

    # 取得件数
    limit = st.sidebar.number_input(
        "取得件数",
        min_value=10,
        max_value=500,
        value=50,
        step=10,
        help="表示するメールの最大件数"
    )

    # リスト更新ボタン
    st.sidebar.markdown("---")
    if st.sidebar.button("🔄 リストを更新", use_container_width=True, key="refresh_email_list"):
        st.rerun()

    # メールを取得（workspace='gmail', doc_typeでフィルタ）
    with st.spinner("メールを取得中..."):
        # Doc typeフィルタの値を変換（"全て"の場合はNone）
        doc_type_value = doc_type_filter if doc_type_filter != "全て" else None

        # get_documents_for_reviewを使用してメールを取得
        emails = db_client.get_documents_for_review(
            limit=limit,
            workspace="gmail",  # Gmailのみ
            doc_type=doc_type_value,  # Doc typeでフィルタ
            review_status="all"  # 全てのステータス
        )

    logger.info(f"DBから取得したメール数: {len(emails)}件")

    if not emails:
        st.info("メールがありません")
        return

    st.sidebar.success(f"✅ {len(emails)}件のメール")

    # メール一覧を表示
    selected_index, edited_df = render_email_list(emails)

    if selected_index is None:
        st.info("メールを選択してください")
        return

    # 選択されたメールを取得
    selected_indices = edited_df[edited_df['選択'] == True].index.tolist() if edited_df is not None else []
    selected_count = len(selected_indices)

    # まとめて操作ボタン（一覧の直下に常に表示）
    col_approve, col_delete, col_spacer = st.columns([1, 1, 2])

    with col_approve:
        if selected_count > 0:
            if st.button(f"✅ まとめて承認 ({selected_count}件)", use_container_width=True, type="primary"):
                with st.spinner(f"{selected_count}件のメールを承認中..."):
                    success_count = 0
                    fail_count = 0

                    for idx in selected_indices:
                        email = emails[idx]
                        doc_id = email.get('id')

                        # レビュー済みとしてマーク
                        if db_client.mark_document_reviewed(doc_id):
                            success_count += 1
                        else:
                            fail_count += 1

                    if success_count > 0:
                        st.success(f"✅ {success_count}件のメールを承認しました")
                    if fail_count > 0:
                        st.error(f"❌ {fail_count}件の承認に失敗しました")

                    st.balloons()
                    import time
                    time.sleep(1)
                    st.rerun()
        else:
            st.button("✅ まとめて承認", use_container_width=True, disabled=True)

    with col_delete:
        # 一括削除確認用のセッション状態
        if 'bulk_delete_confirm_email' not in st.session_state:
            st.session_state.bulk_delete_confirm_email = False

        if selected_count > 0:
            if not st.session_state.bulk_delete_confirm_email:
                if st.button(f"🗑️ まとめて削除 ({selected_count}件)", use_container_width=True, type="secondary"):
                    st.session_state.bulk_delete_confirm_email = True
                    st.rerun()
            else:
                if st.button(f"⚠️ 削除を実行 ({selected_count}件)", use_container_width=True, type="primary"):
                    with st.spinner(f"{selected_count}件のメールを削除中..."):
                        success_count = 0
                        fail_count = 0

                        for idx in selected_indices:
                            email = emails[idx]
                            doc_id = email.get('id')
                            file_id = email.get('source_id')
                            metadata = email.get('metadata', {})
                            if isinstance(metadata, str):
                                try:
                                    metadata = json.loads(metadata)
                                except:
                                    metadata = {}

                            # 1. Gmailのメッセージをゴミ箱に移動
                            message_id = metadata.get('message_id')
                            if message_id:
                                try:
                                    user_email = os.getenv('GMAIL_USER_EMAIL', 'ookubo.y@workspace-o.com')
                                    gmail_connector = GmailConnector(user_email)
                                    gmail_connector.trash_message(message_id)
                                    logger.info(f"Gmailメッセージをゴミ箱に移動: {message_id}")
                                except Exception as e:
                                    logger.error(f"Gmailゴミ箱移動エラー: {e}")
                            else:
                                logger.warning(f"message_idが見つかりません（古いデータの可能性）: doc_id={doc_id}")

                            # 2. Google DriveからHTMLファイルを削除
                            if file_id:
                                try:
                                    drive_connector.trash_file(file_id)
                                except Exception as e:
                                    logger.error(f"Google Drive削除エラー: {e}")

                            # 3. データベースから削除
                            if db_client.delete_document(doc_id):
                                success_count += 1
                            else:
                                fail_count += 1

                        if success_count > 0:
                            st.success(f"✅ {success_count}件のメールを削除しました")
                        if fail_count > 0:
                            st.error(f"❌ {fail_count}件の削除に失敗しました")

                        st.session_state.bulk_delete_confirm_email = False
                        st.balloons()
                        import time
                        time.sleep(1)
                        st.rerun()
        else:
            st.button("🗑️ まとめて削除", use_container_width=True, disabled=True)

    # 削除確認中はキャンセルボタン表示
    if st.session_state.get('bulk_delete_confirm_email', False):
        if st.button("❌ キャンセル", use_container_width=True):
            st.session_state.bulk_delete_confirm_email = False
            st.rerun()

    # メール詳細表示
    st.markdown("---")
    st.subheader("📧 メール詳細")

    selected_email = emails[selected_index]

    # レイアウト: 左にHTMLプレビュー、右に詳細情報
    col_left, col_right = st.columns([1, 1.2])

    with col_left:
        st.markdown("### 📄 メールプレビュー")
        render_email_html_preview(selected_email, drive_connector)

    with col_right:
        st.markdown("### ✏️ メール情報 & メタデータ編集")

        # metadataを取得してパース
        metadata = selected_email.get('metadata') or {}
        if isinstance(metadata, str):
            try:
                metadata = json.loads(metadata)
            except json.JSONDecodeError:
                logger.error(f"Failed to parse metadata JSON for email: {selected_email.get('id')}")
                metadata = {}

        # スキーマ検出器の初期化
        schema_detector = SchemaDetector()
        doc_type = selected_email.get('doc_type', '')
        doc_id = selected_email.get('id')

        # スキーマを検出
        detected_schema = schema_detector.detect_schema(doc_type, metadata)

        if detected_schema:
            st.info(f"🎯 検出されたスキーマ: **{detected_schema}**")
            editable_fields = schema_detector.get_editable_fields(detected_schema)
        else:
            editable_fields = []

        # 構造化フィールドを検出
        structured_fields = detect_structured_fields(metadata)

        # 構造化フィールドのキーセットを作成（フォーム編集から除外するため）
        structured_field_keys = {field["key"] for field in structured_fields}

        # タブリストを動的に構築
        tab_names = ["📝 基本情報"]  # 基本情報タブ

        # 構造化データごとにタブを追加
        for field in structured_fields:
            tab_names.append(field["label"])

        # 固定タブ：表を追加、JSONプレビュー
        tab_names.append("➕ 表を追加")
        tab_names.append("🔍 JSONプレビュー")

        # タブを動的に生成
        tabs = st.tabs(tab_names)
        edited_metadata = None

        # タブ1: 基本情報 & フォーム編集
        with tabs[0]:
            # 基本情報を表示
            render_email_detail(selected_email)

            st.markdown("---")
            st.markdown("#### メタデータ編集")

            if editable_fields:
                # 構造化データフィールドをフォームから除外
                form_fields = [f for f in editable_fields if f["name"] not in structured_field_keys]

                if form_fields:
                    edited_metadata = render_form_editor(metadata, form_fields, doc_id)
                else:
                    st.info("このメールのフィールドは全て専用タブで編集できます")
            else:
                st.info("フォーム編集には対応するスキーマが必要です。JSONプレビュータブをご利用ください。")

        # タブ2以降: 構造化データタブ（動的に生成）
        for idx, field in enumerate(structured_fields):
            with tabs[idx + 1]:  # 基本情報の次から
                st.markdown(f"### {field['label']}")
                st.markdown("表形式で編集できます")
                st.markdown("---")

                # 表エディタでレンダリング
                edited_value = _render_array_table(
                    f"{field['key']}_{doc_id}",
                    field["data"],
                    field["label"]
                )

                # edited_metadataを初期化（必要に応じて）
                if edited_metadata is None:
                    edited_metadata = metadata.copy()

                edited_metadata[field["key"]] = edited_value

        # 最後から2番目のタブ: 表を追加
        with tabs[-2]:
            updated_metadata = render_table_creator(doc_id, metadata.copy())

            if updated_metadata:
                edited_metadata = updated_metadata
                st.info("💡 追加した表を保存するには、下の「💾 保存」ボタンを押してください")

        # 最後のタブ: JSONプレビュー
        with tabs[-1]:
            edited_metadata = render_json_preview(metadata, editable=True, key_suffix=doc_id)

        # 保存ボタンエリア
        st.markdown("---")

        col_save, col_validate, col_cancel = st.columns([1, 1, 1])

        with col_validate:
            if st.button("🔍 変更を確認", use_container_width=True, key=f"validate_{doc_id}"):
                if edited_metadata:
                    with st.expander("変更内容の詳細", expanded=True):
                        render_json_diff(metadata, edited_metadata)

        with col_save:
            if st.button("💾 保存", type="primary", use_container_width=True, key=f"save_{doc_id}"):
                if edited_metadata is None:
                    st.error("編集されたデータがありません")
                else:
                    # スキーマ検証
                    if detected_schema:
                        is_valid, errors = schema_detector.validate_metadata(detected_schema, edited_metadata)
                        if not is_valid:
                            st.error("❌ スキーマ検証エラー:")
                            for error in errors:
                                st.error(f"  - {error}")
                        else:
                            # データベース更新（修正履歴を記録）
                            success = db_client.record_correction(
                                doc_id=doc_id,
                                new_metadata=edited_metadata,
                                new_doc_type=doc_type,
                                corrector_email=None,
                                notes="Email Inbox UIからの手動修正"
                            )

                            if success:
                                st.success("✅ 保存に成功しました！")
                                st.balloons()
                                import time
                                time.sleep(1)
                                st.rerun()
                            else:
                                st.error("❌ 保存に失敗しました")
                    else:
                        # スキーマなしでも保存可能
                        success = db_client.record_correction(
                            doc_id=doc_id,
                            new_metadata=edited_metadata,
                            new_doc_type=doc_type,
                            corrector_email=None,
                            notes="Email Inbox UIからの手動修正"
                        )

                        if success:
                            st.success("✅ 保存に成功しました！")
                            st.balloons()
                            import time
                            time.sleep(1)
                            st.rerun()
                        else:
                            st.error("❌ 保存に失敗しました")

        with col_cancel:
            if st.button("🔄 リセット", use_container_width=True, key=f"reset_{doc_id}"):
                st.rerun()

    # 削除機能（危険な操作のため、別セクションに配置）
    st.markdown("---")
    st.markdown("### ⚠️ 危険な操作")

    doc_id = selected_email.get('id')

    # 削除確認用のセッション状態
    delete_confirm_key = f"delete_confirm_email_{doc_id}"
    if delete_confirm_key not in st.session_state:
        st.session_state[delete_confirm_key] = False

    col_delete1, col_delete2, col_spacer = st.columns([1, 1, 2])

    with col_delete1:
        if not st.session_state[delete_confirm_key]:
            if st.button("🗑️ このメールを削除", use_container_width=True, type="secondary", key="single_delete"):
                st.session_state[delete_confirm_key] = True
                st.rerun()
        else:
            st.warning("本当に削除しますか？")

    with col_delete2:
        if st.session_state[delete_confirm_key]:
            if st.button("✅ 削除を実行", use_container_width=True, type="primary", key="single_delete_confirm"):
                with st.spinner("削除中..."):
                    metadata = selected_email.get('metadata', {})
                    if isinstance(metadata, str):
                        try:
                            metadata = json.loads(metadata)
                        except:
                            metadata = {}

                    # 1. Gmailのメッセージをゴミ箱に移動
                    message_id = metadata.get('message_id')
                    if message_id:
                        try:
                            user_email = os.getenv('GMAIL_USER_EMAIL', 'ookubo.y@workspace-o.com')
                            gmail_connector = GmailConnector(user_email)
                            gmail_connector.trash_message(message_id)
                            st.success(f"✅ Gmailのメッセージをゴミ箱に移動しました")
                        except Exception as e:
                            st.error(f"Gmailゴミ箱移動エラー: {e}")
                            st.warning(f"⚠️ Gmailメッセージのゴミ箱移動に失敗しましたが、続行します")
                    else:
                        st.warning(f"⚠️ message_idが見つかりません（古いデータのため、Gmailの削除はスキップされました）")

                    # 2. Google DriveからHTMLファイルを削除
                    file_id = selected_email.get('source_id')
                    if file_id:
                        try:
                            drive_connector.trash_file(file_id)
                            st.success(f"✅ Google Driveのファイルをゴミ箱に移動しました")
                        except Exception as e:
                            st.error(f"Google Drive削除エラー: {e}")
                            st.warning(f"⚠️ Google Driveファイルの削除に失敗しましたが、データベースからは削除します")

                    # 3. データベースから削除
                    db_success = db_client.delete_document(doc_id)

                    if db_success:
                        st.success("✅ データベースからメールを削除しました")
                        st.balloons()
                        st.session_state[delete_confirm_key] = False
                        import time
                        time.sleep(1)
                        st.rerun()
                    else:
                        st.error("❌ データベースからの削除に失敗しました")
                        st.session_state[delete_confirm_key] = False

            if st.button("❌ キャンセル", use_container_width=True, key="single_delete_cancel"):
                st.session_state[delete_confirm_key] = False
                st.rerun()

    # フッター
    st.markdown("---")
    st.caption("Document Management System - Email Inbox App v1.0")


def main():
    """メインUIロジック"""
    st.set_page_config(
        page_title="Email Inbox - Document Management System",
        page_icon="📬",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # メール受信トレイUIを表示
    email_inbox_ui()


if __name__ == "__main__":
    main()
```

### frontend\expired_email_manager.py

```py
#!/usr/bin/env python
"""
期限切れメール管理UI
ベクトル検索で期限切れメールを検出して、選択的に削除できるStreamlitアプリ
"""
import sys
import os
from pathlib import Path

# プロジェクトのルートディレクトリをPythonパスに追加
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

import streamlit as st
from datetime import datetime, timedelta
import re
from typing import List, Dict, Any
from dotenv import load_dotenv

from shared.common.database.client import DatabaseClient
from shared.common.connectors.google_drive import GoogleDriveConnector
from shared.common.connectors.gmail_connector import GmailConnector
from shared.ai.llm_client.llm_client import LLMClient

load_dotenv()

# ページ設定
st.set_page_config(
    page_title="期限切れメール管理",
    page_icon="📧",
    layout="wide"
)

# 期限関連のキーワード
EXPIRATION_KEYWORDS = [
    "セール 終了",
    "配送期限",
    "注文期限",
    "有効期限",
    "締切日",
    "まもなく終了",
    "本日最終日",
    "本日限定",
    "今日まで",
    "キャンペーン 終了"
]


@st.cache_data(ttl=300)  # 5分間キャッシュ
def extract_dates_from_text(text: str, title: str = "") -> List[datetime]:
    """テキストから日付を抽出"""
    dates = []
    now = datetime.now()
    combined_text = f"{title} {text}"

    # パターン1: YYYY年MM月DD日
    pattern1 = r'(\d{4})年(\d{1,2})月(\d{1,2})日'
    for match in re.finditer(pattern1, combined_text):
        try:
            year = int(match.group(1))
            month = int(match.group(2))
            day = int(match.group(3))
            date = datetime(year, month, day, 23, 59, 59)
            dates.append(date)
        except ValueError:
            continue

    # パターン2: MM月DD日（年なし）
    pattern2 = r'(\d{1,2})月(\d{1,2})日'
    for match in re.finditer(pattern2, combined_text):
        try:
            month = int(match.group(1))
            day = int(match.group(2))
            year = now.year
            date = datetime(year, month, day, 23, 59, 59)
            if date < now:
                date = datetime(year + 1, month, day, 23, 59, 59)
            dates.append(date)
        except ValueError:
            continue

    # パターン3: MM/DD
    pattern3 = r'(\d{1,2})/(\d{1,2})'
    for match in re.finditer(pattern3, combined_text):
        try:
            month = int(match.group(1))
            day = int(match.group(2))
            year = now.year
            date = datetime(year, month, day, 23, 59, 59)
            if date < now:
                date = datetime(year + 1, month, day, 23, 59, 59)
            dates.append(date)
        except ValueError:
            continue

    # パターン4: YYYY-MM-DD
    pattern4 = r'(\d{4})-(\d{1,2})-(\d{1,2})'
    for match in re.finditer(pattern4, combined_text):
        try:
            year = int(match.group(1))
            month = int(match.group(2))
            day = int(match.group(3))
            date = datetime(year, month, day, 23, 59, 59)
            dates.append(date)
        except ValueError:
            continue

    return dates


def find_expired_emails(grace_days: int = 0, progress_callback=None) -> List[Dict[str, Any]]:
    """ベクトル検索で期限切れメールを検出"""
    db_client = DatabaseClient()
    llm_client = LLMClient()

    expired_emails = []
    seen_ids = set()
    now = datetime.now()

    total_keywords = len(EXPIRATION_KEYWORDS)

    for i, keyword in enumerate(EXPIRATION_KEYWORDS):
        if progress_callback:
            progress_callback(i / total_keywords, f"検索中: {keyword}")

        try:
            embedding = llm_client.generate_embedding(keyword)
            results = db_client.search_documents_sync(
                keyword,
                embedding,
                limit=50,
                doc_types=['DM-mail']
            )

            for doc in results:
                doc_id = doc.get('id')
                if doc_id in seen_ids:
                    continue

                title = doc.get('file_name', '') or doc.get('title', '')
                content = ""
                all_chunks = doc.get('all_chunks', [])
                if all_chunks:
                    chunk_contents = [chunk.get('chunk_content', '') for chunk in all_chunks]
                    content = '\n'.join(chunk_contents)
                else:
                    content = doc.get('content', '') or doc.get('summary', '') or doc.get('attachment_text', '')

                dates = extract_dates_from_text(content, title)
                if not dates:
                    continue

                expiration_date = min(dates)
                grace = timedelta(days=grace_days)

                if (expiration_date + grace) < now:
                    doc['expiration_date'] = expiration_date
                    doc['search_keyword'] = keyword
                    doc['content_preview'] = content[:500]  # プレビュー用
                    expired_emails.append(doc)
                    seen_ids.add(doc_id)

        except Exception as e:
            st.error(f"検索エラー ({keyword}): {e}")
            continue

    if progress_callback:
        progress_callback(1.0, "検索完了")

    expired_emails.sort(key=lambda x: x.get('expiration_date', datetime.max))
    return expired_emails


def delete_email(email: Dict[str, Any]) -> bool:
    """メールを削除"""
    try:
        db_client = DatabaseClient()
        drive_connector = GoogleDriveConnector()
        user_email = os.getenv('GMAIL_USER_EMAIL', 'ookubo.y@workspace-o.com')
        gmail_connector = GmailConnector(user_email)

        email_id = email['id']
        source_id = email.get('source_id')
        metadata = email.get('metadata', {})

        if isinstance(metadata, str):
            import json
            try:
                metadata = json.loads(metadata)
            except:
                metadata = {}

        success = True

        # 1. Gmail
        message_id = metadata.get('message_id')
        if message_id:
            try:
                gmail_connector.trash_message(message_id)
            except Exception as e:
                st.warning(f"Gmail削除エラー: {e}")
                success = False

        # 2. Google Drive
        if source_id:
            try:
                drive_connector.trash_file(source_id)
            except Exception as e:
                st.warning(f"Drive削除エラー: {e}")
                success = False

        # 3. Database
        if not db_client.delete_document(email_id):
            st.error("データベース削除失敗")
            return False

        return success

    except Exception as e:
        st.error(f"削除エラー: {e}")
        return False


# メイン画面
st.title("📧 期限切れメール管理")
st.markdown("---")

# サイドバー設定
with st.sidebar:
    st.header("⚙️ 設定")
    grace_days = st.number_input(
        "猶予日数",
        min_value=0,
        max_value=30,
        value=0,
        help="期限から何日後まで削除対象外にするか"
    )

    st.markdown("---")
    st.markdown("### 検索キーワード")
    st.markdown("\n".join([f"- {kw}" for kw in EXPIRATION_KEYWORDS]))

# 検索ボタン
if st.button("🔍 期限切れメールを検索", type="primary", use_container_width=True):
    with st.spinner("検索中..."):
        progress_bar = st.progress(0)
        status_text = st.empty()

        def update_progress(value, text):
            progress_bar.progress(value)
            status_text.text(text)

        expired_emails = find_expired_emails(grace_days, update_progress)
        st.session_state['expired_emails'] = expired_emails

        progress_bar.empty()
        status_text.empty()

        if expired_emails:
            st.success(f"✅ {len(expired_emails)}件の期限切れメールを発見しました")
        else:
            st.info("期限切れメールはありません")

# 検索結果の表示
if 'expired_emails' in st.session_state:
    expired_emails = st.session_state['expired_emails']

    if expired_emails:
        st.markdown("---")
        st.subheader(f"📋 期限切れメール一覧 ({len(expired_emails)}件)")

        # 全選択/全解除
        col1, col2 = st.columns([1, 5])
        with col1:
            select_all = st.checkbox("全選択", value=False)

        # 選択されたメールID
        if 'selected_emails' not in st.session_state:
            st.session_state['selected_emails'] = set()

        if select_all:
            st.session_state['selected_emails'] = {email['id'] for email in expired_emails}
        elif not select_all and len(st.session_state['selected_emails']) == len(expired_emails):
            st.session_state['selected_emails'] = set()

        # メール一覧
        for i, email in enumerate(expired_emails):
            email_id = email['id']
            title = email.get('file_name', '') or email.get('title', '(タイトルなし)')
            expiration = email.get('expiration_date')
            keyword = email.get('search_keyword', '')
            content_preview = email.get('content_preview', '')

            exp_str = expiration.strftime('%Y年%m月%d日') if expiration else '不明'

            with st.expander(f"{'✅' if email_id in st.session_state['selected_emails'] else '⬜'} {title[:80]}", expanded=False):
                col1, col2 = st.columns([1, 5])

                with col1:
                    selected = st.checkbox(
                        "削除対象",
                        key=f"checkbox_{email_id}",
                        value=email_id in st.session_state['selected_emails']
                    )
                    if selected:
                        st.session_state['selected_emails'].add(email_id)
                    else:
                        st.session_state['selected_emails'].discard(email_id)

                with col2:
                    st.markdown(f"**期限日:** {exp_str}")
                    st.markdown(f"**検索キーワード:** {keyword}")

                    # 日付を含む行を抽出して表示
                    lines_with_dates = [
                        line for line in content_preview.split('\n')
                        if re.search(r'\d{4}年|\d{1,2}月\d{1,2}日|\d{1,2}/\d{1,2}', line)
                    ]
                    if lines_with_dates:
                        st.markdown("**本文抜粋:**")
                        for line in lines_with_dates[:3]:
                            st.text(line[:150])

        # 削除ボタン
        st.markdown("---")
        selected_count = len(st.session_state['selected_emails'])

        if selected_count > 0:
            col1, col2, col3 = st.columns([2, 2, 2])

            with col2:
                if st.button(
                    f"🗑️ 選択した {selected_count} 件を削除",
                    type="primary",
                    use_container_width=True
                ):
                    st.session_state['confirm_delete'] = True

            # 削除確認
            if st.session_state.get('confirm_delete', False):
                st.warning(f"⚠️ 本当に {selected_count} 件のメールを削除しますか？")
                col1, col2, col3 = st.columns([2, 2, 2])

                with col1:
                    if st.button("✅ はい、削除します", type="primary", use_container_width=True):
                        progress_bar = st.progress(0)
                        success_count = 0
                        fail_count = 0

                        emails_to_delete = [
                            email for email in expired_emails
                            if email['id'] in st.session_state['selected_emails']
                        ]

                        for i, email in enumerate(emails_to_delete):
                            progress_bar.progress((i + 1) / len(emails_to_delete))
                            if delete_email(email):
                                success_count += 1
                            else:
                                fail_count += 1

                        progress_bar.empty()

                        if fail_count == 0:
                            st.success(f"✅ {success_count}件のメールを削除しました")
                        else:
                            st.warning(f"⚠️ 完了: 成功={success_count}, 失敗={fail_count}")

                        # リセット
                        st.session_state['confirm_delete'] = False
                        st.session_state['selected_emails'] = set()
                        if st.button("🔄 再検索", use_container_width=True):
                            st.rerun()

                with col2:
                    if st.button("❌ キャンセル", use_container_width=True):
                        st.session_state['confirm_delete'] = False
                        st.rerun()
        else:
            st.info("削除するメールを選択してください")
```

### frontend\packages.txt

```txt
# Streamlit Cloud用 packages.txt
# システムレベルの依存パッケージ

# PDF処理用
poppler-utils

# Tesseract OCR
tesseract-ocr
tesseract-ocr-jpn
libtesseract-dev

# 画像処理用
libpng-dev
libjpeg-dev
```

### frontend\requirements.txt

```txt
# Streamlit Cloud用 requirements.txt (ui/review_ui.py専用)
# Playwrightを除外してデプロイ時間を短縮

# Google API
google-api-python-client==2.108.0
google-auth==2.25.2
google-auth-oauthlib==1.2.0
google-auth-httplib2==0.2.0
google-generativeai==0.3.0

# AI APIs
anthropic==0.39.0
openai==1.54.0

# Database
supabase==2.10.0
pgvector==0.2.4

# Document Processing
pdfplumber==0.10.3
python-docx==1.1.0
python-pptx==0.6.21
openpyxl==3.1.2
Pillow
pytesseract==0.3.10
pypdf==3.17.4
pdf2image==1.17.0

# Data Processing
numpy>=1.26.2,<2.0.0
pyarrow==22.0.0
pandas>=2.1.4,<3.0.0

# Utilities
python-dotenv==1.0.0
loguru==0.7.2
pydantic
pydantic-settings
tenacity>=8.2.0
jsonschema>=4.20.0
pyyaml>=6.0
json_repair
beautifulsoup4==4.12.3
lxml>=5.1.0

# Streamlit
streamlit>=1.29.0,<2.0.0
streamlit-pdf-viewer
```

### frontend\schemas\generic.json

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "title": "汎用ドキュメントスキーマ",
  "description": "全てのドキュメントタイプに対応する汎用的な構造定義",
  "properties": {
    "basic_info": {
      "type": "object",
      "title": "基本情報",
      "description": "学校名、学年、発行日などの基本メタデータ",
      "properties": {
        "school_name": {
          "type": ["string", "null"],
          "title": "学校名",
          "description": "学校の正式名称"
        },
        "grade": {
          "type": ["string", "null"],
          "title": "学年",
          "description": "対象学年（例: 5年生）"
        },
        "issue_date": {
          "type": ["string", "null"],
          "format": "date",
          "title": "発行日",
          "description": "文書の発行日（YYYY-MM-DD形式）"
        },
        "period": {
          "type": ["string", "null"],
          "title": "対象期間",
          "description": "文書が対象とする期間（例: 2024年11月18日-21日）"
        },
        "document_title": {
          "type": ["string", "null"],
          "title": "文書タイトル",
          "description": "文書の表題・タイトル"
        },
        "document_number": {
          "type": ["string", "null"],
          "title": "文書番号",
          "description": "学年通信などの号数（例: 第12号）"
        }
      }
    },
    "text_blocks": {
      "type": ["array", "null"],
      "title": "文章セクション（記事ブロック）",
      "description": "表以外の場所にある、まとまった文章セクション（朝会の話、振り返り、道徳の内容など）",
      "items": {
        "type": "object",
        "properties": {
          "title": {
            "type": "string",
            "title": "見出し",
            "description": "セクションのタイトル（例: 朝会「マナーとルールについて」、今日のふりかえり）"
          },
          "content": {
            "type": "string",
            "title": "本文",
            "description": "セクションの本文（長文可、原文そのまま）"
          }
        },
        "required": ["title", "content"]
      }
    },
    "weekly_schedule": {
      "type": ["array", "null"],
      "title": "週間予定・時間割",
      "description": "曜日・時限・クラスで構成される時間割表データ（既存の表エディタロジックをそのまま使用可能）",
      "items": {
        "type": "object",
        "properties": {
          "date": {
            "type": "string",
            "title": "日付",
            "description": "MM-DD形式またはYYYY-MM-DD形式"
          },
          "day": {
            "type": "string",
            "title": "曜日",
            "description": "月、火、水など"
          },
          "day_of_week": {
            "type": "string",
            "title": "曜日（フル）",
            "description": "月曜日、火曜日など"
          },
          "events": {
            "type": "array",
            "title": "行事・イベント",
            "description": "その日の特別な行事やイベント",
            "items": {
              "type": "string"
            }
          },
          "class_schedules": {
            "type": "array",
            "title": "クラス別時間割",
            "description": "各クラスの授業スケジュール（5A、5Bなどクラスごと）",
            "items": {
              "type": "object",
              "properties": {
                "class": {
                  "type": "string",
                  "title": "クラス名",
                  "description": "例: 5A, 5B"
                },
                "subjects": {
                  "type": "array",
                  "title": "科目リスト",
                  "description": "時限順の科目リスト（例: [\"1限:国語\", \"2限:算数\"]）",
                  "items": {
                    "type": "string"
                  }
                },
                "periods": {
                  "type": "array",
                  "title": "時限別科目",
                  "description": "時限ごとの詳細情報（periodとsubjectのペア）",
                  "items": {
                    "type": "object",
                    "properties": {
                      "period": {
                        "type": ["integer", "string"],
                        "title": "時限",
                        "description": "1, 2, 3 または \"朝\""
                      },
                      "subject": {
                        "type": "string",
                        "title": "科目",
                        "description": "国語、算数など"
                      },
                      "time": {
                        "type": "string",
                        "title": "時間",
                        "description": "開始-終了時刻（例: 8:45-9:30）"
                      }
                    },
                    "required": ["period", "subject"]
                  }
                }
              },
              "required": ["class"]
            }
          },
          "note": {
            "type": "string",
            "title": "備考・連絡事項",
            "description": "その日の持ち物や連絡事項"
          }
        },
        "required": ["date"]
      }
    },
    "structured_tables": {
      "type": ["array", "null"],
      "title": "その他の構造化表データ",
      "description": "weekly_schedule以外の汎用的な表データ（持ち物リスト、成績表、イベント一覧など）",
      "items": {
        "type": "object",
        "properties": {
          "table_title": {
            "type": "string",
            "title": "表のタイトル",
            "description": "表の見出し・タイトル"
          },
          "table_type": {
            "type": "string",
            "title": "表の種類",
            "description": "requirements（持ち物）、events（行事）、scores（成績）など"
          },
          "headers": {
            "type": "array",
            "title": "列ヘッダー",
            "description": "表の列名リスト",
            "items": {
              "type": "string"
            }
          },
          "rows": {
            "type": "array",
            "title": "行データ",
            "description": "表の各行のデータ（オブジェクトまたは配列）",
            "items": {
              "type": ["object", "array"]
            }
          }
        },
        "required": ["rows"]
      }
    },
    "special_events": {
      "type": ["array", "null"],
      "title": "特別イベント",
      "description": "通常授業以外の特別な予定",
      "items": {
        "type": "string"
      }
    }
  },
  "required": []
}
```

### frontend\streamlit_app.py

```py
"""
Streamlit UI for Document Processing
Cloud Run APIを呼び出してドキュメント処理を実行
"""
import streamlit as st
import requests
import time

# Backend API URL
BACKEND_URL = "https://doc-processor-983922127476.asia-northeast1.run.app"

st.set_page_config(
    page_title="ドキュメント処理システム",
    page_icon="📄",
    layout="wide"
)

st.title("📄 ドキュメント処理システム")
st.markdown("---")

# Initialize session state
if 'processing' not in st.session_state:
    st.session_state.processing = False
if 'last_result' not in st.session_state:
    st.session_state.last_result = None

# Sidebar settings
with st.sidebar:
    st.header("⚙️ 処理設定")

    workspace = st.text_input(
        "ワークスペース",
        value="all",
        help="'all' で全ワークスペース、または特定のワークスペース名を入力"
    )

    limit = st.number_input(
        "処理件数上限",
        min_value=1,
        max_value=1000,
        value=100,
        help="一度に処理する最大件数"
    )

    preserve_workspace = st.checkbox(
        "ワークスペースを保持",
        value=True,
        help="ドキュメントのworkspaceを保持するか"
    )

    st.markdown("---")

    if st.button("🔄 統計情報を更新", use_container_width=True):
        st.rerun()

# Main content
col1, col2 = st.columns([2, 1])

with col1:
    st.header("📊 処理キューの状態")

    try:
        response = requests.get(
            f"{BACKEND_URL}/api/process/stats",
            params={"workspace": workspace},
            timeout=10
        )

        if response.status_code == 200:
            data = response.json()
            if data.get('success'):
                stats = data.get('stats', {})

                # Metrics in columns
                metric_cols = st.columns(5)

                with metric_cols[0]:
                    st.metric("⏳ 待機中", stats.get('pending', 0))
                with metric_cols[1]:
                    st.metric("🔄 処理中", stats.get('processing', 0))
                with metric_cols[2]:
                    st.metric("✅ 完了", stats.get('completed', 0))
                with metric_cols[3]:
                    st.metric("❌ 失敗", stats.get('failed', 0))
                with metric_cols[4]:
                    st.metric("📝 未処理", stats.get('null', 0))

                st.markdown("---")

                # Summary
                col_summary1, col_summary2 = st.columns(2)
                with col_summary1:
                    st.metric("📦 合計", stats.get('total', 0))
                with col_summary2:
                    success_rate = stats.get('success_rate', 0)
                    st.metric("✨ 成功率", f"{success_rate:.1f}%")

                pending_count = stats.get('pending', 0)
            else:
                st.error(f"エラー: {data.get('error', '不明なエラー')}")
                pending_count = 0
        else:
            st.error(f"❌ API エラー: {response.status_code}")
            pending_count = 0

    except Exception as e:
        st.error(f"❌ 統計取得エラー: {str(e)}")
        pending_count = 0

with col2:
    st.header("🚀 処理実行")

    if pending_count == 0:
        st.info("処理待ちのドキュメントはありません")
        process_button_disabled = True
    else:
        st.success(f"{pending_count} 件のドキュメントが処理待ちです")
        process_button_disabled = False

    if st.button(
        "▶️ 処理を開始",
        type="primary",
        use_container_width=True,
        disabled=process_button_disabled or st.session_state.processing
    ):
        st.session_state.processing = True
        st.rerun()

# Processing section
if st.session_state.processing:
    st.markdown("---")
    st.header("🔄 処理中...")

    with st.spinner("Cloud Runで処理を実行中..."):
        try:
            response = requests.post(
                f"{BACKEND_URL}/api/process/start",
                json={
                    "workspace": workspace,
                    "limit": limit,
                    "preserve_workspace": preserve_workspace
                },
                timeout=3600  # 1時間のタイムアウト
            )

            if response.status_code == 200:
                data = response.json()
                if data.get('success'):
                    st.session_state.last_result = data
                    st.success("✅ 処理が完了しました！")

                    # Show results
                    col_res1, col_res2, col_res3 = st.columns(3)
                    with col_res1:
                        st.metric("処理数", data.get('processed', 0))
                    with col_res2:
                        st.metric("成功", data.get('success_count', 0))
                    with col_res3:
                        st.metric("失敗", data.get('failed_count', 0))
                else:
                    st.error(f"❌ エラー: {data.get('error', '不明なエラー')}")
            else:
                st.error(f"❌ API エラー: {response.status_code}")

        except requests.exceptions.Timeout:
            st.error("❌ タイムアウトエラー: 処理に時間がかかりすぎています")
        except Exception as e:
            st.error(f"❌ エラー: {str(e)}")

        finally:
            st.session_state.processing = False

        if st.button("🔄 ページを更新"):
            st.rerun()

# Instructions
with st.sidebar:
    st.markdown("---")
    st.markdown("### ℹ️ 使い方")
    st.markdown("""
    1. **ワークスペースを選択**
       - 'all' で全ワークスペース
       - または特定のワークスペース名

    2. **処理件数を設定**
       - 一度に処理する最大件数

    3. **処理を開始**
       - ▶️ ボタンをクリック
       - Cloud RunでOCR処理を実行

    4. **結果を確認**
       - 処理完了後に結果を表示
    """)

    st.markdown("---")
    st.info(f"バックエンド: {BACKEND_URL}")

# Footer
st.markdown("---")
st.markdown("**注意:** 処理には時間がかかる場合があります。ブラウザを閉じずにお待ちください。")
```

### frontend\utils\__init__.py

```py
"""
UI Utilities
"""
```

### frontend\utils\schema_detector.py

```py
"""
Schema Detector
メタデータの構造から適切なスキーマを判定
"""
import json
from pathlib import Path
from typing import Dict, Any, Optional, List


class SchemaDetector:
    """スキーマ検出クラス"""

    def __init__(self):
        """スキーマファイルをロード"""
        self.schemas = {}
        # frontend/utils/schema_detector.py → frontend/schemas/
        self.schema_dir = Path(__file__).parent.parent / "schemas"
        self._load_schemas()

        # デフォルトの汎用スキーマを定義（ファイルがない場合のフォールバック）
        if "generic" not in self.schemas:
            self.schemas["generic"] = {
                "$schema": "http://json-schema.org/draft-07/schema#",
                "title": "Generic Document Schema",
                "type": "object",
                "properties": {
                    "title": {
                        "type": ["string", "null"],
                        "title": "タイトル",
                        "description": "ドキュメントのタイトル"
                    },
                    "summary": {
                        "type": ["string", "null"],
                        "title": "要約",
                        "description": "ドキュメントの要約"
                    },
                    "document_date": {
                        "type": ["string", "null"],
                        "format": "date",
                        "title": "文書日付",
                        "description": "文書の日付（YYYY-MM-DD）"
                    },
                    "tags": {
                        "type": "array",
                        "title": "タグ",
                        "description": "ドキュメントに関連するタグ",
                        "items": {
                            "type": "string"
                        }
                    },
                    "category": {
                        "type": ["string", "null"],
                        "title": "カテゴリー",
                        "description": "ドキュメントのカテゴリー"
                    },
                    "sender": {
                        "type": ["string", "null"],
                        "title": "送信者",
                        "description": "送信者名またはメールアドレス"
                    },
                    "subject": {
                        "type": ["string", "null"],
                        "title": "件名",
                        "description": "メールまたは文書の件名"
                    },
                    "content_type": {
                        "type": ["string", "null"],
                        "title": "内容タイプ",
                        "description": "文書の内容タイプ"
                    }
                },
                "required": []
            }

    def _load_schemas(self):
        """schemas/ディレクトリから全てのスキーマをロード"""
        if not self.schema_dir.exists():
            print(f"スキーマディレクトリが存在しません: {self.schema_dir}")
            return

        for schema_file in self.schema_dir.glob("*.json"):
            try:
                with open(schema_file, 'r', encoding='utf-8') as f:
                    schema_name = schema_file.stem
                    self.schemas[schema_name] = json.load(f)
            except Exception as e:
                print(f"スキーマ読み込みエラー ({schema_file.name}): {e}")

    def detect_schema(self, doc_type: str, metadata: Dict[str, Any]) -> Optional[str]:
        """
        doc_typeとmetadataからスキーマを判定

        現在は全てのドキュメントに汎用スキーマを適用

        Args:
            doc_type: ドキュメントタイプ（サンプル用、現在は使用しない）
            metadata: メタデータ辞書（サンプル用、現在は使用しない）

        Returns:
            スキーマ名（常に "generic"）
        """
        # 全てのドキュメントに汎用スキーマを適用
        if "generic" in self.schemas:
            return "generic"

        return None

    def _is_ikuya_school_structure(self, metadata: Dict[str, Any]) -> bool:
        """ikuya_school の構造かどうか判定"""
        # Noneの場合に空の辞書に置き換える
        metadata = metadata or {}

        # basic_info フィールドの存在をチェック
        if "basic_info" in metadata and isinstance(metadata["basic_info"], dict):
            return True

        # 新しい構造化フィールドの存在をチェック（優先）
        if "monthly_schedule_list" in metadata or "learning_content_list" in metadata:
            return True

        # weekly_schedule with class_schedules の存在をチェック（後方互換性）
        if "weekly_schedule" in metadata:
            weekly_schedule = metadata["weekly_schedule"]
            if isinstance(weekly_schedule, list) and len(weekly_schedule) > 0:
                first_day = weekly_schedule[0]
                # class_schedules フィールドがあれば ikuya_school
                if "class_schedules" in first_day:
                    return True

        # text_blocks と structured_tables の両方があれば ikuya_school の可能性が高い
        if "text_blocks" in metadata and "structured_tables" in metadata:
            return True

        return False

    def _is_timetable_structure(self, metadata: Dict[str, Any]) -> bool:
        """時間割の構造かどうか判定"""
        # Noneの場合に空の辞書に置き換える
        metadata = metadata or {}

        # daily_scheduleフィールドの存在をチェック
        if "daily_schedule" in metadata:
            daily_schedule = metadata["daily_schedule"]
            if isinstance(daily_schedule, list) and len(daily_schedule) > 0:
                # 最初の要素がdate, day_of_week, periodsを持つかチェック
                first_day = daily_schedule[0]
                return all(key in first_day for key in ["date", "day_of_week", "periods"])
        return False

    def _is_school_notice_structure(self, metadata: Dict[str, Any]) -> bool:
        """学校通知の構造かどうか判定"""
        # Noneの場合に空の辞書に置き換える
        metadata = metadata or {}

        # notice_typeまたはweekly_scheduleフィールドの存在をチェック
        if "notice_type" in metadata:
            return True
        if "weekly_schedule" in metadata:
            return True
        # school_nameとgradeの両方が存在する場合も学校通知として扱う
        if "school_name" in metadata and "grade" in metadata:
            return True
        return False

    def get_schema(self, schema_name: str) -> Optional[Dict[str, Any]]:
        """
        スキーマ名からスキーマ定義を取得

        Args:
            schema_name: スキーマ名

        Returns:
            スキーマ定義辞書、存在しない場合はNone
        """
        return self.schemas.get(schema_name)

    def get_editable_fields(self, schema_name: str) -> List[Dict[str, Any]]:
        """
        スキーマから編集可能なフィールドリストを取得

        Args:
            schema_name: スキーマ名

        Returns:
            フィールド定義のリスト
        """
        schema = self.get_schema(schema_name)
        if not schema or "properties" not in schema:
            return []

        fields = []
        for field_name, field_def in schema["properties"].items():
            fields.append({
                "name": field_name,
                "type": field_def.get("type", "string"),
                "title": field_def.get("title", field_name),
                "description": field_def.get("description", ""),
                "required": field_name in schema.get("required", []),
                "enum": field_def.get("enum"),
                "format": field_def.get("format"),
                "items": field_def.get("items")
            })

        return fields

    def validate_metadata(self, schema_name: str, metadata: Dict[str, Any]) -> tuple[bool, List[str]]:
        """
        メタデータがスキーマに適合しているか検証

        Args:
            schema_name: スキーマ名
            metadata: 検証するメタデータ

        Returns:
            (検証結果, エラーメッセージリスト)
        """
        # メタデータがNoneの場合、安全のために空の辞書に変換
        if metadata is None:
            metadata = {}

        schema = self.get_schema(schema_name)
        if not schema:
            return False, [f"スキーマ '{schema_name}' が見つかりません"]

        errors = []

        # 必須フィールドのチェック
        required_fields = schema.get("required", [])
        for field in required_fields:
            if field not in metadata:
                errors.append(f"必須フィールド '{field}' が欠けています")

        # 型のチェック（簡易版）
        properties = schema.get("properties", {})
        for field_name, value in metadata.items():
            if field_name in properties:
                expected_type = properties[field_name].get("type")
                if expected_type and not self._check_type(value, expected_type):
                    errors.append(f"フィールド '{field_name}' の型が不正です（期待: {expected_type}）")

        return len(errors) == 0, errors

    def _check_type(self, value: Any, expected_type: Any) -> bool:
        """値の型をチェック（配列形式の型定義にも対応）"""
        type_map = {
            "string": str,
            "integer": int,
            "number": (int, float),
            "boolean": bool,
            "array": list,
            "object": dict,
            "null": type(None)
        }

        # expected_type が配列の場合（例: ["string", "null"]）
        if isinstance(expected_type, list):
            # いずれかの型に一致すればOK
            return any(self._check_type(value, t) for t in expected_type)

        # expected_type が文字列の場合
        if not isinstance(expected_type, str):
            return True  # 不明な型は検証をスキップ

        expected_python_type = type_map.get(expected_type)
        if expected_python_type is None:
            return True  # 不明な型は検証をスキップ

        return isinstance(value, expected_python_type)
```

### frontend\utils\schemas\school_common.json

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "title": "学校関連文書 共通フォーム",
  "description": "学年通信、時間割、テスト通知など、全ての学校関連文書の共通構造。表データとテキストブロックを論理的に分離し、GPT/Claudeの構造化精度を高める。",
  "properties": {
    "issue_date": {
      "type": ["string", "null"],
      "format": "date",
      "title": "発行日",
      "description": "文書の発行日（YYYY-MM-DD形式）"
    },
    "relevant_date": {
      "type": ["string", "null"],
      "format": "date",
      "title": "関連日付",
      "description": "イベントやテストの実施日（YYYY-MM-DD形式）"
    },
    "grade": {
      "type": ["string", "null"],
      "title": "対象学年",
      "description": "文書の対象学年（例: 5年生）"
    },
    "event_type": {
      "type": ["string", "null"],
      "title": "文書の目的",
      "description": "timetable, test, notice, event など、文書の主な種類",
      "enum": ["timetable", "test", "notice", "event", "newsletter", "homework", "meeting", "other", null]
    },
    "things_to_bring": {
      "type": ["array", "null"],
      "title": "持ち物",
      "description": "イベントや授業で持参するもの",
      "items": {
        "type": "string"
      }
    },
    "schedule_details": {
      "type": ["string", "null"],
      "title": "スケジュール詳細",
      "description": "時間割やイベント日程の詳細（フリーテキスト）"
    },
    "key_topics": {
      "type": ["array", "null"],
      "title": "主要トピック",
      "description": "文書の主要な話題（健康、集金など）",
      "items": {
        "type": "string"
      }
    },
    "extracted_tables": {
      "type": ["array", "null"],
      "title": "抽出された表データ",
      "description": "pdfplumberまたはVisionで抽出された構造化データ（生データ）",
      "items": {
        "type": ["object", "null"]
      }
    },
    "weekly_schedule": {
      "type": ["array", "null"],
      "title": "週間予定・時間割",
      "description": "曜日・時限・クラスで構成される時間割表データ（表形式データの分離）",
      "items": {
        "type": "object",
        "properties": {
          "date": {
            "type": "string",
            "title": "日付",
            "description": "MM-DD形式またはYYYY-MM-DD形式"
          },
          "day": {
            "type": "string",
            "title": "曜日",
            "description": "月、火、水など"
          },
          "day_of_week": {
            "type": "string",
            "title": "曜日（フル）",
            "description": "月曜日、火曜日など"
          },
          "events": {
            "type": "array",
            "title": "行事・イベント",
            "items": {
              "type": "string"
            }
          },
          "class_schedules": {
            "type": "array",
            "title": "クラス別時間割",
            "description": "各クラスの授業スケジュール（5A、5Bなどクラスごと）",
            "items": {
              "type": "object",
              "properties": {
                "class": {
                  "type": "string",
                  "title": "クラス名"
                },
                "subjects": {
                  "type": "array",
                  "title": "科目リスト",
                  "items": {
                    "type": "string"
                  }
                }
              }
            }
          },
          "note": {
            "type": "string",
            "title": "備考・連絡事項"
          }
        }
      }
    },
    "monthly_schedule_blocks": {
      "type": ["array", "null"],
      "title": "月間予定表ブロック",
      "description": "学年便りの月間予定表から抽出した日付・イベント・備考のリスト",
      "items": {
        "type": "object",
        "properties": {
          "date": {
            "type": "string",
            "title": "日付",
            "description": "MM-DD形式またはYYYY-MM-DD形式"
          },
          "day_of_week": {
            "type": ["string", "null"],
            "title": "曜日",
            "description": "月、火、水など"
          },
          "event": {
            "type": ["string", "null"],
            "title": "イベント・行事",
            "description": "その日の予定や行事の内容"
          },
          "time": {
            "type": ["string", "null"],
            "title": "時刻",
            "description": "集合時刻、下校時刻など（例: 7:45 集合、11:30 下校）"
          },
          "notes": {
            "type": ["string", "null"],
            "title": "備考",
            "description": "持ち物、場所などの補足情報"
          }
        },
        "required": ["date", "event"]
      }
    },
    "learning_content_blocks": {
      "type": ["array", "null"],
      "title": "教科別学習予定ブロック",
      "description": "学年便りの教科別学習予定から抽出した教科・内容・持ち物のリスト",
      "items": {
        "type": "object",
        "properties": {
          "subject": {
            "type": "string",
            "title": "教科名",
            "description": "国語、算数、理科、社会など"
          },
          "teacher": {
            "type": ["string", "null"],
            "title": "担当教員",
            "description": "その教科の担当教員名"
          },
          "content": {
            "type": "string",
            "title": "学習内容",
            "description": "その教科で学習する内容の詳細"
          },
          "materials": {
            "type": ["string", "null"],
            "title": "持ち物・準備物",
            "description": "その教科で必要な持ち物や教材"
          }
        },
        "required": ["subject", "content"]
      }
    },
    "structured_tables": {
      "type": ["array", "null"],
      "title": "その他の構造化表データ",
      "description": "weekly_schedule以外の汎用的な表データ（持ち物リスト、成績表、イベント一覧など）",
      "items": {
        "type": "object",
        "properties": {
          "table_title": {
            "type": "string",
            "title": "表のタイトル"
          },
          "table_type": {
            "type": "string",
            "title": "表の種類",
            "description": "requirements（持ち物）、events（行事）、scores（成績）など"
          },
          "headers": {
            "type": "array",
            "title": "列ヘッダー",
            "items": {
              "type": "string"
            }
          },
          "rows": {
            "type": "array",
            "title": "行データ",
            "items": {
              "type": "object"
            }
          }
        }
      }
    },
    "text_blocks": {
      "type": ["array", "null"],
      "title": "文章セクション（トピックごと）",
      "description": "文書内のすべてのテキストコンテンツをトピックごとに分けたもの（朝会の話、振り返り、道徳、連絡事項、お知らせなど、すべてのテキストセクション）。テキストと表の論理的分離。",
      "items": {
        "type": "object",
        "properties": {
          "title": {
            "type": "string",
            "title": "見出し",
            "description": "セクションのタイトル"
          },
          "content": {
            "type": "string",
            "title": "本文",
            "description": "セクションの本文（長文可、原文そのまま）"
          }
        },
        "required": ["title", "content"]
      }
    }
  },
  "required": []
}
```

### frontend\utils\stage_h_reprocessor.py

```py
"""
統合Stage H再実行ユーティリティ

全ての編集箇所（全文編集、行単位編集、フォーム編集、表形式編集）から
Stage H（構造化）を再実行できる共通機能を提供します。
"""
import streamlit as st
from typing import Dict, Any, Optional
from loguru import logger


def reprocess_with_stageh(
    doc_id: str,
    attachment_text: str,
    file_name: str,
    metadata: Dict[str, Any],
    workspace: str,
    db_client,
    trigger_source: str = "manual_edit"
) -> bool:
    """
    Stage H（構造化）を再実行し、データベースに保存

    Args:
        doc_id: ドキュメントID
        attachment_text: 補正後の添付ファイルテキスト
        file_name: ファイル名
        metadata: 既存のメタデータ
        workspace: ワークスペース
        db_client: データベースクライアント
        trigger_source: 再実行のトリガー元（ログ用）

    Returns:
        成功した場合True、失敗した場合False
    """
    from G_unified_pipeline import UnifiedDocumentPipeline
    from pathlib import Path
    import tempfile
    import asyncio

    logger.info(f"[Stage H-K 再実行] 開始 - トリガー: {trigger_source}")
    logger.info(f"  ドキュメントID: {doc_id}")
    logger.info(f"  テキスト長: {len(attachment_text)} 文字")
    logger.info(f"  Workspace: {workspace}")

    try:
        # 統合パイプラインを初期化
        pipeline = UnifiedDocumentPipeline(db_client=db_client)

        # 補正されたテキストを一時ファイルとして保存
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as tmp:
            tmp.write(attachment_text)
            temp_file_path = tmp.name

        temp_path = Path(temp_file_path)

        try:
            # Stage H-K を再実行（テキストが既にあるので、Stage E-G はスキップ）
            with st.spinner(f"🔄 Stage H-K（構造化〜埋め込み）を再実行中... ({trigger_source})"):
                async def run_pipeline():
                    return await pipeline.process_document(
                        file_path=temp_path,
                        file_name=file_name,
                        doc_type=metadata.get('doc_type', 'other'),
                        workspace=workspace,
                        mime_type='text/plain',  # テキストファイルとして処理
                        source_id=doc_id,
                        existing_document_id=doc_id,  # 既存ドキュメントを更新
                        extra_metadata={
                            'manually_corrected': True,
                            'correction_trigger': trigger_source,
                            'correction_timestamp': __import__('datetime').datetime.now().isoformat(),
                            'corrected_text_length': len(attachment_text)
                        }
                    )

                # asyncioループで実行
                if asyncio.get_event_loop().is_running():
                    # 既にイベントループが動いている場合（Streamlit環境）
                    import nest_asyncio
                    nest_asyncio.apply()
                    result = asyncio.get_event_loop().run_until_complete(run_pipeline())
                else:
                    result = asyncio.run(run_pipeline())

        finally:
            # 一時ファイル削除
            if temp_path.exists():
                temp_path.unlink()

        if result.get('success'):
            st.success(f"✅ Stage H-K再実行が完了しました！（トリガー: {trigger_source}）")
            logger.info(f"[Stage H-K 再実行] 成功")
            logger.info(f"  チャンク数: {result.get('chunks_count', 0)}")

            # 補正前後の比較を表示
            with st.expander("📊 再実行結果の比較", expanded=True):
                col_before, col_after = st.columns(2)

                with col_before:
                    st.markdown("**補正前**")
                    st.metric("メタデータフィールド数", len(metadata.keys()))

                with col_after:
                    st.markdown("**補正後**")
                    st.metric("チャンク数", result.get('chunks_count', 0))

            return True
        else:
            st.error(f"❌ 再実行に失敗しました: {result.get('error')}")
            logger.error(f"[Stage H-K 再実行] 失敗: {result.get('error')}")
            return False

    except Exception as e:
        logger.error(f"[Stage H-K 再実行] エラー: {e}", exc_info=True)
        st.error(f"❌ Stage H-K再実行エラー: {e}")
        return False


def show_reprocess_button(
    doc_id: str,
    attachment_text: str,
    original_text: str,
    file_name: str,
    metadata: Dict[str, Any],
    workspace: str,
    db_client,
    trigger_source: str = "manual_edit",
    button_label: str = "🔄 変更を反映してStage H再実行"
) -> bool:
    """
    Stage H再実行ボタンを表示

    Args:
        doc_id: ドキュメントID
        attachment_text: 補正後の添付ファイルテキスト
        original_text: 元のテキスト
        file_name: ファイル名
        metadata: 既存のメタデータ
        workspace: ワークスペース
        db_client: データベースクライアント
        trigger_source: 再実行のトリガー元（ログ用）
        button_label: ボタンのラベル

    Returns:
        再実行が成功した場合True
    """
    # 変更検知
    text_changed = attachment_text != original_text

    if not text_changed:
        st.info("💡 テキストは変更されていませんが、スキーマ変更を反映するため再実行できます。")
        # スキーマ変更を反映するため、テキスト未変更でも処理を続行

    # 変更があることを表示
    char_diff = len(attachment_text) - len(original_text)
    if char_diff > 0:
        st.success(f"✅ {char_diff} 文字追加されました（合計: {len(attachment_text)} 文字）")
    elif char_diff < 0:
        st.warning(f"⚠️ {abs(char_diff)} 文字削除されました（合計: {len(attachment_text)} 文字）")

    # 再実行ボタン
    col_btn1, col_btn2 = st.columns([3, 1])

    with col_btn1:
        if st.button(
            button_label,
            type="primary",
            use_container_width=True,
            key=f"reprocess_{trigger_source}_{doc_id}"
        ):
            success = reprocess_with_stageh(
                doc_id=doc_id,
                attachment_text=attachment_text,
                file_name=file_name,
                metadata=metadata,
                workspace=workspace,
                db_client=db_client,
                trigger_source=trigger_source
            )

            if success:
                st.balloons()
                import time
                time.sleep(2)
                st.rerun()

            return success

    with col_btn2:
        if st.button(
            "↩️ リセット",
            use_container_width=True,
            key=f"reset_{trigger_source}_{doc_id}"
        ):
            st.rerun()

    return False
```

### frontend\utils\table_parser.py

```py
"""
Markdown表をパースして構造化データに変換するユーティリティ
"""

from typing import List, Dict, Any
import re


def parse_markdown_table(markdown_table: str) -> Dict[str, Any]:
    """
    Markdown形式の表を構造化データに変換

    Args:
        markdown_table: Markdown形式の表文字列

    Returns:
        構造化された表データ: {"headers": List[str], "rows": List[List[str]]}
    """
    if not markdown_table or not markdown_table.strip():
        return {"headers": [], "rows": []}

    lines = markdown_table.strip().split('\n')

    if len(lines) < 2:
        # ヘッダーと区切り行がない場合
        return {"headers": [], "rows": []}

    # ヘッダー行を抽出
    header_line = lines[0]
    headers = [cell.strip() for cell in header_line.split('|') if cell.strip()]

    # データ行を抽出（区切り行をスキップ）
    rows = []
    for line in lines[2:]:  # 最初の2行（ヘッダーと区切り）をスキップ
        if line.strip():
            # パイプで分割し、空でないセルのみを保持
            cells = [cell.strip() for cell in line.split('|')]
            # 最初と最後の空セルを除去（Markdown形式の両端のパイプ）
            if cells and cells[0] == '':
                cells = cells[1:]
            if cells and cells[-1] == '':
                cells = cells[:-1]
            if cells:
                rows.append(cells)

    return {
        "headers": headers,
        "rows": rows
    }


def parse_extracted_tables(extracted_tables: Any) -> List[Dict[str, Any]]:
    """
    `extracted_tables`フィールドから構造化データを抽出

    Args:
        extracted_tables: データベースから取得した`extracted_tables`
                         - 新形式: 辞書のリスト [{"table_id": ..., "headers": [...], "rows": [...]}, ...]
                         - 旧形式: Markdown文字列のリストのリスト

    Returns:
        構造化された表データのリスト
    """
    if not extracted_tables:
        return []

    if not isinstance(extracted_tables, list):
        return []

    # 新形式: 既に構造化された辞書のリストの場合
    if len(extracted_tables) > 0 and isinstance(extracted_tables[0], dict):
        # 'headers'キーと'rows'キーが存在するか確認
        if 'headers' in extracted_tables[0] and 'rows' in extracted_tables[0]:
            # そのまま返す（既に正しい形式）
            return extracted_tables

    # 旧形式: extracted_tablesがリストのリストの場合（ページごとの表）
    all_tables = []
    for page_idx, page_tables in enumerate(extracted_tables, 1):
        if isinstance(page_tables, list):
            for table_idx, table_data in enumerate(page_tables, 1):
                if isinstance(table_data, str):
                    # Markdown形式の文字列をパース
                    parsed_table = parse_markdown_table(table_data)
                    if parsed_table["headers"] or parsed_table["rows"]:
                        all_tables.append({
                            "page": page_idx,
                            "table_number": table_idx,
                            "headers": parsed_table["headers"],
                            "rows": parsed_table["rows"]
                        })

    return all_tables
```

### frontend\utils\text_structurer.py

```py
"""
テキスト構造化ユーティリティ

テキストを行単位で解析し、意味のあるブロックに分類します。
挨拶、本文、署名、日付など、全ての行を構造化します。
"""
import re
from typing import List, Dict, Any
from loguru import logger


class TextStructurer:
    """テキストを構造化ブロックに分類するクラス"""

    # 挨拶パターン
    GREETING_PATTERNS = [
        r'^(お世話になっております|お疲れ様です|おはようございます|こんにちは|こんばんは)',
        r'^(いつもお世話になっております|平素より大変お世話になっております)',
        r'^(お忙しいところ|突然のご連絡|ご連絡ありがとうございます)',
        r'^(拝啓|敬具|草々|謹啓|謹白)',
    ]

    # 署名パターン
    SIGNATURE_PATTERNS = [
        r'^(よろしくお願い|何卒よろしく|引き続きよろしく)',
        r'^(\S+\s+\S+)$',  # 名前っぽい（2語）
        r'^(株式会社|合同会社|\S+部|\S+課)',
        r'^(Tel:|TEL:|電話:|Email:|E-mail:|メール:)',
        r'^(〒\d{3}-\d{4})',  # 郵便番号
    ]

    # 日付パターン
    DATE_PATTERNS = [
        r'\d{4}年\d{1,2}月\d{1,2}日',
        r'\d{4}/\d{1,2}/\d{1,2}',
        r'\d{4}-\d{1,2}-\d{1,2}',
        r'\d{1,2}月\d{1,2}日',
    ]

    # タイトル・見出しパターン
    TITLE_PATTERNS = [
        r'^【[^】]+】',  # 【タイトル】
        r'^■\s*.+',     # ■ タイトル
        r'^◆\s*.+',     # ◆ タイトル
        r'^##\s*.+',    # ## タイトル（Markdown）
        r'^#\s*.+',     # # タイトル（Markdown）
    ]

    # リスト項目パターン
    LIST_PATTERNS = [
        r'^[-・*]\s+',   # - 項目、・項目、* 項目
        r'^\d+\.\s+',    # 1. 項目
        r'^[①-⑳]\s*',   # ① 項目
    ]

    # 空行パターン
    EMPTY_LINE_PATTERN = r'^\s*$'

    @classmethod
    def structure_text(cls, text: str) -> List[Dict[str, Any]]:
        """
        テキストを構造化ブロックに分類

        Args:
            text: 入力テキスト

        Returns:
            構造化されたブロックのリスト
            [
                {"type": "greeting", "content": "お世話になっております", "line_number": 1},
                {"type": "body", "content": "本文の内容", "line_number": 2},
                ...
            ]
        """
        if not text:
            return []

        lines = text.split('\n')
        structured_blocks = []

        for line_num, line in enumerate(lines, start=1):
            block_type = cls._classify_line(line)

            structured_blocks.append({
                "type": block_type,
                "content": line,
                "line_number": line_num,
                "length": len(line)
            })

        logger.info(f"テキスト構造化完了: {len(structured_blocks)} 行を {len(set(b['type'] for b in structured_blocks))} 種類のブロックに分類")
        return structured_blocks

    @classmethod
    def _classify_line(cls, line: str) -> str:
        """
        1行を分類

        Args:
            line: 入力行

        Returns:
            ブロックタイプ（greeting, signature, date, title, list, body, empty）
        """
        # 空行チェック
        if re.match(cls.EMPTY_LINE_PATTERN, line):
            return "empty"

        # 挨拶チェック
        for pattern in cls.GREETING_PATTERNS:
            if re.search(pattern, line):
                return "greeting"

        # タイトル・見出しチェック
        for pattern in cls.TITLE_PATTERNS:
            if re.match(pattern, line):
                return "title"

        # リスト項目チェック
        for pattern in cls.LIST_PATTERNS:
            if re.match(pattern, line):
                return "list_item"

        # 日付チェック
        for pattern in cls.DATE_PATTERNS:
            if re.search(pattern, line):
                return "date"

        # 署名チェック
        for pattern in cls.SIGNATURE_PATTERNS:
            if re.search(pattern, line):
                return "signature"

        # その他は本文
        return "body"

    @classmethod
    def group_by_type(cls, structured_blocks: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """
        ブロックをタイプごとにグループ化

        Args:
            structured_blocks: 構造化されたブロックのリスト

        Returns:
            タイプごとにグループ化された辞書
        """
        grouped = {}
        for block in structured_blocks:
            block_type = block["type"]
            if block_type not in grouped:
                grouped[block_type] = []
            grouped[block_type].append(block)

        return grouped

    @classmethod
    def get_statistics(cls, structured_blocks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        構造化されたブロックの統計情報を取得

        Args:
            structured_blocks: 構造化されたブロックのリスト

        Returns:
            統計情報の辞書
        """
        total_lines = len(structured_blocks)
        type_counts = {}

        for block in structured_blocks:
            block_type = block["type"]
            type_counts[block_type] = type_counts.get(block_type, 0) + 1

        return {
            "total_lines": total_lines,
            "type_counts": type_counts,
            "unique_types": len(type_counts)
        }

    @classmethod
    def format_as_table(cls, structured_blocks: List[Dict[str, Any]]) -> str:
        """
        構造化されたブロックを表形式の文字列に変換

        Args:
            structured_blocks: 構造化されたブロックのリスト

        Returns:
            表形式の文字列
        """
        if not structured_blocks:
            return "（空のテキスト）"

        lines = []
        lines.append("行番号 | タイプ | 内容")
        lines.append("------|--------|------")

        for block in structured_blocks:
            line_num = block["line_number"]
            block_type = cls._translate_type(block["type"])
            content = block["content"][:50]  # 最初の50文字のみ
            lines.append(f"{line_num:04d} | {block_type} | {content}")

        return "\n".join(lines)

    @classmethod
    def _translate_type(cls, block_type: str) -> str:
        """
        ブロックタイプを日本語に翻訳

        Args:
            block_type: ブロックタイプ

        Returns:
            日本語のブロックタイプ名
        """
        translations = {
            "greeting": "挨拶",
            "signature": "署名",
            "date": "日付",
            "title": "タイトル",
            "list_item": "リスト項目",
            "body": "本文",
            "empty": "空行"
        }
        return translations.get(block_type, block_type)
```

### README.md

```md
# ドキュメント管理システム

Supabase + マルチAIモデル（Gemini, OpenAI）を使用した、統合ドキュメント処理・検索システムです。

## 概要

Google Drive/Gmail/Classroomから取得したドキュメント（PDF、画像、テキスト）を7つのステージ（E→K）で処理し、ベクトル検索可能な形式でSupabaseに保存します。

**主な機能:**
- **マルチソース対応**: Google Drive, Gmail, Classroom からの自動取り込み
- **統合処理パイプライン**: Stage E（前処理）→ K（ベクトル化）の7段階処理
- **ベクトル検索**: OpenAI Embeddings + Supabase pgvector による高精度検索
- **AI回答生成**: Gemini 2.5 Flash による自然な回答
- **柔軟な設定**: ドキュメントタイプ・ワークスペースごとにモデル・プロンプトを切り替え

---

## システム構成

```
┌─────────────────┐
│  データソース    │
│ Drive/Gmail/    │
│  Classroom      │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  services/data-ingestion    │  ← データ取り込み
│  (監視・取得)    │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ G_unified_      │  ← 統合処理パイプライン
│  pipeline       │     Stage E-K（7段階）
│  (処理・保存)    │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│   Supabase      │  ← データベース
│  (pgvector)     │     - Rawdata_FILE_AND_MAIL
│                 │     - search_index
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  services/doc-search    │  ← 検索・回答API
│  (Flask)        │
└─────────────────┘
```

---

## 前提条件

### 必要な環境

- Python 3.12+
- Supabase アカウント
- Google Cloud プロジェクト（Drive/Gmail/Classroom API有効化）
- OpenAI API キー
- Google AI Studio API キー

### 環境変数

`.env` ファイルに以下を設定：

```bash
# Supabase
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=***REDACTED***

# OpenAI
OPENAI_API_KEY=***REDACTED***

# Google AI (Gemini)
GOOGLE_API_KEY=***REDACTED***

# Google Drive/Gmail/Classroom
GOOGLE_CREDENTIALS_PATH=_runtime/credentials/google_credentials.json
```

---

## セットアップ手順

### 1. リポジトリの準備

```bash
cd /path/to/document_management_system
```

### 2. 仮想環境の作成とアクティベート

```bash
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate  # Windows
```

### 3. 依存パッケージのインストール

```bash
pip install -r requirements.txt
```

### 4. Supabase データベースのセットアップ

**重要:** データベーススキーマは Supabase で直接作成されています。新規環境の場合、[SQL_REFERENCE.md](SQL_REFERENCE.md) を参照してください。

既存環境からのマイグレーション、または追加設定が必要な場合、Supabase SQL Editor で以下を実行：

```bash
# 1. pgvector 拡張機能（必須）
database/migrations/enable_pgvector.sql

# 2. Stage 出力カラムの追加（必要に応じて）
migrations/add_stage_output_columns.sql

# 3. 検索関数
J_resources/sql/add_match_documents_function.sql

# 4. オプション: チラシ/家計簿機能
database/migrations/create_flyer_schema.sql  # チラシ機能
shared/kakeibo/schema.sql                         # 家計簿機能
```

詳細な手順と全SQLファイルの説明は [SQL_REFERENCE.md](SQL_REFERENCE.md) を参照。

**主要テーブル:**
- `Rawdata_FILE_AND_MAIL`: ドキュメントのメタデータとStage E-K処理結果
- `search_index`: 検索用チャンクとベクトル埋め込み（1536次元）
- `Rawdata_RECEIPT_*`, `Rawdata_FLYER_*`, `Rawdata_NETSUPER_*`: サブシステム用テーブル

### 5. Google認証情報の設定

1. Google Cloud Console でプロジェクトを作成
2. Drive API, Gmail API, Classroom API を有効化
3. サービスアカウントを作成し、JSONキーをダウンロード
4. `_runtime/credentials/google_credentials.json` に配置

```bash
mkdir -p _runtime/credentials
mv ~/Downloads/your-credentials.json _runtime/credentials/google_credentials.json
```

---

## 使い方

### ドキュメントの処理

#### 方法1: 自動監視（推奨）

Google Driveの特定フォルダを監視し、新規ファイルを自動処理：

```bash
python services/data-ingestion/monitoring/inbox_monitor.py
```

設定: `services/data-ingestion/monitoring/config.yaml`

#### 方法2: 手動処理

特定のドキュメントIDを指定して処理：

```bash
python process_specific_docs.py
```

`process_specific_docs.py` の `doc_ids` リストを編集：

```python
doc_ids = [
    'your-document-id-1',
    'your-document-id-2'
]
```

#### 方法3: キュー処理

`processing_status='pending'` のドキュメントを一括処理：

```bash
python process_queued_documents.py --limit 10
```

### 統合パイプライン（Stage E-K）

7つのステージで順次処理：

```
E: 前処理 → F: Vision解析 → G: テキスト整形 →
H: 構造化 → I: 統合・要約 → J: チャンク化 → K: ベクトル化
```

**各ステージの出力はDBに保存:**
- `stage_e1_text` ~ `stage_e5_text`: 前処理（5エンジン）
- `stage_f_text_ocr`, `stage_f_layout_ocr`, `stage_f_visual_elements`: Vision解析
- `stage_h_normalized`: 構造化入力テキスト
- `stage_i_structured`: 構造化データ（JSON）
- `stage_j_chunks_json`: チャンク（JSON）

詳細は [ARCHITECTURE.md](ARCHITECTURE.md) 参照。

### 検索・回答API

Flask APIサーバーを起動：

```bash
cd services/doc-search
python app.py
```

ブラウザで http://localhost:5001 にアクセス。

**APIエンドポイント:**
- `POST /api/search` - ベクトル検索
- `POST /api/answer` - AI回答生成
- `GET /api/health` - ヘルスチェック

---

## 設定ファイル

### config/models.yaml

各ステージで使用するAIモデルを定義：

```yaml
models:
  stage_f:
    default: "gemini-2.5-flash"
    flyer: "gemini-2.5-pro"       # チラシは視覚理解重視
    classroom: "gemini-2.5-flash" # Classroomはコスト重視
```

### config/pipeline_routing.yaml

workspace と doc_type に基づいてルーティング：

```yaml
routing:
  by_workspace:
    ikuya_classroom:
      schema: "classroom"
      stages:
        stage_f:
          prompt_key: "classroom"
          model_key: "classroom"
```

### config/prompts.yaml

全ステージのプロンプトを一元管理（15個のMDファイルを統合）：

```yaml
prompts:
  stage_f:
    classroom: |
      あなたはGoogle Classroom課題ドキュメントの...
    default: |
      あなたはドキュメントから視覚情報を...
```

---

## プロジェクト構成

```
document_management_system/
├── shared/common/                     # 共通モジュール
│   ├── database/                # Supabaseクライアント
│   ├── processors/              # PDF/Office処理
│   ├── connectors/              # Drive/Gmail/Classroom
│   └── processing/              # チャンク処理
│
├── services/data-ingestion/                  # データ取り込み
│   ├── gmail/                   # Gmail取り込み
│   ├── google_drive/            # Drive取り込み
│   ├── google_classroom/        # Classroom取り込み
│   └── monitoring/              # 監視スクリプト
│
├── shared/ai/                  # AI共通機能
│   ├── llm_client/              # LLMクライアント
│   └── embeddings/              # ベクトル埋め込み
│
├── shared/pipeline/          # 統合処理パイプライン
│   ├── stage_e_preprocessing.py   # Stage E: 前処理
│   ├── stage_f_visual.py          # Stage F: Vision解析
│   ├── stage_g_formatting.py      # Stage G: テキスト整形
│   ├── stage_h_structuring.py     # Stage H: 構造化
│   ├── stage_i_synthesis.py       # Stage I: 統合・要約
│   ├── stage_j_chunking.py        # Stage J: チャンク化
│   ├── stage_k_embedding.py       # Stage K: ベクトル化
│   ├── pipeline.py                # パイプライン本体
│   ├── config_loader.py           # 設定ローダー
│   └── config/                    # 設定ファイル
│       ├── models.yaml           # モデル定義
│       ├── pipeline_routing.yaml # ルーティング設定
│       └── prompts.yaml          # プロンプト（統合版）
│
├── services/doc-search/                  # Flask API
│   ├── app.py                   # メインアプリ
│   ├── templates/               # HTMLテンプレート
│   └── requirements.txt
│
├── migrations/                   # DBマイグレーション
│   └── add_stage_output_columns.sql
│
├── .env                         # 環境変数
├── README.md                    # このファイル
└── ARCHITECTURE.md              # 技術詳細
```

---

## トラブルシューティング

### プロンプトが見つからない

**症状:** `プロンプトが見つかりません: stage_f/classroom`

**原因:** prompts.yaml が読み込まれていない

**対処:**
```bash
# prompts.yaml の存在確認
ls shared/pipeline/config/prompts.yaml

# ConfigLoader のログを確認
# "✅ prompts.yaml を読み込みました" が表示されるはず
```

### ドキュメントが消失する

**症状:** 処理後にドキュメントが消える

**原因:** 古いバージョンの DELETE→INSERT 処理（修正済み）

**対処:** 最新版では UPDATE を使用しているため、この問題は発生しません

### Stage出力が空

**症状:** `stage_e1_text` などが NULL

**原因:** 古いバージョンのpipeline.py

**対処:** 最新版では全ステージ出力をDBに保存します（pipeline.py 378-388行目）

### Gemini API エラー

**症状:** `GOOGLE_API_KEY not found`

**対処:**
```bash
# .env ファイルを確認
cat .env | grep GOOGLE_API_KEY

# 環境変数が読み込まれているか確認
python -c "import os; print(os.getenv('GOOGLE_API_KEY'))"
```

---

## 技術スタック

- **Python**: 3.12+
- **データベース**: Supabase (PostgreSQL + pgvector)
- **AI/ML**:
  - Gemini 2.5 Flash/Pro (Vision解析・構造化・回答生成)
  - OpenAI text-embedding-3-small (1536次元ベクトル)
- **ベクトル検索**: pgvector (cosine類似度)
- **Web API**: Flask 3.0
- **ファイル処理**: PyPDF2, pdfplumber, python-docx, openpyxl
- **外部連携**: Google Drive API, Gmail API, Classroom API

---

## セキュリティ注意事項

- `.env` ファイルを `.gitignore` に追加
- `google_credentials.json` を公開リポジトリにコミットしない
- 本番環境では `debug=False` に設定
- Supabase の Service Role Key は慎重に管理

---

## サポート

詳細な技術情報は [ARCHITECTURE.md](ARCHITECTURE.md) を参照してください。

問題が発生した場合：
1. ログを確認（`logs/` ディレクトリ）
2. 環境変数の設定を確認
3. Supabase のテーブル構造を確認
4. 最新版にアップデート

```

### requirements.txt

```txt
a l t a i r = = 5 . 5 . 0 
 a n n o t a t e d - t y p e s = = 0 . 7 . 0 
 a n t h r o p i c = = 0 . 3 9 . 0 
 a n y i o = = 4 . 1 1 . 0 
 a t t r s = = 2 5 . 4 . 0 
 b e a u t i f u l s o u p 4 = = 4 . 1 4 . 3 
 b l i n k e r = = 1 . 9 . 0 
 c a c h e t o o l s = = 5 . 5 . 2 
 c e r t i f i = = 2 0 2 5 . 1 1 . 1 2 
 c f f i = = 2 . 0 . 0 
 c f g v = = 3 . 5 . 0 
 c h a r s e t - n o r m a l i z e r = = 3 . 4 . 4 
 c l i c k = = 8 . 3 . 1 
 c o h e r e = = 5 . 2 0 . 0 
 c o l o r a m a = = 0 . 4 . 6 
 c r y p t o g r a p h y = = 4 6 . 0 . 3 
 d e p r e c a t i o n = = 2 . 1 . 0 
 d i s t l i b = = 0 . 4 . 0 
 d i s t r o = = 1 . 9 . 0 
 e i n o p s = = 0 . 8 . 1 
 e t _ x m l f i l e = = 2 . 0 . 0 
 f a s t a v r o = = 1 . 1 2 . 1 
 f i l e l o c k = = 3 . 2 0 . 0 
 f i l e t y p e = = 1 . 2 . 0 
 F l a s k = = 3 . 1 . 0 
 F l a s k - C o r s = = 5 . 0 . 0 
 f s s p e c = = 2 0 2 5 . 1 2 . 0 
 g i t d b = = 4 . 0 . 1 2 
 G i t P y t h o n = = 3 . 1 . 4 5 
 g o o g l e - a i - g e n e r a t i v e l a n g u a g e = = 0 . 6 . 1 5 
 g o o g l e - a p i - c o r e = = 2 . 2 5 . 2 
 g o o g l e - a p i - p y t h o n - c l i e n t = = 2 . 1 0 8 . 0 
 g o o g l e - a u t h = = 2 . 2 5 . 2 
 g o o g l e - a u t h - h t t p l i b 2 = = 0 . 2 . 0 
 g o o g l e - a u t h - o a u t h l i b = = 1 . 2 . 0 
 g o o g l e - g e n e r a t i v e a i = = 0 . 8 . 5 
 g o o g l e a p i s - c o m m o n - p r o t o s = = 1 . 7 2 . 0 
 g o t r u e = = 2 . 1 2 . 4 
 g r e e n l e t = = 3 . 3 . 0 
 g r p c i o = = 1 . 7 6 . 0 
 g r p c i o - s t a t u s = = 1 . 6 2 . 3 
 g u n i c o r n = = 2 3 . 0 . 0 
 h 1 1 = = 0 . 1 6 . 0 
 h 2 = = 4 . 3 . 0 
 h p a c k = = 4 . 1 . 0 
 h t t p c o r e = = 1 . 0 . 9 
 h t t p l i b 2 = = 0 . 3 1 . 0 
 h t t p x = = 0 . 2 7 . 2 
 h t t p x - s s e = = 0 . 4 . 0 
 h u g g i n g f a c e - h u b = = 0 . 3 6 . 0 
 h y p e r f r a m e = = 6 . 1 . 0 
 i d e n t i f y = = 2 . 6 . 1 5 
 i d n a = = 3 . 1 1 
 i t s d a n g e r o u s = = 2 . 2 . 0 
 J i n j a 2 = = 3 . 1 . 6 
 j i t e r = = 0 . 1 2 . 0 
 j o b l i b = = 1 . 5 . 2 
 j s o n _ r e p a i r = = 0 . 5 4 . 2 
 j s o n s c h e m a = = 4 . 2 5 . 1 
 j s o n s c h e m a - s p e c i f i c a t i o n s = = 2 0 2 5 . 9 . 1 
 l o g u r u = = 0 . 7 . 2 
 l x m l = = 6 . 0 . 2 
 M a r k u p S a f e = = 3 . 0 . 3 
 m p m a t h = = 1 . 3 . 0 
 n a r w h a l s = = 2 . 1 2 . 0 
 n e t w o r k x = = 3 . 6 . 1 
 n o d e e n v = = 1 . 1 0 . 0 
 n u m p y = = 2 . 2 . 6 
 o a u t h l i b = = 3 . 3 . 1 
 o p e n a i = = 1 . 5 4 . 0 
 o p e n c v - p y t h o n = = 4 . 1 1 . 0 . 8 6 
 o p e n c v - p y t h o n - h e a d l e s s = = 4 . 1 1 . 0 . 8 6 
 o p e n p y x l = = 3 . 1 . 2 
 p a c k a g i n g = = 2 5 . 0 
 p a d d l e o c r = = 3 . 3 . 2 
 p a n d a s = = 2 . 3 . 3 
 p d f 2 i m a g e = = 1 . 1 7 . 0 
 p d f m i n e r . s i x = = 2 0 2 2 1 1 0 5 
 p d f p l u m b e r = = 0 . 1 0 . 3 
 p g v e c t o r = = 0 . 2 . 4 
 p i l l o w = = 1 0 . 4 . 0 
 p l a t f o r m d i r s = = 4 . 5 . 1 
 p l a y w r i g h t = = 1 . 5 7 . 0 
 p o s t g r e s t = = 0 . 1 8 . 0 
 p r e _ c o m m i t = = 4 . 5 . 1 
 p r o t o - p l u s = = 1 . 2 6 . 1 
 p r o t o b u f = = 5 . 2 9 . 5 
 p s u t i l = = 5 . 9 . 8 
 p y a r r o w = = 2 2 . 0 . 0 
 p y a s n 1 = = 0 . 6 . 1 
 p y a s n 1 _ m o d u l e s = = 0 . 4 . 2 
 p y c p a r s e r = = 2 . 2 3 
 p y d a n t i c = = 2 . 1 2 . 5 
 p y d a n t i c - s e t t i n g s = = 2 . 1 2 . 0 
 p y d a n t i c _ c o r e = = 2 . 4 1 . 5 
 p y d e c k = = 0 . 9 . 1 
 p y e e = = 1 3 . 0 . 0 
 P y J W T = = 2 . 1 0 . 1 
 p y p a r s i n g = = 3 . 2 . 5 
 p y p d f = = 3 . 1 7 . 4 
 p y p d f i u m 2 = = 4 . 3 0 . 0 
 p y t e s s e r a c t = = 0 . 3 . 1 0 
 p y t h o n - d a t e u t i l = = 2 . 9 . 0 . p o s t 0 
 p y t h o n - d o c x = = 1 . 1 . 0 
 p y t h o n - d o t e n v = = 1 . 0 . 0 
 p y t h o n - p p t x = = 0 . 6 . 2 1 
 p y t z = = 2 0 2 5 . 2 
 P y Y A M L = = 6 . 0 . 2 
 r e a l t i m e = = 2 . 2 4 . 0 
 r e f e r e n c i n g = = 0 . 3 7 . 0 
 r e g e x = = 2 0 2 5 . 1 1 . 3 
 r e q u e s t s = = 2 . 3 2 . 5 
 r e q u e s t s - o a u t h l i b = = 2 . 0 . 0 
 r p d s - p y = = 0 . 2 9 . 0 
 r s a = = 4 . 9 . 1 
 s a f e t e n s o r s = = 0 . 7 . 0 
 s c i k i t - l e a r n = = 1 . 8 . 0 
 s c i p y = = 1 . 1 6 . 3 
 s e n t e n c e - t r a n s f o r m e r s = = 5 . 1 . 2 
 s e t u p t o o l s = = 8 0 . 9 . 0 
 s i x = = 1 . 1 7 . 0 
 s m m a p = = 5 . 0 . 2 
 s n i f f i o = = 1 . 3 . 1 
 s o u p s i e v e = = 2 . 8 
 s t o r a g e 3 = = 0 . 9 . 0 
 s t r e a m l i t = = 1 . 5 0 . 0 
 s t r e a m l i t - p d f - v i e w e r = = 0 . 0 . 2 6 
 s u p a b a s e = = 2 . 1 0 . 0 
 s u p a f u n c = = 0 . 7 . 0 
 s u r y a - o c r = = 0 . 1 7 . 0 
 s y m p y = = 1 . 1 4 . 0 
 t e n a c i t y = = 9 . 1 . 2 
 t h r e a d p o o l c t l = = 3 . 6 . 0 
 t o k e n i z e r s = = 0 . 2 2 . 1 
 t o m l = = 0 . 1 0 . 2 
 t o r c h = = 2 . 9 . 1 
 t o r n a d o = = 6 . 5 . 4 
 t q d m = = 4 . 6 7 . 1 
 t r a n s f o r m e r s = = 4 . 5 7 . 3 
 t y p e s - r e q u e s t s = = 2 . 3 2 . 4 . 2 0 2 5 0 9 1 3 
 t y p i n g - i n s p e c t i o n = = 0 . 4 . 2 
 t y p i n g _ e x t e n s i o n s = = 4 . 1 5 . 0 
 t z d a t a = = 2 0 2 5 . 2 
 u r i t e m p l a t e = = 4 . 2 . 0 
 u r l l i b 3 = = 2 . 5 . 0 
 v i r t u a l e n v = = 2 0 . 3 5 . 4 
 w a t c h d o g = = 6 . 0 . 0 
 w e b s o c k e t s = = 1 5 . 0 . 1 
 W e r k z e u g = = 3 . 1 . 3 
 w i n 3 2 _ s e t c t i m e = = 1 . 2 . 0 
 x l s x w r i t e r = = 3 . 2 . 9 
 
```

### run_build.ps1

```ps1
Set-Location "C:\Users\ookub\document-management-system"

# Load .env manually
$envContent = Get-Content .env
foreach ($line in $envContent) {
    if ($line -match "^([^#][^=]*)=(.*)$") {
        $key = $matches[1].Trim()
        $value = $matches[2].Trim()
        Set-Item -Path "Env:$key" -Value $value
    }
}

Write-Host "SUPABASE_URL = $env:SUPABASE_URL"

# Build command
$subs = "_GOOGLE_AI_API_KEY=***REDACTED***"

Write-Host "Running gcloud builds submit..."
& gcloud builds submit --region=asia-northeast1 --config=cloudbuild.yaml --substitutions="$subs"

Write-Host "Build complete. Now deploying..."
& gcloud run deploy doc-processor --image asia-northeast1-docker.pkg.dev/consummate-yew-479020-u2/cloud-run-source-deploy/doc-processor:latest --region asia-northeast1 --allow-unauthenticated --service-account document-management-system@consummate-yew-479020-u2.iam.gserviceaccount.com --timeout 3600 --memory 16Gi --cpu 4

Write-Host "Done!"
```

### scripts\processing\process_daiei.py

```py
"""
ダイエーネットスーパー 商品データ取得メインスクリプト

使い方:
    # 全カテゴリーを処理（デフォルト）
    python process_daiei.py

    # 特定のカテゴリーのみ処理
    python process_daiei.py --category "野菜" --category "果物"

    # ヘッドレスモードをオフにして動作確認
    python process_daiei.py --no-headless
"""

import os
import sys
import json
import asyncio
import logging
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))
sys.path.insert(0, str(root_dir / "services" / "data-ingestion"))

# .envファイルを読み込む
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from daiei.product_ingestion import DaieiProductIngestionPipeline

# ロガー設定
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('daiei_output.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


async def process_all_categories(
    pipeline: DaieiProductIngestionPipeline,
    target_categories: List[str] = None,
    max_pages_per_category: int = 100
) -> Dict[str, Any]:
    """
    すべてのカテゴリーを処理

    Args:
        pipeline: 商品取得パイプライン
        target_categories: 処理対象のカテゴリー名リスト（Noneなら全カテゴリー）
        max_pages_per_category: カテゴリーあたりの最大ページ数

    Returns:
        処理結果のサマリー
    """
    logger.info("=" * 80)
    logger.info("ダイエーネットスーパー 商品データ取得開始")
    logger.info("=" * 80)

    start_time = datetime.now()

    # ログイン後に取得した店舗IDを使用してカテゴリーURLを構築
    store_id = pipeline.scraper.store_id
    if not store_id:
        logger.error("❌ 店舗IDが取得できていません")
        return {}

    # カテゴリー定義（classL, classS, ilc_code のパラメータで指定）
    # 実際のカテゴリーはトップページから動的に取得するのが理想だが、まずは主要カテゴリーを手動定義
    category_params = [
        {"name": "野菜・果物", "classL": "2", "classS": "1", "ilc_code": "1001"},
        {"name": "肉加工品", "classL": "2", "classS": "2", "ilc_code": "1002"},
        {"name": "魚", "classL": "2", "classS": "3", "ilc_code": "1003"},
        {"name": "惣菜・弁当", "classL": "2", "classS": "4", "ilc_code": "1004"},
        {"name": "パン・乳製品", "classL": "2", "classS": "5", "ilc_code": "1005"},
        {"name": "冷凍食品・アイス", "classL": "2", "classS": "6", "ilc_code": "1006"},
        {"name": "冷蔵食品", "classL": "2", "classS": "7", "ilc_code": "1007"},
        {"name": "調味料・即席食品", "classL": "2", "classS": "8", "ilc_code": "1008"},
        {"name": "米・乾物", "classL": "2", "classS": "9", "ilc_code": "1009"},
        {"name": "菓子", "classL": "2", "classS": "10", "ilc_code": "1010"},
        {"name": "飲料", "classL": "2", "classS": "11", "ilc_code": "1011"},
        {"name": "酒類", "classL": "2", "classS": "12", "ilc_code": "1012"},
        {"name": "医薬品", "classL": "2", "classS": "13", "ilc_code": "1013"},
        {"name": "健康・美容・日用品", "classL": "2", "classS": "14", "ilc_code": "1014"},
        {"name": "住まい・衣料", "classL": "2", "classS": "15", "ilc_code": "1015"},
        {"name": "ベビー・衣料", "classL": "2", "classS": "16", "ilc_code": "1016"},
    ]

    # URLを構築
    categories = []
    for params in category_params:
        url = f"https://netsuper.daiei.co.jp/{store_id}/item/item.php?classL={params['classL']}&classS={params['classS']}&ilc_code={params['ilc_code']}"
        categories.append({
            "name": params["name"],
            "url": url
        })

    # 対象カテゴリーをフィルタリング
    if target_categories:
        categories = [
            cat for cat in categories
            if cat["name"] in target_categories
        ]
        logger.info(f"処理対象カテゴリー: {', '.join(target_categories)}")
    else:
        logger.info(f"全{len(categories)}カテゴリーを処理")

    # 各カテゴリーを処理
    results = []
    total_products = 0
    total_new = 0
    total_updated = 0

    for i, category in enumerate(categories, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"[{i}/{len(categories)}] カテゴリー: {category['name']}")
        logger.info(f"{'='*80}")

        try:
            result = await pipeline.process_category_all_pages(
                category_url=category["url"],
                category_name=category["name"],
                max_pages=max_pages_per_category
            )

            results.append(result)
            total_products += result["total_products"]
            total_new += result["new_products"]
            total_updated += result["updated_products"]

        except Exception as e:
            logger.error(f"カテゴリー '{category['name']}' の処理でエラー: {e}", exc_info=True)
            continue

    # サマリー出力
    end_time = datetime.now()
    duration = end_time - start_time

    logger.info("\n" + "=" * 80)
    logger.info("処理完了サマリー")
    logger.info("=" * 80)
    logger.info(f"処理時間: {duration}")
    logger.info(f"カテゴリー数: {len(results)}/{len(categories)}")
    logger.info(f"総商品数: {total_products}件")
    logger.info(f"  新規: {total_new}件")
    logger.info(f"  更新: {total_updated}件")

    # カテゴリー別の詳細
    logger.info("\nカテゴリー別詳細:")
    for result in results:
        logger.info(
            f"  {result['category_name']}: "
            f"{result['total_products']}件 "
            f"(新規{result['new_products']}件、更新{result['updated_products']}件、"
            f"{result['pages_processed']}ページ)"
        )

    summary = {
        "start_time": start_time.isoformat(),
        "end_time": end_time.isoformat(),
        "duration_seconds": duration.total_seconds(),
        "categories_processed": len(results),
        "total_categories": len(categories),
        "total_products": total_products,
        "new_products": total_new,
        "updated_products": total_updated,
        "category_results": results
    }

    return summary


async def main():
    """メイン処理"""
    parser = argparse.ArgumentParser(
        description='ダイエーネットスーパー 商品データ取得'
    )
    parser.add_argument(
        '--category',
        action='append',
        help='処理対象のカテゴリー名（複数指定可）'
    )
    parser.add_argument(
        '--max-pages',
        type=int,
        default=100,
        help='カテゴリーあたりの最大ページ数（デフォルト: 100）'
    )
    parser.add_argument(
        '--no-headless',
        action='store_true',
        help='ブラウザを表示する（デバッグ用）'
    )

    args = parser.parse_args()

    # 環境変数チェック
    login_id = os.getenv("DAIEI_LOGIN_ID")
    password = ***REDACTED***"DAIEI_PASSWORD")

    if not login_id or not password:
        logger.error("❌ 環境変数 DAIEI_LOGIN_ID と DAIEI_PASSWORD を設定してください")
        sys.exit(1)

    # パイプライン初期化
    pipeline = DaieiProductIngestionPipeline(
        login_id=login_id,
        password=***REDACTED***
        headless=not args.no_headless
    )

    try:
        # スクレイパー起動
        success = await pipeline.start()
        if not success:
            logger.error("❌ スクレイパー起動失敗")
            sys.exit(1)

        # カテゴリー処理
        summary = await process_all_categories(
            pipeline=pipeline,
            target_categories=args.category,
            max_pages_per_category=args.max_pages
        )

        # サマリーをJSONファイルに保存
        output_file = f"daiei_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        logger.info(f"\n✅ 処理結果を {output_file} に保存しました")

    except KeyboardInterrupt:
        logger.info("\n⚠️ ユーザーによって中断されました")

    except Exception as e:
        logger.error(f"❌ エラーが発生しました: {e}", exc_info=True)
        sys.exit(1)

    finally:
        # クリーンアップ
        await pipeline.close()
        logger.info("✅ スクレイパー終了")


if __name__ == "__main__":
    asyncio.run(main())
```

### scripts\processing\process_queued_documents.py

```py
"""
シンプル版ドキュメント処理スクリプト

キューテーブルを使わず、Rawdata_FILE_AND_MAIL.processing_status で直接管理

処理内容:
1. processing_status='pending' のドキュメントを取得
2. 統合パイプライン（Stage E-K）で処理
3. 成功: processing_status='completed'
4. 失敗: processing_status='failed'

使い方:
    # 全ワークスペースを処理
    python process_queued_documents_v3.py --limit=100

    # 特定のワークスペースのみ
    python process_queued_documents_v3.py --workspace=ema_classroom --limit=20

    # pendingにリセット（再処理用）
    python process_queued_documents_v3.py --reset-to-pending --workspace=all
"""

import asyncio
from typing import List, Dict, Any, Optional
import sys
from datetime import datetime
from pathlib import Path
import mimetypes

from loguru import logger
from shared.common.database.client import DatabaseClient
from shared.common.connectors.google_drive import GoogleDriveConnector
from shared.pipeline import UnifiedDocumentPipeline


class DocumentProcessor:
    """ドキュメント処理（シンプル版）"""

    VIDEO_EXTENSIONS = ['.mp4', '.mov', '.avi', '.mkv', '.webm', '.flv', '.wmv', '.m4v', '.mpeg', '.mpg']

    def __init__(self):
        self.db = DatabaseClient()
        self.pipeline = UnifiedDocumentPipeline(db_client=self.db)
        self.drive = GoogleDriveConnector()
        self.temp_dir = Path("./temp")
        self.temp_dir.mkdir(parents=True, exist_ok=True)

    def get_pending_documents(self, workspace: str = 'all', limit: int = 100) -> List[Dict[str, Any]]:
        """
        processing_status='pending' のドキュメントを取得

        Args:
            workspace: 対象ワークスペース ('all' で全ワークスペース)
            limit: 取得する最大件数

        Returns:
            ドキュメントリスト
        """
        query = self.db.client.table('Rawdata_FILE_AND_MAIL').select('*').eq('processing_status', 'pending')

        if workspace != 'all':
            query = query.eq('workspace', workspace)

        result = query.limit(limit).execute()
        return result.data if result.data else []

    def mark_as_processing(self, document_id: str):
        """処理中にマーク"""
        try:
            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                'processing_status': 'processing',
                'processing_stage': '開始',
                'processing_progress': 0.0
            }).eq('id', document_id).execute()
        except Exception as e:
            logger.error( f"処理中マークエラー: {e}")

    def update_progress(self, document_id: str, stage: str, progress: float):
        """進捗を更新"""
        try:
            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                'processing_stage': stage,
                'processing_progress': progress
            }).eq('id', document_id).execute()
            logger.debug(f"進捗更新: {stage} ({progress*100:.0f}%)")
        except Exception as e:
            logger.error(f"進捗更新エラー: {e}")

    def mark_as_completed(self, document_id: str):
        """完了にマーク"""
        try:
            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                'processing_status': 'completed',
                'processing_stage': '完了',
                'processing_progress': 1.0
            }).eq('id', document_id).execute()
        except Exception as e:
            logger.error( f"完了マークエラー: {e}")

    def mark_as_failed(self, document_id: str, error_message: str = ""):
        """エラーにマーク"""
        try:
            update_data = {
                'processing_status': 'failed',
                'processing_stage': 'エラー',
                'processing_progress': 0.0
            }

            # エラーメッセージをメタデータに保存
            if error_message:
                # 既存のメタデータを取得
                doc_result = self.db.client.table('Rawdata_FILE_AND_MAIL').select('metadata').eq('id', document_id).execute()
                if doc_result.data and len(doc_result.data) > 0:
                    metadata = doc_result.data[0].get('metadata', {}) or {}
                else:
                    metadata = {}

                metadata['last_error'] = error_message
                metadata['last_error_time'] = datetime.now().isoformat()
                update_data['metadata'] = metadata

            self.db.client.table('Rawdata_FILE_AND_MAIL').update(update_data).eq('id', document_id).execute()
        except Exception as e:
            logger.error( f"失敗マークエラー: {e}")

    def get_queue_stats(self, workspace: str = 'all') -> Dict[str, int]:
        """
        統計情報を取得

        Args:
            workspace: 対象ワークスペース ('all' で全て)

        Returns:
            統計情報の辞書
        """
        try:
            query = self.db.client.table('Rawdata_FILE_AND_MAIL').select('processing_status, workspace')

            if workspace != 'all':
                query = query.eq('workspace', workspace)

            response = query.execute()

            stats = {
                'pending': 0,
                'processing': 0,
                'completed': 0,
                'failed': 0,
                'null': 0  # 未処理（processing_statusがnull）
            }

            for doc in response.data:
                status = doc.get('processing_status')
                if status is None:
                    stats['null'] += 1
                else:
                    stats[status] = stats.get(status, 0) + 1

            stats['total'] = len(response.data)

            # 成功率を計算
            processed = stats['completed'] + stats['failed']
            if processed > 0:
                stats['success_rate'] = round(stats['completed'] / processed * 100, 1)
            else:
                stats['success_rate'] = 0.0

            return stats

        except Exception as e:
            logger.error(f" 統計取得エラー: {e}")
            return {}

    def print_queue_stats(self, workspace: str = 'all'):
        """
        統計情報を表示

        Args:
            workspace: 対象ワークスペース ('all' で全て)
        """
        stats = self.get_queue_stats(workspace)

        if not stats:
            logger.info("統計情報の取得に失敗しました")
            return

        logger.info("\n" + "="*80)
        if workspace == 'all':
            logger.info("📊 全体統計")
        else:
            logger.info(f"📊 統計 (workspace: {workspace})")
        logger.info("="*80)
        logger.info(f"待機中 (pending):      {stats.get('pending', 0):>5}件")
        logger.info(f"処理中 (processing):   {stats.get('processing', 0):>5}件")
        logger.info(f"完了   (completed):    {stats.get('completed', 0):>5}件")
        logger.info(f"失敗   (failed):       {stats.get('failed', 0):>5}件")
        logger.info(f"未処理 (null):         {stats.get('null', 0):>5}件")
        logger.info("-" * 80)
        logger.info(f"合計:                  {stats.get('total', 0):>5}件")

        # 成功率を表示
        processed = stats.get('completed', 0) + stats.get('failed', 0)
        if processed > 0:
            logger.info(f"成功率:                {stats.get('success_rate', 0):>5.1f}% ({stats.get('completed', 0)}/{processed})")

        logger.info("="*80 + "\n")

    async def process_document(
        self,
        doc: Dict[str, Any],
        preserve_workspace: bool = True,
        progress_callback=None
    ) -> bool:
        """
        ドキュメントを処理

        Args:
            doc: ドキュメントデータ
            preserve_workspace: workspaceを保持するか

        Returns:
            成功したかどうか
        """
        document_id = doc['id']
        file_name = doc.get('file_name', 'unknown')
        title = doc.get('title', '')
        display_name = title if title else '(タイトル未生成)'
        source_type = doc.get('source_type', '')

        completed_or_failed = False  # 処理が完了または失敗したかのフラグ

        try:
            # 処理中にマーク
            self.mark_as_processing(document_id)

            # source_idの有無で判断（source_typeには依存しない）
            drive_file_id = doc.get('source_id')

            if drive_file_id:
                # 添付ファイルあり（Drive File IDが存在）
                result = await self._process_with_attachment(doc, preserve_workspace, progress_callback)
            else:
                # テキストのみ（添付ファイルなし）
                result = await self._process_text_only(doc, preserve_workspace, progress_callback)

            # 結果がboolの場合（後方互換性）
            if isinstance(result, bool):
                success = result
                error_msg = "処理失敗（詳細なし）" if not success else None
            else:
                # 結果がdictの場合（詳細エラー付き）
                success = result.get('success', False)
                error_msg = result.get('error', "不明なエラー") if not success else None

            # ステータス更新
            if success:
                # screenshot_url がある場合：PNGを削除してクリア
                screenshot_url = doc.get('screenshot_url')
                if screenshot_url:
                    try:
                        # screenshot_url からファイルIDを抽出
                        import re
                        match = re.search(r'/d/([a-zA-Z0-9_-]+)', screenshot_url)
                        if match:
                            png_file_id = match.group(1)

                            # PNGをゴミ箱に移動（共有ドライブでは完全削除不可）
                            from shared.common.connectors.google_drive import GoogleDriveConnector
                            drive = GoogleDriveConnector()
                            drive.trash_file(png_file_id)
                            logger.info(f"[OK] OCR用PNGをゴミ箱に移動: {png_file_id}")

                            # screenshot_url をクリア
                            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                                'screenshot_url': None
                            }).eq('id', document_id).execute()
                            logger.info(f"[OK] screenshot_url をクリアしました")

                    except Exception as e:
                        logger.warning(f" PNG削除処理でエラー（処理は継続）: {e}")

                self.mark_as_completed(document_id)
                completed_or_failed = True
                logger.info(f"[OK] 処理成功: {display_name}")
            else:
                self.mark_as_failed(document_id, error_msg)
                completed_or_failed = True
                logger.error( f"[FAIL] 処理エラー: {display_name} - {error_msg}")

            return success

        except Exception as e:
            # 明確なエラー発生: エラーとして記録
            error_msg = f"処理中にエラー: {str(e)}"
            logger.error("=" * 80)
            logger.error(f"[FAIL] 明確なエラーが発生しました → エラーとして記録")
            logger.error(f"  ├─ ドキュメント: {display_name}")
            logger.error(f"  ├─ エラータイプ: {type(e).__name__}")
            logger.error(f"  └─ エラー内容: {error_msg}")
            logger.error("=" * 80)
            self.mark_as_failed(document_id, error_msg)
            completed_or_failed = True
            return False

        finally:
            # 強制終了や中断時はpendingに差し戻し（completed_or_failedがFalseの場合）
            # エラーが出ていないがcompletedになっていない → pendingに戻す（エラーにしない）
            if not completed_or_failed:
                logger.warning("=" * 80)
                logger.warning(f"[ROLLBACK] 処理が中断されました → pendingに差し戻し（エラーにしません）")
                logger.warning(f"  ├─ ドキュメント: {display_name}")
                logger.warning(f"  └─ 理由: 明確なエラーが出ていないため、エラーではなくpendingに戻します")
                logger.warning("=" * 80)
                try:
                    self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                        'processing_status': 'pending'
                    }).eq('id', document_id).execute()
                    logger.info(f"[OK] pendingに差し戻しました: {display_name}")
                except Exception as e:
                    logger.error(f"差し戻しエラー: {e}")

    async def _process_text_only(
        self,
        doc: Dict[str, Any],
        preserve_workspace: bool = True,
        progress_callback=None
    ) -> bool:
        """テキストのみドキュメントを処理（統合パイプラインのStage H-K部分のみ使用）"""
        from shared.common.processing.metadata_chunker import MetadataChunker

        document_id = doc['id']
        file_name = doc.get('file_name', 'text_only')
        workspace_to_use = doc.get('workspace', 'unknown') if preserve_workspace else 'unknown'

        display_subject = doc.get('display_subject', '')
        display_post_text = doc.get('display_post_text', '')
        attachment_text = doc.get('attachment_text', '')

        # テキスト結合
        text_parts = []
        if display_subject:
            text_parts.append(f"【件名】\n{display_subject}")
        if display_post_text:
            text_parts.append(f"【本文】\n{display_post_text}")
        if attachment_text:
            text_parts.append(f"【添付ファイル】\n{attachment_text}")

        combined_text = '\n\n'.join(text_parts)

        if not combined_text.strip():
            error_msg = "テキストが空です"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        # 統合パイプラインの Stage H-K を使用
        # config から設定を取得
        stage_h_config = self.pipeline.config.get_stage_config('stage_h', doc.get('doc_type', 'other'), workspace_to_use)

        # Stage H: 構造化
        self.update_progress(document_id, 'Stage H: 構造化', 0.3)
        stageh_result = self.pipeline.stage_h.process(
            file_name=file_name,
            doc_type=doc.get('doc_type', 'unknown'),
            workspace=workspace_to_use,
            combined_text=combined_text,
            prompt=stage_h_config['prompt'],
            model=stage_h_config['model']
        )

        # Stage H の結果をチェック
        if not stageh_result or not isinstance(stageh_result, dict):
            error_msg = "Stage H失敗: 構造化結果が不正です"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        stageh_metadata = stageh_result.get('metadata', {})
        if stageh_metadata.get('extraction_failed'):
            error_msg = "Stage H失敗: JSON抽出に失敗しました"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        document_date = stageh_result.get('document_date')
        tags = stageh_result.get('tags', [])

        # Stage I はスキップ（テキストのみなので要約不要）

        # Stage J: チャンク化
        self.update_progress(document_id, 'Stage J: チャンク化', 0.6)
        metadata_chunker = MetadataChunker()
        document_data = {
            'file_name': file_name,
            'summary': '',
            'document_date': document_date,
            'tags': tags,
            'doc_type': doc.get('doc_type'),
            'display_subject': display_subject,
            'display_post_text': display_post_text,
            'display_sender': doc.get('display_sender'),
            'display_type': doc.get('display_type'),
            'display_sent_at': doc.get('display_sent_at'),
            'classroom_sender_email': doc.get('classroom_sender_email'),
            'attachment_text': attachment_text,
            'persons': stageh_metadata.get('persons', []) if isinstance(stageh_metadata, dict) else [],
            'organizations': stageh_metadata.get('organizations', []) if isinstance(stageh_metadata, dict) else [],
            'people': stageh_metadata.get('people', []) if isinstance(stageh_metadata, dict) else [],
            # Stage H の構造化データを追加
            'text_blocks': stageh_metadata.get('text_blocks', []) if isinstance(stageh_metadata, dict) else [],
            'structured_tables': stageh_metadata.get('structured_tables', []) if isinstance(stageh_metadata, dict) else [],
            'weekly_schedule': stageh_metadata.get('weekly_schedule', []) if isinstance(stageh_metadata, dict) else [],
            'other_text': stageh_metadata.get('other_text', []) if isinstance(stageh_metadata, dict) else []
        }

        chunks = metadata_chunker.create_metadata_chunks(document_data)

        # 既存チャンクを削除
        try:
            self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
        except Exception as e:
            logger.warning( f"既存チャンク削除エラー（継続）: {e}")

        # Stage K: Embedding + 保存
        self.update_progress(document_id, 'Stage K: Embedding', 0.8)
        stage_k_result = self.pipeline.stage_k.embed_and_save(document_id, chunks)

        if not stage_k_result.get('success'):
            error_msg = f"Stage K失敗: {stage_k_result.get('failed_count', 0)}/{len(chunks)}チャンク保存失敗"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        # 部分的失敗もエラーとして扱う（厳格モード）
        failed_count = stage_k_result.get('failed_count', 0)
        if failed_count > 0:
            error_msg = f"Stage K部分失敗: {failed_count}/{len(chunks)}チャンク保存失敗"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        logger.info(f"チャンク保存完了: {stage_k_result.get('saved_count', 0)}/{len(chunks)}件")

        # ドキュメント更新
        try:
            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                'tags': tags,
                'document_date': document_date,
                'metadata': stageh_metadata
            }).eq('id', document_id).execute()
        except Exception as e:
            error_msg = f"ドキュメント更新エラー: {e}"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        return {'success': True}

    async def _process_with_attachment(
        self,
        doc: Dict[str, Any],
        preserve_workspace: bool = True,
        progress_callback=None
    ) -> bool:
        """添付ファイルありドキュメントを処理"""
        document_id = doc['id']
        file_name = doc.get('file_name', 'unknown')
        drive_file_id = doc.get('source_id')

        if not drive_file_id:
            logger.error( "source_id（Drive File ID）がありません")
            return False

        # ファイル拡張子チェック
        file_extension = Path(file_name).suffix.lower()
        if file_extension in self.VIDEO_EXTENSIONS:
            logger.info(f"⏭️  動画ファイルをスキップ: {file_name}")
            # 動画ファイルはスキップ扱いで成功とする
            return True

        # screenshot_url があればPNGをダウンロード（OCR用）、なければ通常ファイル
        screenshot_url = doc.get('screenshot_url')
        screenshot_file_id = None
        download_file_id = drive_file_id
        download_file_name = file_name

        if screenshot_url:
            # screenshot_url からファイルIDを抽出
            import re
            match = re.search(r'/d/([a-zA-Z0-9_-]+)', screenshot_url)
            if match:
                screenshot_file_id = match.group(1)
                download_file_id = screenshot_file_id
                # PNGファイル名に変更
                base_name = Path(file_name).stem
                download_file_name = f"{base_name}.png"
                logger.info(f"[OCR用] PNGをダウンロード: {download_file_name} (screenshot_url使用)")
            else:
                logger.warning( f"screenshot_url からファイルIDを抽出できません: {screenshot_url}")

        # Driveからダウンロード
        self.update_progress(document_id, 'ダウンロード中', 0.1)
        try:
            self.drive.download_file(download_file_id, download_file_name, str(self.temp_dir))
            local_path = self.temp_dir / download_file_name
        except Exception as e:
            # 404エラー（ファイルが存在しない）の場合、テキストのみ処理にフォールバック
            error_str = str(e)
            if 'File not found' in error_str or '404' in error_str:
                logger.warning( f"Driveにファイルが存在しません。テキストのみ処理にフォールバック: {file_name}")
                return await self._process_text_only(doc, preserve_workspace)
            else:
                logger.error( f"ダウンロード失敗: {e}")
                return False

        # MIMEタイプを推測
        mime_type = doc.get('mimeType')
        if not mime_type:
            # データベースにない場合は、ファイル名から推測
            mime_type, _ = mimetypes.guess_type(file_name)
        if not mime_type:
            # それでも不明な場合は汎用バイナリとして扱う
            mime_type = 'application/octet-stream'

        # 統合パイプラインで処理
        self.update_progress(document_id, 'Stage E-K: 処理中', 0.3)
        try:
            workspace_to_use = doc.get('workspace', 'unknown') if preserve_workspace else 'unknown'

            result = await self.pipeline.process_document(
                file_path=Path(local_path),
                file_name=file_name,
                doc_type=doc.get('doc_type', 'other'),
                workspace=workspace_to_use,
                mime_type=mime_type,
                source_id=drive_file_id,
                existing_document_id=document_id,
                extra_metadata={
                    'display_subject': doc.get('display_subject'),
                    'display_post_text': doc.get('display_post_text'),
                    'attachment_text': doc.get('attachment_text'),
                    'display_sender': doc.get('display_sender'),
                    'display_sender_email': doc.get('display_sender_email'),
                    'display_type': doc.get('display_type'),
                    'display_sent_at': doc.get('display_sent_at'),
                    'classroom_sender_email': doc.get('classroom_sender_email')
                },
                progress_callback=progress_callback
            )

            # 結果全体を返す（エラーメッセージを含む）
            return result

        finally:
            # 一時ファイル削除
            if local_path.exists():
                local_path.unlink()
                logger.debug( f"一時ファイル削除: {local_path}")

    async def run(
        self,
        workspace: str = 'all',
        limit: int = 100,
        preserve_workspace: bool = True
    ):
        """
        処理を実行

        Args:
            workspace: 対象ワークスペース
            limit: 処理する最大件数
            preserve_workspace: workspaceを保持するか
        """
        logger.info("="*80)
        logger.info("ドキュメント処理スクリプト（シンプル版）")
        logger.info("="*80)

        # pending ドキュメントを取得
        docs = self.get_pending_documents(workspace, limit)

        if not docs:
            logger.info("処理対象のドキュメントがありません")
            return

        logger.info(f"処理対象: {len(docs)}件")
        logger.info("")

        # 統計
        stats = {'success': 0, 'failed': 0, 'total': len(docs)}

        # 順次処理
        for i, doc in enumerate(docs, 1):
            file_name = doc.get('file_name', 'unknown')
            title = doc.get('title', '')
            # タイトルがあればタイトルを表示、なければ「タイトル未生成」
            display_name = title if title else '(タイトル未生成)'
            logger.info(f"\n{'='*80}")
            logger.info(f"[{i}/{len(docs)}] 処理開始: {display_name}")
            logger.info(f"Document ID: {doc['id']}")

            success = await self.process_document(doc, preserve_workspace)

            if success:
                stats['success'] += 1
            else:
                stats['failed'] += 1

            logger.info(f"進捗: 成功={stats['success']}, エラー={stats['failed']}, 残り={len(docs)-i}")

        # 最終結果
        logger.info("\n" + "="*80)
        logger.info("処理完了")
        logger.info("="*80)
        logger.info(f"[OK] 成功: {stats['success']}件")
        logger.error(f"[FAIL] エラー: {stats['failed']}件")
        logger.info(f"[TOTAL] 合計: {stats['total']}件")
        logger.info("="*80)


async def main():
    """メイン関数"""
    import argparse

    parser = argparse.ArgumentParser(description='ドキュメント処理スクリプト（シンプル版）')
    parser.add_argument('--workspace', default='all', help='対象ワークスペース (デフォルト: all)')
    parser.add_argument('--limit', type=int, default=100, help='処理する最大件数 (デフォルト: 100)')
    parser.add_argument('--no-preserve-workspace', action='store_true', help='workspaceを保持しない')
    parser.add_argument('--stats', action='store_true', help='統計情報のみを表示')

    args = parser.parse_args()

    processor = DocumentProcessor()

    # 統計情報のみ表示
    if args.stats:
        processor.print_queue_stats(workspace=args.workspace)
        return

    # 通常の処理
    await processor.run(
        workspace=args.workspace,
        limit=args.limit,
        preserve_workspace=not args.no_preserve_workspace
    )


async def continuous_processing_loop():
    """継続的な処理ループ（自動処理用）"""
    processor = DocumentProcessor()

    logger.info("="*80)
    logger.info("自動処理ループを開始します")
    logger.info("="*80)

    while True:
        try:
            # pending ドキュメントを取得
            docs = processor.get_pending_documents(workspace='all', limit=10)

            if docs:
                logger.info(f"\n処理対象: {len(docs)}件")

                # 順次処理
                for i, doc in enumerate(docs, 1):
                    title = doc.get('title', '')
                    display_name = title if title else '(タイトル未生成)'
                    logger.info(f"\n[{i}/{len(docs)}] 処理中: {display_name}")

                    await processor.process_document(doc, preserve_workspace=True)
            else:
                logger.debug("処理対象のドキュメントがありません（5秒後に再チェック）")

            # 5秒待機してから次のループ
            await asyncio.sleep(5)

        except Exception as e:
            logger.error(f"処理ループでエラー: {e}")
            # エラーが発生しても10秒待機して継続
            await asyncio.sleep(10)


if __name__ == '__main__':
    import sys

    # --loop フラグがある場合は継続ループモード
    if '--loop' in sys.argv:
        asyncio.run(continuous_processing_loop())
    else:
        # 通常モード（1回実行）
        asyncio.run(main())
```

### scripts\processing\process_queued_flyers.py

```py
"""
チラシ処理パイプライン

Rawdata_FLYER_shopsテーブルの processing_status='pending' のチラシを処理

処理フロー:
1. Pre-processing: 画像ファイルダウンロード
2. Stage B (Gemini Vision):
   - Step 1: OCR + レイアウト解析
   - Step 2: 商品情報の構造化抽出
3. Stage C (Gemini Flash): 構造化データの最終整理
4. Stage A (Gemini): 要約生成
5. チャンク化・ベクトル化: search_indexに保存

使い方:
    # 全てのpendingチラシを処理（デフォルト10件）
    python process_queued_flyers.py

    # 処理件数を指定
    python process_queued_flyers.py --limit=50

    # 特定の店舗のみ処理
    python process_queued_flyers.py --store="フーディアム 武蔵小杉"

    # ドライラン（確認のみ）
    python process_queued_flyers.py --dry-run
"""

import asyncio
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from loguru import logger
import hashlib

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))
sys.path.insert(0, str(root_dir / "services" / "data-ingestion"))

from shared.common.database.client import DatabaseClient
from shared.common.connectors.google_drive import GoogleDriveConnector
from shared.pipeline import UnifiedDocumentPipeline


class FlyerProcessor:
    """チラシ処理パイプライン"""

    def __init__(self, temp_dir: str = "./temp"):
        """
        Args:
            temp_dir: 一時ファイル保存ディレクトリ
        """
        self.db = DatabaseClient()
        self.drive = GoogleDriveConnector()

        # 統合パイプラインを初期化
        self.pipeline = UnifiedDocumentPipeline(db_client=self.db)

        self.temp_dir = Path(temp_dir)
        self.temp_dir.mkdir(parents=True, exist_ok=True)

        logger.info("FlyerProcessor初期化完了（G_unified_pipeline使用）")

    def get_pending_flyers(
        self,
        limit: int = 10,
        store_name: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        処理待ちのチラシを取得

        Args:
            limit: 取得件数
            store_name: 店舗名（指定された場合のみ）

        Returns:
            チラシ情報のリスト
        """
        try:
            query = self.db.client.table('Rawdata_FLYER_shops').select('*').eq(
                'processing_status', 'pending'
            )

            if store_name:
                query = query.eq('organization', store_name)

            result = query.limit(limit).execute()

            if result.data:
                logger.info(f"処理待ちチラシ: {len(result.data)}件")
                return result.data

            return []

        except Exception as e:
            logger.error(f"チラシ取得エラー: {e}")
            return []

    async def process_single_flyer(
        self,
        flyer_doc: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        1件のチラシを処理（G_unified_pipeline使用）

        Args:
            flyer_doc: チラシドキュメント情報

        Returns:
            処理結果
        """
        flyer_doc_id = flyer_doc['id']
        file_name = flyer_doc.get('file_name', '不明')
        source_id = flyer_doc.get('source_id')  # Google Drive ID
        organization = flyer_doc.get('organization', '不明')

        logger.info(f"\n{'='*80}")
        logger.info(f"チラシ処理開始: {file_name}")
        logger.info(f"  店舗: {organization}")
        logger.info(f"  ID: {flyer_doc_id}")
        logger.info(f"{'='*80}")

        result = {
            'flyer_doc_id': flyer_doc_id,
            'file_name': file_name,
            'success': False,
            'products_count': 0,
            'chunks_count': 0,
            'error': None
        }

        local_path = None

        try:
            # ステータスを 'processing' に更新
            self._update_status(flyer_doc_id, 'processing')

            # ============================================
            # Pre-processing: ファイルダウンロード
            # ============================================
            logger.info("[Pre-processing] 画像ダウンロード中...")
            local_path = self.drive.download_file(source_id, file_name, self.temp_dir)

            if not local_path or not Path(local_path).exists():
                raise Exception("画像ダウンロード失敗")

            logger.info(f"  ダウンロード完了: {local_path}")

            # ============================================
            # G_unified_pipeline で処理
            # ============================================
            logger.info("[G_unified_pipeline] チラシ処理開始...")

            # doc_type="flyer" で統合パイプラインを実行
            # → config/prompts/stage_f/flyer.md
            # → config/prompts/stage_g/flyer.md
            # → config/prompts/stage_h/flyer.md
            # → config/prompts/stage_i/flyer.md
            pipeline_result = await self.pipeline.process_document(
                file_path=Path(local_path),
                file_name=file_name,
                doc_type='flyer',  # ← これで自動的にチラシ用プロンプト・モデルが選択される
                workspace='shopping',
                mime_type='image/jpeg',  # チラシは通常JPEG
                source_id=source_id,
                extra_metadata={
                    'organization': organization,
                    'flyer_title': flyer_doc.get('flyer_title'),
                    'flyer_period': flyer_doc.get('flyer_period'),
                    'page_number': flyer_doc.get('page_number'),
                    'flyer_doc_id': flyer_doc_id
                }
            )

            if not pipeline_result.get('success'):
                raise Exception(f"パイプライン処理失敗: {pipeline_result.get('error')}")

            document_id = pipeline_result['document_id']
            chunks_count = pipeline_result.get('chunks_count', 0)

            logger.info(f"[G_unified_pipeline完了] document_id={document_id}, chunks={chunks_count}")

            # ============================================
            # 成功
            # ============================================
            self._update_status(flyer_doc_id, 'completed')

            result.update({
                'success': True,
                'document_id': document_id,
                'chunks_count': chunks_count
            })

            logger.info(f"✅ チラシ処理成功: {file_name}")
            return result

        except Exception as e:
            error_msg = str(e)
            logger.error(f"❌ チラシ処理エラー: {error_msg}", exc_info=True)

            self._update_status(flyer_doc_id, 'error', error_message=error_msg)

            result['error'] = error_msg
            return result

        finally:
            # 一時ファイル削除
            if local_path and Path(local_path).exists():
                try:
                    Path(local_path).unlink()
                    logger.debug(f"一時ファイル削除: {local_path}")
                except Exception as e:
                    logger.warning(f"一時ファイル削除失敗: {e}")

    async def _save_products(
        self,
        flyer_doc_id: str,
        products: List[Dict[str, Any]],
        page_number: int
    ) -> int:
        """
        商品情報をRawdata_FLYER_itemsテーブルに保存

        Args:
            flyer_doc_id: チラシドキュメントID
            products: 商品リスト
            page_number: ページ番号

        Returns:
            保存成功件数
        """
        success_count = 0

        for product in products:
            try:
                # 商品名の正規化（検索用）
                product_name = product.get('product_name', '')
                product_name_normalized = product_name.lower().strip()

                product_data = {
                    'flyer_document_id': flyer_doc_id,
                    'product_name': product_name,
                    'product_name_normalized': product_name_normalized,
                    'price': product.get('price'),
                    'original_price': product.get('original_price'),
                    'discount_rate': product.get('discount_rate'),
                    'price_unit': product.get('price_unit', '円'),
                    'price_text': product.get('price_text'),
                    'category': product.get('category', 'その他'),
                    'subcategory': product.get('subcategory'),
                    'brand': product.get('brand'),
                    'quantity': product.get('quantity'),
                    'origin': product.get('origin'),
                    'is_special_offer': product.get('is_special_offer', False),
                    'offer_type': product.get('offer_type'),
                    'page_number': page_number,
                    'extracted_text': product.get('extracted_text'),
                    'confidence': product.get('confidence', 0.5),
                    'metadata': {
                        'extraction_date': datetime.now().isoformat(),
                        'extraction_model': 'gemini-2.0-flash-exp'
                    }
                }

                result = await self.db.insert_document('Rawdata_FLYER_items', product_data)
                if result:
                    success_count += 1

            except Exception as e:
                logger.error(f"商品保存エラー: {e}")
                logger.debug(f"商品データ: {product}")

        return success_count

    def _update_status(
        self,
        flyer_doc_id: str,
        status: str,
        error: str = None
    ):
        """
        チラシの処理ステータスを更新

        Args:
            flyer_doc_id: チラシドキュメントID
            status: ステータス（processing, completed, failed）
            error: エラーメッセージ
        """
        try:
            update_data = {
                'processing_status': status,
                'updated_at': datetime.now().isoformat()
            }

            if error:
                update_data['processing_error'] = error

            self.db.client.table('Rawdata_FLYER_shops').update(update_data).eq(
                'id', flyer_doc_id
            ).execute()

        except Exception as e:
            logger.error(f"ステータス更新エラー: {e}")

    async def process_pending_flyers(
        self,
        limit: int = 10,
        store_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        処理待ちのチラシを一括処理

        Args:
            limit: 処理件数
            store_name: 店舗名（指定された場合のみ）

        Returns:
            処理結果のサマリー
        """
        logger.info("=" * 80)
        logger.info("チラシ処理パイプライン開始")
        logger.info("=" * 80)

        # 処理待ちチラシを取得
        pending_flyers = self.get_pending_flyers(limit, store_name)

        if not pending_flyers:
            logger.info("処理待ちのチラシはありません")
            return {'total': 0, 'success': 0, 'failed': 0, 'total_products': 0, 'total_chunks': 0}

        logger.info(f"処理対象: {len(pending_flyers)}件")

        results = []
        for i, flyer in enumerate(pending_flyers, 1):
            logger.info(f"\n[{i}/{len(pending_flyers)}] 処理中...")
            result = await self.process_single_flyer(flyer)
            results.append(result)

        # サマリー
        success_count = sum(1 for r in results if r['success'])
        failed_count = len(results) - success_count
        total_products = sum(r.get('products_count', 0) for r in results)
        total_chunks = sum(r.get('chunks_count', 0) for r in results)

        logger.info("\n" + "=" * 80)
        logger.info("処理完了")
        logger.info(f"  成功: {success_count}/{len(results)}")
        logger.info(f"  失敗: {failed_count}/{len(results)}")
        logger.info(f"  抽出商品数: {total_products}件")
        logger.info(f"  生成チャンク数: {total_chunks}個")
        logger.info("=" * 80)

        return {
            'total': len(results),
            'success': success_count,
            'failed': failed_count,
            'total_products': total_products,
            'total_chunks': total_chunks,
            'results': results
        }


async def main():
    """メインエントリーポイント"""
    # コマンドライン引数のパース
    dry_run = '--dry-run' in sys.argv
    limit = 10
    store_name = None

    for arg in sys.argv:
        if arg.startswith('--limit='):
            try:
                limit = int(arg.split('=')[1])
            except:
                pass
        elif arg.startswith('--store='):
            store_name = arg.split('=')[1]

    processor = FlyerProcessor()

    if dry_run:
        logger.info("🔍 DRY RUN モード: 実際の処理は行いません")
        pending = processor.get_pending_flyers(limit, store_name)
        logger.info(f"処理対象: {len(pending)}件")
        for flyer in pending:
            logger.info(f"  - {flyer.get('organization')}: {flyer.get('file_name')}")
        return

    result = await processor.process_pending_flyers(limit, store_name)

    # 結果を表示
    print("\n" + "=" * 80)
    print("🛒 チラシ処理結果")
    print("=" * 80)
    print(f"処理件数: {result['total']}")
    print(f"成功: {result['success']}")
    print(f"失敗: {result['failed']}")
    print(f"抽出商品数: {result['total_products']}")
    print(f"生成チャンク数: {result['total_chunks']}")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())
```

### scripts\processing\process_rakuten_seiyu.py

```py
"""
楽天西友ネットスーパー 商品データ定期取得スクリプト

使用方法:
    # 初回: ログインしてCookie取得
    python process_rakuten_seiyu.py --auth

    # 商品データ取得
    python process_rakuten_seiyu.py --once              # 1回だけ実行
    python process_rakuten_seiyu.py --continuous        # 継続実行（24時間ごと）
    python process_rakuten_seiyu.py --categories 110001,110003  # 特定カテゴリーのみ

    # ヘッドレスモードなしでログイン（デバッグ用）
    python process_rakuten_seiyu.py --auth --no-headless
"""

import asyncio
import argparse
import os
import sys
import json
import logging
from pathlib import Path
from typing import Optional, List
from dotenv import load_dotenv

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))
sys.path.insert(0, str(root_dir / "services" / "data-ingestion"))

from rakuten_seiyu.auth_manager import RakutenSeiyuAuthManager
from rakuten_seiyu.product_ingestion import RakutenSeiyuProductIngestionPipeline

# 環境変数を読み込み
load_dotenv()

# ロガー設定
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def authenticate(headless: bool = True) -> bool:
    """
    ログインしてCookieを保存

    Args:
        headless: ヘッドレスモードで実行するか

    Returns:
        成功したらTrue
    """
    logger.info("=" * 60)
    logger.info("🔐 楽天西友にログイン中...")
    logger.info("=" * 60)

    rakuten_id = os.getenv("RAKUTEN_ID")
    password = ***REDACTED***"RAKUTEN_PASSWORD")
    zip_code = os.getenv("DELIVERY_ZIP_CODE", "211-0063")

    if not rakuten_id or not password:
        logger.error("❌ 環境変数 RAKUTEN_ID と RAKUTEN_PASSWORD を設定してください")
        logger.error("   .env ファイルを確認してください")
        return False

    try:
        async with RakutenSeiyuAuthManager(headless=headless) as auth:
            # ログイン
            success = await auth.login(
                rakuten_id=rakuten_id,
                password=***REDACTED***
            )

            if not success:
                logger.error("❌ ログイン失敗")
                return False

            # Cookie保存
            await auth.save_cookies("B_ingestion/rakuten_seiyu/rakuten_seiyu_cookies.json")

        logger.info("=" * 60)
        logger.info("✅ 認証完了！Cookie保存しました")
        logger.info("=" * 60)
        return True

    except Exception as e:
        logger.error(f"❌ 認証処理エラー: {e}", exc_info=True)
        return False


async def run_ingestion(
    categories: Optional[str] = None,
    category_config_file: str = "B_ingestion/rakuten_seiyu/categories_config.json",
    headless: bool = True
) -> bool:
    """
    商品データ取得を実行

    Args:
        categories: カンマ区切りのカテゴリーID（指定時は指定カテゴリーのみ）
        category_config_file: カテゴリー設定ファイルのパス
        headless: ヘッドレスモードで実行するか

    Returns:
        成功したらTrue
    """
    logger.info("=" * 60)
    logger.info("🛒 楽天西友商品データ取得開始")
    logger.info("=" * 60)

    # 認証情報を取得
    rakuten_id = os.getenv("RAKUTEN_ID")
    password = ***REDACTED***"RAKUTEN_PASSWORD")

    if not rakuten_id or not password:
        logger.error("❌ 環境変数 RAKUTEN_ID と RAKUTEN_PASSWORD を設定してください")
        return False

    # パイプライン初期化
    pipeline = RakutenSeiyuProductIngestionPipeline(
        rakuten_id=rakuten_id,
        password=***REDACTED***
        headless=headless
    )

    # ログインしてスクレイパー起動
    login_success = await pipeline.start()
    if not login_success:
        logger.error("❌ スクレイパーの起動またはログインに失敗しました")
        return False

    # カテゴリーを動的に取得（毎回ログイン後に取得）
    all_categories = await pipeline.discover_categories()

    if not all_categories:
        logger.error("❌ カテゴリーが見つかりませんでした")
        await pipeline.close()
        return False

    # カテゴリーIDでフィルタリング（指定がある場合）
    target_categories = []

    if categories:
        # コマンドライン引数でカテゴリーID指定
        category_ids = [c.strip() for c in categories.split(",")]
        target_categories = [
            cat for cat in all_categories
            if cat.get("category_id") in category_ids
        ]

        if not target_categories:
            logger.warning(f"指定されたカテゴリーID {category_ids} が見つかりません")
            logger.info("利用可能なカテゴリー:")
            for cat in all_categories[:10]:  # 最初の10件を表示
                logger.info(f"  ID: {cat.get('category_id')}, 名前: {cat.get('name')}")
            await pipeline.close()
            return False

        logger.info(f"指定カテゴリー: {len(target_categories)}件")
    else:
        # 全カテゴリーを対象（ただし/search/で始まるもののみ）
        target_categories = [
            cat for cat in all_categories
            if cat.get("category_id") and cat.get("category_id").isdigit()
        ]
        logger.info(f"全カテゴリーを処理: {len(target_categories)}件")

    # 各カテゴリーを処理
    total_stats = {
        "total_products": 0,
        "new_products": 0,
        "updated_products": 0,
        "categories_processed": 0
    }

    for category in target_categories:
        try:
            category_url = category["url"]
            category_name = category["name"]
            category_id = category.get("category_id", "不明")

            logger.info("-" * 60)
            logger.info(f"📦 カテゴリー処理中: {category_name} (ID: {category_id})")
            logger.info("-" * 60)

            result = await pipeline.process_category_all_pages(
                category_url=category_url,
                category_name=category_name
            )

            if result["success"]:
                total_stats["total_products"] += result["total_products"]
                total_stats["new_products"] += result["new_products"]
                total_stats["updated_products"] += result["updated_products"]
                total_stats["categories_processed"] += 1

            # カテゴリー間の待機（礼儀正しくアクセス）
            if len(target_categories) > 1:
                import time
                import random
                wait_time = random.uniform(3.0, 5.0)
                logger.info(f"⏳ 次のカテゴリーまで {wait_time:.1f}秒待機...")
                time.sleep(wait_time)

        except Exception as e:
            logger.error(f"❌ カテゴリー処理エラー ({category['name']}): {e}", exc_info=True)
            continue

    # スクレイパーを終了
    await pipeline.close()

    # 最終結果
    logger.info("=" * 60)
    logger.info("✅ 処理完了")
    logger.info(f"   処理カテゴリー数: {total_stats['categories_processed']}")
    logger.info(f"   合計商品数: {total_stats['total_products']}件")
    logger.info(f"   新規: {total_stats['new_products']}件")
    logger.info(f"   更新: {total_stats['updated_products']}件")
    logger.info("=" * 60)

    return True


async def main():
    """メイン関数"""
    parser = argparse.ArgumentParser(
        description='楽天西友ネットスーパー 商品データ取得ツール',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
使用例:
  # 初回認証
  python process_rakuten_seiyu.py --auth

  # 商品データ取得（1回）
  python process_rakuten_seiyu.py --once

  # 特定カテゴリーのみ取得
  python process_rakuten_seiyu.py --once --categories 110001,110003

  # 継続実行（24時間ごと）
  python process_rakuten_seiyu.py --continuous

  # デバッグモード（ブラウザ表示）
  python process_rakuten_seiyu.py --auth --no-headless
        """
    )

    parser.add_argument(
        '--auth',
        action='store_true',
        help='ログインしてCookie取得'
    )
    parser.add_argument(
        '--once',
        action='store_true',
        help='1回だけ実行'
    )
    parser.add_argument(
        '--continuous',
        action='store_true',
        help='継続実行（24時間ごと）'
    )
    parser.add_argument(
        '--categories',
        type=str,
        help='カンマ区切りのカテゴリーID（例: 110001,110003）'
    )
    parser.add_argument(
        '--no-headless',
        action='store_true',
        help='ヘッドレスモードなしで実行（デバッグ用）'
    )

    args = parser.parse_args()

    # 引数チェック
    if not any([args.auth, args.once, args.continuous]):
        parser.print_help()
        return

    # 認証処理
    if args.auth:
        headless = not args.no_headless
        success = await authenticate(headless=headless)
        if not success:
            sys.exit(1)
        return

    # 商品データ取得
    if args.once:
        headless = not args.no_headless
        success = await run_ingestion(categories=args.categories, headless=headless)
        if not success:
            sys.exit(1)

    elif args.continuous:
        logger.info("🔄 継続実行モード開始（24時間ごとに実行）")
        logger.info("   Ctrl+C で終了します")
        headless = not args.no_headless

        while True:
            try:
                await run_ingestion(categories=args.categories, headless=headless)

                # 24時間待機
                logger.info("⏳ 次回実行まで24時間待機します...")
                await asyncio.sleep(86400)

            except KeyboardInterrupt:
                logger.info("⚠️  ユーザーによる中断")
                break
            except Exception as e:
                logger.error(f"❌ エラー発生: {e}", exc_info=True)
                logger.info("⏳ 1時間後にリトライします...")
                await asyncio.sleep(3600)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("⚠️  プログラムを終了します")
        sys.exit(0)
```

### scripts\processing\process_single_doc.py

```py
"""単一ドキュメントを処理"""
import asyncio
import sys
from pathlib import Path

# プロジェクトルートをパスに追加
sys.path.insert(0, str(Path(__file__).parent))

from process_queued_documents import DocumentProcessor

async def main():
    doc_id = '2a16467c-435b-44ab-80f8-d9f8c1670495'

    processor = DocumentProcessor()

    # ドキュメント取得
    doc_result = processor.db.client.table('Rawdata_FILE_AND_MAIL').select('*').eq('id', doc_id).execute()

    if not doc_result.data:
        print(f"ERROR: ドキュメントが見つかりません: {doc_id}")
        return

    doc = doc_result.data[0]
    print(f"処理開始: {doc.get('file_name', 'unknown')}")
    print(f"Document ID: {doc_id}")
    print(f"Workspace: {doc.get('workspace')}")
    print(f"Status: {doc.get('processing_status')}")
    print("=" * 80)

    # 処理実行
    success = await processor.process_document(doc, preserve_workspace=True)

    if success:
        print("\n✅ 処理成功")
    else:
        print("\n❌ 処理失敗")

if __name__ == '__main__':
    asyncio.run(main())
```

### scripts\processing\process_tokyu_store.py

```py
"""
東急ストア ネットスーパー 商品データ取得メインスクリプト

使い方:
    # 全カテゴリーを処理（デフォルト）
    python process_tokyu_store.py

    # 特定のカテゴリーのみ処理
    python process_tokyu_store.py --category "野菜" --category "果物"

    # ヘッドレスモードをオフにして動作確認
    python process_tokyu_store.py --no-headless
"""

import os
import sys
import json
import asyncio
import logging
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))
sys.path.insert(0, str(root_dir / "services" / "data-ingestion"))

# .envファイルを読み込む
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from tokyu_store.product_ingestion import TokyuStoreProductIngestionPipeline

# ロガー設定
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tokyu_store_output.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


async def process_all_categories(
    pipeline: TokyuStoreProductIngestionPipeline,
    target_categories: List[str] = None,
    max_pages_per_category: int = 100
) -> Dict[str, Any]:
    """
    すべてのカテゴリーを処理

    Args:
        pipeline: 商品取得パイプライン
        target_categories: 処理対象のカテゴリー名リスト（Noneなら全カテゴリー）
        max_pages_per_category: カテゴリーあたりの最大ページ数

    Returns:
        処理結果のサマリー
    """
    logger.info("=" * 80)
    logger.info("東急ストア ネットスーパー 商品データ取得開始")
    logger.info("=" * 80)

    start_time = datetime.now()

    # カテゴリーを動的に取得
    all_categories = await pipeline.discover_categories()

    if not all_categories:
        logger.error("❌ カテゴリーが見つかりませんでした")
        logger.info("💡 手動でカテゴリーURLを指定してください")

        # 手動カテゴリー定義（実際のサイトから取得したURL）
        manual_categories = [
            {"name": "野菜", "url": f"{pipeline.scraper.base_url}/shop/c/cC10"},
            {"name": "果物", "url": f"{pipeline.scraper.base_url}/shop/c/cC11"},
            {"name": "お魚", "url": f"{pipeline.scraper.base_url}/shop/c/cC20"},
            {"name": "お肉", "url": f"{pipeline.scraper.base_url}/shop/c/cC30"},
            {"name": "惣菜", "url": f"{pipeline.scraper.base_url}/shop/c/cC40"},
            {"name": "牛乳・乳製品・卵", "url": f"{pipeline.scraper.base_url}/shop/c/cC50"},
            {"name": "パン・生菓子・シリアル", "url": f"{pipeline.scraper.base_url}/shop/c/cC51"},
            {"name": "チルド総菜・豆腐・納豆・漬物", "url": f"{pipeline.scraper.base_url}/shop/c/cC52"},
            {"name": "冷凍食品・アイス", "url": f"{pipeline.scraper.base_url}/shop/c/cC53"},
            {"name": "米・餅", "url": f"{pipeline.scraper.base_url}/shop/c/cC54"},
            {"name": "麺類", "url": f"{pipeline.scraper.base_url}/shop/c/cC55"},
            {"name": "乾物・瓶缶詰・粉類", "url": f"{pipeline.scraper.base_url}/shop/c/cC56"},
            {"name": "調味料・中華材料", "url": f"{pipeline.scraper.base_url}/shop/c/cC57"},
            {"name": "お菓子", "url": f"{pipeline.scraper.base_url}/shop/c/cC58"},
            {"name": "水・飲料", "url": f"{pipeline.scraper.base_url}/shop/c/cC59"},
            {"name": "酒類", "url": f"{pipeline.scraper.base_url}/shop/c/cC60"},
        ]
        all_categories = manual_categories
        logger.info(f"📝 手動カテゴリー定義を使用: {len(all_categories)}件")

    # 対象カテゴリーをフィルタリング
    if target_categories:
        categories = [
            cat for cat in all_categories
            if cat["name"] in target_categories
        ]
        logger.info(f"処理対象カテゴリー: {', '.join(target_categories)}")
    else:
        categories = all_categories
        logger.info(f"全{len(categories)}カテゴリーを処理")

    # 各カテゴリーを処理
    results = []
    total_products = 0
    total_new = 0
    total_updated = 0

    for i, category in enumerate(categories, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"[{i}/{len(categories)}] カテゴリー: {category['name']}")
        logger.info(f"{'='*80}")

        try:
            result = await pipeline.process_category_all_pages(
                category_url=category["url"],
                category_name=category["name"],
                max_pages=max_pages_per_category
            )

            results.append(result)
            total_products += result["total_products"]
            total_new += result["new_products"]
            total_updated += result["updated_products"]

        except Exception as e:
            logger.error(f"カテゴリー '{category['name']}' の処理でエラー: {e}", exc_info=True)
            continue

    # サマリー出力
    end_time = datetime.now()
    duration = end_time - start_time

    logger.info("\n" + "=" * 80)
    logger.info("処理完了サマリー")
    logger.info("=" * 80)
    logger.info(f"処理時間: {duration}")
    logger.info(f"カテゴリー数: {len(results)}/{len(categories)}")
    logger.info(f"総商品数: {total_products}件")
    logger.info(f"  新規: {total_new}件")
    logger.info(f"  更新: {total_updated}件")

    # カテゴリー別の詳細
    logger.info("\nカテゴリー別詳細:")
    for result in results:
        logger.info(
            f"  {result['category_name']}: "
            f"{result['total_products']}件 "
            f"(新規{result['new_products']}件、更新{result['updated_products']}件、"
            f"{result['pages_processed']}ページ)"
        )

    summary = {
        "start_time": start_time.isoformat(),
        "end_time": end_time.isoformat(),
        "duration_seconds": duration.total_seconds(),
        "categories_processed": len(results),
        "total_categories": len(categories),
        "total_products": total_products,
        "new_products": total_new,
        "updated_products": total_updated,
        "category_results": results
    }

    return summary


async def main():
    """メイン処理"""
    parser = argparse.ArgumentParser(
        description='東急ストア ネットスーパー 商品データ取得'
    )
    parser.add_argument(
        '--category',
        action='append',
        help='処理対象のカテゴリー名（複数指定可）'
    )
    parser.add_argument(
        '--max-pages',
        type=int,
        default=100,
        help='カテゴリーあたりの最大ページ数（デフォルト: 100）'
    )
    parser.add_argument(
        '--no-headless',
        action='store_true',
        help='ブラウザを表示する（デバッグ用）'
    )
    parser.add_argument(
        '--zip-code',
        type=str,
        default=None,
        help='配達エリアの郵便番号（デフォルト: 環境変数またはコード内の設定）'
    )

    args = parser.parse_args()

    # 環境変数チェック
    login_id = os.getenv("TOKYU_STORE_LOGIN_ID")
    password = ***REDACTED***"TOKYU_STORE_PASSWORD")
    zip_code = args.zip_code or os.getenv("DELIVERY_ZIP_CODE", "158-0094")

    if not login_id or not password:
        logger.error("❌ 環境変数 TOKYU_STORE_LOGIN_ID と TOKYU_STORE_PASSWORD を設定してください")
        logger.error("   .env ファイルに以下を追加してください:")
        logger.error("   TOKYU_STORE_LOGIN_ID=your_email@example.com")
        logger.error("   TOKYU_STORE_PASSWORD=***REDACTED***")
        logger.error("   DELIVERY_ZIP_CODE=158-0094")
        sys.exit(1)

    # パイプライン初期化
    pipeline = TokyuStoreProductIngestionPipeline(
        login_id=login_id,
        password=***REDACTED***
        zip_code=zip_code,
        headless=not args.no_headless
    )

    try:
        # スクレイパー起動
        success = await pipeline.start()
        if not success:
            logger.error("❌ スクレイパー起動失敗")
            sys.exit(1)

        # カテゴリー処理
        summary = await process_all_categories(
            pipeline=pipeline,
            target_categories=args.category,
            max_pages_per_category=args.max_pages
        )

        # サマリーをJSONファイルに保存
        output_file = f"_runtime/data/tokyu_store/tokyu_store_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        logger.info(f"\n✅ 処理結果を {output_file} に保存しました")

    except KeyboardInterrupt:
        logger.info("\n⚠️ ユーザーによって中断されました")

    except Exception as e:
        logger.error(f"❌ エラーが発生しました: {e}", exc_info=True)
        sys.exit(1)

    finally:
        # クリーンアップ
        await pipeline.close()
        logger.info("✅ スクレイパー終了")


if __name__ == "__main__":
    asyncio.run(main())
```

### scripts\reset\reset_3docs.py

```py
from shared.common.database.client import DatabaseClient

db = DatabaseClient()

doc_ids = [
    '299d6d5d-d105-4503-982f-8db48e77eded',
    '485f2b52-1914-4adf-b04a-2c9c94b0760c',
    '87ba1314-9ef0-49bd-a6b4-0a558d32bf0d'
]

for doc_id in doc_ids:
    result = db.client.table('Rawdata_FILE_AND_MAIL').update({
        'processing_status': 'pending',
        'stage_e1_text': None,
        'stage_e2_text': None,
        'stage_e3_text': None,
        'stage_e4_text': None,
        'stage_e5_text': None,
        'attachment_text': None,
        'stage_f_text_ocr': None,
        'stage_f_layout_ocr': None,
        'stage_f_visual_elements': None,
        'stage_h_normalized': None,
        'stage_i_structured': None,
        'summary': None,
        'tags': None,
        'title': None,
        'stage_j_chunks_json': None,
        'chunk_count': 0
    }).eq('id', doc_id).execute()
    print(f'Reset to pending: {doc_id}')

print('\n3件をpendingに戻しました')
```

### scripts\reset\reset_all_documents.py

```py
"""
全ドキュメントをpendingに戻して生成データを削除するスクリプト
"""
from shared.common.database.client import DatabaseClient

def reset_all_documents():
    db = DatabaseClient()

    # 全ドキュメント数を確認
    print("全ドキュメント数を確認中...")
    result = db.client.table('Rawdata_FILE_AND_MAIL')\
        .select('id', count='exact')\
        .execute()

    total = result.count
    print(f"全ドキュメント数: {total}件\n")

    # completed状態のドキュメント数
    completed = db.client.table('Rawdata_FILE_AND_MAIL')\
        .select('id', count='exact')\
        .eq('processing_status', 'completed')\
        .execute()

    print(f"completed状態: {completed.count}件")

    # 全ドキュメントをpendingに戻し、生成データを削除
    print(f"\n全{total}件をpendingに戻して生成データを削除中...")

    # バッチ処理で更新
    db.client.table('Rawdata_FILE_AND_MAIL').update({
        'processing_status': 'pending',
        'title': None,
        'stage_i_structured': None,
        'stage_j_chunks_json': None,
        'document_date': None
    }).neq('id', '00000000-0000-0000-0000-000000000000').execute()

    print(f"\n[完了] 全{total}件をpendingに戻しました")
    print("生成データ（title, stage_i_structured, stage_j_chunks_json, document_date）を削除しました")

    # 確認
    pending_count = db.client.table('Rawdata_FILE_AND_MAIL')\
        .select('id', count='exact')\
        .eq('processing_status', 'pending')\
        .execute()

    print(f"\n現在のpending状態: {pending_count.count}件")

if __name__ == '__main__':
    reset_all_documents()
```

### scripts\reset\reset_all_stages_e_to_k.py

```py
"""
全ステータスのドキュメントのステージE～Kのデータを削除し、processing_statusをpendingに戻す
"""
from shared.common.database.client import DatabaseClient
import sys

def reset_all_stages_e_to_k(workspace=None, all_workspaces=False, skip_confirm=False):
    """
    全ステータスのドキュメントのステージE～Kのフィールドをクリアし、processing_statusをpendingに戻す

    Args:
        workspace: 対象のワークスペース（Noneの場合、all_workspaces=Trueが必要）
        all_workspaces: Trueの場合、全ワークスペースを対象
        skip_confirm: Trueの場合、確認プロンプトをスキップ
    """
    db = DatabaseClient(use_service_role=True)  # RLSバイパスのためService Role使用

    # クリアするフィールドのリスト
    fields_to_clear = {
        # ステージE
        'stage_e1_text': None,
        'stage_e2_text': None,
        'stage_e3_text': None,
        'stage_e4_text': None,
        'stage_e5_text': None,
        # ステージF
        'stage_f_text_ocr': None,
        'stage_f_layout_ocr': None,
        'stage_f_visual_elements': None,
        # ステージH
        'stage_h_normalized': None,
        # ステージI
        'stage_i_structured': None,
        # ステージJ
        'stage_j_chunks_json': None,
        # ステータスをpendingに戻す
        'processing_status': 'pending',
        'processing_stage': None,
    }

    # 対象ドキュメントを取得（全ステータス）
    if all_workspaces:
        # 全ワークスペースを対象
        query = db.client.table('Rawdata_FILE_AND_MAIL')\
            .select('id, file_name, title, processing_status, workspace, stage_e1_text, stage_f_text_ocr, stage_h_normalized, stage_i_structured, stage_j_chunks_json')
    elif workspace:
        # 指定ワークスペースを対象
        query = db.client.table('Rawdata_FILE_AND_MAIL')\
            .select('id, file_name, title, processing_status, workspace, stage_e1_text, stage_f_text_ocr, stage_h_normalized, stage_i_structured, stage_j_chunks_json')\
            .eq('workspace', workspace)
    else:
        print("エラー: workspace または all_workspaces=True を指定してください")
        return

    result = query.execute()

    if not result.data:
        print(f"対象のドキュメントが見つかりません")
        return

    # ステージE～Kのデータが存在するドキュメントのみフィルタリング
    docs_with_stages = []
    for doc in result.data:
        has_stage_data = (
            doc.get('stage_e1_text') is not None or
            doc.get('stage_f_text_ocr') is not None or
            doc.get('stage_h_normalized') is not None or
            doc.get('stage_i_structured') is not None or
            doc.get('stage_j_chunks_json') is not None
        )
        if has_stage_data:
            docs_with_stages.append(doc)

    if not docs_with_stages:
        print("ステージE～Kのデータが存在するドキュメントが見つかりません")
        return

    print(f"ステージE～Kのデータが存在するドキュメント: {len(docs_with_stages)}件")
    print(f"（全ドキュメント数: {len(result.data)}件）\n")

    # ステータス別の集計
    status_counts = {}
    for doc in docs_with_stages:
        status = doc.get('processing_status', '(不明)')
        status_counts[status] = status_counts.get(status, 0) + 1

    print("ステータス別:")
    for status, count in status_counts.items():
        print(f"  - {status}: {count}件")

    # ワークスペース別の集計
    workspace_counts = {}
    for doc in docs_with_stages:
        ws = doc.get('workspace', '(不明)')
        workspace_counts[ws] = workspace_counts.get(ws, 0) + 1

    print("\nワークスペース別:")
    for ws, count in workspace_counts.items():
        print(f"  - {ws}: {count}件")

    print("\nドキュメント一覧（最初の10件）:")
    for i, doc in enumerate(docs_with_stages[:10]):
        title = doc.get('title', doc.get('file_name', '(名前なし)'))
        status = doc.get('processing_status', '不明')
        ws = doc.get('workspace', '不明')

        # どのステージにデータがあるか表示
        stages = []
        if doc.get('stage_e1_text'): stages.append('E')
        if doc.get('stage_f_text_ocr'): stages.append('F')
        if doc.get('stage_h_normalized'): stages.append('H')
        if doc.get('stage_i_structured'): stages.append('I')
        if doc.get('stage_j_chunks_json'): stages.append('J')
        stages_str = ','.join(stages)

        print(f"  {i+1}. [{ws}] {title}")
        print(f"      ステータス: {status}, データあり: ステージ {stages_str}")

    if len(docs_with_stages) > 10:
        print(f"  ... 他 {len(docs_with_stages) - 10}件")

    # 確認プロンプト
    if not skip_confirm:
        confirm = input(f"\n{len(docs_with_stages)}件のドキュメントのステージE～Kをクリアし、pendingに戻しますか? (yes/no): ")
        if confirm.lower() != 'yes':
            print("キャンセルしました")
            return

    # 各ドキュメントを更新
    print("\n処理中...")
    success_count = 0
    error_count = 0
    for doc in docs_with_stages:
        try:
            db.client.table('Rawdata_FILE_AND_MAIL')\
                .update(fields_to_clear)\
                .eq('id', doc['id'])\
                .execute()
            title = doc.get('title', doc.get('file_name', '(名前なし)'))
            print(f"  [OK] {title}")
            success_count += 1
        except Exception as e:
            print(f"  [ERROR] (id: {doc['id']}): {e}")
            error_count += 1

    print(f"\n完了: {success_count}件成功, {error_count}件エラー")

if __name__ == '__main__':
    # コマンドライン引数の処理
    skip_confirm = '--yes' in sys.argv or '-y' in sys.argv

    if '--all' in sys.argv:
        reset_all_stages_e_to_k(all_workspaces=True, skip_confirm=skip_confirm)
    elif len(sys.argv) > 1 and not sys.argv[1].startswith('--'):
        workspace = sys.argv[1]
        reset_all_stages_e_to_k(workspace=workspace, skip_confirm=skip_confirm)
    else:
        print("使用方法:")
        print("  python reset_all_stages_e_to_k.py --all [--yes]         # 全ワークスペース")
        print("  python reset_all_stages_e_to_k.py <workspace> [--yes]   # 指定ワークスペース")
        print("\nオプション:")
        print("  --yes, -y    確認プロンプトをスキップ")
```

### scripts\reset\reset_doc.py

```py
from shared.common.database.client import DatabaseClient
import sys

db = DatabaseClient()
doc_id = '2a16467c-435b-44ab-80f8-d9f8c1670495'

# 現在のステータス確認
current = db.client.table('Rawdata_FILE_AND_MAIL').select('processing_status').eq('id', doc_id).execute()
if current.data:
    print(f'Current status: {current.data[0]["processing_status"]}')

# pendingに戻す
result = db.client.table('Rawdata_FILE_AND_MAIL').update({
    'processing_status': 'pending'
}).eq('id', doc_id).execute()

print(f'Reset to pending: {doc_id}')
```

### scripts\reset\reset_fabricated_dates.py

```py
"""
捏造された日付を持つドキュメントを特定し、再処理用にpendingに戻すスクリプト

使い方:
    # 日付サフィックス付きのドキュメントを確認
    python reset_fabricated_dates.py --check --workspace=ema_classroom

    # 特定のワークスペースを再処理用にリセット
    python reset_fabricated_dates.py --reset --workspace=ema_classroom --limit=50

    # 全ワークスペースを確認
    python reset_fabricated_dates.py --check --workspace=all
"""

import argparse
import re
from typing import List, Dict, Any
from shared.common.database.client import DatabaseClient


class FabricatedDateResetter:
    """捏造日付リセッター"""

    def __init__(self):
        self.db = DatabaseClient()

    def get_documents_with_date_suffix(
        self,
        workspace: str = 'all',
        limit: int = 1000
    ) -> List[Dict[str, Any]]:
        """
        日付サフィックス付きのドキュメントを取得

        パターン:
        - _(YYYY_MM_DD)
        - _(YYYY_MM)
        - _(YYYY)
        - _(2025_12_04) など
        """
        query = self.db.client.table('Rawdata_FILE_AND_MAIL')\
            .select('id, file_name, title, workspace, processing_status, metadata')\
            .eq('processing_status', 'completed')\
            .not_.is_('title', 'null')

        if workspace != 'all':
            query = query.eq('workspace', workspace)

        result = query.limit(limit).execute()

        if not result.data:
            return []

        # 日付サフィックスパターン
        date_pattern = re.compile(r'_\((\d{4}|\d{4}_\d{2}|\d{4}_\d{2}_\d{2}|YYYY|YYYY_MM|YYYY_MM_DD)\)$')

        docs_with_date = []
        for doc in result.data:
            title = doc.get('title', '')
            if date_pattern.search(title):
                docs_with_date.append(doc)

        return docs_with_date

    def categorize_by_date_format(self, documents: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """日付形式でドキュメントを分類"""
        categories = {
            'YYYY_MM_DD': [],  # 完全な日付
            'YYYY_MM': [],     # 年月のみ
            'YYYY': [],        # 年のみ
            'placeholder': []  # _(YYYY) プレースホルダー
        }

        for doc in documents:
            title = doc.get('title', '')

            # 日付サフィックスを抽出
            if title.endswith('_(YYYY)'):
                categories['placeholder'].append(doc)
            elif re.search(r'_\(\d{4}_\d{2}_\d{2}\)$', title):
                categories['YYYY_MM_DD'].append(doc)
            elif re.search(r'_\(\d{4}_\d{2}\)$', title):
                categories['YYYY_MM'].append(doc)
            elif re.search(r'_\(\d{4}\)$', title):
                categories['YYYY'].append(doc)

        return categories

    def check_documents(self, workspace: str = 'all', limit: int = 1000):
        """日付サフィックス付きドキュメントの統計を表示"""
        print(f"\n日付サフィックス付きドキュメントを確認中 (workspace: {workspace})...\n")

        docs = self.get_documents_with_date_suffix(workspace, limit)

        if not docs:
            print(f"日付サフィックス付きのドキュメントが見つかりませんでした")
            return

        print(f"合計: {len(docs)}件\n")

        # 日付形式で分類
        categories = self.categorize_by_date_format(docs)

        print("=== 日付形式別の統計 ===")
        print(f"完全な日付 _(YYYY_MM_DD): {len(categories['YYYY_MM_DD'])}件")
        print(f"年月のみ _(YYYY_MM): {len(categories['YYYY_MM'])}件")
        print(f"年のみ _(YYYY): {len(categories['YYYY'])}件")
        print(f"プレースホルダー _(YYYY): {len(categories['placeholder'])}件")
        print()

        # サンプル表示
        print("=== 完全な日付のサンプル (最初の10件) ===")
        for doc in categories['YYYY_MM_DD'][:10]:
            title = doc.get('title', '(なし)')
            workspace = doc.get('workspace', '(なし)')
            print(f"  - {title} (workspace: {workspace})")

        if len(categories['YYYY_MM_DD']) > 10:
            print(f"  ... 他 {len(categories['YYYY_MM_DD']) - 10}件")
        print()

        print("=== 年月のみのサンプル (最初の10件) ===")
        for doc in categories['YYYY_MM'][:10]:
            title = doc.get('title', '(なし)')
            workspace = doc.get('workspace', '(なし)')
            print(f"  - {title} (workspace: {workspace})")

        if len(categories['YYYY_MM']) > 10:
            print(f"  ... 他 {len(categories['YYYY_MM']) - 10}件")
        print()

        print("=== 再処理の推奨 ===")
        print(f"完全な日付 _(YYYY_MM_DD) の {len(categories['YYYY_MM_DD'])}件:")
        print("  → 文書内に明示的な発行日があるか確認が必要")
        print("  → sended_date と一致しているか確認が必要")
        print()
        print(f"年月のみ _(YYYY_MM) の {len(categories['YYYY_MM'])}件:")
        print("  → 文書内に年月の記載があるか確認が必要")
        print()
        print(f"プレースホルダー _(YYYY) の {len(categories['placeholder'])}件:")
        print("  → 正常（日付不明）")
        print()

    def reset_to_pending(
        self,
        workspace: str = 'all',
        limit: int = 100,
        dry_run: bool = True
    ):
        """
        日付サフィックス付きドキュメントをpendingに戻す

        Args:
            workspace: 対象ワークスペース
            limit: 処理する最大件数
            dry_run: Trueの場合、実際には更新せずに表示のみ
        """
        docs = self.get_documents_with_date_suffix(workspace, limit)

        if not docs:
            print(f"対象ドキュメントが見つかりませんでした")
            return

        # プレースホルダー_(YYYY)は除外（これは正常）
        docs_to_reset = [doc for doc in docs if not doc.get('title', '').endswith('_(YYYY)')]

        print(f"\n再処理対象: {len(docs_to_reset)}件")

        if dry_run:
            print("\n[DRY RUN] 以下のドキュメントがpendingに戻されます:")
            for i, doc in enumerate(docs_to_reset[:20], 1):
                title = doc.get('title', '(なし)')
                workspace = doc.get('workspace', '(なし)')
                print(f"{i}. {title} (workspace: {workspace})")

            if len(docs_to_reset) > 20:
                print(f"... 他 {len(docs_to_reset) - 20}件")

            print("\n実際に実行するには --no-dry-run オプションを追加してください")
            return

        # 実際に更新
        print("\n[実行中] ドキュメントをpendingに戻しています...")

        for doc in docs_to_reset:
            doc_id = doc['id']
            try:
                self.db.client.table('Rawdata_FILE_AND_MAIL')\
                    .update({'processing_status': 'pending'})\
                    .eq('id', doc_id)\
                    .execute()
                print(f"✓ {doc.get('title', '(なし)')}")
            except Exception as e:
                print(f"✗ {doc.get('title', '(なし)')}: {e}")

        print(f"\n[完了] {len(docs_to_reset)}件をpendingに戻しました")


def main():
    parser = argparse.ArgumentParser(
        description='捏造された日付を持つドキュメントを特定し、再処理用にpendingに戻す'
    )
    parser.add_argument(
        '--check',
        action='store_true',
        help='日付サフィックス付きドキュメントの統計を表示'
    )
    parser.add_argument(
        '--reset',
        action='store_true',
        help='日付サフィックス付きドキュメントをpendingに戻す'
    )
    parser.add_argument(
        '--workspace',
        type=str,
        default='all',
        help='対象ワークスペース（デフォルト: all）'
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=1000,
        help='処理する最大件数（デフォルト: 1000）'
    )
    parser.add_argument(
        '--no-dry-run',
        action='store_true',
        help='実際に更新を実行（デフォルトはdry run）'
    )

    args = parser.parse_args()

    resetter = FabricatedDateResetter()

    if args.check:
        resetter.check_documents(args.workspace, args.limit)
    elif args.reset:
        resetter.reset_to_pending(
            args.workspace,
            args.limit,
            dry_run=not args.no_dry_run
        )
    else:
        print("エラー: --check または --reset のいずれかを指定してください")
        print("使い方: python reset_fabricated_dates.py --help")


if __name__ == '__main__':
    main()
```

### scripts\reset\reset_stages_e_to_k.py

```py
"""
ステージE～Kのデータを削除し、processing_statusをpendingに戻す
"""
from shared.common.database.client import DatabaseClient
import sys

def reset_stages_e_to_k(workspace=None, doc_id=None, all_workspaces=False, skip_confirm=False):
    """
    ステージE～Kのフィールドをクリアし、processing_statusをpendingに戻す

    Args:
        workspace: 対象のワークスペース（Noneの場合、all_workspaces=Trueが必要）
        doc_id: 特定のドキュメントIDを指定（Noneの場合は全ドキュメント）
        all_workspaces: Trueの場合、全ワークスペースを対象
        skip_confirm: Trueの場合、確認プロンプトをスキップ
    """
    db = DatabaseClient(use_service_role=True)  # RLSバイパスのためService Role使用

    # クリアするフィールドのリスト
    fields_to_clear = {
        # ステージE
        'stage_e1_text': None,
        'stage_e2_text': None,
        'stage_e3_text': None,
        'stage_e4_text': None,
        'stage_e5_text': None,
        # ステージF
        'stage_f_text_ocr': None,
        'stage_f_layout_ocr': None,
        'stage_f_visual_elements': None,
        # ステージH
        'stage_h_normalized': None,
        # ステージI
        'stage_i_structured': None,
        # ステージJ
        'stage_j_chunks_json': None,
        # ステータスをpendingに戻す
        'processing_status': 'pending',
        'processing_stage': None,
    }

    # 対象ドキュメントを取得
    if doc_id:
        # 特定のドキュメントを対象
        query = db.client.table('Rawdata_FILE_AND_MAIL')\
            .select('id, file_name, title, processing_status, workspace')\
            .eq('id', doc_id)
    elif all_workspaces:
        # 全ワークスペースを対象（completedステータスのみ）
        query = db.client.table('Rawdata_FILE_AND_MAIL')\
            .select('id, file_name, title, processing_status, workspace')\
            .eq('processing_status', 'completed')
    elif workspace:
        # 指定ワークスペースを対象（completedステータスのみ）
        query = db.client.table('Rawdata_FILE_AND_MAIL')\
            .select('id, file_name, title, processing_status, workspace')\
            .eq('workspace', workspace)\
            .eq('processing_status', 'completed')
    else:
        print("エラー: workspace または all_workspaces=True を指定してください")
        return

    result = query.execute()

    if not result.data:
        print(f"対象のドキュメントが見つかりません")
        if doc_id:
            print(f"  doc_id: {doc_id}")
        elif all_workspaces:
            print(f"  全ワークスペース, status: completed")
        else:
            print(f"  workspace: {workspace}, status: completed")
        return

    print(f"対象ドキュメント: {len(result.data)}件")

    # ワークスペース別の集計
    workspace_counts = {}
    for doc in result.data:
        ws = doc.get('workspace', '(不明)')
        workspace_counts[ws] = workspace_counts.get(ws, 0) + 1

    print("\nワークスペース別:")
    for ws, count in workspace_counts.items():
        print(f"  - {ws}: {count}件")

    print("\nドキュメント一覧（最初の10件）:")
    for i, doc in enumerate(result.data[:10]):
        title = doc.get('title', doc.get('file_name', '(名前なし)'))
        status = doc.get('processing_status', '不明')
        ws = doc.get('workspace', '不明')
        print(f"  {i+1}. [{ws}] {title} (現在: {status})")

    if len(result.data) > 10:
        print(f"  ... 他 {len(result.data) - 10}件")

    # 確認プロンプト
    if not doc_id and len(result.data) > 0 and not skip_confirm:
        confirm = input(f"\n{len(result.data)}件のドキュメントのステージE～Kをクリアし、pendingに戻しますか? (yes/no): ")
        if confirm.lower() != 'yes':
            print("キャンセルしました")
            return

    # 各ドキュメントを更新
    print("\n処理中...")
    success_count = 0
    error_count = 0
    for doc in result.data:
        try:
            db.client.table('Rawdata_FILE_AND_MAIL')\
                .update(fields_to_clear)\
                .eq('id', doc['id'])\
                .execute()
            title = doc.get('title', doc.get('file_name', '(名前なし)'))
            print(f"  [OK] {title}")
            success_count += 1
        except Exception as e:
            print(f"  [ERROR] (id: {doc['id']}): {e}")
            error_count += 1

    print(f"\n完了: {success_count}件成功, {error_count}件エラー")

if __name__ == '__main__':
    # コマンドライン引数の処理
    # 使用例:
    #   python reset_stages_e_to_k.py --all --yes               # 全ワークスペースの全completedドキュメント（確認なし）
    #   python reset_stages_e_to_k.py my_workspace              # 指定ワークスペースの全completedドキュメント
    #   python reset_stages_e_to_k.py --id <doc_id>             # 特定のドキュメントのみ

    skip_confirm = '--yes' in sys.argv or '-y' in sys.argv

    if '--id' in sys.argv:
        idx = sys.argv.index('--id')
        if idx + 1 < len(sys.argv):
            doc_id = sys.argv[idx + 1]
            reset_stages_e_to_k(doc_id=doc_id, skip_confirm=skip_confirm)
        else:
            print("エラー: --id の後にドキュメントIDを指定してください")
    elif '--all' in sys.argv:
        reset_stages_e_to_k(all_workspaces=True, skip_confirm=skip_confirm)
    elif len(sys.argv) > 1 and not sys.argv[1].startswith('--'):
        workspace = sys.argv[1]
        reset_stages_e_to_k(workspace=workspace, skip_confirm=skip_confirm)
    else:
        print("使用方法:")
        print("  python reset_stages_e_to_k.py --all [--yes]         # 全ワークスペース")
        print("  python reset_stages_e_to_k.py <workspace> [--yes]   # 指定ワークスペース")
        print("  python reset_stages_e_to_k.py --id <doc_id>         # 特定ドキュメント")
        print("\nオプション:")
        print("  --yes, -y    確認プロンプトをスキップ")
```

### scripts\reset\reset_to_pending.py

```py
"""
processing 状態のドキュメントを pending に戻す
"""
from shared.common.database.client import DatabaseClient

def reset_to_pending(workspace='ikuya_classroom'):
    db = DatabaseClient()

    # processing 状態のドキュメントを取得
    result = db.client.table('Rawdata_FILE_AND_MAIL')\
        .select('id, file_name, title')\
        .eq('workspace', workspace)\
        .eq('processing_status', 'processing')\
        .execute()

    if not result.data:
        print(f"processing 状態のドキュメントが見つかりません (workspace: {workspace})")
        return

    print(f"processing 状態のドキュメント: {len(result.data)}件")
    for doc in result.data:
        title = doc.get('title', '(タイトル未生成)')
        print(f"  - {title}")

    # pending に戻す
    for doc in result.data:
        db.client.table('Rawdata_FILE_AND_MAIL')\
            .update({'processing_status': 'pending'})\
            .eq('id', doc['id'])\
            .execute()

    print(f"\n[OK] {len(result.data)}件を pending に戻しました")

if __name__ == '__main__':
    import sys
    workspace = sys.argv[1] if len(sys.argv) > 1 else 'ikuya_classroom'
    reset_to_pending(workspace)
```

### scripts\utils\check_database.py

```py
import os
import sys
from dotenv import load_dotenv
from supabase import create_client

# 出力をUTF-8に設定
sys.stdout.reconfigure(encoding='utf-8')

load_dotenv()

url = os.getenv('SUPABASE_URL')
key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
client = create_client(url, key)

print("=== processing_lock確認 ===\n")

# id=1のレコードを取得
result = client.table('processing_lock').select('*').eq('id', 1).execute()

if result.data:
    print("id=1のレコード:")
    record = result.data[0]
    for key, value in record.items():
        print(f"  {key}: {value}")
else:
    print("id=1のレコードが存在しません！")
    print("\n作成します...")
    client.table('processing_lock').insert({
        'id': 1,
        'is_processing': False,
        'max_parallel': 3,
        'current_workers': 0
    }).execute()
    print("作成完了")
```

### scripts\utils\check_state.py

```py
"""Check current processing state in Supabase"""
from shared.common.database.client import DatabaseClient

db = DatabaseClient(use_service_role=True)

# processing_workers テーブルを確認
workers = db.client.table('processing_workers').select('*').execute()
print('=== processing_workers ===')
for w in workers.data:
    print(f'  - {w}')
print(f'Total: {len(workers.data)}')

# processing_lock テーブルを確認
lock = db.client.table('processing_lock').select('*').eq('id', 1).execute()
print()
print('=== processing_lock ===')
print(lock.data)

# processing状態のドキュメント数を確認
docs = db.client.table('Rawdata_FILE_AND_MAIL').select('id', count='exact').eq('processing_status', 'processing').execute()
print()
print(f'=== processing状態のドキュメント数: {docs.count} ===')
```

### scripts\utils\collect_project.py

```py
"""
プロジェクト構造・コード収集ツール
- ディレクトリ構造をツリー表示
- 全コードを収集
- 機密キーを自動で伏字化
"""
import os
import re
from pathlib import Path
from datetime import datetime

# 除外するディレクトリ
EXCLUDE_DIRS = {
    '.git', '__pycache__', 'node_modules', 'venv', '.venv',
    'dist', 'build', '.pytest_cache', '.mypy_cache',
    'htmlcov', '.tox', 'eggs', '*.egg-info', '.local',
    '_runtime', '.devcontainer', 'cache', 'temp', 'tmp'
}

# 除外するファイル
EXCLUDE_FILES = {
    '.DS_Store', 'Thumbs.db', '*.pyc', '*.pyo', '*.so',
    '*.dll', '*.exe', '*.bin', '*.pkl', '*.model',
    '*.jpg', '*.jpeg', '*.png', '*.gif', '*.ico', '*.svg',
    '*.pdf', '*.doc', '*.docx', '*.xls', '*.xlsx',
    '*.zip', '*.tar', '*.gz', '*.rar', '*.7z',
    '*.mp3', '*.mp4', '*.wav', '*.avi', '*.mov',
    '*.woff', '*.woff2', '*.ttf', '*.eot',
    'package-lock.json', 'yarn.lock', '*.lock'
}

# コードファイルの拡張子
CODE_EXTENSIONS = {
    '.py', '.js', '.ts', '.tsx', '.jsx', '.html', '.css', '.scss',
    '.json', '.yaml', '.yml', '.toml', '.ini', '.cfg',
    '.md', '.txt', '.sh', '.bash', '.ps1', '.bat',
    '.sql', '.dockerfile', '.env.example'
}

# 機密キーのパターン
SECRET_PATTERNS = [
    (r'(SUPABASE_KEY\s*=\s*["\']?)([^"\'"\s]+)', r'\1***REDACTED***'),
    (r'(SUPABASE_SERVICE_ROLE_KEY\s*=\s*["\']?)([^"\'"\s]+)', r'\1***REDACTED***'),
    (r'(OPENAI_API_KEY\s*=\s*["\']?)([^"\'"\s]+)', r'\1***REDACTED***'),
    (r'(ANTHROPIC_API_KEY\s*=\s*["\']?)([^"\'"\s]+)', r'\1***REDACTED***'),
    (r'(GOOGLE_API_KEY\s*=\s*["\']?)([^"\'"\s]+)', r'\1***REDACTED***'),
    (r'(API_KEY\s*=\s*["\']?)([^"\'"\s]+)', r'\1***REDACTED***'),
    (r'(SECRET_KEY\s*=\s*["\']?)([^"\'"\s]+)', r'\1***REDACTED***'),
    (r'(PASSWORD\s*=\s*["\']?)([^"\'"\s]+)', r'\1***REDACTED***'),
    (r'(TOKEN\s*=\s*["\']?)([^"\'"\s]+)', r'\1***REDACTED***'),
    (r'(PRIVATE_KEY\s*=\s*["\']?)([^"\'"\s]+)', r'\1***REDACTED***'),
    (r'("private_key"\s*:\s*")([^"]+)', r'\1***REDACTED***'),
    (r'("client_secret"\s*:\s*")([^"]+)', r'\1***REDACTED***'),
    (r'(eyJ[a-zA-Z0-9_-]+\.eyJ[a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]+)', '***JWT_REDACTED***'),
    (r'(sk-[a-zA-Z0-9]{20,})', '***OPENAI_KEY_REDACTED***'),
    (r'(ghp_[a-zA-Z0-9]{36})', '***GITHUB_TOKEN_REDACTED***'),
]


def should_exclude_dir(name):
    """ディレクトリを除外すべきか"""
    return name in EXCLUDE_DIRS or name.startswith('.')


def should_exclude_file(name):
    """ファイルを除外すべきか"""
    if name in EXCLUDE_FILES:
        return True
    for pattern in EXCLUDE_FILES:
        if pattern.startswith('*') and name.endswith(pattern[1:]):
            return True
    return False


def is_code_file(name):
    """コードファイルかどうか"""
    ext = Path(name).suffix.lower()
    return ext in CODE_EXTENSIONS or name in {'.env', '.env.example', 'Dockerfile', 'Makefile'}


def redact_secrets(content):
    """機密情報を伏字化"""
    for pattern, replacement in SECRET_PATTERNS:
        content = re.sub(pattern, replacement, content, flags=re.IGNORECASE)
    return content


def get_tree(root_path, prefix=""):
    """ディレクトリツリーを生成"""
    lines = []
    root = Path(root_path)

    items = sorted(root.iterdir(), key=lambda x: (x.is_file(), x.name.lower()))
    dirs = [i for i in items if i.is_dir() and not should_exclude_dir(i.name)]
    files = [i for i in items if i.is_file() and not should_exclude_file(i.name)]

    all_items = dirs + files

    for i, item in enumerate(all_items):
        is_last = i == len(all_items) - 1
        connector = "└── " if is_last else "├── "

        if item.is_dir():
            lines.append(f"{prefix}{connector}{item.name}/")
            extension = "    " if is_last else "│   "
            lines.extend(get_tree(item, prefix + extension))
        else:
            lines.append(f"{prefix}{connector}{item.name}")

    return lines


def collect_code(root_path):
    """全コードを収集"""
    files_content = []
    root = Path(root_path)

    for path in sorted(root.rglob('*')):
        # 除外ディレクトリ内のファイルはスキップ
        if any(part in EXCLUDE_DIRS or part.startswith('.') for part in path.parts):
            continue

        if path.is_file() and not should_exclude_file(path.name):
            if is_code_file(path.name) or path.name == '.env':
                try:
                    rel_path = path.relative_to(root)
                    content = path.read_text(encoding='utf-8', errors='ignore')
                    content = redact_secrets(content)
                    files_content.append({
                        'path': str(rel_path),
                        'content': content
                    })
                except Exception as e:
                    files_content.append({
                        'path': str(rel_path),
                        'content': f'# Error reading file: {e}'
                    })

    return files_content


def main():
    project_root = Path(__file__).resolve().parent.parent.parent
    output_file = project_root / 'PROJECT_SNAPSHOT.md'

    print(f"プロジェクト収集中: {project_root}")

    # ツリー生成
    print("ディレクトリ構造を収集中...")
    tree_lines = get_tree(project_root)

    # コード収集
    print("コードファイルを収集中...")
    code_files = collect_code(project_root)

    # 出力
    print(f"出力ファイル生成中: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# Project Snapshot\n\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write("## Directory Structure\n\n")
        f.write("```\n")
        f.write(f"{project_root.name}/\n")
        f.write('\n'.join(tree_lines))
        f.write("\n```\n\n")

        f.write("## Source Files\n\n")
        for file_info in code_files:
            ext = Path(file_info['path']).suffix or 'text'
            f.write(f"### {file_info['path']}\n\n")
            f.write(f"```{ext.lstrip('.')}\n")
            f.write(file_info['content'])
            if not file_info['content'].endswith('\n'):
                f.write('\n')
            f.write("```\n\n")

    print(f"\n完了! {len(code_files)} ファイルを収集しました")
    print(f"出力: {output_file}")


if __name__ == '__main__':
    main()
```

### scripts\utils\fix_database.py

```py
"""
Supabaseのprocessing_lockテーブルを自動修正するスクリプト
"""
import os
from dotenv import load_dotenv
from supabase import create_client

load_dotenv()

# Supabase接続
url = os.getenv('SUPABASE_URL')
key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
client = create_client(url, key)

print("=== processing_lockテーブル修正開始 ===\n")

# 1. 現在のテーブル構造を確認
print("1. 現在のテーブル状態を確認中...")
try:
    result = client.table('processing_lock').select('*').execute()
    print(f"   既存レコード数: {len(result.data)}")
    if result.data:
        print(f"   既存カラム: {list(result.data[0].keys())}")
except Exception as e:
    print(f"   エラー: {e}")

# 2. カラム追加（RPC経由でSQLを実行）
print("\n2. 必要なカラムを追加中...")
sql = """
ALTER TABLE processing_lock
ADD COLUMN IF NOT EXISTS current_index INTEGER DEFAULT 0,
ADD COLUMN IF NOT EXISTS total_count INTEGER DEFAULT 0,
ADD COLUMN IF NOT EXISTS current_file TEXT DEFAULT '',
ADD COLUMN IF NOT EXISTS success_count INTEGER DEFAULT 0,
ADD COLUMN IF NOT EXISTS error_count INTEGER DEFAULT 0,
ADD COLUMN IF NOT EXISTS logs JSONB DEFAULT '[]'::jsonb,
ADD COLUMN IF NOT EXISTS cpu_percent REAL DEFAULT 0.0,
ADD COLUMN IF NOT EXISTS memory_percent REAL DEFAULT 0.0,
ADD COLUMN IF NOT EXISTS memory_used_gb REAL DEFAULT 0.0,
ADD COLUMN IF NOT EXISTS memory_total_gb REAL DEFAULT 0.0,
ADD COLUMN IF NOT EXISTS throttle_delay REAL DEFAULT 0.0,
ADD COLUMN IF NOT EXISTS adjustment_count INTEGER DEFAULT 0,
ADD COLUMN IF NOT EXISTS max_parallel INTEGER DEFAULT 3,
ADD COLUMN IF NOT EXISTS current_workers INTEGER DEFAULT 0;
"""

try:
    client.rpc('exec_sql', {'sql': sql}).execute()
    print("   ✓ カラム追加完了")
except Exception as e:
    # RPCが使えない場合、postgrestでDDLは実行できないので、手動実行が必要
    print(f"   ⚠ RPCでカラム追加できませんでした: {e}")
    print("   → 以下のSQLをSupabase SQL Editorで手動実行してください：")
    print(sql)

# 3. id=1のレコードを確認・作成
print("\n3. 初期レコード（id=1）を確認中...")
try:
    result = client.table('processing_lock').select('*').eq('id', 1).execute()
    if not result.data:
        print("   初期レコードが存在しません。作成します...")
        client.table('processing_lock').insert({
            'id': 1,
            'is_processing': False,
            'max_parallel': 3,
            'current_workers': 0,
            'current_index': 0,
            'total_count': 0,
            'current_file': '',
            'success_count': 0,
            'error_count': 0,
            'logs': [],
            'cpu_percent': 0.0,
            'memory_percent': 0.0,
            'memory_used_gb': 0.0,
            'memory_total_gb': 0.0,
            'throttle_delay': 0.0,
            'adjustment_count': 0
        }).execute()
        print("   ✓ 初期レコード作成完了")
    else:
        print("   ✓ 初期レコードは既に存在します")
        print(f"   内容: {result.data[0]}")
except Exception as e:
    print(f"   エラー: {e}")

print("\n=== 完了 ===")
print("ブラウザでCloud Runのページを開いて、処理を開始してください。")
print("リアルタイム表示が動作するはずです。")
```

### scripts\utils\fix_rls.py

```py
import os
import sys
from dotenv import load_dotenv
from supabase import create_client

sys.stdout.reconfigure(encoding='utf-8')
load_dotenv()

url = os.getenv('SUPABASE_URL')
key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
client = create_client(url, key)

print("=== RLS無効化 ===\n")

# RLSを無効化するSQLを実行
# postgrest経由では実行できないため、RPCを作成する必要がある
# 代わりに、データを直接確認して問題を特定

# まずデータを取得
result = client.table('processing_lock').select('*').eq('id', 1).execute()

if result.data:
    data = result.data[0]
    print("取得できたカラム:")
    for key in data.keys():
        print(f"  {key}")

    print("\n確認: cpu_percent が存在するか?")
    if 'cpu_percent' in data:
        print(f"  はい: {data['cpu_percent']}")
    else:
        print("  いいえ - RLSまたはカラムが存在しない問題")
        print("\n解決方法: Supabase Dashboard > Authentication > Policies")
        print("  processing_lock テーブルの SELECT policy を全て削除するか、")
        print("  または全カラムへのアクセスを許可してください")
else:
    print("データ取得失敗")
```

### scripts\utils\setup_missing_files.py

```py
"""
セットアップスクリプト: 不足しているディレクトリとファイルを作成
"""
import os
import json
from pathlib import Path

# プロジェクトルート
project_root = Path(__file__).parent

# frontend/schemas ディレクトリを作成
schemas_dir = project_root / "frontend" / "schemas"
schemas_dir.mkdir(parents=True, exist_ok=True)
print(f"✓ ディレクトリ作成: {schemas_dir}")

# generic.json スキーマファイルを作成
generic_schema = {
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic Document Schema",
    "type": "object",
    "properties": {
        "title": {
            "type": ["string", "null"],
            "title": "タイトル",
            "description": "ドキュメントのタイトル"
        },
        "summary": {
            "type": ["string", "null"],
            "title": "要約",
            "description": "ドキュメントの要約"
        },
        "document_date": {
            "type": ["string", "null"],
            "format": "date",
            "title": "文書日付",
            "description": "文書の日付（YYYY-MM-DD）"
        },
        "tags": {
            "type": "array",
            "title": "タグ",
            "description": "ドキュメントに関連するタグ",
            "items": {
                "type": "string"
            }
        },
        "category": {
            "type": ["string", "null"],
            "title": "カテゴリー",
            "description": "ドキュメントのカテゴリー"
        },
        "sender": {
            "type": ["string", "null"],
            "title": "送信者",
            "description": "送信者名またはメールアドレス"
        },
        "subject": {
            "type": ["string", "null"],
            "title": "件名",
            "description": "メールまたは文書の件名"
        },
        "content_type": {
            "type": ["string", "null"],
            "title": "内容タイプ",
            "description": "文書の内容タイプ"
        }
    },
    "required": []
}

generic_schema_file = schemas_dir / "generic.json"
with open(generic_schema_file, 'w', encoding='utf-8') as f:
    json.dump(generic_schema, f, ensure_ascii=False, indent=2)
print(f"✓ ファイル作成: {generic_schema_file}")

print("\n✅ セットアップ完了!")
```

### scripts\utils\test_flask_startup.py

```py
"""
Flaskアプリの起動テスト
環境変数の読み込みと依存関係をチェック
"""
import sys
from pathlib import Path

# プロジェクトルートをパスに追加
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

print("=" * 70)
print("🔍 Flask アプリ起動テスト")
print("=" * 70)

# Step 1: 環境変数の読み込みテスト
print("\n[Step 1] 環境変数の読み込みテスト...")
try:
    from shared.common.config.settings import settings
    print(f"  ✓ SUPABASE_URL: {settings.SUPABASE_URL[:30]}..." if settings.SUPABASE_URL else "  ✗ SUPABASE_URL: 未設定")
    print(f"  ✓ SUPABASE_KEY: {'設定済み' if settings.SUPABASE_KEY else '未設定'}")
    print(f"  ✓ OPENAI_API_KEY: {'設定済み' if settings.OPENAI_API_KEY else '未設定'}")
    print(f"  ✓ GOOGLE_AI_API_KEY: {'設定済み' if settings.GOOGLE_AI_API_KEY else '未設定'}")

    if not all([settings.SUPABASE_URL, settings.SUPABASE_KEY, settings.OPENAI_API_KEY, settings.GOOGLE_AI_API_KEY]):
        print("\n  ⚠️ 一部の環境変数が未設定です")
    else:
        print("\n  ✅ 全ての環境変数が設定されています")
except Exception as e:
    print(f"  ❌ エラー: {e}")
    sys.exit(1)

# Step 2: 依存モジュールのインポートテスト
print("\n[Step 2] 依存モジュールのインポートテスト...")
modules_to_test = [
    "shared.common.database.client",
    "shared.ai.llm_client.llm_client",
    "shared.common.utils.query_expansion",
    "shared.common.utils.context_extractor",
    "shared.common.config.yaml_loader",
    "shared.common.config.model_tiers",
]

all_ok = True
for module_name in modules_to_test:
    try:
        __import__(module_name)
        print(f"  ✓ {module_name}")
    except Exception as e:
        print(f"  ❌ {module_name}: {e}")
        all_ok = False

if not all_ok:
    print("\n  ⚠️ 一部のモジュールのインポートに失敗しました")
else:
    print("\n  ✅ 全てのモジュールのインポートに成功しました")

# Step 3: データベース接続テスト
print("\n[Step 3] データベース接続テスト...")
try:
    from shared.common.database.client import DatabaseClient
    db_client = DatabaseClient()

    # 簡単なクエリを実行
    response = db_client.client.table('Rawdata_FILE_AND_MAIL').select('id').limit(1).execute()
    print(f"  ✓ データベース接続成功")
    print(f"  ✓ テーブルアクセス成功")
except Exception as e:
    print(f"  ❌ データベース接続エラー: {e}")
    all_ok = False

# Step 4: Flaskアプリのインポートテスト
print("\n[Step 4] Flaskアプリのインポートテスト...")
try:
    # services/doc-search をパスに追加
    doc_search_path = project_root / "services" / "doc-search"
    sys.path.insert(0, str(doc_search_path))

    # appモジュールをインポート
    import app as flask_app
    print(f"  ✓ Flaskアプリのインポート成功")
    print(f"  ✓ エンドポイント数: {len(flask_app.app.url_map._rules)}")
except Exception as e:
    print(f"  ❌ Flaskアプリのインポートエラー: {e}")
    import traceback
    traceback.print_exc()
    all_ok = False

# 最終結果
print("\n" + "=" * 70)
if all_ok:
    print("✅ 全てのテストに成功しました!")
    print("\nFlaskアプリを起動するには:")
    print("  cd C:\\Users\\ookub\\document-management-system")
    print("  python services\\doc-search\\app.py")
else:
    print("❌ 一部のテストに失敗しました。上記のエラーを確認してください。")
print("=" * 70)
```

### scripts\utils\watch_updates.py

```py
import os
import sys
import time
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client

sys.stdout.reconfigure(encoding='utf-8')
load_dotenv()

url = os.getenv('SUPABASE_URL')
key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
client = create_client(url, key)

print("=== processing_lock 更新監視 ===")
print("2秒ごとにチェックします...\n")

last_updated = None

for i in range(10):
    result = client.table('processing_lock').select('updated_at,cpu_percent,memory_percent,current_index,total_count,is_processing').eq('id', 1).execute()

    if result.data:
        data = result.data[0]
        now = datetime.now().strftime('%H:%M:%S')

        if last_updated != data['updated_at']:
            print(f"[{now}] 🔄 更新されました！")
            print(f"  updated_at: {data['updated_at']}")
            print(f"  cpu_percent: {data['cpu_percent']}")
            print(f"  memory_percent: {data['memory_percent']}")
            print(f"  進捗: {data['current_index']}/{data['total_count']}")
            print(f"  is_processing: {data['is_processing']}")
            print()
            last_updated = data['updated_at']
        else:
            print(f"[{now}] ⏸️  変化なし (updated_at: {data['updated_at']})")

    time.sleep(2)

print("\n監視終了")
```

### services\data-ingestion\__init__.py

```py

```

### services\data-ingestion\common\__init__.py

```py
"""
共通商品取り込みモジュール
"""

from .base_product_ingestion import BaseProductIngestionPipeline

__all__ = ["BaseProductIngestionPipeline"]
```

### services\data-ingestion\common\base_product_ingestion.py

```py
"""
共通商品取り込みパイプライン基盤クラス
全ネットスーパースクリプトで共有する処理を定義
"""

from abc import ABC, abstractmethod
from datetime import date, datetime
from typing import Dict, List, Optional, Set
from uuid import UUID
import uuid
import os

from shared.common.database.client import DatabaseClient
from shared.ai.llm_client.llm_client import LLMClient
from loguru import logger
from openai import OpenAI


class BaseProductIngestionPipeline(ABC):
    """商品取り込みの共通基盤クラス"""

    def __init__(self, organization_name: str, headless: bool = True):
        """
        初期化

        Args:
            organization_name: 組織名（東急ストア、楽天西友、ダイエー）
            headless: ブラウザのヘッドレスモード
        """
        self.organization_name = organization_name
        self.headless = headless
        self.db = DatabaseClient(use_service_role=True)
        self.llm_client = LLMClient()
        self.scraper = None

        # OpenAI clientの初期化（embedding生成用）
        openai_api_key = ***REDACTED***"OPENAI_API_KEY")
        if openai_api_key:
            self.openai_client = OpenAI(api_key=***REDACTED***
            self.embedding_enabled = True
            logger.info("OpenAI Embedding機能が有効化されました")
        else:
            self.openai_client = None
            self.embedding_enabled = False
            logger.warning("OPENAI_API_KEYが設定されていません。Embedding生成は無効です")

    @abstractmethod
    async def start(self) -> bool:
        """
        スクレイパーの起動とログイン
        各ストア固有の実装が必要
        """
        pass

    @abstractmethod
    async def close(self):
        """スクレイパーのクリーンアップ"""
        pass

    async def check_existing_products(self, jan_codes: List[str]) -> Set[str]:
        """
        既存商品のチェック（JANコードで重複排除）

        Args:
            jan_codes: チェックするJANコードリスト

        Returns:
            既存のJANコードのセット
        """
        if not jan_codes:
            return set()

        # 空文字・Noneを除外
        valid_jan_codes = [code for code in jan_codes if code]

        if not valid_jan_codes:
            return set()

        result = self.db.client.table('Rawdata_NETSUPER_items').select(
            'jan_code'
        ).in_('jan_code', valid_jan_codes).execute()

        return {row['jan_code'] for row in result.data if row.get('jan_code')}

    async def check_existing_products_by_name(self, products: List[Dict]) -> Dict[tuple, str]:
        """
        既存商品のチェック（商品名+組織で重複排除）
        JANコードがない商品用

        Args:
            products: 商品データのリスト（product_nameとjan_codeを含む）

        Returns:
            (product_name, organization) -> product_id のマッピング
        """
        # JANコードがない商品のみを抽出
        no_jan_products = [p for p in products if not p.get('jan_code')]

        if not no_jan_products:
            return {}

        # 商品名のリストを作成
        product_names = list(set([p.get('product_name') for p in no_jan_products if p.get('product_name')]))

        if not product_names:
            return {}

        # 該当する商品名と組織の組み合わせで既存レコードを検索
        result = self.db.client.table('Rawdata_NETSUPER_items').select(
            'id, product_name, organization'
        ).in_('product_name', product_names).eq(
            'organization', self.organization_name
        ).is_('jan_code', 'null').execute()

        # (product_name, organization) -> id のマッピングを作成
        existing_map = {}
        for row in result.data:
            key = (row['product_name'], row['organization'])
            existing_map[key] = row['id']

        return existing_map

    def _generate_embedding(self, text: str) -> Optional[List[float]]:
        """
        商品名からembeddingを生成

        Args:
            text: 商品名

        Returns:
            1536次元のベクトル（失敗時はNone）
        """
        if not self.embedding_enabled or not self.openai_client:
            return None

        if not text:
            return None

        try:
            response = self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Embedding生成エラー: {e}")
            return None

    def _prepare_product_data(
        self,
        product: Dict,
        category_name: Optional[str] = None,
        general_name: Optional[str] = None,
        category_id: Optional[UUID] = None,
        confidence: Optional[float] = None
    ) -> Dict:
        """
        商品データの正規化と準備

        Args:
            product: スクレイパーから取得した生データ
            category_name: カテゴリ名（スクレイパー取得値）
            general_name: 一般名詞（分類済みの場合）
            category_id: カテゴリID（分類済みの場合）
            confidence: 分類信頼度

        Returns:
            データベース挿入用の正規化済みデータ
        """
        today = date.today()

        # 商品名を取得（サイト表記のまま保存）
        product_name = product.get("product_name", "")

        # 価格のパース（本体価格と税込価格の両方）
        # 本体価格（税抜）
        price = product.get("price")
        current_price = None
        if price is not None:
            try:
                if isinstance(price, (int, float)):
                    current_price = float(price)
                else:
                    current_price = float(str(price).replace(",", "").replace("円", "").replace("¥", "").strip())
            except (ValueError, AttributeError):
                current_price = None

        # 税込価格
        price_tax_included = product.get("price_tax_included")
        current_price_tax_included = None
        if price_tax_included is not None:
            try:
                if isinstance(price_tax_included, (int, float)):
                    current_price_tax_included = float(price_tax_included)
                else:
                    current_price_tax_included = float(str(price_tax_included).replace(",", "").replace("円", "").replace("¥", "").strip())
            except (ValueError, AttributeError):
                current_price_tax_included = None

        # price_text: 元のテキスト形式を保持（本体価格 / 税込価格）
        price_text_parts = []
        if price is not None:
            price_text_parts.append(f"本体¥{price}")
        if price_tax_included is not None:
            price_text_parts.append(f"税込¥{price_tax_included}")
        price_text = " / ".join(price_text_parts) if price_text_parts else ""

        # メタデータ
        metadata = {
            "raw_data": product,
            "scraping_timestamp": datetime.now().isoformat()
        }

        # データ構築
        data = {
            # 基本情報
            "source_type": "online_supermarket",
            "workspace": "shopping",
            "doc_type": "online_grocery_item",
            "organization": self.organization_name,

            # 商品情報
            "product_name": product_name,
            "jan_code": product.get("jan_code"),

            # 価格情報
            "current_price": current_price,
            "current_price_tax_included": current_price_tax_included,
            "price_text": price_text,

            # 分類情報
            "category": category_name,
            "general_name": general_name,
            "category_id": str(category_id) if category_id else None,
            "classification_confidence": confidence,
            "needs_approval": general_name is None,  # 未分類の場合は承認待ち

            # その他
            "manufacturer": product.get("manufacturer"),
            "image_url": product.get("image_url"),
            "in_stock": product.get("in_stock", True),
            "is_available": product.get("is_available", True),

            # メタデータ
            "metadata": metadata,
            "document_date": today.isoformat(),
            "last_scraped_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }

        # 複数Embeddingを生成（ハイブリッド検索用）
        # 1. general_name_embedding
        if general_name:
            general_name_emb = self._generate_embedding(general_name)
            if general_name_emb:
                data["general_name_embedding"] = '[' + ','.join(map(str, general_name_emb)) + ']'
                logger.debug(f"general_name_embedding生成成功: {general_name[:30]}...")

        # 2. small_category_embedding
        if category_name:
            category_emb = self._generate_embedding(category_name)
            if category_emb:
                data["small_category_embedding"] = '[' + ','.join(map(str, category_emb)) + ']'
                logger.debug(f"small_category_embedding生成成功: {category_name[:30]}...")

        # 3. keywords_embedding
        keywords = product.get("keywords")
        if keywords:
            # キーワードを文字列に変換
            if isinstance(keywords, list):
                keywords_text = " ".join(str(k) for k in keywords if k)
            else:
                keywords_text = str(keywords)

            if keywords_text.strip():
                keywords_emb = self._generate_embedding(keywords_text)
                if keywords_emb:
                    data["keywords_embedding"] = '[' + ','.join(map(str, keywords_emb)) + ']'
                    logger.debug(f"keywords_embedding生成成功: {keywords_text[:30]}...")

        return data

    async def process_category_page(
        self,
        category_url: str,
        page: int = 1,
        category_name: Optional[str] = None
    ) -> Dict:
        """
        カテゴリページの処理（共通ロジック）

        Args:
            category_url: カテゴリURL
            page: ページ番号
            category_name: カテゴリ名

        Returns:
            処理結果
        """
        if not self.scraper:
            raise RuntimeError("Scraper not initialized. Call start() first.")

        logger.info(f"ページ {page} を処理中...")

        # スクレイパー固有の実装を呼び出し（タプル返却）
        products, pagination_info = await self.scraper.fetch_products_page(category_url, page)

        # ページネーション情報をログ出力
        if page == 1 and pagination_info:
            logger.info(f"ページネーション情報: {pagination_info}")

        if not products:
            logger.warning("商品が見つかりませんでした")
            return {
                "success": True,
                "total_products": 0,
                "new_products": 0,
                "updated_products": 0,
                "pagination_info": pagination_info
            }

        # 商品データの正規化と保存
        jan_codes = [p.get("jan_code") for p in products if p.get("jan_code")]
        existing_jan_codes = await self.check_existing_products(jan_codes)

        # JANコードがない商品の既存チェック（商品名+組織）
        existing_by_name = await self.check_existing_products_by_name(products)

        insert_count = 0
        update_count = 0

        for product in products:
            # 商品データ準備（分類は後で実施）
            product_data = self._prepare_product_data(product, category_name)
            jan_code = product.get("jan_code")
            product_name = product.get("product_name")

            try:
                if jan_code and jan_code in existing_jan_codes:
                    # JANコードで既存商品を更新
                    # general_name と keywords は除外（AI生成済みデータを保持）
                    update_data = {k: v for k, v in product_data.items() if k not in ['general_name', 'keywords']}
                    self.db.client.table('Rawdata_NETSUPER_items').update(
                        update_data
                    ).eq('jan_code', jan_code).execute()
                    update_count += 1
                elif not jan_code and (product_name, self.organization_name) in existing_by_name:
                    # JANコードなし商品を商品名+組織で更新
                    # general_name と keywords は除外（AI生成済みデータを保持）
                    existing_id = existing_by_name[(product_name, self.organization_name)]
                    update_data = {k: v for k, v in product_data.items() if k not in ['general_name', 'keywords']}
                    self.db.client.table('Rawdata_NETSUPER_items').update(
                        update_data
                    ).eq('id', existing_id).execute()
                    update_count += 1
                else:
                    # 新規商品を挿入
                    product_data["created_at"] = datetime.now().isoformat()
                    self.db.client.table('Rawdata_NETSUPER_items').insert(
                        product_data
                    ).execute()
                    insert_count += 1

            except Exception as e:
                logger.error(f"Failed to save product {product_name}: {e}")

        logger.info(f"✅ 処理完了: 合計{len(products)}件（新規{insert_count}件、更新{update_count}件）")

        return {
            "success": True,
            "total_products": len(products),
            "new_products": insert_count,
            "updated_products": update_count,
            "pagination_info": pagination_info
        }

    async def process_category_all_pages(
        self,
        category_url: str,
        category_name: Optional[str] = None,
        max_pages: int = 100
    ) -> Dict:
        """
        カテゴリの全ページを処理

        Args:
            category_url: カテゴリURL
            category_name: カテゴリ名
            max_pages: 最大ページ数

        Returns:
            処理結果
        """
        logger.info(f"カテゴリー '{category_name}' の全ページ処理開始")

        page = 1
        total_products = 0
        total_new = 0
        total_updated = 0
        total_pages_from_pagination = None

        while page <= max_pages:
            result = await self.process_category_page(category_url, page, category_name)

            # 初回のページネーション情報を取得
            if page == 1 and result.get("pagination_info"):
                pagination = result["pagination_info"]
                total_pages_from_pagination = pagination.get("totalPages")
                if total_pages_from_pagination:
                    logger.info(f"📄 検出された総ページ数: {total_pages_from_pagination}")
                    # 総ページ数がmax_pagesより少ない場合、max_pagesを更新
                    if total_pages_from_pagination < max_pages:
                        max_pages = total_pages_from_pagination
                        logger.info(f"✅ 処理ページ数を {total_pages_from_pagination} に制限")

            if not result.get("success") or result.get("total_products", 0) == 0:
                logger.info(f"ページ {page} で商品なし、カテゴリー処理終了")
                break

            total_products += result.get("total_products", 0)
            total_new += result.get("new_products", 0)
            total_updated += result.get("updated_products", 0)

            # ページネーション情報に基づく終了判定
            if total_pages_from_pagination and page >= total_pages_from_pagination:
                logger.info(f"✅ 全{total_pages_from_pagination}ページの処理完了")
                break

            page += 1

        logger.info(f"✅ カテゴリー '{category_name}' 完了")
        logger.info(f"   合計: {total_products}件（新規{total_new}件、更新{total_updated}件）")

        return {
            "success": True,
            "category_url": category_url,
            "category_name": category_name,
            "total_products": total_products,
            "new_products": total_new,
            "updated_products": total_updated,
            "pages_processed": page - 1,
            "total_pages": total_pages_from_pagination
        }
```

### services\data-ingestion\common\category_config.json

```json
{
  "rakuten_seiyu": [
    {
      "name": "【パワーアップ迎春】今年最後の超特価！！",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/700300/?l-id=_leftnavi_700300&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "スゴ得",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/120004/?l-id=_leftnavi_120004&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "ただ今お得",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/120002/?l-id=_leftnavi_120002&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "西友オリジナル・旬の特集",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/120003/?l-id=_leftnavi_120003&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "野菜",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110001/?l-id=_leftnavi_110001&sort=1",
      "enabled": true,
      "start_date": "2026-01-04",
      "interval_days": 7,
      "last_run": "2025-12-27",
      "notes": ""
    },
    {
      "name": "果物",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110002/?l-id=_leftnavi_110002&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "お肉",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110003/?l-id=_leftnavi_110003&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "お魚",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110004/?l-id=_leftnavi_110004&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "お惣菜・お弁当",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110005/?l-id=_leftnavi_110005&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "ハム・ソーセージ・チルド調理品",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110006/?l-id=_leftnavi_110006&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "卵・牛乳・乳製品",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110007/?l-id=_leftnavi_110007&sort=1",
      "enabled": true,
      "start_date": "2026-01-01",
      "interval_days": 7,
      "last_run": "2025-12-24",
      "notes": ""
    },
    {
      "name": "豆腐・納豆・漬物・練物",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110008/?l-id=_leftnavi_110008&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "冷凍食品・アイス",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110009/?l-id=_leftnavi_110009&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "お米・麺・パスタ",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110010/?l-id=_leftnavi_110010&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "パン・ジャム・シリアル",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110011/?l-id=_leftnavi_110011&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "食油・カレー・スープ・調味料",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110012/?l-id=_leftnavi_110012&sort=1",
      "enabled": true,
      "start_date": "2026-01-04",
      "interval_days": 7,
      "last_run": "2025-12-27",
      "notes": ""
    },
    {
      "name": "缶詰・粉類・乾物",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110013/?l-id=_leftnavi_110013&sort=1",
      "enabled": true,
      "start_date": "2026-01-04",
      "interval_days": 7,
      "last_run": "2025-12-27",
      "notes": ""
    },
    {
      "name": "お菓子・スイーツ",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110014/?l-id=_leftnavi_110014&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "飲料・お水",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110015/?l-id=_leftnavi_110015&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "お酒・ノンアルコール",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110016/?l-id=_leftnavi_110016&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "紙・生理用品・介護",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110017/?l-id=_leftnavi_110017&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "美容・衛生",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110018/?l-id=_leftnavi_110018&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "日用品・雑貨",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110019/?l-id=_leftnavi_110019&sort=1",
      "enabled": true,
      "start_date": "2026-01-07",
      "interval_days": 7,
      "last_run": "2025-12-30",
      "notes": ""
    },
    {
      "name": "キッチン用品",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110020/?l-id=_leftnavi_110020&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "ベビー",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110021/?l-id=_leftnavi_110021&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "ペット",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/110022/?l-id=_leftnavi_110022&sort=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "よりどり割でお得",
      "url": "https://netsuper.rakuten.co.jp/seiyu/choice?l-id=_leftnavi_choice_top",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "【パワーアップ迎春】年末年始の超特価！！",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/700300/?l-id=_leftnavi_700300&sort=1",
      "enabled": true,
      "start_date": "2025-12-30",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "その他",
      "url": "https://netsuper.rakuten.co.jp/seiyu/search/880001/?l-id=_leftnavi_880001&sort=1",
      "enabled": true,
      "start_date": "2025-12-30",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    }
  ],
  "tokyu_store": [
    {
      "name": "野菜",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC10",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "果物",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC11",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "お魚",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC20",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "お肉",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC30",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "惣菜",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC40",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "牛乳・乳製品・卵",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC50",
      "enabled": true,
      "start_date": "2026-01-04",
      "interval_days": 7,
      "last_run": "2025-12-27",
      "notes": ""
    },
    {
      "name": "パン・生菓子・シリアル",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC51",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "チルド総菜・豆腐・納豆・漬物など",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC52",
      "enabled": true,
      "start_date": "2026-01-04",
      "interval_days": 7,
      "last_run": "2025-12-27",
      "notes": ""
    },
    {
      "name": "冷凍食品・アイス",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC53",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "米・餅",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC54",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "麺類",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC55",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "乾物・瓶缶詰・粉類",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC56",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "調味料・中華材料",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC57",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "お菓子",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC58",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "水・飲料",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC59",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "酒類",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC60",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "ケース販売商品",
      "url": "https://ns.tokyu-bell.jp/shop/r/rJ20",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "ベビーフード他",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC81",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "日用雑貨",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC80",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "ペット用品",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC80",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "プライベートブランド",
      "url": "https://ns.tokyu-bell.jp/shop/r/rJ10",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "有機野菜",
      "url": "https://ns.tokyu-bell.jp/shop/r/rJ30",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "flower & green",
      "url": "https://ns.tokyu-bell.jp/shop/c/cC82",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": ""
    }
  ],
  "daiei": [
    {
      "name": "野菜・果物",
      "url": "https://daiei.eorder.ne.jp/category/vegetables",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 14,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "肉・加工品",
      "url": "https://netsuper.daiei.co.jp/0726/item/item.php?classL=2&classS=1&ilc_code=1002",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 14,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "魚",
      "url": "https://netsuper.daiei.co.jp/0726/item/item.php?classL=2&classS=1&ilc_code=1003",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 14,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "惣菜・弁当",
      "url": "https://netsuper.daiei.co.jp/0726/item/item.php?classL=2&classS=1&ilc_code=1004",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 14,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "パン・乳製品",
      "url": "https://netsuper.daiei.co.jp/0726/item/item.php?classL=2&classS=1&ilc_code=1005",
      "enabled": true,
      "start_date": "2026-01-12",
      "interval_days": 14,
      "last_run": "2025-12-28",
      "notes": ""
    },
    {
      "name": "冷凍食品・アイス",
      "url": "https://netsuper.daiei.co.jp/0726/item/item.php?classL=2&classS=1&ilc_code=1006",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 14,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "冷蔵食品",
      "url": "https://netsuper.daiei.co.jp/0726/item/item.php?classL=2&classS=1&ilc_code=1007",
      "enabled": true,
      "start_date": "2026-01-11",
      "interval_days": 14,
      "last_run": "2025-12-27",
      "notes": ""
    },
    {
      "name": "調味料・即席食品",
      "url": "https://netsuper.daiei.co.jp/0726/item/item.php?classL=2&classS=1&ilc_code=1008",
      "enabled": true,
      "start_date": "2026-01-11",
      "interval_days": 14,
      "last_run": "2025-12-27",
      "notes": ""
    },
    {
      "name": "米・乾物",
      "url": "https://netsuper.daiei.co.jp/0726/item/item.php?classL=2&classS=1&ilc_code=1009",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 14,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "菓子",
      "url": "https://daiei.eorder.ne.jp/category/snacks",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 14,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "飲料",
      "url": "https://daiei.eorder.ne.jp/category/drinks",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 14,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "酒類",
      "url": "https://netsuper.daiei.co.jp/0726/item/item.php?classL=2&classS=1&ilc_code=1012",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 14,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "医薬品",
      "url": "https://netsuper.daiei.co.jp/0726/item/item.php?classL=2&classS=1&ilc_code=1013",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 14,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "健康美容・医薬部外",
      "url": "https://netsuper.daiei.co.jp/0726/item/item.php?classL=2&classS=1&ilc_code=1014",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 14,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "住まい・暮らし",
      "url": "https://netsuper.daiei.co.jp/0726/item/item.php?classL=2&classS=1&ilc_code=1015",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 14,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "ベビー・衣料",
      "url": "https://netsuper.daiei.co.jp/0726/item/item.php?classL=2&classS=1&ilc_code=1016",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 14,
      "last_run": null,
      "notes": ""
    },
    {
      "name": "広告の品",
      "url": "https://netsuper.daiei.co.jp/0726/item/item.php?classL=4&classS=1",
      "enabled": true,
      "start_date": "2025-12-23",
      "interval_days": 7,
      "last_run": null,
      "notes": "特売品・広告商品"
    }
  ]
}
```

### services\data-ingestion\common\category_manager.py

```py
"""
ネットスーパーカテゴリー実行スケジュール管理

カテゴリーごとに次回実行日時とインターバルを指定し、
実行すべきかどうかを判定する機能を提供します。
"""

import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from loguru import logger


class CategoryManager:
    """カテゴリーの実行スケジュールを管理"""

    def __init__(self, config_path: Optional[Path] = None):
        """
        Args:
            config_path: 設定ファイルのパス（指定しない場合はデフォルト）
        """
        if config_path is None:
            # デフォルトパス: B_ingestion/common/category_config.json
            self.config_path = Path(__file__).parent / "category_config.json"
        else:
            self.config_path = Path(config_path)

        self.config: Dict[str, List[Dict[str, Any]]] = {}
        self.load_config()

    def load_config(self):
        """設定ファイルを読み込む"""
        if self.config_path.exists():
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
                logger.info(f"設定ファイルを読み込みました: {self.config_path}")
                # 旧フォーマットから新フォーマットへ自動変換
                self._migrate_old_format()
            except Exception as e:
                logger.error(f"設定ファイル読み込みエラー: {e}")
                self.config = {}
        else:
            logger.warning(f"設定ファイルが見つかりません: {self.config_path}")
            self.config = {}

    def _migrate_old_format(self):
        """旧フォーマット（next_run_datetime）から新フォーマット（start_date）へ変換"""
        migrated = False
        for store_name, categories in self.config.items():
            for category in categories:
                # next_run_datetimeがあり、start_dateがない場合
                if "next_run_datetime" in category and "start_date" not in category:
                    # next_run_datetime（日時）を start_date（日付のみ）に変換
                    next_run_datetime = category["next_run_datetime"]
                    # YYYY-MM-DD HH:MM から YYYY-MM-DD を抽出
                    category["start_date"] = next_run_datetime.split()[0] if " " in next_run_datetime else next_run_datetime
                    del category["next_run_datetime"]
                    migrated = True
                    logger.info(f"カテゴリー {category['name']} を新フォーマットに変換しました")

                # last_run_datetimeをlast_runに変換
                if "last_run_datetime" in category and "last_run" not in category:
                    last_run_datetime = category["last_run_datetime"]
                    if last_run_datetime:
                        category["last_run"] = last_run_datetime.split()[0] if " " in last_run_datetime else last_run_datetime
                    else:
                        category["last_run"] = None
                    del category["last_run_datetime"]
                    migrated = True

        if migrated:
            self.save_config()
            logger.info("設定ファイルを新フォーマットに変換しました")

    def save_config(self):
        """設定ファイルに保存"""
        try:
            self.config_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, ensure_ascii=False, indent=2)
            logger.info(f"設定ファイルを保存しました: {self.config_path}")
        except Exception as e:
            logger.error(f"設定ファイル保存エラー: {e}")

    def initialize_store_categories(
        self,
        store_name: str,
        categories: List[Dict[str, str]],
        default_interval_days: int = 7,
        default_start_date: Optional[str] = None
    ):
        """
        店舗のカテゴリーを初期化

        Args:
            store_name: 店舗名（例: "rakuten_seiyu"）
            categories: カテゴリーリスト [{"name": "xxx", "url": "xxx"}, ...]
            default_interval_days: デフォルトのインターバル日数
            default_start_date: デフォルトの開始日（YYYY-MM-DD）、指定しない場合は明日
        """
        if default_start_date is None:
            # デフォルト: 明日
            tomorrow = datetime.now() + timedelta(days=1)
            default_start_date = tomorrow.strftime("%Y-%m-%d")

        if store_name not in self.config:
            self.config[store_name] = []

        existing_names = {cat["name"] for cat in self.config[store_name]}

        for category in categories:
            if category["name"] not in existing_names:
                self.config[store_name].append({
                    "name": category["name"],
                    "url": category.get("url", ""),
                    "enabled": True,
                    "start_date": default_start_date,
                    "interval_days": default_interval_days,
                    "last_run": None,
                    "notes": ""
                })

        self.save_config()
        logger.info(f"{store_name} のカテゴリーを初期化しました（{len(categories)}件）")

    def should_run_category(
        self,
        store_name: str,
        category_name: str,
        now: Optional[datetime] = None
    ) -> bool:
        """
        カテゴリーを今実行すべきかどうかを判定

        ロジック: 現在日付 >= start_date なら True

        Args:
            store_name: 店舗名
            category_name: カテゴリー名
            now: 現在時刻（指定しない場合は現在時刻）

        Returns:
            実行すべきなら True
        """
        if now is None:
            now = datetime.now()

        if store_name not in self.config:
            logger.warning(f"店舗 {store_name} が設定ファイルに存在しません")
            return False

        category = self._find_category(store_name, category_name)
        if category is None:
            logger.warning(f"カテゴリー {category_name} が見つかりません（店舗: {store_name}）")
            return False

        # 無効化されている場合はスキップ
        if not category.get("enabled", True):
            logger.info(f"カテゴリー {category_name} は無効化されています")
            return False

        # 開始日を取得
        start_date_str = category.get("start_date")
        if not start_date_str:
            logger.warning(f"カテゴリー {category_name} の開始日が設定されていません")
            return False

        # 日付をパース
        try:
            start_date = datetime.strptime(start_date_str, "%Y-%m-%d").date()
        except ValueError as e:
            logger.error(f"開始日のパースエラー: {start_date_str}, {e}")
            return False

        # 判定（日付のみで比較）
        should_run = now.date() >= start_date
        if should_run:
            logger.info(f"カテゴリー {category_name}: 実行可能（開始日: {start_date_str}）")

        return should_run

    def mark_as_run(
        self,
        store_name: str,
        category_name: str,
        run_datetime: Optional[datetime] = None
    ):
        """
        カテゴリーを実行済みとしてマークし、次回開始日を更新

        次回開始日 = 実行日 + interval_days + 1日

        Args:
            store_name: 店舗名
            category_name: カテゴリー名
            run_datetime: 実行日時（指定しない場合は現在時刻）
        """
        if run_datetime is None:
            run_datetime = datetime.now()

        category = self._find_category(store_name, category_name)
        if category is not None:
            # 最終実行日を記録
            category["last_run"] = run_datetime.strftime("%Y-%m-%d")

            # 次回開始日を計算: 実行日 + interval_days + 1
            interval_days = category.get("interval_days", 7)
            next_start_date = run_datetime.date() + timedelta(days=interval_days + 1)
            category["start_date"] = next_start_date.strftime("%Y-%m-%d")

            self.save_config()
            logger.info(f"カテゴリー {category_name} を実行済みとしてマークしました")
            logger.info(f"  最終実行: {category['last_run']}")
            logger.info(f"  次回開始日: {category['start_date']}")
        else:
            logger.warning(f"カテゴリー {category_name} が見つかりません（店舗: {store_name}）")

    def get_start_date(
        self,
        store_name: str,
        category_name: str
    ) -> Optional[str]:
        """
        カテゴリーの開始日を取得

        Returns:
            開始日（YYYY-MM-DD形式）、または None
        """
        category = self._find_category(store_name, category_name)
        if category is None:
            return None

        return category.get("start_date")

    def _find_category(
        self,
        store_name: str,
        category_name: str
    ) -> Optional[Dict[str, Any]]:
        """カテゴリーを検索"""
        if store_name not in self.config:
            return None

        for category in self.config[store_name]:
            if category["name"] == category_name:
                return category

        return None

    def get_all_categories(self, store_name: str) -> List[Dict[str, Any]]:
        """店舗のすべてのカテゴリーを取得"""
        return self.config.get(store_name, [])

    def update_category(
        self,
        store_name: str,
        category_name: str,
        updates: Dict[str, Any]
    ):
        """
        カテゴリー情報を更新（存在しない場合は新規作成）

        Args:
            store_name: 店舗名
            category_name: カテゴリー名
            updates: 更新する内容（enabled, next_run_datetime, interval_days など）
        """
        # 店舗が存在しない場合は作成
        if store_name not in self.config:
            self.config[store_name] = []

        category = self._find_category(store_name, category_name)
        if category is not None:
            # 既存カテゴリーを更新
            category.update(updates)
            self.save_config()
            logger.info(f"カテゴリー {category_name} を更新しました")
        else:
            # 新規カテゴリーを作成
            new_category = {
                "name": category_name,
                "url": updates.get("url", ""),
                "enabled": updates.get("enabled", True),
                "start_date": updates.get("start_date", datetime.now().strftime("%Y-%m-%d")),
                "interval_days": updates.get("interval_days", 7),
                "last_run": updates.get("last_run", None),
                "notes": updates.get("notes", "")
            }
            self.config[store_name].append(new_category)
            self.save_config()
            logger.info(f"✅ 新規カテゴリー {category_name} を追加しました（店舗: {store_name}）")


if __name__ == "__main__":
    # テスト実行
    manager = CategoryManager()

    # サンプルデータで初期化
    sample_categories = [
        {"name": "野菜", "url": "https://example.com/vegetables"},
        {"name": "果物", "url": "https://example.com/fruits"},
        {"name": "肉類", "url": "https://example.com/meat"},
    ]

    manager.initialize_store_categories(
        "rakuten_seiyu",
        sample_categories,
        default_interval_days=7,
        default_next_run="2025-12-25 01:00"
    )

    # 実行判定テスト
    print("野菜を実行すべきか:", manager.should_run_category("rakuten_seiyu", "野菜"))
    print("次回実行予定日時:", manager.get_next_run_datetime("rakuten_seiyu", "野菜"))
```

### services\data-ingestion\common\category_manager_db.py

```py
"""
ネットスーパーカテゴリー実行スケジュール管理（Supabaseベース）

カテゴリーごとに次回実行日時とインターバルを指定し、
実行すべきかどうかを判定する機能を提供します。
Streamlit Cloud対応のため、設定をSupabaseテーブルに保存します。
"""

from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from loguru import logger
from shared.common.database.client import DatabaseClient


class CategoryManagerDB:
    """カテゴリーの実行スケジュールを管理（Supabaseベース）"""

    def __init__(self):
        """初期化"""
        self.db = DatabaseClient(use_service_role=True)
        self.table_name = "99_lg_scraping_schedule"

    def load_config_from_db(self, store_name: str) -> List[Dict[str, Any]]:
        """
        Supabaseから設定を読み込む

        Args:
            store_name: 店舗名

        Returns:
            カテゴリーリスト
        """
        try:
            result = self.db.client.table(self.table_name).select(
                '*'
            ).eq('store_name', store_name).execute()

            return result.data
        except Exception as e:
            logger.error(f"設定読み込みエラー: {e}")
            return []

    def save_category_to_db(
        self,
        store_name: str,
        category_name: str,
        updates: Dict[str, Any]
    ):
        """
        カテゴリー設定をSupabaseに保存（upsert）

        Args:
            store_name: 店舗名
            category_name: カテゴリー名
            updates: 更新内容
        """
        try:
            # 既存レコードを検索
            existing = self.db.client.table(self.table_name).select(
                'id'
            ).eq('store_name', store_name).eq('category_name', category_name).execute()

            data = {
                'store_name': store_name,
                'category_name': category_name,
                **updates
            }

            if existing.data:
                # 更新
                self.db.client.table(self.table_name).update(
                    updates
                ).eq('store_name', store_name).eq('category_name', category_name).execute()
                logger.info(f"カテゴリー {category_name} を更新しました")
            else:
                # 新規作成
                self.db.client.table(self.table_name).insert(data).execute()
                logger.info(f"✅ 新規カテゴリー {category_name} を追加しました（店舗: {store_name}）")

        except Exception as e:
            logger.error(f"カテゴリー保存エラー: {e}")

    def initialize_store_categories(
        self,
        store_name: str,
        categories: List[Dict[str, str]],
        default_interval_days: int = 7,
        default_start_date: Optional[str] = None
    ):
        """
        店舗のカテゴリーを初期化

        Args:
            store_name: 店舗名（例: "rakuten_seiyu"）
            categories: カテゴリーリスト [{"name": "xxx", "url": "xxx"}, ...]
            default_interval_days: デフォルトのインターバル日数
            default_start_date: デフォルトの開始日（YYYY-MM-DD）、指定しない場合は明日
        """
        if default_start_date is None:
            # デフォルト: 明日
            tomorrow = datetime.now() + timedelta(days=1)
            default_start_date = tomorrow.strftime("%Y-%m-%d")

        # 既存カテゴリーを取得
        existing_categories = self.load_config_from_db(store_name)
        existing_names = {cat["category_name"] for cat in existing_categories}

        for category in categories:
            if category["name"] not in existing_names:
                self.save_category_to_db(
                    store_name,
                    category["name"],
                    {
                        "url": category.get("url", ""),
                        "enabled": True,
                        "start_date": default_start_date,
                        "interval_days": default_interval_days,
                        "last_run": None,
                        "notes": ""
                    }
                )

        logger.info(f"{store_name} のカテゴリーを初期化しました（{len(categories)}件）")

    def should_run_category(
        self,
        store_name: str,
        category_name: str,
        now: Optional[datetime] = None
    ) -> bool:
        """
        カテゴリーを今実行すべきかどうかを判定

        ロジック: 現在日付 >= start_date なら True

        Args:
            store_name: 店舗名
            category_name: カテゴリー名
            now: 現在時刻（指定しない場合は現在時刻）

        Returns:
            実行すべきなら True
        """
        if now is None:
            now = datetime.now()

        category = self._find_category_in_db(store_name, category_name)
        if category is None:
            logger.warning(f"カテゴリー {category_name} が見つかりません（店舗: {store_name}）")
            return False

        # 無効化されている場合はスキップ
        if not category.get("enabled", True):
            logger.info(f"カテゴリー {category_name} は無効化されています")
            return False

        # 開始日を取得
        start_date_str = category.get("start_date")
        if not start_date_str:
            logger.warning(f"カテゴリー {category_name} の開始日が設定されていません")
            return False

        # 日付をパース（DATE型なので文字列で返ってくる）
        try:
            start_date = datetime.strptime(start_date_str, "%Y-%m-%d").date()
        except ValueError as e:
            logger.error(f"開始日のパースエラー: {start_date_str}, {e}")
            return False

        # 判定（日付のみで比較）
        should_run = now.date() >= start_date
        if should_run:
            logger.info(f"カテゴリー {category_name}: 実行可能（開始日: {start_date_str}）")

        return should_run

    def mark_as_run(
        self,
        store_name: str,
        category_name: str,
        run_datetime: Optional[datetime] = None
    ):
        """
        カテゴリーを実行済みとしてマークし、次回開始日を更新

        次回開始日 = 実行日 + interval_days + 1日

        Args:
            store_name: 店舗名
            category_name: カテゴリー名
            run_datetime: 実行日時（指定しない場合は現在時刻）
        """
        if run_datetime is None:
            run_datetime = datetime.now()

        category = self._find_category_in_db(store_name, category_name)
        if category is not None:
            # 最終実行日を記録
            last_run_date = run_datetime.strftime("%Y-%m-%d")

            # 次回開始日を計算: 実行日 + interval_days + 1
            interval_days = category.get("interval_days", 7)
            next_start_date = run_datetime.date() + timedelta(days=interval_days + 1)
            next_start_date_str = next_start_date.strftime("%Y-%m-%d")

            # Supabaseを更新
            self.save_category_to_db(
                store_name,
                category_name,
                {
                    "last_run": last_run_date,
                    "start_date": next_start_date_str
                }
            )

            logger.info(f"カテゴリー {category_name} を実行済みとしてマークしました")
            logger.info(f"  最終実行: {last_run_date}")
            logger.info(f"  次回開始日: {next_start_date_str}")
        else:
            logger.warning(f"カテゴリー {category_name} が見つかりません（店舗: {store_name}）")

    def get_start_date(
        self,
        store_name: str,
        category_name: str
    ) -> Optional[str]:
        """
        カテゴリーの開始日を取得

        Returns:
            開始日（YYYY-MM-DD形式）、または None
        """
        category = self._find_category_in_db(store_name, category_name)
        if category is None:
            return None

        return category.get("start_date")

    def _find_category_in_db(
        self,
        store_name: str,
        category_name: str
    ) -> Optional[Dict[str, Any]]:
        """Supabaseからカテゴリーを検索"""
        try:
            result = self.db.client.table(self.table_name).select(
                '*'
            ).eq('store_name', store_name).eq('category_name', category_name).execute()

            if result.data:
                return result.data[0]
            return None
        except Exception as e:
            logger.error(f"カテゴリー検索エラー: {e}")
            return None

    def get_all_categories(self, store_name: str) -> List[Dict[str, Any]]:
        """店舗のすべてのカテゴリーを取得"""
        return self.load_config_from_db(store_name)

    def update_category(
        self,
        store_name: str,
        category_name: str,
        updates: Dict[str, Any]
    ):
        """
        カテゴリー情報を更新（存在しない場合は新規作成）

        Args:
            store_name: 店舗名
            category_name: カテゴリー名
            updates: 更新する内容（enabled, start_date, interval_days など）
        """
        self.save_category_to_db(store_name, category_name, updates)


if __name__ == "__main__":
    # テスト実行
    manager = CategoryManagerDB()

    # サンプルデータで初期化
    sample_categories = [
        {"name": "野菜", "url": "https://example.com/vegetables"},
        {"name": "果物", "url": "https://example.com/fruits"},
        {"name": "肉類", "url": "https://example.com/meat"},
    ]

    manager.initialize_store_categories(
        "rakuten_seiyu",
        sample_categories,
        default_interval_days=7
    )

    # 実行判定テスト
    print("野菜を実行すべきか:", manager.should_run_category("rakuten_seiyu", "野菜"))
    print("開始日:", manager.get_start_date("rakuten_seiyu", "野菜"))
```

### services\data-ingestion\daiei\__init__.py

```py
# Daiei Net Super Scraping Module
```

### services\data-ingestion\daiei\daiei_scraper_playwright.py

```py
"""
ダイエーネットスーパー スクレイピングモジュール (Playwright版)

Playwrightを使用してログイン状態を保持したまま商品データを取得します。
"""

import json
import re
import time
import random
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from playwright.async_api import async_playwright, Browser, Page, BrowserContext

# ロガー設定
logger = logging.getLogger(__name__)


class DaieiScraperPlaywright:
    """ダイエーネットスーパーのスクレイピングクラス (Playwright版)"""

    def __init__(self):
        self.base_url = "https://netsuper.daiei.co.jp"
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        self.store_id: Optional[str] = None  # 店舗ID（ログイン後に取得）

    async def __aenter__(self):
        """async with構文でのコンテキスト開始"""
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """async with構文でのコンテキスト終了"""
        await self.close()

    async def start(self, headless: bool = True):
        """
        ブラウザを起動

        Args:
            headless: ヘッドレスモードで起動するか
        """
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(headless=headless)
            self.context = await self.browser.new_context(
                viewport={'width': 1280, 'height': 800},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            self.page = await self.context.new_page()
            logger.info("✅ Playwrightブラウザ起動完了")

        except Exception as e:
            logger.error(f"ブラウザ起動エラー: {e}", exc_info=True)
            raise

    async def close(self):
        """ブラウザを閉じる"""
        try:
            if self.page:
                await self.page.close()
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
            logger.info("✅ ブラウザ終了")

        except Exception as e:
            logger.error(f"ブラウザ終了エラー: {e}", exc_info=True)

    async def _is_logged_in(self) -> bool:
        """ログイン状態をチェック"""
        try:
            # ログインフォームが表示されているかチェック
            login_form = await self.page.query_selector('input[name="login_id"]')
            if login_form:
                # ログインフォームが見つかった = ログインしていない
                return False

            # 店舗IDを含むURL（/0XXX/）ならログイン済み
            current_url = self.page.url
            import re
            if re.search(r'/0\d{3}/', current_url):
                return True

            return False
        except:
            return False

    async def login(self, login_id: str, password: str) -> bool:
        """
        ダイエーネットスーパーにログイン

        Args:
            login_id: ログインID
            password: パスワード

        Returns:
            成功したらTrue
        """
        import asyncio

        try:
            logger.info("🔐 ダイエーネットスーパーにログイン中...")

            # トップページにアクセス
            await self.page.goto(self.base_url, wait_until="domcontentloaded", timeout=60000)
            await self.page.wait_for_timeout(2000)

            # 既にログイン済みかチェック
            if await self._is_logged_in():
                logger.info("✅ 既にログイン済みです")
                return True

            # ログインフォームを見つける
            logger.info("ステップ1: ログインIDを入力中...")

            # ログインIDを入力
            login_id_input = await self.page.wait_for_selector(
                'input[name="login_id"]',
                timeout=10000,
                state="visible"
            )
            await login_id_input.click()
            await login_id_input.fill(login_id)
            logger.info("✅ ログインID入力完了")

            # パスワードを入力
            logger.info("ステップ2: パスワードを入力中...")
            password_input = await self.page.wait_for_selector(
                'input[name="password"]',
                timeout=5000,
                state="visible"
            )
            await password_input.click()
            await password_input.fill(password)
            logger.info("✅ パスワード入力完了")

            # ログインボタンをクリック
            await asyncio.sleep(0.5)
            login_button = await self.page.wait_for_selector(
                'button.p-mvLogin__submit, button[onclick*="LoginRun"]',
                timeout=5000,
                state="visible"
            )
            await login_button.click()
            logger.info("ログインボタンをクリック")

            # ログイン完了を待機
            await asyncio.sleep(3)
            await self.page.wait_for_load_state("domcontentloaded")

            current_url = self.page.url
            logger.info(f"ログイン後のURL: {current_url}")

            # ログイン成功確認
            if await self._is_logged_in():
                logger.info("✅ ログイン成功")

                # WAONポイント案内ページが表示される場合があるので、スキップ
                await asyncio.sleep(1)
                shopping_button = await self.page.query_selector('a[href*="submit_apc_shopping"]')
                if shopping_button:
                    logger.info("WAONポイント案内ページをスキップ中...")
                    await shopping_button.click()
                    await asyncio.sleep(2)
                    await self.page.wait_for_load_state("domcontentloaded")
                    logger.info("✅ お買い物ページに遷移")

                return True
            else:
                logger.error("❌ ログイン失敗")
                return False

        except Exception as e:
            logger.error(f"ログインエラー: {e}", exc_info=True)
            return False

    async def select_delivery_slot(self) -> bool:
        """
        配達日時を選択（利用可能な最初の配達便を選択）

        Returns:
            成功したらTrue
        """
        import asyncio

        try:
            logger.info("📦 配達日時を選択中...")

            # 配達便選択ページを待機
            await self.page.wait_for_timeout(2000)

            # 利用可能な配達便のラベルを探す
            # class="acceptable" (○受付中) または class="a-little" (△残りわずか)
            available_labels = await self.page.query_selector_all(
                'label.acceptable, label.a-little'
            )

            if not available_labels:
                logger.error("❌ 利用可能な配達便が見つかりません")
                return False

            logger.info(f"利用可能な配達便: {len(available_labels)}件")

            # 最初の利用可能な配達便を選択
            first_label = available_labels[0]
            label_id = await first_label.get_attribute("id")
            logger.info(f"配達便ラベルを選択: {label_id}")

            # ラベルをクリック（内部のinputが選択される）
            await first_label.click()
            await asyncio.sleep(1)

            # 「お買い物を始める」ボタンをクリック
            # このボタンは配達便選択後に有効になる
            start_shopping_button = await self.page.wait_for_selector(
                'button[type="button"], input[type="button"], a[href*="index.php"]',
                timeout=10000,
                state="visible"
            )

            # ボタンのテキストを確認
            button_text = await start_shopping_button.inner_text()
            logger.info(f"ボタンテキスト: {button_text}")

            await start_shopping_button.click()
            logger.info("お買い物を始めるボタンをクリック")

            await asyncio.sleep(2)
            await self.page.wait_for_load_state("domcontentloaded")

            # 店舗IDをURLから取得
            current_url = self.page.url
            match = re.search(r'/(\d{4})/', current_url)
            if match:
                self.store_id = match.group(1)
                logger.info(f"✅ 店舗ID取得: {self.store_id}")
            else:
                logger.warning("⚠️ 店舗IDが取得できませんでした")

            logger.info(f"現在のURL: {current_url}")
            logger.info("✅ 配達日時選択完了")
            return True

        except Exception as e:
            logger.error(f"配達日時選択エラー: {e}", exc_info=True)
            return False

    async def fetch_products_page(
        self,
        category_url: str,
        page: int = 1
    ) -> tuple[List[Dict[str, Any]], Optional[dict]]:
        """
        カテゴリーページの商品データを取得

        Args:
            category_url: カテゴリーの完全URL
            page: ページ番号（1始まり、内部的にpage=0に変換される）

        Returns:
            (商品データのリスト, ページネーション情報)
        """
        try:
            # ダイエーのページ番号は0始まり（page=0が1ページ目）
            # 引数は1始まりで受け取り、内部で0始まりに変換
            actual_page = page - 1

            # URLにページ番号を追加
            if '?' in category_url:
                url = f"{category_url}&page={actual_page}"
            else:
                url = f"{category_url}?page={actual_page}"

            logger.info(f"商品ページ取得中 (page={page}→{actual_page}): {url}")

            await self.page.goto(url, wait_until="networkidle", timeout=60000)
            await self.page.wait_for_timeout(3000)

            # 商品データを抽出（HTMLベース）
            products, pagination_info = await self._extract_products_from_html()

            # アクセス間隔制御
            await self.page.wait_for_timeout(random.randint(1000, 2000))

            return products, pagination_info

        except Exception as e:
            logger.error(f"商品ページ取得エラー: {e}", exc_info=True)
            return [], None

    async def _extract_products_from_html(self) -> tuple[List[Dict[str, Any]], Optional[dict]]:
        """
        HTMLから商品データを抽出

        Returns:
            (商品データのリスト, ページネーション情報)
        """
        try:
            logger.info("✅ HTML解析開始")

            # デバッグ用にスクリーンショット・HTML保存
            await self.page.screenshot(path="daiei_product_page.png")
            logger.info("スクリーンショット保存: daiei_product_page.png")

            html_content = await self.page.content()
            with open("daiei_product_page.html", "w", encoding="utf-8") as f:
                f.write(html_content)
            logger.info("HTML保存: daiei_product_page.html")

            # 商品コンテナを取得
            product_containers = await self.page.query_selector_all('div.item_ct')
            logger.info(f"商品コンテナ数: {len(product_containers)}")

            products = []

            for container in product_containers:
                try:
                    # 商品IDを取得
                    id_anchor = await container.query_selector('a[id]')
                    product_id = await id_anchor.get_attribute('id') if id_anchor else None

                    # 商品名とURLを取得
                    name_elem = await container.query_selector('div.item_name a')
                    product_name = await name_elem.inner_text() if name_elem else None
                    product_url = None
                    if name_elem:
                        href = await name_elem.get_attribute('href')
                        if href:
                            if href.startswith('http'):
                                product_url = href
                            elif href.startswith('/'):
                                product_url = f"https://netsuper.daiei.co.jp{href}"
                            else:
                                product_url = f"https://netsuper.daiei.co.jp/{href}"

                    # 商品画像URLを取得
                    img_elem = await container.query_selector('div.item_img img')
                    img_src = await img_elem.get_attribute('src') if img_elem else None
                    if img_src and not img_src.startswith('http'):
                        img_src = f"https://netsuper.daiei.co.jp{img_src}"

                    # 本体価格を取得
                    price_elem = await container.query_selector('span.item_price')
                    base_price_text = await price_elem.inner_text() if price_elem else None
                    base_price = float(base_price_text) if base_price_text and base_price_text.isdigit() else None

                    # 税込価格を取得
                    tax_price_elem = await container.query_selector('p.item_price2')
                    tax_price_text = await tax_price_elem.inner_text() if tax_price_elem else None
                    tax_price = None
                    if tax_price_text:
                        import re
                        match = re.search(r'([\d,]+\.?\d*)円', tax_price_text)
                        if match:
                            tax_price = float(match.group(1).replace(',', ''))

                    product = {
                        "product_id": product_id,
                        "product_name": product_name,
                        "price": base_price,
                        "price_tax_included": tax_price,
                        "image_url": img_src,
                        "url": product_url,  # URLを追加
                        "in_stock": True,  # ページに表示されている = 在庫あり
                        "is_available": True,
                        "raw_data": {
                            "product_id": product_id,
                            "base_price": base_price,
                            "tax_price": tax_price,
                            "url": product_url  # raw_dataにも追加
                        }
                    }

                    products.append(product)

                except Exception as e:
                    logger.warning(f"商品データ抽出エラー（スキップ）: {e}")
                    continue

            # ページネーション情報を取得
            pagination_info = None
            try:
                pagination_text_elem = await self.page.query_selector('text="ヒット数："')
                if pagination_text_elem:
                    parent = await pagination_text_elem.evaluate_handle('node => node.parentElement')
                    text = await parent.inner_text()
                    import re
                    # "ヒット数：205件 2/6ページ" のような形式
                    total_match = re.search(r'(\d+)件', text)
                    page_match = re.search(r'(\d+)/(\d+)ページ', text)

                    if total_match and page_match:
                        total_items = int(total_match.group(1))
                        current_page = int(page_match.group(1))
                        total_pages = int(page_match.group(2))

                        pagination_info = {
                            "totalItems": total_items,
                            "currentPage": current_page,
                            "totalPages": total_pages,
                            "itemsPerPage": len(products),
                            "source": "html:pagination_text"
                        }
            except Exception as e:
                logger.warning(f"ページネーション情報取得エラー: {e}")

            logger.info(f"✅ 商品抽出完了: {len(products)}件")

            return products, pagination_info

        except Exception as e:
            logger.error(f"商品抽出エラー: {e}", exc_info=True)
            return [], None


async def main():
    """テスト実行用のメイン関数"""
    import os
    from dotenv import load_dotenv
    load_dotenv()

    login_id = os.getenv("DAIEI_LOGIN_ID")
    password = ***REDACTED***"DAIEI_PASSWORD")

    if not login_id or not password:
        logger.error("❌ 環境変数 DAIEI_LOGIN_ID と DAIEI_PASSWORD を設定してください")
        return

    scraper = DaieiScraperPlaywright()

    try:
        # ヘッドレスモードをオフにしてブラウザを表示
        await scraper.start(headless=False)

        # ログイン
        success = await scraper.login(login_id, password)
        if not success:
            logger.error("❌ ログイン失敗")
            return

        # 配達日時選択ページのHTMLを保存
        await scraper.page.screenshot(path="daiei_delivery_page.png")
        html_content = await scraper.page.content()
        with open("daiei_delivery_page.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        logger.info("📸 配達日時選択ページのスクリーンショット・HTML保存完了")

        # 配達日時選択
        success = await scraper.select_delivery_slot()
        if not success:
            logger.error("❌ 配達日時選択失敗")
            # 失敗時も少し待つ（画面確認のため）
            await scraper.page.wait_for_timeout(5000)
            return

        # 成功後、商品ページのスクリーンショット・HTML保存
        await scraper.page.screenshot(path="daiei_product_top.png")
        html_content = await scraper.page.content()
        with open("daiei_product_top.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        logger.info("📸 商品トップページのスクリーンショット・HTML保存完了")

        # テスト: カテゴリーページにアクセス（野菜果物）
        test_category_url = f"https://netsuper.daiei.co.jp/{scraper.store_id}/item/item.php?classL=2&classS=1&ilc_code=1001"
        logger.info(f"📦 テストカテゴリーページにアクセス: {test_category_url}")

        products, pagination = await scraper.fetch_products_page(test_category_url, page=1)
        logger.info(f"✅ 商品取得完了: {len(products)}件")
        if pagination:
            logger.info(f"ページネーション情報: {pagination}")

        logger.info("✅ テスト完了")

        # 画面を確認できるように5秒待つ
        await scraper.page.wait_for_timeout(5000)

    finally:
        await scraper.close()


if __name__ == "__main__":
    import asyncio
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    asyncio.run(main())
```

### services\data-ingestion\daiei\process_with_schedule.py

```py
"""
ダイエーネットスーパー スケジュール管理対応版

カテゴリーごとの実行スケジュールを管理し、
サーバー負荷を最小限に抑える待機時間を実装します。

ダイエーは規約が厳しいため、特に注意深くアクセスします。
"""

import os
import sys
import asyncio
import random
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from common.category_manager_db import CategoryManagerDB
from daiei.product_ingestion import DaieiProductIngestionPipeline

# ロガー設定
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class PoliteDaieiPipeline:
    """サーバー負荷に配慮したスケジュール管理パイプライン（ダイエー用）"""

    def __init__(
        self,
        login_id: str,
        password: str,
        headless: bool = True,
        dry_run: bool = False
    ):
        """
        Args:
            login_id: ダイエーログインID
            password: パスワード
            headless: ヘッドレスモード
            dry_run: Dry Run モード（設定ファイルの初期化のみ）
        """
        self.pipeline = DaieiProductIngestionPipeline(
            login_id=login_id,
            password=***REDACTED***
            headless=headless
        )
        self.manager = CategoryManagerDB()
        self.dry_run = dry_run
        self.store_name = "daiei"

    async def polite_wait_between_pages(self):
        """ページ遷移間の待機（5秒〜10秒のランダム・ダイエーは長めに）"""
        wait_time = random.uniform(5.0, 10.0)
        logger.info(f"⏳ ページ遷移待機: {wait_time:.1f}秒")
        await asyncio.sleep(wait_time)

    async def polite_wait_between_categories(self):
        """カテゴリー切替時の待機（20秒〜40秒のランダム・ダイエーは長めに）"""
        wait_time = random.uniform(20.0, 40.0)
        logger.info(f"⏳ カテゴリー切替待機: {wait_time:.1f}秒")
        await asyncio.sleep(wait_time)

    async def initialize_categories(self):
        """カテゴリーを初期化（初回実行時）"""
        logger.info("📋 カテゴリーを初期化します...")
        logger.warning("⚠️ ダイエーは規約が厳しいため、慎重にアクセスします")

        # スクレイパー起動
        success = await self.pipeline.start()
        if not success:
            logger.error("❌ スクレイパー起動失敗")
            return False

        try:
            # カテゴリーを手動で定義（ダイエーの場合）
            # ※ダイエーは動的取得が難しい場合があるため、ハードコードも検討
            categories = [
                {"name": "野菜・果物", "url": "https://daiei.eorder.ne.jp/category/"},
                # 他のカテゴリーはスクレイピングで取得するか、手動で追加
            ]

            # または動的に取得を試みる
            # categories = await self.pipeline.discover_categories()

            if not categories:
                logger.warning("カテゴリーが見つかりませんでした")
                return False

            # CategoryManagerに登録
            category_list = [
                {"name": cat["name"], "url": cat["url"]}
                for cat in categories
            ]

            # デフォルト設定で初期化
            # ダイエーは特に慎重に：開始日は明日、インターバル: 14日（2週間）
            tomorrow = datetime.now().strftime("%Y-%m-%d")
            self.manager.initialize_store_categories(
                self.store_name,
                category_list,
                default_interval_days=14,  # ダイエーは長めに設定
                default_start_date=tomorrow
            )

            logger.info(f"✅ {len(categories)}件のカテゴリーを初期化しました")
            logger.info("管理画面で設定を調整してください:")
            logger.info("  streamlit run B_ingestion/netsuper_category_manager_ui.py")
            logger.warning("⚠️ ダイエーは規約遵守のため、インターバルを長めに設定することを推奨します")

            return True

        finally:
            await self.pipeline.close()

    async def run_scheduled_categories(self, manual_categories: List[str] = None):
        """スケジュールに基づいてカテゴリーを処理

        Args:
            manual_categories: 手動実行時に指定されたカテゴリー名のリスト（Noneの場合はスケジュールに従う）
        """
        if manual_categories:
            logger.info("="*80)
            logger.info("ダイエーネットスーパー 手動実行開始")
            logger.info(f"対象カテゴリー: {', '.join(manual_categories)}")
            logger.info("="*80)
        else:
            logger.info("="*80)
            logger.info("ダイエーネットスーパー スケジュール実行開始")
            logger.info("="*80)
        logger.warning("⚠️ ダイエーは規約が厳しいため、慎重にアクセスします")

        # スクレイパー起動
        success = await self.pipeline.start()
        if not success:
            logger.error("❌ スクレイパー起動失敗")
            return

        try:
            # カテゴリーを動的に取得して更新（ダイエーは静的リスト）
            logger.info("🔄 カテゴリーを最新化中...")
            discovered_categories = await self.pipeline.discover_categories()

            if discovered_categories:
                logger.info(f"✅ {len(discovered_categories)}件のカテゴリーを取得")

                # 既存の設定を取得
                existing_categories = self.manager.get_all_categories(self.store_name)
                existing_names = {cat["name"]: cat for cat in existing_categories} if existing_categories else {}

                # 新規カテゴリーを追加
                for cat in discovered_categories:
                    if cat["name"] not in existing_names:
                        logger.info(f"  📝 新規カテゴリー追加: {cat['name']}")
                        self.manager.update_category(
                            self.store_name,
                            cat["name"],
                            {
                                "url": cat["url"],
                                "enabled": True,
                                "interval_days": 14,  # ダイエーは長めに設定
                                "start_date": datetime.now().strftime("%Y-%m-%d")
                            }
                        )
                    else:
                        # URLが変更されている場合は更新
                        if existing_names[cat["name"]].get("url") != cat["url"]:
                            logger.info(f"  🔄 URL更新: {cat['name']}")
                            self.manager.update_category(
                                self.store_name,
                                cat["name"],
                                {"url": cat["url"]}
                            )

            # 設定からカテゴリーを取得
            categories = self.manager.get_all_categories(self.store_name)

            if not categories:
                logger.warning("カテゴリーが設定されていません。初回実行してください:")
                logger.warning("  python -m B_ingestion.daiei.process_with_schedule --init")
                return

            # 実行すべきカテゴリーをフィルタリング
            today = datetime.now()
            runnable_categories = []

            if manual_categories:
                # 手動実行時: 指定されたカテゴリーのみ
                for cat in categories:
                    if cat["name"] in manual_categories:
                        runnable_categories.append(cat)
            else:
                # スケジュール実行時: 今日実行すべきカテゴリー
                for cat in categories:
                    if self.manager.should_run_category(self.store_name, cat["name"], today):
                        runnable_categories.append(cat)

            logger.info(f"📊 総カテゴリー数: {len(categories)}件")
            logger.info(f"✅ 本日実行対象: {len(runnable_categories)}件")

            if not runnable_categories:
                logger.info("本日実行するカテゴリーはありません")
                return

            # カテゴリーごとに処理
            for idx, cat in enumerate(runnable_categories, 1):
                logger.info("")
                logger.info("="*80)
                logger.info(f"📦 カテゴリー {idx}/{len(runnable_categories)}: {cat['name']}")
                logger.info(f"   URL: {cat['url']}")
                logger.info("="*80)

                try:
                    # カテゴリーの商品データを取得してSupabaseに保存
                    result = await self.pipeline.process_category_all_pages(
                        category_url=cat['url'],
                        category_name=cat['name']
                    )

                    if result:
                        logger.info(f"✅ カテゴリー {cat['name']} の処理完了")
                        logger.info(f"   商品数: {result.get('total_products', 0)}件")
                        logger.info(f"   新規: {result.get('new_products', 0)}件, 更新: {result.get('updated_products', 0)}件")
                    else:
                        logger.warning(f"⚠️ カテゴリー {cat['name']} の処理に問題がありました")

                except Exception as e:
                    logger.error(f"❌ カテゴリー {cat['name']} 処理エラー: {e}", exc_info=True)
                    # エラー時は特に長めに待機（ダイエーは厳しいため）
                    logger.warning("⚠️ エラー発生のため2分間待機します")
                    await asyncio.sleep(120)

                finally:
                    # 成功・失敗に関わらず実行済みとしてマーク
                    self.manager.mark_as_run(self.store_name, cat["name"], today)

                # カテゴリー間の待機（ダイエーは長めに）
                if idx < len(runnable_categories):
                    await self.polite_wait_between_categories()

            logger.info("")
            logger.info("="*80)
            logger.info("✅ すべてのカテゴリー処理完了")
            logger.info("="*80)

        finally:
            await self.pipeline.close()


async def main():
    """メイン処理"""
    import argparse

    parser = argparse.ArgumentParser(description="ダイエースクレイピング（スケジュール管理対応）")
    parser.add_argument("--init", action="store_true", help="カテゴリーを初期化（初回実行時のみ）")
    parser.add_argument("--manual", action="store_true", help="手動実行モード（環境変数MANUAL_CATEGORIESからカテゴリーを取得）")
    parser.add_argument("--headless", action="store_true", default=True, help="ヘッドレスモード")
    args = parser.parse_args()

    login_id = os.getenv("DAIEI_LOGIN_ID")
    password = ***REDACTED***"DAIEI_PASSWORD")

    if not login_id or not password:
        logger.error("❌ 環境変数 DAIEI_LOGIN_ID と DAIEI_PASSWORD を設定してください")
        return

    pipeline = PoliteDaieiPipeline(
        login_id=login_id,
        password=***REDACTED***
        headless=args.headless
    )

    if args.init:
        # 初期化モード
        await pipeline.initialize_categories()
    elif args.manual:
        # 手動実行モード
        manual_categories_str = os.getenv("MANUAL_CATEGORIES", "")
        if not manual_categories_str:
            logger.error("❌ 環境変数 MANUAL_CATEGORIES が設定されていません")
            return

        manual_categories = [cat.strip() for cat in manual_categories_str.split(",") if cat.strip()]
        if not manual_categories:
            logger.error("❌ カテゴリーが指定されていません")
            return

        await pipeline.run_scheduled_categories(manual_categories=manual_categories)

        # 商品データ取得後、自動的にembedding生成を実行
        await generate_embeddings_if_needed()
    else:
        # 通常実行モード（スケジュール）
        await pipeline.run_scheduled_categories()

        # 商品データ取得後、自動的にembedding生成を実行
        await generate_embeddings_if_needed()


async def generate_embeddings_if_needed():
    """商品データの分類・embedding生成（未生成のものがあれば実行）"""
    try:
        logger.info("")
        logger.info("="*80)
        logger.info("🔄 商品分類・Embedding生成チェック開始")
        logger.info("="*80)

        # ステップ1: Gemini 2.5 Flash で general_name, small_category, keywords を生成
        logger.info("ステップ1: Gemini 2.5 Flash で商品分類生成")
        classification_path = Path(__file__).parent.parent.parent / "L_product_classification"
        sys.path.insert(0, str(classification_path))

        from daily_auto_classifier import DailyAutoClassifier

        classifier = DailyAutoClassifier()
        result = await classifier.process_unclassified_products()
        logger.info(f"✅ 分類完了: {result.get('classified_count', 0)}件")

        # ステップ2: Embedding生成
        logger.info("ステップ2: Embedding生成")
        netsuper_app_path = Path(__file__).parent.parent.parent / "netsuper_search_app"
        sys.path.insert(0, str(netsuper_app_path))

        from generate_multi_embeddings import MultiEmbeddingGenerator

        # embedding生成を実行
        generator = MultiEmbeddingGenerator()
        generator.process_products(delay=0.1)

        logger.info("✅ 商品分類・Embedding生成処理完了")

    except Exception as e:
        logger.error(f"⚠️ 商品分類・Embedding生成エラー（スキップして続行）: {e}")


if __name__ == "__main__":
    asyncio.run(main())
```

### services\data-ingestion\daiei\product_ingestion.py

```py
"""
ダイエーネットスーパー 商品データ取得パイプライン

商品データを取得してSupabaseに保存します。

処理フロー:
1. ログインして配達日時を選択
2. カテゴリーページの商品データを取得
3. JANコードで既存商品をチェック
4. Supabaseに保存（新規 or 更新）
"""

import os
import sys
import logging
from pathlib import Path
from typing import List, Dict, Any

# プロジェクトルートをパスに追加
root_dir = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(root_dir))

# .envファイルを読み込む
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from common.base_product_ingestion import BaseProductIngestionPipeline
from daiei.daiei_scraper_playwright import DaieiScraperPlaywright

# ロガー設定
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class DaieiProductIngestionPipeline(BaseProductIngestionPipeline):
    """ダイエー商品データ取得パイプライン（共通基盤クラス継承）"""

    def __init__(self, login_id: str, password: str, headless: bool = True):
        """
        Args:
            login_id: ダイエーログインID
            password: パスワード
            headless: ヘッドレスモードで実行するか
        """
        super().__init__(organization_name="ダイエーネットスーパー", headless=headless)
        self.login_id = login_id
        self.password = ***REDACTED***

        logger.info("DaieiProductIngestionPipeline初期化完了（Service Role使用）")

    async def start(self) -> bool:
        """
        スクレイパーを起動してログイン（ダイエー固有）

        Returns:
            成功したらTrue
        """
        try:
            self.scraper = DaieiScraperPlaywright()
            await self.scraper.start(headless=self.headless)

            # ログイン
            success = await self.scraper.login(self.login_id, self.password)
            if not success:
                logger.error("❌ ログイン失敗")
                await self.scraper.close()
                return False

            # 配達日時選択
            success = await self.scraper.select_delivery_slot()
            if not success:
                logger.error("❌ 配達日時選択失敗")
                await self.scraper.close()
                return False

            logger.info("✅ スクレイパー起動・ログイン・配達日時選択完了")
            return True

        except Exception as e:
            logger.error(f"スクレイパー起動エラー: {e}", exc_info=True)
            return False

    async def close(self):
        """スクレイパーを終了"""
        if self.scraper:
            await self.scraper.close()

    async def discover_categories(self) -> List[Dict[str, str]]:
        """
        カテゴリーを取得（ダイエーは静的リスト）

        Returns:
            カテゴリー情報のリスト [{"name": "カテゴリー名", "url": "URL"}]
        """
        logger.info("🔍 カテゴリーを取得中（ダイエーは静的リスト）...")

        # ダイエーのカテゴリーは動的取得が難しいため、静的リストを使用
        # 今後、スクレイピングで取得できるようになったら実装を変更
        categories = [
            {"name": "野菜・果物", "url": "https://daiei.eorder.ne.jp/category/vegetables"},
            {"name": "精肉", "url": "https://daiei.eorder.ne.jp/category/meat"},
            {"name": "鮮魚", "url": "https://daiei.eorder.ne.jp/category/fish"},
            {"name": "惣菜", "url": "https://daiei.eorder.ne.jp/category/deli"},
            {"name": "冷凍食品", "url": "https://daiei.eorder.ne.jp/category/frozen"},
            {"name": "乳製品・卵", "url": "https://daiei.eorder.ne.jp/category/dairy"},
            {"name": "パン・シリアル", "url": "https://daiei.eorder.ne.jp/category/bread"},
            {"name": "麺類", "url": "https://daiei.eorder.ne.jp/category/noodles"},
            {"name": "缶詰・瓶詰", "url": "https://daiei.eorder.ne.jp/category/canned"},
            {"name": "調味料", "url": "https://daiei.eorder.ne.jp/category/seasoning"},
            {"name": "飲料", "url": "https://daiei.eorder.ne.jp/category/drinks"},
            {"name": "菓子", "url": "https://daiei.eorder.ne.jp/category/snacks"},
            {"name": "日用品", "url": "https://daiei.eorder.ne.jp/category/household"},
        ]

        logger.info(f"✅ {len(categories)}件のカテゴリーを取得")
        return categories


async def main():
    """テスト実行用のメイン関数"""
    logger.info("ダイエー商品データ取得パイプライン開始")

    login_id = os.getenv("DAIEI_LOGIN_ID")
    password = ***REDACTED***"DAIEI_PASSWORD")

    if not login_id or not password:
        logger.error("❌ 環境変数 DAIEI_LOGIN_ID と DAIEI_PASSWORD を設定してください")
        return

    pipeline = DaieiProductIngestionPipeline(
        login_id=login_id,
        password=***REDACTED***
        headless=False
    )

    try:
        # スクレイパー起動
        success = await pipeline.start()
        if not success:
            logger.error("❌ スクレイパー起動失敗")
            return

        logger.info("✅ テスト完了")

    finally:
        await pipeline.close()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

### services\data-ingestion\gmail\__init__.py

```py

```

### services\data-ingestion\gmail\config.yaml

```yaml
# Gmail Ingestion設定ファイル
# デフォルト設定で問題ない場合は空でOK

# 必要に応じて以下を設定可能:
# max_results: 100
# dry_run: false
```

### services\data-ingestion\gmail\gmail_ingestion.py

```py
"""
Gmailメール取り込みパイプライン

Gmail API → Google Drive（添付ファイル） → Supabase (pending)

処理フロー:
1. Gmail APIでメール一覧を取得（設定ファイルのクエリに基づく）
2. Supabaseで既存データをチェックして新着メールを抽出
3. 添付ファイルがあればGoogle Driveに保存
4. Supabaseに基本情報を登録（processing_status='pending'）
5. 別途 process_queued_documents.py で処理（PDF抽出、Stage E-K）
"""
import os
import sys
import hashlib
import yaml
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from loguru import logger
import asyncio

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

# .envファイルを読み込む
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

# HTML to Screenshot utility
from shared.common.utils.html_screenshot import HTMLScreenshotGenerator

from shared.common.connectors.gmail_connector import GmailConnector
from shared.common.connectors.google_drive import GoogleDriveConnector
from shared.common.database.client import DatabaseClient


class GmailIngestionPipeline:
    """Gmailメール取り込みパイプライン"""

    def __init__(
        self,
        mail_type: str = "DM",
        user_email: Optional[str] = None,
        config_file: Optional[str] = None,
        attachment_folder_id: Optional[str] = None,
        email_folder_id: Optional[str] = None
    ):
        """
        Args:
            mail_type: メールタイプ（'DM', 'JOB'など）環境変数のプレフィックスに使用
            user_email: アクセス対象のメールアドレス（Noneの場合は環境変数から取得）
            config_file: 設定ファイルのパス（Noneの場合はデフォルト設定を使用）
            attachment_folder_id: 添付ファイル保存先のDriveフォルダID（Noneの場合は環境変数から取得）
            email_folder_id: メール本文HTML保存先のDriveフォルダID（Noneの場合は環境変数から取得）
        """
        self.mail_type = mail_type.upper()

        # メールタイプに基づいて環境変数を取得
        self.user_email = user_email or os.getenv(f"GMAIL_{self.mail_type}_USER_EMAIL")
        self.attachment_folder_id = attachment_folder_id or os.getenv(f"GMAIL_{self.mail_type}_ATTACHMENT_FOLDER_ID")
        self.email_folder_id = email_folder_id or os.getenv(f"GMAIL_{self.mail_type}_EMAIL_FOLDER_ID")

        if not self.user_email:
            raise ValueError(f"user_emailが指定されていません。GMAIL_{self.mail_type}_USER_EMAILを.envに設定するか、引数で指定してください。")

        # 設定ファイルの読み込み
        self.config = self._load_config(config_file)

        # コネクタの初期化
        self.gmail = GmailConnector(user_email=self.user_email)
        self.drive = GoogleDriveConnector()
        self.db = DatabaseClient()

        # ラベルキャッシュ
        self._label_cache = None

        logger.info(f"GmailIngestionPipeline初期化完了")
        logger.info(f"  - Mail type: {self.mail_type}")
        logger.info(f"  - User email: {self.user_email}")
        logger.info(f"  - Email folder: {self.email_folder_id}")
        logger.info(f"  - Attachment folder: {self.attachment_folder_id}")

    def _load_config(self, config_file: Optional[Path] = None) -> Dict[str, Any]:
        """設定ファイルを読み込む"""
        if config_file is None:
            config_file = Path(__file__).parent / "config.yaml"

        if not config_file.exists():
            logger.warning(f"設定ファイルが見つかりません: {config_file}")
            logger.info("デフォルト設定を使用します")
            return self._get_default_config()

        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)

                # 空のファイルまたはコメントのみの場合、Noneが返される
                if config is None:
                    logger.info(f"設定ファイルが空です: {config_file}")
                    logger.info("デフォルト設定を使用します")
                    return self._get_default_config()

                logger.info(f"設定ファイルを読み込みました: {config_file}")
                return config
        except Exception as e:
            logger.error(f"設定ファイルの読み込みエラー: {e}")
            logger.info("デフォルト設定を使用します")
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """デフォルト設定を返す（メールタイプに基づく）"""
        # 環境変数からラベル名を取得
        label = os.getenv(f"GMAIL_{self.mail_type}_LABEL", self.mail_type)
        processed_label = os.getenv(f"GMAIL_{self.mail_type}_PROCESSED_LABEL", "Processed")

        return {
            'gmail': {
                'query': f'label:{label}',
                'max_results': 100,
                'processed_label': processed_label,
                'remove_source_label_after_import': True
            },
            'import_settings': {
                'workspace': 'gmail',
                'doc_type': f'{self.mail_type}-mail',
                'person': ['宜紀'],  # 固定値
                'organization': [],
                'save_attachments': True,
                'attachment_extensions': ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.png', '.jpg', '.jpeg']
            }
        }

    def _get_labels(self) -> List[Dict[str, Any]]:
        """ラベル一覧を取得（キャッシュ付き）"""
        if self._label_cache is None:
            self._label_cache = self.gmail.list_labels()
        return self._label_cache

    def _get_label_id(self, label_name: str) -> Optional[str]:
        """ラベル名からラベルIDを取得"""
        labels = self._get_labels()
        for label in labels:
            if label.get('name') == label_name:
                return label.get('id')
        return None

    def _get_or_create_label(self, label_name: str) -> Optional[str]:
        """ラベルを取得または作成"""
        # 既存ラベルをチェック
        label_id = self._get_label_id(label_name)
        if label_id:
            return label_id

        # ラベルが存在しない場合は作成
        try:
            label_object = {
                'name': label_name,
                'labelListVisibility': 'labelShow',
                'messageListVisibility': 'show'
            }
            created_label = self.gmail.service.users().labels().create(
                userId='me',
                body=label_object
            ).execute()

            # キャッシュを更新
            self._label_cache = None

            logger.info(f"ラベル作成: {label_name}")
            return created_label.get('id')
        except Exception as e:
            logger.error(f"ラベル作成エラー: {e}")
            return None

    async def check_existing_messages(self, message_ids: List[str]) -> set:
        """
        Supabaseで既存のメッセージIDをチェック

        Args:
            message_ids: チェックするメッセージIDのリスト

        Returns:
            既に存在するメッセージIDのセット
        """
        try:
            # Rawdata_FILE_AND_MAIL テーブルで source_type='gmail' のドキュメントを取得
            result = self.db.client.table('Rawdata_FILE_AND_MAIL').select('metadata').eq(
                'source_type', 'gmail'
            ).execute()

            # metadata->message_id を抽出
            existing_ids = set()
            if result.data:
                for doc in result.data:
                    metadata = doc.get('metadata', {})
                    if isinstance(metadata, dict):
                        msg_id = metadata.get('message_id')
                        if msg_id:
                            existing_ids.add(msg_id)

            logger.info(f"既存のメール: {len(existing_ids)}件")
            return existing_ids

        except Exception as e:
            logger.error(f"Supabase検索エラー: {e}")
            return set()

    def save_attachment_to_drive(
        self,
        attachment_data: bytes,
        filename: str,
        message_id: str
    ) -> Optional[str]:
        """
        添付ファイルをGoogle Driveに保存

        Args:
            attachment_data: 添付ファイルのバイトデータ
            filename: ファイル名
            message_id: メッセージID

        Returns:
            DriveのファイルID、失敗時はNone
        """
        # 安全なファイル名を生成
        safe_filename = "".join(c for c in filename if c.isalnum() or c in (' ', '-', '_', '.', '　')).strip()
        if not safe_filename:
            safe_filename = "attachment"

        # タイムスタンプ付きファイル名（重複防止）
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_name = f"{timestamp}_{safe_filename}"

        # MIMEタイプの推測（拡張子から）
        mime_type = 'application/octet-stream'
        ext = Path(filename).suffix.lower()
        mime_types = {
            '.pdf': 'application/pdf',
            '.doc': 'application/msword',
            '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            '.xls': 'application/vnd.ms-excel',
            '.xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            '.png': 'image/png',
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
        }
        mime_type = mime_types.get(ext, mime_type)

        # Driveにアップロード
        file_id = self.drive.upload_file(
            file_content=attachment_data,
            file_name=file_name,
            mime_type=mime_type,
            folder_id=self.attachment_folder_id
        )

        if file_id:
            logger.info(f"添付ファイルをDriveに保存: {file_name}")
        else:
            logger.error(f"添付ファイルの保存に失敗: {file_name}")

        return file_id

    async def generate_and_upload_png_from_html(
        self,
        html_content: str,
        message_id: str,
        subject: str
    ) -> Optional[str]:
        """
        HTMLからPNG画像を生成してGoogle Driveに保存（一時ファイル）

        Args:
            html_content: HTMLコンテンツ（CID画像が既にBASE64形式に変換済み）
            message_id: Gmail message ID
            subject: メール件名

        Returns:
            Drive file ID（失敗時はNone）
        """
        if not html_content:
            logger.warning(f"HTML本文が空: {message_id}")
            return None

        if not self.email_folder_id:
            logger.error("email_folder_idが設定されていません")
            return None

        try:
            # ファイル名を生成
            safe_subject = subject[:50].replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
            timestamp = message_id[:10]
            png_file_name = f"{timestamp}_{safe_subject}.png"

            # HTML → PNG変換
            logger.info(f"HTML→PNG変換開始: {png_file_name}")
            screenshot_gen = HTMLScreenshotGenerator(viewport_width=800, viewport_height=800)
            png_bytes = await screenshot_gen.html_to_screenshot(
                html_content=html_content,
                output_path=None,  # ローカル保存不要
                full_page=True
            )

            # PNGをDriveにアップロード（一時ファイル）
            file_id = self.drive.upload_file(
                file_content=png_bytes,
                file_name=png_file_name,
                mime_type='image/png',
                folder_id=self.email_folder_id
            )

            if file_id:
                logger.info(f"PNG画像をDriveに保存（一時）: {png_file_name}")
            else:
                logger.error(f"PNG画像のDriveアップロード失敗: {png_file_name}")

            return file_id

        except Exception as e:
            logger.error(f"HTML→PNG変換エラー: {e}")
            return None

    def save_email_html_to_drive(
        self,
        html_content: str,
        message_id: str,
        subject: str
    ) -> Optional[str]:
        """
        メール本文（HTML）をGoogle Driveに保存

        Args:
            html_content: HTMLコンテンツ（CID画像が既にBASE64形式に変換済み）
            message_id: Gmail message ID
            subject: メール件名

        Returns:
            Drive file ID（失敗時はNone）
        """
        if not html_content:
            logger.warning(f"HTML本文が空: {message_id}")
            return None

        if not self.email_folder_id:
            logger.error("email_folder_idが設定されていません")
            return None

        # Base64埋め込み画像をリサイズ（モバイル対応）
        try:
            from shared.common.utils.html_screenshot import HTMLScreenshotGenerator
            screenshot_gen = HTMLScreenshotGenerator()
            html_content = screenshot_gen._resize_embedded_images(html_content, max_height=300)
            logger.info(f"HTML内のBase64画像をリサイズしました（max_height=300px）")
        except Exception as e:
            logger.warning(f"Base64画像のリサイズに失敗（元のまま保存）: {e}")

        # ファイル名を生成（件名が長すぎる場合は切り詰める）
        # 無効な文字を除去
        safe_subject = subject[:50].replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
        timestamp = message_id[:10]  # メッセージIDの最初の10文字
        file_name = f"{timestamp}_{safe_subject}.html"

        # HTMLコンテンツをバイト列に変換
        html_bytes = html_content.encode('utf-8')

        # Driveにアップロード
        file_id = self.drive.upload_file(
            file_content=html_bytes,
            file_name=file_name,
            mime_type='text/html',
            folder_id=self.email_folder_id
        )

        if file_id:
            logger.info(f"メールHTMLをDriveに保存: {file_name}")
        else:
            logger.error(f"メールHTMLの保存に失敗: {file_name}")

        return file_id

    async def process_single_message(
        self,
        message_id: str
    ) -> Dict[str, Any]:
        """
        1件のメールを処理

        Args:
            message_id: メッセージID

        Returns:
            処理結果の辞書
        """
        result = {
            'message_id': message_id,
            'success': False,
            'attachment_file_ids': [],
            'document_ids': [],
            'error': None
        }

        try:
            # メールの詳細を取得
            message = self.gmail.get_message(message_id, format='full')
            if not message:
                logger.error(f"メールの取得に失敗: {message_id}")
                result['error'] = "メールの取得に失敗"
                return result

            # ヘッダー情報を抽出
            headers = self.gmail.parse_message_headers(message)
            subject = headers.get('Subject', '（件名なし）')
            from_header = headers.get('From', '不明')
            to_email = headers.get('To', '')
            date_str = headers.get('Date', '')

            # From ヘッダーをパース（"名前 <email@example.com>" → (名前, email) に分離）
            from email.utils import parseaddr
            sender_name, sender_email = parseaddr(from_header)
            if not sender_name:
                sender_name = sender_email  # 名前がない場合はメールアドレスを使用

            logger.info(f"メール処理開始: {subject}")

            # 本文と添付ファイルを抽出
            parts = self.gmail.extract_message_parts(message)
            text_plain = parts.get('text_plain', '')
            text_html = parts.get('text_html', '')
            attachments = parts.get('attachments', [])

            # HTMLメール内のCID参照画像をBASE64形式に変換
            if text_html and attachments:
                text_html = self.gmail.convert_html_with_inline_images(
                    message_id, text_html, attachments
                )

            # 受信日時をISO形式に変換
            sent_at = None
            if date_str:
                try:
                    from email.utils import parsedate_to_datetime
                    dt = parsedate_to_datetime(date_str)
                    sent_at = dt.isoformat()
                except Exception as e:
                    logger.warning(f"日付のパースに失敗: {date_str}, {e}")

            # 添付ファイルの処理
            attachment_info_list = []
            if self.config['import_settings']['save_attachments'] and attachments:
                allowed_extensions = self.config['import_settings']['attachment_extensions']

                for att in attachments:
                    filename = att.get('filename', '')
                    attachment_id = att.get('attachmentId')
                    ext = Path(filename).suffix.lower()

                    # 拡張子フィルタ
                    if allowed_extensions and ext not in allowed_extensions:
                        logger.debug(f"スキップ（拡張子フィルタ）: {filename}")
                        continue

                    if not attachment_id:
                        logger.warning(f"添付ファイルIDが空: {filename}")
                        continue

                    # 添付ファイルのデータを取得
                    att_data = self.gmail.get_attachment(message_id, attachment_id)
                    if not att_data:
                        logger.error(f"添付ファイルの取得に失敗: {filename}")
                        continue

                    # Driveに保存
                    file_id = self.save_attachment_to_drive(att_data, filename, message_id)
                    if file_id:
                        result['attachment_file_ids'].append(file_id)
                        attachment_info_list.append({
                            'filename': filename,
                            'drive_file_id': file_id,
                            'size': att.get('size', 0),
                            'mime_type': att.get('mimeType', '')
                        })

            # メタデータ準備
            metadata = {
                'message_id': message_id,
                'thread_id': message.get('threadId', ''),
                'subject': subject,
                'from': from_header,
                'from_name': sender_name,
                'from_email': sender_email,
                'to': to_email,
                'date': date_str,
                'labels': message.get('labelIds', []),
                'attachments': attachment_info_list,
                'has_attachments': len(attachment_info_list) > 0
            }

            # 本文テキストを結合（プレーンテキスト優先）
            email_body = text_plain if text_plain else text_html

            # コンテンツハッシュ（重複検出用）
            content_for_hash = f"{subject}|{sender_email}|{date_str}|{email_body}"
            content_hash = hashlib.sha256(content_for_hash.encode('utf-8')).hexdigest()

            # メール本文（HTML）をDriveに保存
            email_html_file_id = None
            email_png_file_id = None  # 画像処理用PNG（一時ファイル）
            if text_html:
                # HTML保存（表示用・永続）
                email_html_file_id = self.save_email_html_to_drive(text_html, message_id, subject)
                if email_html_file_id:
                    result['attachment_file_ids'].append(email_html_file_id)

                    # PNG生成&保存（AI処理用・一時）
                    email_png_file_id = await self.generate_and_upload_png_from_html(text_html, message_id, subject)
                    if email_png_file_id:
                        result['attachment_file_ids'].append(email_png_file_id)
                        logger.info(f"HTMLメール処理: HTML={email_html_file_id}, PNG={email_png_file_id}")

            # Supabaseに保存するデータ
            # 1. メール本文（HTML→PNG）のレコードを作成
            if email_html_file_id and email_png_file_id:
                # HTMLからテキストを抽出（取り込み時点で抽出）
                from bs4 import BeautifulSoup
                try:
                    soup = BeautifulSoup(text_html, 'html.parser')
                    # scriptとstyleタグを除去
                    for script in soup(["script", "style"]):
                        script.decompose()
                    # テキスト抽出
                    extracted_text = soup.get_text(separator='\n', strip=True)
                    logger.info(f"HTMLからテキスト抽出: {len(extracted_text)}文字")
                except Exception as e:
                    logger.warning(f"HTML解析エラー（フォールバック使用）: {e}")
                    extracted_text = email_body  # フォールバック

                # ファイル名の共通部分
                safe_subject = subject[:50].replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
                timestamp = message_id[:10]

                email_doc_data = {
                    'source_type': 'gmail',
                    'source_id': email_html_file_id,  # HTML（表示用・永続）
                    'source_url': f"https://drive.google.com/file/d/{email_html_file_id}/view",
                    'screenshot_url': f"https://drive.google.com/file/d/{email_png_file_id}/view",  # PNG（OCR用・一時）
                    'file_name': f"{timestamp}_{safe_subject}.html",  # HTML拡張子
                    'file_type': 'html',  # HTMLとして保存
                    'doc_type': self.config['import_settings']['doc_type'],
                    'workspace': self.config['import_settings']['workspace'],
                    'person': self.config['import_settings']['person'],
                    'organization': self.config['import_settings']['organization'],
                    'attachment_text': extracted_text,  # HTMLから抽出したテキスト（取り込み時点）
                    'summary': '',  # process_queued_documents.py で生成
                    'tags': ['gmail', 'email_html'],
                    'document_date': sent_at,
                    'metadata': metadata,  # シンプルなメタデータのみ
                    'content_hash': content_hash,
                    'processing_status': 'pending',
                    'processing_stage': 'gmail_html',
                    # 表示用フィールド
                    'display_type': 'Email',
                    'display_subject': subject,
                    'display_sent_at': sent_at,
                    'display_sender': sender_name,
                    'display_sender_email': sender_email,
                    'display_post_text': email_body  # 全文
                }

                try:
                    doc_result = await self.db.insert_document('Rawdata_FILE_AND_MAIL', email_doc_data)
                    if doc_result:
                        doc_id = doc_result.get('doc_id')
                        result['document_ids'].append(doc_id)
                        logger.info(f"Supabase保存完了（メール本文HTML）: {subject}")
                except Exception as db_error:
                    logger.error(f"Supabase保存エラー（メール本文）: {db_error}")
                    result['error'] = str(db_error)

            # 2. 添付ファイルのレコードを作成（ある場合のみ）
            if attachment_info_list:
                # 添付ファイルがある場合：各添付ファイルごとにレコードを作成
                for att_info in attachment_info_list:
                    doc_data = {
                        'source_type': 'gmail',
                        'source_id': att_info['drive_file_id'],
                        'source_url': f"https://drive.google.com/file/d/{att_info['drive_file_id']}/view",
                        'file_name': att_info['filename'],
                        'file_type': Path(att_info['filename']).suffix.lower().replace('.', ''),
                        'doc_type': self.config['import_settings']['doc_type'],
                        'workspace': self.config['import_settings']['workspace'],
                        'person': self.config['import_settings']['person'],
                        'organization': self.config['import_settings']['organization'],
                        'attachment_text': '',  # process_queued_documents.py で抽出
                        'summary': '',  # process_queued_documents.py で生成
                        'tags': ['gmail', 'attachment'],
                        'document_date': sent_at,
                        'metadata': metadata,
                        'content_hash': content_hash,
                        'processing_status': 'pending',
                        'processing_stage': 'gmail_attachment_downloaded',
                        # 表示用フィールド
                        'display_type': 'Email',
                        'display_subject': subject,
                        'display_sent_at': sent_at,
                        'display_sender': sender_name,
                        'display_sender_email': sender_email,
                        'display_post_text': email_body  # 全文
                    }

                    try:
                        doc_result = await self.db.insert_document('Rawdata_FILE_AND_MAIL', doc_data)
                        if doc_result:
                            doc_id = doc_result.get('doc_id')
                            result['document_ids'].append(doc_id)
                            logger.info(f"Supabase保存完了（添付ファイル）: {att_info['filename']}")
                    except Exception as db_error:
                        logger.error(f"Supabase保存エラー: {db_error}")
                        result['error'] = str(db_error)

            # メールのラベルを変更（DMラベルを削除し、Processedラベルに移動）
            processed_label = self.config['gmail'].get('processed_label', 'Processed')
            remove_source_label = self.config['gmail'].get('remove_source_label_after_import', True)

            # Processedラベルを取得または作成
            processed_label_id = self._get_or_create_label(processed_label)

            if processed_label_id:
                labels_to_add = [processed_label_id]
                labels_to_remove = []

                # DMラベルを削除する設定の場合
                if remove_source_label:
                    dm_label_id = self._get_label_id('DM')
                    if dm_label_id:
                        labels_to_remove.append(dm_label_id)

                self.gmail.modify_labels(message_id, add_labels=labels_to_add, remove_labels=labels_to_remove)
                logger.info(f"ラベル変更完了: {message_id} -> {processed_label}")

            result['success'] = True
            logger.info(f"メール処理完了: {subject} ({len(result['attachment_file_ids'])} attachments)")

        except Exception as e:
            logger.error(f"メール処理エラー: {e}", exc_info=True)
            result['error'] = str(e)

        return result

    async def run(self):
        """パイプラインを実行"""
        try:
            # Gmail設定を取得
            gmail_config = self.config['gmail']
            query = gmail_config.get('query', 'label:DM')
            max_results = gmail_config.get('max_results', 100)

            logger.info(f"メール一覧を取得中...")
            logger.info(f"  - Query: {query}")
            logger.info(f"  - Max results: {max_results}")

            # メール一覧を取得
            messages = self.gmail.list_messages(
                query=query,
                max_results=max_results
            )

            if not messages:
                logger.info("メールが見つかりませんでした")
                return

            logger.info(f"メール一覧取得完了: {len(messages)}件")

            # メッセージIDを抽出
            message_ids = [m['id'] for m in messages]

            # 既存のメッセージIDをチェック
            existing_ids = await self.check_existing_messages(message_ids)

            # 新規メッセージを抽出
            new_message_ids = [mid for mid in message_ids if mid not in existing_ids]

            logger.info(f"現在のメール: {len(messages)}件")
            logger.info(f"既存のメール: {len(existing_ids)}件")
            logger.info(f"新規メール: {len(new_message_ids)}件")

            if not new_message_ids:
                logger.info("新規メールはありません")
                return

            # 新規メッセージを処理
            results = []
            for i, message_id in enumerate(new_message_ids, 1):
                logger.info(f"[{i}/{len(new_message_ids)}] 処理中...")
                result = await self.process_single_message(message_id)
                results.append(result)

            # サマリー
            success_count = sum(1 for r in results if r['success'])
            total_attachments = sum(len(r['attachment_file_ids']) for r in results)
            total_docs = sum(len(r['document_ids']) for r in results)

            logger.info("=" * 60)
            logger.info("処理完了")
            logger.info(f"  成功: {success_count}/{len(results)}")
            logger.info(f"  失敗: {len(results) - success_count}/{len(results)}")
            logger.info(f"  処理した添付ファイル: {total_attachments}件")
            logger.info(f"  登録したドキュメント: {total_docs}件（pending状態）")
            logger.info("=" * 60)
            logger.info("")
            logger.info("次のステップ:")
            logger.info(f"  python process_queued_documents.py --workspace={self.config['import_settings']['workspace']}")
            logger.info("=" * 60)

            # 結果を表示
            print("\n" + "=" * 80)
            print("Gmail取り込み結果")
            print("=" * 80)

            for result in results:
                print(f"\nMessage ID: {result['message_id']}")
                print(f"  Success: {result['success']}")
                print(f"  Attachments: {len(result['attachment_file_ids'])}")
                for file_id in result['attachment_file_ids']:
                    print(f"    - https://drive.google.com/file/d/{file_id}/view")
                print(f"  Documents: {len(result['document_ids'])} (pending)")
                if result['error']:
                    print(f"  Error: {result['error']}")

            print("\n" + "=" * 80)
            print("次のステップ:")
            print(f"  python process_queued_documents.py --workspace={self.config['import_settings']['workspace']}")
            print("=" * 80)

        except Exception as e:
            logger.error(f"パイプライン実行エラー: {e}", exc_info=True)
            raise


async def main():
    """メインエントリーポイント"""
    import argparse

    parser = argparse.ArgumentParser(description='Gmail取り込みパイプライン')
    parser.add_argument('--mail-type', type=str, default='DM', help='メールタイプ（DM, JOBなど）デフォルト: DM')
    parser.add_argument('--email', type=str, help='アクセス対象のメールアドレス（省略時は環境変数から取得）')
    parser.add_argument('--config', type=str, help='設定ファイルのパス')
    parser.add_argument('--email-folder-id', type=str, help='メール本文HTML保存先のDriveフォルダID（省略時は環境変数から取得）')
    parser.add_argument('--attachment-folder-id', type=str, help='添付ファイル保存先のDriveフォルダID（省略時は環境変数から取得）')
    args = parser.parse_args()

    # パイプラインの初期化
    config_file = Path(args.config) if args.config else None
    pipeline = GmailIngestionPipeline(
        mail_type=args.mail_type,
        user_email=args.email,
        config_file=config_file,
        email_folder_id=args.email_folder_id,
        attachment_folder_id=args.attachment_folder_id
    )

    # 実行
    await pipeline.run()


if __name__ == "__main__":
    asyncio.run(main())
```

### services\data-ingestion\monitoring\__init__.py

```py

```

### services\data-ingestion\monitoring\inbox_monitor.py

```py
"""
InBox自動監視スクリプト (v1.0)

目的: Google Driveの特定のInBoxフォルダをポーリングし、
     新規追加されたPDFファイルを検出、既存の2段階AIパイプラインに渡す。

設計: AUTO_INBOX_COMPLETE_v3.0.md の Phase 2 (Track 3) に準拠
     「受信箱自動監視システム」のアーキテクチャ定義に基づく

実行頻度: GitHub Actions (毎時実行)
"""

import os
import sys
import asyncio
import tempfile
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from loguru import logger
import traceback

# パス設定
sys.path.insert(0, str(Path(__file__).parent.parent))

from shared.common.connectors.google_drive import GoogleDriveConnector
from shared.common.database.client import DatabaseClient
from shared.common.processors.pdf import calculate_content_hash
from shared.pipeline import UnifiedDocumentPipeline

# ログ設定
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)

logger.add(log_dir / f'inbox_monitor_{datetime.now():%Y%m%d_%H%M%S}.log', rotation="10 MB", level="INFO")
logger.add(sys.stdout, level="INFO")


class InBoxMonitor:
    """InBox自動監視クラス"""

    def __init__(self):
        """初期化"""
        self.drive = GoogleDriveConnector()
        self.db = DatabaseClient()

        # 統合パイプラインを初期化
        self.pipeline = UnifiedDocumentPipeline(db_client=self.db)

        # 一時ディレクトリ
        self.temp_dir = Path("./temp")
        self.temp_dir.mkdir(parents=True, exist_ok=True)

        # InBoxフォルダIDとArchiveフォルダIDを取得
        self.inbox_folder_id = self.drive.get_inbox_folder_id()
        self.archive_folder_id = self.drive.get_archive_folder_id()

        if not self.inbox_folder_id:
            raise ValueError("INBOX_FOLDER_ID が環境変数に設定されていません")

        logger.info(f"InBox監視システム初期化完了")
        logger.info(f"InBox Folder ID: {self.inbox_folder_id}")
        logger.info(f"Archive Folder ID: {self.archive_folder_id if self.archive_folder_id else 'Not Set'}")

    def get_processed_file_ids(self) -> List[str]:
        """
        既に処理済みのファイルIDをデータベースから取得

        Returns:
            処理済みファイルIDのリスト
        """
        logger.info("📊 データベースから処理済みファイルIDを取得中...")
        processed_ids = self.db.get_processed_file_ids()
        logger.info(f"✅ {len(processed_ids)} 件の処理済みファイルIDを取得")
        return processed_ids

    def scan_inbox_for_new_files(self, processed_file_ids: List[str]) -> List[Dict[str, Any]]:
        """
        InBoxフォルダから新規ファイルをスキャン

        Args:
            processed_file_ids: 既に処理済みのファイルIDリスト

        Returns:
            新規ファイルのメタデータリスト
        """
        logger.info(f"📁 InBoxフォルダ [{self.inbox_folder_id}] をスキャン中...")

        new_files = self.drive.list_inbox_files(
            folder_id=self.inbox_folder_id,
            processed_file_ids=processed_file_ids
        )

        if new_files:
            logger.info(f"🆕 {len(new_files)} 件の新規ファイルを検出:")
            for file in new_files:
                logger.info(f"  - {file['name']} (ID: {file['id'][:8]}...)")
        else:
            logger.info("新規ファイルは見つかりませんでした")

        return new_files

    def check_duplicate_by_hash(self, file_meta: Dict[str, Any]) -> Optional[str]:
        """
        ファイルのcontent_hashを計算し、重複をチェック

        Args:
            file_meta: ファイルメタデータ

        Returns:
            content_hash: 重複していない場合はハッシュ値を返す
            None: 重複している場合はNoneを返す
        """
        file_id = file_meta['id']
        file_name = file_meta['name']

        try:
            # 一時ディレクトリにファイルをダウンロード
            temp_dir = tempfile.gettempdir()
            logger.info(f"🔍 重複チェック: {file_name} をダウンロード中...")

            file_path = self.drive.download_file(file_id, file_name, temp_dir)

            # content_hashを計算
            content_hash = calculate_content_hash(file_path)
            logger.info(f"   計算されたハッシュ: {content_hash[:16]}...")

            # 重複チェック
            is_duplicate = self.db.check_duplicate_hash(content_hash)

            # 一時ファイルを削除
            try:
                Path(file_path).unlink()
            except Exception:
                pass

            if is_duplicate:
                logger.warning(f"⚠️  重複検知: {file_name} は既に処理済みです（AI処理スキップ）")
                return None

            logger.info(f"✅ 重複なし: {file_name} は新規ファイルです")
            return content_hash

        except Exception as e:
            logger.error(f"❌ 重複チェックエラー: {file_name} - {e}")
            logger.error(traceback.format_exc())
            # エラー時は処理を続行（安全側に倒す）
            return "error_skip_hash_check"

    async def process_file(self, file_meta: Dict[str, Any]) -> bool:
        """
        新規ファイルを Stage E-K パイプラインで処理

        Args:
            file_meta: ファイルメタデータ

        Returns:
            処理が成功した場合True
        """
        file_name = file_meta['name']
        file_id = file_meta['id']
        mime_type = file_meta.get('mimeType', 'application/octet-stream')

        logger.info(f"⚙️  ファイル処理開始: {file_name}")

        local_path = None
        try:
            # ファイルをダウンロード
            local_path = self.drive.download_file(file_id, file_name, self.temp_dir)
            logger.info(f"ダウンロード完了: {local_path}")

            # Stage E-K で処理（inbox workspace）
            # doc_type は'other'として扱う（inbox からの自動取り込み）
            result = await self.pipeline.process_document(
                file_path=Path(local_path),
                file_name=file_name,
                doc_type='other',
                workspace='inbox',
                mime_type=mime_type,
                source_id=file_id
            )

            if result and result.get('success'):
                logger.info(f"✅ ファイル処理成功: {file_name}")
                return True
            else:
                error_msg = result.get('error', 'unknown error') if result else 'no result'
                logger.error(f"❌ ファイル処理失敗: {error_msg}")
                return False

        except Exception as e:
            logger.error(f"❌ ファイル処理中に致命的なエラーが発生: {file_name}")
            logger.error(traceback.format_exc())
            return False

        finally:
            # 一時ファイルを削除
            if local_path and Path(local_path).exists():
                Path(local_path).unlink()
                logger.debug(f"一時ファイル削除: {local_path}")

    def move_to_archive(self, file_id: str, file_name: str) -> bool:
        """
        処理済みファイルをArchiveフォルダに移動

        Args:
            file_id: ファイルID
            file_name: ファイル名

        Returns:
            移動が成功した場合True
        """
        if not self.archive_folder_id:
            logger.warning(f"⚠️  ARCHIVE_FOLDER_ID が設定されていないため、{file_name} の移動をスキップします")
            return False

        logger.info(f"📦 ファイルをArchiveに移動中: {file_name}")

        success = self.drive.move_file(file_id, self.archive_folder_id)

        if success:
            logger.info(f"✅ ファイル移動成功: {file_name} -> Archive")
        else:
            logger.error(f"❌ ファイル移動失敗: {file_name}")

        return success

    async def run_monitoring_cycle(self):
        """監視サイクルのメイン実行"""
        logger.info("=" * 70)
        logger.info("🔍 InBox自動監視システム 開始")
        logger.info(f"実行時刻: {datetime.now():%Y-%m-%d %H:%M:%S}")
        logger.info("=" * 70)

        stats = {
            'new_files_detected': 0,
            'duplicates_skipped': 0,
            'processed_success': 0,
            'processed_failed': 0,
            'archived_success': 0,
            'archived_failed': 0
        }

        try:
            # Step 1: 処理済みファイルIDを取得
            processed_file_ids = self.get_processed_file_ids()

            # Step 2: InBoxから新規ファイルを検出
            new_files = self.scan_inbox_for_new_files(processed_file_ids)
            stats['new_files_detected'] = len(new_files)

            # Step 3: 各ファイルを処理
            for file_meta in new_files:
                file_id = file_meta['id']
                file_name = file_meta['name']

                # Step 3-1: 重複チェック（content_hash）
                content_hash = self.check_duplicate_by_hash(file_meta)

                if content_hash is None:
                    # 重複ファイル：AI処理をスキップ
                    stats['duplicates_skipped'] += 1
                    logger.info(f"💰 コスト削減: {file_name} のAI処理をスキップしました")

                    # 重複ファイルもArchiveに移動
                    if self.archive_folder_id:
                        archive_success = self.move_to_archive(file_id, file_name)
                        if archive_success:
                            stats['archived_success'] += 1
                        else:
                            stats['archived_failed'] += 1
                    continue

                # Step 3-2: ファイルを処理（重複なしの場合）
                success = await self.process_file(file_meta)

                if success:
                    stats['processed_success'] += 1

                    # 処理成功後、Archiveに移動
                    if self.archive_folder_id:
                        archive_success = self.move_to_archive(file_id, file_name)
                        if archive_success:
                            stats['archived_success'] += 1
                        else:
                            stats['archived_failed'] += 1
                else:
                    stats['processed_failed'] += 1
                    logger.warning(f"⚠️  処理失敗のため、{file_name} はInBoxに残します")

        except Exception as e:
            logger.error(f"❌ 監視サイクル実行中に致命的なエラーが発生: {e}")
            logger.error(traceback.format_exc())

        # サマリー表示
        logger.info("=" * 70)
        logger.info("📊 InBox自動監視システム 完了サマリー")
        logger.info(f"新規ファイル検出数: {stats['new_files_detected']}")
        logger.info(f"重複によりスキップ: {stats['duplicates_skipped']} 件 💰")
        logger.info(f"AI処理成功数: {stats['processed_success']}")
        logger.info(f"AI処理失敗数: {stats['processed_failed']}")
        logger.info(f"アーカイブ成功数: {stats['archived_success']}")
        logger.info(f"アーカイブ失敗数: {stats['archived_failed']}")
        logger.info("=" * 70)

        return stats


async def main():
    """メイン関数"""
    try:
        monitor = InBoxMonitor()
        stats = await monitor.run_monitoring_cycle()

        # 終了コード決定
        if stats['new_files_detected'] == 0:
            logger.info("✨ 新規ファイルがありませんでした（正常終了）")
            sys.exit(0)

        if stats['processed_failed'] == 0:
            logger.info("✅ すべてのファイルが正常に処理されました")
            sys.exit(0)

        failure_rate = stats['processed_failed'] / stats['new_files_detected']

        if failure_rate >= 0.5:
            logger.error(f"❌ 失敗率が高すぎます ({failure_rate:.1%})。システムレベルの問題の可能性があります。")
            sys.exit(1)
        else:
            logger.warning(f"⚠️  {stats['processed_failed']}件のファイル処理が失敗しました。")
            sys.exit(2)

    except Exception as e:
        logger.error(f"❌ プログラム実行中に致命的なエラーが発生: {e}")
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
```

### services\data-ingestion\netsuper_category_manager_ui.py

```py
"""
ネットスーパーカテゴリー管理UI

Streamlitを使用して、カテゴリーごとの実行スケジュールを管理します。
"""

import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import sys

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

from common.category_manager_db import CategoryManagerDB

# ページ設定
st.set_page_config(
    page_title="ネットスーパーカテゴリー管理",
    page_icon="🛒",
    layout="wide"
)

st.title("🛒 ネットスーパーカテゴリー管理")

# CategoryManagerの初期化（Supabaseベース）
manager = CategoryManagerDB()

# タブで店舗を切り替え
tabs = st.tabs(["楽天西友", "東急ストア", "ダイエー", "設定"])

# 各店舗の共通処理
def show_store_categories(store_name: str, store_display_name: str):
    """店舗のカテゴリー管理画面を表示"""
    st.header(f"{store_display_name} カテゴリー管理")

    categories = manager.get_all_categories(store_name)

    if not categories:
        st.info(f"{store_display_name} のカテゴリーが登録されていません。")
        st.markdown("初期化するには、スクレイピングスクリプトを一度実行してください。")
        return

    # 統計情報
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("総カテゴリー数", len(categories))
    with col2:
        enabled_count = sum(1 for cat in categories if cat.get("enabled", True))
        st.metric("有効", enabled_count)
    with col3:
        disabled_count = len(categories) - enabled_count
        st.metric("無効", disabled_count)
    with col4:
        now = datetime.now()
        runnable_count = sum(
            1 for cat in categories
            if cat.get("enabled", True) and manager.should_run_category(store_name, cat["category_name"], now)
        )
        st.metric("実行可能", runnable_count)

    st.divider()

    # カテゴリー一覧を表形式で表示・編集
    st.subheader("カテゴリー一覧")

    # データフレームに変換
    df_data = []
    for cat in categories:
        df_data.append({
            "名前": cat["category_name"],
            "有効": cat.get("enabled", True),
            "開始日": cat.get("start_date", ""),
            "インターバル（日）": cat.get("interval_days", 7),
            "前回実行日": cat.get("last_run", "未実行"),
            "備考": cat.get("notes", "")
        })

    df = pd.DataFrame(df_data)

    # データエディタで編集
    edited_df = st.data_editor(
        df,
        column_config={
            "名前": st.column_config.TextColumn("カテゴリー名", disabled=True, width="medium"),
            "有効": st.column_config.CheckboxColumn("有効", width="small"),
            "開始日": st.column_config.TextColumn(
                "開始日",
                help="YYYY-MM-DD形式で入力（例: 2025-12-24）",
                width="medium"
            ),
            "インターバル（日）": st.column_config.NumberColumn(
                "インターバル（日）",
                min_value=1,
                max_value=365,
                step=1,
                width="small"
            ),
            "前回実行日": st.column_config.TextColumn("前回実行日", disabled=True, width="medium"),
            "備考": st.column_config.TextColumn("備考", width="large")
        },
        hide_index=True,
        width="stretch",
        key=f"editor_{store_name}"
    )

    # ボタン行
    col1, col2, col3 = st.columns([2, 1, 1])

    with col1:
        if st.button("💾 変更を保存", type="primary", key=f"save_{store_name}"):
            # 変更内容を反映
            for idx, row in edited_df.iterrows():
                category_name = row["名前"]
                manager.update_category(
                    store_name,
                    category_name,
                    {
                        "enabled": row["有効"],
                        "start_date": row["開始日"],
                        "interval_days": int(row["インターバル（日）"]),
                        "notes": row["備考"]
                    }
                )
            st.success("✅ 変更を保存しました")
            st.rerun()

    with col2:
        if st.button("🔄 最終実行日をリセット", key=f"reset_{store_name}"):
            for cat in categories:
                manager.update_category(store_name, cat["category_name"], {"last_run": None})
            st.success("✅ すべてのカテゴリーの最終実行日をリセットしました")
            st.rerun()

    with col3:
        if st.button("✅ すべて有効化", key=f"enable_all_{store_name}"):
            for cat in categories:
                manager.update_category(store_name, cat["category_name"], {"enabled": True})
            st.success("✅ すべてのカテゴリーを有効化しました")
            st.rerun()

    st.divider()

    # 実行日・インターバルの説明
    with st.expander("ℹ️ 開始日・インターバルの仕組み"):
        st.markdown("""
        ### 実行の仕組み

        1. **開始日**
           - 手動で自由に設定可能（YYYY-MM-DD形式）
           - GitHub Actionsが毎日午前2時に実行
           - 「現在日付 >= 開始日」のカテゴリーが処理される

        2. **実行後の自動更新**
           - 実行後、開始日が自動更新される
           - 計算式: `実行日 + インターバル日数 + 1日`
           - 例: 12/24実行、インターバル7日 → 次回開始日は 1/1

        3. **手動実行（ローカルのターミナルから）**
           ```bash
           # 楽天西友 - 特定カテゴリーを手動実行
           MANUAL_CATEGORIES="野菜,果物" python -m B_ingestion.rakuten_seiyu.process_with_schedule --manual

           # 東急ストア - 特定カテゴリーを手動実行
           MANUAL_CATEGORIES="野菜,果物" python -m B_ingestion.tokyu_store.process_with_schedule --manual

           # ダイエー - 特定カテゴリーを手動実行
           MANUAL_CATEGORIES="野菜・果物" python -m B_ingestion.daiei.process_with_schedule --manual

           # スケジュール通りに実行（オプション指定なし）
           python -m B_ingestion.rakuten_seiyu.process_with_schedule
           ```

        ### 例

        - **通常運用**: 開始日 = 2025-12-28、インターバル = 7日
          - 12/28 午前2時のGitHub Actions実行時に処理
          - 開始日が自動的に 2026-01-05 に更新

        - **翌日午前2時に実行したい場合**:
          - 開始日を過去の日付（例: 2025-12-23）に設定
          - 翌日午前2時のGitHub Actions実行時に処理される
        """)

# 各タブに店舗を表示
with tabs[0]:
    show_store_categories("rakuten_seiyu", "楽天西友ネットスーパー")

with tabs[1]:
    show_store_categories("tokyu_store", "東急ストア")

with tabs[2]:
    show_store_categories("daiei", "ダイエーネットスーパー")

# 設定タブ
with tabs[3]:
    st.header("⚙️ 全般設定")

    st.subheader("データベース情報")
    st.text(f"テーブル名: {manager.table_name}")
    st.caption("Supabaseテーブルでスケジュール管理（Streamlit Cloud対応）")

    # 各店舗のカテゴリー数を表示
    col1, col2, col3 = st.columns(3)
    with col1:
        rakuten_cats = manager.get_all_categories("rakuten_seiyu")
        st.metric("楽天西友", f"{len(rakuten_cats)}カテゴリー")
    with col2:
        tokyu_cats = manager.get_all_categories("tokyu_store")
        st.metric("東急ストア", f"{len(tokyu_cats)}カテゴリー")
    with col3:
        daiei_cats = manager.get_all_categories("daiei")
        st.metric("ダイエー", f"{len(daiei_cats)}カテゴリー")

    st.divider()

    st.subheader("一括設定")

    col1, col2, col3 = st.columns(3)

    with col1:
        default_interval = st.number_input(
            "デフォルトインターバル（日）",
            min_value=1,
            max_value=365,
            value=7,
            key="default_interval"
        )

    with col2:
        # 日付入力
        default_date = st.date_input(
            "デフォルト開始日",
            value=datetime.now() + timedelta(days=1),
            key="default_date"
        )

    with col3:
        target_store = st.selectbox(
            "対象店舗",
            ["rakuten_seiyu", "tokyu_store", "daiei"],
            format_func=lambda x: {
                "rakuten_seiyu": "楽天西友",
                "tokyu_store": "東急ストア",
                "daiei": "ダイエー"
            }[x],
            key="target_store"
        )

    if st.button("🔄 選択した店舗のすべてのカテゴリーに一括適用"):
        # 日付を文字列に変換
        start_date_str = default_date.strftime("%Y-%m-%d")

        categories = manager.get_all_categories(target_store)
        for cat in categories:
            manager.update_category(
                target_store,
                cat["category_name"],
                {
                    "start_date": start_date_str,
                    "interval_days": default_interval
                }
            )
        st.success(f"✅ {target_store} のすべてのカテゴリーに設定を適用しました")
        st.rerun()

    st.divider()

    st.subheader("データベースの内容")

    # 全店舗のスケジュールデータを取得して表示
    all_stores = ["rakuten_seiyu", "tokyu_store", "daiei"]
    all_schedules = []

    for store in all_stores:
        categories = manager.get_all_categories(store)
        for cat in categories:
            all_schedules.append({
                "店舗": store,
                "カテゴリー": cat.get("category_name"),
                "有効": cat.get("enabled", True),
                "開始日": cat.get("start_date"),
                "インターバル": cat.get("interval_days", 7),
                "前回実行": cat.get("last_run", "未実行")
            })

    if all_schedules:
        import pandas as pd
        df = pd.DataFrame(all_schedules)
        st.dataframe(df, hide_index=True, use_container_width=True)
    else:
        st.info("スケジュールデータがありません。スクレイピングスクリプトを実行してカテゴリーを初期化してください。")
```

### services\data-ingestion\netsuper_classification_manager_ui.py

```py
"""
ネットスーパー商品分類管理UI

商品のgeneral_nameとsmall_categoryを分類ごとにレビュー・修正します。
"""

import streamlit as st
import pandas as pd
import os
from datetime import datetime, timezone
from supabase import create_client

# ページ設定
st.set_page_config(
    page_title="商品分類管理",
    page_icon="🏷️",
    layout="wide"
)

st.title("🏷️ ネットスーパー商品分類管理")

# Supabase接続（サービスロールキーを使用してRLS制限を回避）
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_ROLE_KEY = ***REDACTED***"SUPABASE_SERVICE_ROLE_KEY")

if not SUPABASE_URL or not SUPABASE_SERVICE_ROLE_KEY:
    st.error("環境変数 SUPABASE_URL と SUPABASE_SERVICE_ROLE_KEY を設定してください")
    st.stop()

db = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)

# デバッグ: UIが見ているデータベースの商品数を表示
_debug_count = db.table('Rawdata_NETSUPER_items').select('id', count='exact').execute()
st.warning(f"🔍 デバッグ: このUIが見ている全商品数 = {_debug_count.count}件 | URL: {SUPABASE_URL[:30]}...")

# セッション状態の初期化
if 'general_name_index' not in st.session_state:
    st.session_state.general_name_index = 0
if 'large_category' not in st.session_state:
    st.session_state.large_category = None
if 'medium_category' not in st.session_state:
    st.session_state.medium_category = None
if 'small_category' not in st.session_state:
    st.session_state.small_category = None

# 一般名詞リスト取得（キャッシュ）
@st.cache_data(ttl=60)
def get_general_names():
    result = db.table('Rawdata_NETSUPER_items').select(
        'general_name'
    ).not_.is_('general_name', 'null').execute()
    return sorted(list(set([r['general_name'] for r in result.data if r.get('general_name')])))

# カテゴリ階層を構築（キャッシュ）
@st.cache_data(ttl=60)
def build_category_hierarchy():
    """MASTER_Categories_productから階層パスを構築"""
    result = db.table('MASTER_Categories_product').select('id, name, parent_id').execute()

    categories = {cat['id']: cat for cat in result.data}

    def get_path(cat_id):
        """カテゴリIDから階層パスを取得"""
        path = []
        current_id = cat_id
        while current_id:
            cat = categories.get(current_id)
            if cat:
                path.insert(0, cat['name'])
                current_id = cat['parent_id']
            else:
                break
        return ' > '.join(path)

    # 各カテゴリのパスを構築
    paths = {}
    for cat_id, cat in categories.items():
        paths[cat['name']] = get_path(cat_id)

    return paths

# 大分類を取得（商品数付き）
@st.cache_data(ttl=0)
def get_large_categories():
    """大分類を取得（商品1件以上のみ、件数表示）"""
    import pandas as pd

    try:
        # PostgreSQL で直接集計（超高速）
        # RPC関数を使うか、生SQLでGROUP BYする
        # まずカテゴリーマッピングを取得（全件）
        all_categories = []
        offset = 0
        while True:
            result = db.table('MASTER_Categories_product').select('id, large_category').range(offset, offset + 999).execute()
            if not result.data:
                break
            all_categories.extend(result.data)
            offset += 1000
        cat_df = pd.DataFrame(all_categories)

        # 商品のcategory_idごとの件数を取得（全件）
        all_products = []
        offset = 0
        while True:
            result = db.table('Rawdata_NETSUPER_items').select('category_id').not_.is_('category_id', 'null').range(offset, offset + 999).execute()
            if not result.data:
                break
            all_products.extend(result.data)
            offset += 1000
        prod_df = pd.DataFrame(all_products)

        # Pandasで超高速集計
        if len(prod_df) > 0 and len(cat_df) > 0:
            # category_idごとの件数
            count_by_cat = prod_df['category_id'].value_counts().to_dict()

            # large_categoryにマッピング
            cat_map = cat_df.set_index('id')['large_category'].to_dict()

            # large_categoryごとに集計
            large_counts = {}
            for cat_id, count in count_by_cat.items():
                large_cat = cat_map.get(cat_id)
                if large_cat:
                    large_counts[large_cat] = large_counts.get(large_cat, 0) + count

            # 辞書に追加
            cat_with_counts = {f"{large} ({count}件)": large for large, count in large_counts.items() if count > 0}
        else:
            cat_with_counts = {}

    except Exception as e:
        st.error(f"大分類取得エラー: {e}")
        cat_with_counts = {}

    # 未分類を追加
    unclassified_result = db.table('Rawdata_NETSUPER_items').select('id', count='exact').is_('category_id', 'null').execute()
    unclassified_count = unclassified_result.count if unclassified_result.count else 0

    if unclassified_count > 0:
        if any("未分類" in key for key in cat_with_counts.keys()):
            cat_with_counts[f"未分類(カテゴリ未設定) ({unclassified_count}件)"] = "未分類(カテゴリ未設定)"
        else:
            cat_with_counts[f"未分類 ({unclassified_count}件)"] = "未分類"

    return cat_with_counts

# 中分類を取得（商品数付き）
@st.cache_data(ttl=60)
def get_medium_categories(large_category_name):
    """指定した大分類の中分類を取得（商品1件以上のみ、件数表示）"""
    import pandas as pd

    cat_with_counts = {}

    # category_id IS NULL（未分類）の場合
    if large_category_name == "未分類" or large_category_name == "未分類(カテゴリ未設定)":
        unclassified_result = db.table('Rawdata_NETSUPER_items').select('id', count='exact').is_('category_id', 'null').execute()
        unclassified_count = unclassified_result.count if unclassified_result.count else 0
        if unclassified_count > 0:
            cat_with_counts[f"未分類 ({unclassified_count}件)"] = "未分類"
        return cat_with_counts

    try:
        # 1. この大分類のカテゴリーから category_id → medium_category のマッピングを取得
        categories = db.table('MASTER_Categories_product').select('id, medium_category').eq('large_category', large_category_name).execute()
        cat_df = pd.DataFrame(categories.data)

        if len(cat_df) == 0:
            return cat_with_counts

        cat_ids = cat_df['id'].tolist()

        # 2. これらのカテゴリーIDに属する商品を取得（バッチ処理）
        all_products = []
        batch_size = 50
        for i in range(0, len(cat_ids), batch_size):
            batch_ids = cat_ids[i:i+batch_size]
            try:
                products = db.table('Rawdata_NETSUPER_items').select('category_id').in_('category_id', batch_ids).execute()
                all_products.extend(products.data)
            except Exception:
                pass

        if not all_products:
            return cat_with_counts

        # 3. Pandasで超高速集計
        prod_df = pd.DataFrame(all_products)
        count_by_cat = prod_df['category_id'].value_counts().to_dict()

        # medium_categoryにマッピング
        cat_map = cat_df.set_index('id')['medium_category'].to_dict()

        # medium_categoryごとに集計
        medium_counts = {}
        for cat_id, count in count_by_cat.items():
            medium_cat = cat_map.get(cat_id)
            if medium_cat:
                medium_counts[medium_cat] = medium_counts.get(medium_cat, 0) + count

        # 辞書に追加
        cat_with_counts = {f"{medium} ({count}件)": medium for medium, count in medium_counts.items() if count > 0}

    except Exception as e:
        st.error(f"中分類取得エラー: {e}")

    return cat_with_counts

# 小分類を取得（商品数付き）
@st.cache_data(ttl=60)
def get_small_categories_by_medium(large_category_name, medium_category_name):
    """指定した大分類・中分類の小分類を取得（商品1件以上のみ、件数表示）"""
    cat_with_counts = {}

    # category_id IS NULL（未分類）の場合
    if medium_category_name == "未分類":
        unclassified_result = db.table('Rawdata_NETSUPER_items').select('id', count='exact').is_('category_id', 'null').execute()
        unclassified_count = unclassified_result.count if unclassified_result.count else 0
        if unclassified_count > 0:
            cat_with_counts[f"未分類 ({unclassified_count}件)"] = "未分類"
        return cat_with_counts

    # この大分類・中分類に属するDISTINCT small_categoryを取得
    categories = db.table('MASTER_Categories_product').select('small_category, id').eq('large_category', large_category_name).eq('medium_category', medium_category_name).execute()

    for cat in categories.data:
        small_name = cat.get('small_category')
        cat_id = cat.get('id')

        if not small_name:
            continue

        # 商品数をカウント
        count_result = db.table('Rawdata_NETSUPER_items').select('id', count='exact').eq('category_id', cat_id).execute()
        count = count_result.count if count_result.count else 0

        # 商品が1件以上ある場合のみ追加
        if count > 0:
            cat_with_counts[f"{small_name} ({count}件)"] = small_name

    return cat_with_counts

# タブで表示方法を切り替え
tabs = st.tabs(["一般名詞で分類", "小カテゴリで分類", "統計情報"])

# =============================================================================
# タブ1: 一般名詞で分類
# =============================================================================
with tabs[0]:
    st.header("一般名詞（general_name）ごとに商品を確認・修正")

    general_names = get_general_names()

    if not general_names:
        st.info("一般名詞が設定されている商品がありません。")
    else:
        # 検索ボックス
        search_term = st.text_input(
            "🔍 一般名詞を検索",
            placeholder="例: 牛乳、卵、パン...",
            key="general_name_search"
        )

        # 検索フィルタリング
        if search_term:
            filtered_names = [name for name in general_names if search_term.lower() in name.lower()]
        else:
            filtered_names = general_names

        if not filtered_names:
            st.warning(f"「{search_term}」に一致する一般名詞が見つかりません。")
        else:
            # 一般名詞を選択
            selected_general_name = st.selectbox(
                f"一般名詞を選択（{len(filtered_names)}件）",
                filtered_names,
                index=min(st.session_state.general_name_index, len(filtered_names)-1),
                key="general_name_select",
                on_change=lambda: setattr(st.session_state, 'general_name_index', filtered_names.index(st.session_state.general_name_select) if st.session_state.general_name_select in filtered_names else 0)
            )

            if selected_general_name:
                # 選択した一般名詞の商品を取得
                products = db.table('Rawdata_NETSUPER_items').select(
                    'id, product_name, general_name, small_category, organization, current_price_tax_included'
                ).eq('general_name', selected_general_name).limit(100).execute()

                st.subheader(f"一般名詞: {selected_general_name} ({len(products.data)}件)")

                if products.data:
                    # データフレームに変換
                    df_data = []
                    for p in products.data:
                        df_data.append({
                            "ID": p['id'],
                            "商品名": p['product_name'],
                            "一般名詞": p.get('general_name', ''),
                            "小カテゴリ": p.get('small_category', ''),
                            "店舗": p.get('organization', ''),
                            "価格": p.get('current_price_tax_included', 0)
                        })

                    df = pd.DataFrame(df_data)

                    # データエディタで編集
                    edited_df = st.data_editor(
                        df,
                        column_config={
                            "ID": st.column_config.TextColumn("ID", disabled=True, width="small"),
                            "商品名": st.column_config.TextColumn("商品名", disabled=True, width="large"),
                            "一般名詞": st.column_config.TextColumn("一般名詞", width="medium"),
                            "小カテゴリ": st.column_config.TextColumn("小カテゴリ", width="medium"),
                            "店舗": st.column_config.TextColumn("店舗", disabled=True, width="medium"),
                            "価格": st.column_config.NumberColumn("価格", disabled=True, width="small")
                        },
                        hide_index=True,
                        key=f"editor_general_{selected_general_name}"
                    )

                    # 保存ボタン
                    if st.button("💾 変更を保存", type="primary", key="save_general"):
                        try:
                            # 変更を反映
                            current_time = datetime.now(timezone.utc).isoformat()
                            success_count = 0
                            has_verified_column = True

                            for idx, row in edited_df.iterrows():
                                product_id = row["ID"]
                                update_data = {
                                    "general_name": row["一般名詞"],
                                    "small_category": row["小カテゴリ"]
                                }

                                # manually_verified カラムが存在する場合のみ追加
                                if has_verified_column:
                                    update_data["manually_verified"] = True
                                    update_data["last_verified_at"] = current_time

                                try:
                                    db.table('Rawdata_NETSUPER_items').update(update_data).eq('id', product_id).execute()
                                    success_count += 1
                                except Exception as e:
                                    # manually_verified カラムが存在しない場合、フラグなしで再試行
                                    if "manually_verified" in str(e) and has_verified_column:
                                        has_verified_column = False
                                        update_data = {
                                            "general_name": row["一般名詞"],
                                            "small_category": row["小カテゴリ"]
                                        }
                                        db.table('Rawdata_NETSUPER_items').update(update_data).eq('id', product_id).execute()
                                        success_count += 1
                                    else:
                                        raise

                            if has_verified_column:
                                st.success(f"✅ {success_count}件の商品を更新しました（検証済みとしてマーク）")
                            else:
                                st.success(f"✅ {success_count}件の商品を更新しました")
                                st.info("💡 ヒント: マイグレーション実行後、検証済みフラグが自動的に付くようになります")
                            st.rerun()
                        except Exception as e:
                            st.error(f"❌ 保存中にエラーが発生しました: {str(e)}")
                            st.exception(e)

# =============================================================================
# タブ2: 小カテゴリで分類（3段階連動プルダウン + 未分類対応）
# =============================================================================
with tabs[1]:
    st.header("小カテゴリ（small_category）ごとに商品を確認・修正")

    # 大分類を取得（{表示名: 実名} の辞書）
    large_categories_dict = get_large_categories()
    large_display_names = list(large_categories_dict.keys())

    col1, col2, col3 = st.columns(3)

    with col1:
        # 大分類プルダウン（未分類は get_large_categories() に含まれる）
        selected_large_display = st.selectbox(
            "🏢 大分類",
            ["選択してください"] + large_display_names,
            key="large_cat_select"
        )

    # 表示名から実名を取得
    if selected_large_display in large_categories_dict:
        selected_large = large_categories_dict[selected_large_display]
    else:
        selected_large = selected_large_display  # "選択してください" or "未分類"

    # 中分類を取得
    medium_categories_dict = {}
    if selected_large and selected_large not in ["選択してください"]:
        medium_categories_dict = get_medium_categories(selected_large)
    medium_display_names = list(medium_categories_dict.keys())

    with col2:
        # 中分類プルダウン
        if selected_large == "選択してください":
            st.selectbox("📂 中分類", ["大分類を選択してください"], disabled=True)
            selected_medium_display = None
            selected_medium = None
        elif medium_display_names:
            selected_medium_display = st.selectbox(
                "📂 中分類",
                ["選択してください"] + medium_display_names,
                key="medium_cat_select"
            )
            # 表示名から実名を取得
            if selected_medium_display in medium_categories_dict:
                selected_medium = medium_categories_dict[selected_medium_display]
            else:
                selected_medium = selected_medium_display
        else:
            st.selectbox("📂 中分類", ["該当なし"], disabled=True)
            selected_medium_display = None
            selected_medium = None

    # 小分類を取得
    small_categories_dict = {}
    if selected_medium and selected_medium not in ["選択してください"]:
        small_categories_dict = get_small_categories_by_medium(selected_large, selected_medium)

    small_display_names = list(small_categories_dict.keys())

    with col3:
        # 小分類プルダウン
        if selected_medium is None or selected_medium == "選択してください":
            st.selectbox("📄 小分類", ["中分類を選択してください"], disabled=True)
            selected_small_display = None
            selected_small = None
        elif small_display_names:
            selected_small_display = st.selectbox(
                "📄 小分類",
                ["選択してください"] + small_display_names,
                key="small_cat_select"
            )
            # 表示名から実名を取得
            if selected_small_display in small_categories_dict:
                selected_small = small_categories_dict[selected_small_display]
            else:
                selected_small = selected_small_display
        else:
            st.selectbox("📄 小分類", ["該当なし"], disabled=True)
            selected_small_display = None
            selected_small = None

    # 商品取得関数（キャッシュ付き）
    @st.cache_data(ttl=300)
    def fetch_products_by_category(large, medium, small):
        """カテゴリに応じた商品を取得（5分間キャッシュ）"""
        # 小分類まで選択されている場合
        if small and small != "選択してください":
            # 未分類（category_id IS NULL）の場合
            if small == "未分類":
                result = db.table('Rawdata_NETSUPER_items').select(
                    'id, product_name, general_name, small_category, category_id, organization, current_price_tax_included'
                ).is_('category_id', 'null').limit(1000).execute()
                return result.data

            cat_result = db.table('MASTER_Categories_product').select('id').eq(
                'large_category', large
            ).eq('medium_category', medium).eq('small_category', small).execute()

            if cat_result.data:
                small_id = cat_result.data[0]['id']
                result = db.table('Rawdata_NETSUPER_items').select(
                    'id, product_name, general_name, small_category, category_id, organization, current_price_tax_included'
                ).eq('category_id', small_id).limit(100).execute()
                return result.data
            return []

        # 大+中分類選択、小分類は未選択
        elif medium and medium not in ["選択してください", None]:
            cat_result = db.table('MASTER_Categories_product').select('id').eq(
                'large_category', large
            ).eq('medium_category', medium).execute()

            all_cat_ids = [cat['id'] for cat in cat_result.data]

            if all_cat_ids:
                # PostgRESTの.in()制限を回避: 50件ずつバッチ処理
                all_products = []
                batch_size = 50
                for i in range(0, len(all_cat_ids), batch_size):
                    batch_ids = all_cat_ids[i:i+batch_size]
                    try:
                        result = db.table('Rawdata_NETSUPER_items').select(
                            'id, product_name, general_name, small_category, category_id, organization, current_price_tax_included'
                        ).in_('category_id', batch_ids).limit(1000).execute()
                        all_products.extend(result.data)
                        if len(all_products) >= 1000:
                            break
                    except Exception as e:
                        # エラーが発生してもスキップして続行
                        pass
                return all_products[:1000]
            return []

        # 大分類のみ選択、中分類は未選択
        elif large and large not in ["選択してください"]:
            cat_result = db.table('MASTER_Categories_product').select('id').eq(
                'large_category', large
            ).execute()

            all_cat_ids = [cat['id'] for cat in cat_result.data]

            if all_cat_ids:
                # PostgRESTの.in()制限を回避: 50件ずつバッチ処理
                all_products = []
                batch_size = 50
                for i in range(0, len(all_cat_ids), batch_size):
                    batch_ids = all_cat_ids[i:i+batch_size]
                    try:
                        result = db.table('Rawdata_NETSUPER_items').select(
                            'id, product_name, general_name, small_category, category_id, organization, current_price_tax_included'
                        ).in_('category_id', batch_ids).limit(1000).execute()
                        all_products.extend(result.data)
                        if len(all_products) >= 1000:
                            break
                    except Exception as e:
                        # エラーが発生してもスキップして続行
                        pass
                return all_products[:1000]
            return []

        return []

    # 商品を取得
    products_data = fetch_products_by_category(selected_large, selected_medium, selected_small)

    # 表示パスを設定
    display_path = ""
    if selected_small and selected_small != "選択してください":
        display_path = f"📂 {selected_large} > {selected_medium} > {selected_small}"
    elif selected_medium and selected_medium not in ["選択してください", None]:
        display_path = f"📂 {selected_large} > {selected_medium} （配下全て）"
    elif selected_large and selected_large not in ["選択してください"]:
        display_path = f"📂 {selected_large} （配下全て）"

    if products_data:
        st.subheader(f"{display_path} ({len(products_data)}件)")

        # category_idからカテゴリ情報を取得するためのキャッシュ
        category_cache = {}

        def get_category_info(category_id):
            """category_idから大中小分類を取得"""
            if not category_id:
                return "未分類", "未分類", "未分類"

            if category_id in category_cache:
                return category_cache[category_id]

            # MASTER_Categories_productから取得
            result = db.table('MASTER_Categories_product').select(
                'large_category, medium_category, small_category'
            ).eq('id', category_id).execute()

            if result.data:
                cat = result.data[0]
                large = cat.get('large_category') or "未分類"
                medium = cat.get('medium_category') or "未分類"
                small = cat.get('small_category') or "未分類"
                category_cache[category_id] = (large, medium, small)
                return large, medium, small

            return "未分類", "未分類", "未分類"

        # データフレームに変換（ID列を削除、大中分類を追加）
        df_data = []
        for p in products_data:
            large, medium, small = get_category_info(p.get('category_id'))

            df_data.append({
                "選択": False,  # チェックボックス
                "_id": p['id'],  # 内部用（非表示）
                "商品名": p['product_name'],
                "一般名詞": p.get('general_name', ''),
                "大分類": large,
                "中分類": medium,
                "小分類": small,
                "店舗": p.get('organization', ''),
                "価格": p.get('current_price_tax_included', 0)
            })

        df = pd.DataFrame(df_data)

        # セッションステートで選択状態を管理
        selection_key = f"selection_{selected_large}_{selected_medium}_{selected_small}"
        if selection_key not in st.session_state:
            st.session_state[selection_key] = {}

        # 全選択・全解除ボタン
        col_select1, col_select2, col_select3 = st.columns([1, 1, 8])

        with col_select1:
            if st.button("☑️ 全選択", key="select_all"):
                for idx in range(len(df)):
                    st.session_state[selection_key][idx] = True
                st.rerun()

        with col_select2:
            if st.button("☐ 全解除", key="deselect_all"):
                st.session_state[selection_key] = {}
                st.rerun()

        # セッションステートの選択状態をデータフレームに反映
        for idx in st.session_state[selection_key]:
            if idx < len(df):
                df.at[idx, "選択"] = st.session_state[selection_key][idx]

        # 一括設定UI
        st.markdown("---")
        st.subheader("📦 選択した商品に一括適用")

        col_bulk1, col_bulk2, col_bulk3, col_bulk4 = st.columns([2, 2, 2, 1])

        with col_bulk1:
            bulk_large = st.text_input("🏢 大分類", key="bulk_large", placeholder="例: 食品類")
        with col_bulk2:
            bulk_medium = st.text_input("📂 中分類", key="bulk_medium", placeholder="例: 調味料")
        with col_bulk3:
            bulk_small = st.text_input("📄 小分類", key="bulk_small", placeholder="例: 味噌")

        st.markdown("---")

        # データエディタで編集
        edited_df = st.data_editor(
            df,
            column_config={
                "選択": st.column_config.CheckboxColumn("選択", default=False, width="small"),
                "_id": None,  # 非表示
                "商品名": st.column_config.TextColumn("商品名", disabled=True, width="large"),
                "一般名詞": st.column_config.TextColumn("一般名詞", width="medium"),
                "大分類": st.column_config.TextColumn("大分類", width="small"),
                "中分類": st.column_config.TextColumn("中分類", width="small"),
                "小分類": st.column_config.TextColumn("小分類", width="medium"),
                "店舗": st.column_config.TextColumn("店舗", disabled=True, width="small"),
                "価格": st.column_config.NumberColumn("価格", disabled=True, width="small")
            },
            hide_index=True,
            key=f"editor_category_{selected_large}_{selected_medium}_{selected_small}"
        )

        # 編集後の選択状態をセッションステートに保存
        for idx, row in edited_df.iterrows():
            if row["選択"]:
                st.session_state[selection_key][idx] = True
            elif idx in st.session_state[selection_key]:
                del st.session_state[selection_key][idx]

        # カテゴリー作成/取得ヘルパー関数
        def get_or_create_category(large_name, medium_name, small_name):
            """カテゴリーを取得、なければ作成（大中小の組み合わせで1つのID）"""
            # トリム
            large_name = str(large_name).strip() if large_name else ""
            medium_name = str(medium_name).strip() if medium_name else ""
            small_name = str(small_name).strip() if small_name else ""

            # 未分類チェック
            if not large_name or not medium_name or not small_name or small_name == "未分類":
                return None

            # 検索・登録用の一つなぎの名前
            category_full_name = f"{large_name}>{medium_name}>{small_name}"

            try:
                # 既存カテゴリーを検索（nameで検索）
                result = db.table('MASTER_Categories_product').select('id').eq('name', category_full_name).execute()

                if result.data:
                    return result.data[0]['id']

                # 新規作成
                new_cat = {
                    'name': category_full_name,
                    'large_category': large_name,
                    'medium_category': medium_name,
                    'small_category': small_name,
                    'parent_id': None
                }
                result = db.table('MASTER_Categories_product').insert(new_cat).execute()

                if not result.data:
                    raise Exception(f"カテゴリ '{category_full_name}' の作成に失敗しました")

                return result.data[0]['id']
            except Exception as e:
                raise Exception(f"カテゴリ '{category_full_name}' の取得/作成中にエラーが発生しました: {str(e)}")

        # 一括適用ボタン
        col_btn1, col_btn2 = st.columns([1, 3])

        with col_btn1:
            if st.button("📦 選択した商品に一括適用", type="primary", key="bulk_apply"):
                # 選択された商品を取得
                selected_rows = edited_df[edited_df["選択"] == True]

                if len(selected_rows) == 0:
                    st.warning("⚠️ 商品が選択されていません")
                elif not bulk_large or not bulk_medium or not bulk_small:
                    st.warning("⚠️ 大分類・中分類・小分類をすべて入力してください")
                else:
                    try:
                        # カテゴリを取得/作成
                        category_id = get_or_create_category(bulk_large, bulk_medium, bulk_small)

                        if not category_id:
                            st.error("❌ カテゴリの作成に失敗しました")
                        else:
                            # 選択された商品を一括更新
                            current_time = datetime.now(timezone.utc).isoformat()
                            success_count = 0

                            for idx, row in selected_rows.iterrows():
                                product_id = row["_id"]

                                update_data = {
                                    "small_category": bulk_small,
                                    "category_id": category_id
                                }

                                try:
                                    db.table('Rawdata_NETSUPER_items').update(update_data).eq('id', product_id).execute()
                                    success_count += 1
                                except Exception as e:
                                    st.error(f"❌ 商品ID {product_id} の更新に失敗: {str(e)}")

                            # キャッシュをクリア
                            st.cache_data.clear()

                            st.success(f"✅ {success_count}件の商品を一括更新しました")
                            st.rerun()

                    except Exception as e:
                        st.error(f"❌ 一括適用中にエラーが発生しました: {str(e)}")
                        st.exception(e)

        with col_btn2:
            st.caption(f"選択中: {len(edited_df[edited_df['選択'] == True])}件")

        # 個別編集保存ボタン
        if st.button("💾 変更を保存", type="primary", key="save_category"):
            try:
                # 変更を反映
                current_time = datetime.now(timezone.utc).isoformat()
                success_count = 0
                has_verified_column = True

                for idx, row in edited_df.iterrows():
                    product_id = row["_id"]

                    # カテゴリー取得/作成
                    large_name = row["大分類"]
                    medium_name = row["中分類"]
                    small_name = row["小分類"]

                    # 大中小の組み合わせで1つのIDを取得/作成
                    category_id = get_or_create_category(large_name, medium_name, small_name)

                    update_data = {
                        "general_name": row["一般名詞"],
                        "small_category": small_name if small_name != "未分類" else None,
                        "category_id": category_id
                    }

                    # manually_verified カラムが存在する場合のみ追加
                    if has_verified_column:
                        update_data["manually_verified"] = True
                        update_data["last_verified_at"] = current_time

                    try:
                        db.table('Rawdata_NETSUPER_items').update(update_data).eq('id', product_id).execute()
                        success_count += 1
                    except Exception as e:
                        # manually_verified カラムが存在しない場合、フラグなしで再試行
                        if "manually_verified" in str(e) and has_verified_column:
                            has_verified_column = False
                            update_data = {
                                "general_name": row["一般名詞"],
                                "small_category": small_name if small_name != "未分類" else None,
                                "category_id": category_id
                            }
                            db.table('Rawdata_NETSUPER_items').update(update_data).eq('id', product_id).execute()
                            success_count += 1
                        else:
                            raise

                # キャッシュをクリア（新しいカテゴリーが追加された場合）
                st.cache_data.clear()

                if has_verified_column:
                    st.success(f"✅ {success_count}件の商品を更新しました（検証済みとしてマーク）")
                else:
                    st.success(f"✅ {success_count}件の商品を更新しました")
                    st.info("💡 ヒント: マイグレーション実行後、検証済みフラグが自動的に付くようになります")
                st.rerun()
            except Exception as e:
                st.error(f"❌ 保存中にエラーが発生しました: {str(e)}")
                st.exception(e)

# =============================================================================
# タブ3: 統計情報
# =============================================================================
with tabs[2]:
    st.header("📊 分類統計情報")

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("一般名詞別の商品数")

        # 一般名詞ごとの商品数を集計
        result = db.table('Rawdata_NETSUPER_items').select(
            'general_name'
        ).not_.is_('general_name', 'null').execute()

        general_name_counts = {}
        for r in result.data:
            name = r.get('general_name', '未分類')
            general_name_counts[name] = general_name_counts.get(name, 0) + 1

        # データフレームに変換して表示
        if general_name_counts:
            df_general = pd.DataFrame([
                {"一般名詞": name, "商品数": count}
                for name, count in sorted(general_name_counts.items(), key=lambda x: x[1], reverse=True)
            ])
            st.dataframe(df_general, hide_index=True, height=400)

    with col2:
        st.subheader("小カテゴリ別の商品数")

        # 小カテゴリごとの商品数を集計
        result = db.table('Rawdata_NETSUPER_items').select(
            'small_category'
        ).not_.is_('small_category', 'null').execute()

        category_counts = {}
        for r in result.data:
            cat = r.get('small_category', '未分類')
            category_counts[cat] = category_counts.get(cat, 0) + 1

        # データフレームに変換して表示
        if category_counts:
            df_category = pd.DataFrame([
                {"小カテゴリ": cat, "商品数": count}
                for cat, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True)
            ])
            st.dataframe(df_category, hide_index=True, height=400)

    # 未分類商品の数
    st.divider()

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        total = db.table('Rawdata_NETSUPER_items').select('id', count='exact').execute()
        st.metric("総商品数", total.count)

    with col2:
        no_general = db.table('Rawdata_NETSUPER_items').select('id', count='exact').is_('general_name', 'null').execute()
        st.metric("一般名詞未設定", no_general.count)

    with col3:
        no_category = db.table('Rawdata_NETSUPER_items').select('id', count='exact').is_('small_category', 'null').execute()
        st.metric("小カテゴリ未設定", no_category.count)

    with col4:
        try:
            verified = db.table('Rawdata_NETSUPER_items').select('id', count='exact').eq('manually_verified', True).execute()
            st.metric("手動検証済み", verified.count, delta="AI学習用データ")
        except Exception:
            # manually_verified カラムがまだ存在しない場合
            st.metric("手動検証済み", 0, delta="要マイグレーション", delta_color="off")
```

### services\data-ingestion\rakuten_seiyu\__init__.py

```py

```

### services\data-ingestion\rakuten_seiyu\auth_manager.py

```py
"""
楽天西友ネットスーパー 認証マネージャー

Playwrightを使用して楽天IDでログインし、配送先を設定した後、
セッションCookieをファイルに保存します。
"""

import json
import logging
from pathlib import Path
from typing import Optional
from playwright.async_api import async_playwright, Browser, Page, BrowserContext

# ロガー設定
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.DEBUG)


class RakutenSeiyuAuthManager:
    """楽天西友ネットスーパーの認証を管理するクラス"""

    def __init__(self, headless: bool = True):
        """
        Args:
            headless: ヘッドレスモードで実行するか（デフォルト: True）
        """
        self.headless = headless
        self.base_url = "https://netsuper.rakuten.co.jp/seiyu"
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        self.playwright = None

    async def __aenter__(self):
        """コンテキストマネージャーのエントリー"""
        await self._launch_browser()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """コンテキストマネージャーの終了"""
        await self.close()

    async def _launch_browser(self):
        """ブラウザを起動"""
        logger.info("ブラウザを起動中...")
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(headless=self.headless)
        self.context = await self.browser.new_context(
            viewport={"width": 1280, "height": 720},
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        )
        self.page = await self.context.new_page()
        logger.info("ブラウザ起動完了")

    async def login(self, rakuten_id: str, password: str) -> bool:
        """
        楽天IDでログイン

        Args:
            rakuten_id: 楽天ID（メールアドレス）
            password: パスワード

        Returns:
            成功したらTrue
        """
        try:
            logger.info("楽天西友トップページにアクセス中...")
            await self.page.goto(self.base_url, wait_until="domcontentloaded", timeout=60000)

            # デバッグ用：ページのスクリーンショットとHTMLを保存
            await self.page.screenshot(path="debug_step1_top_page.png")
            logger.info("スクリーンショット保存: debug_step1_top_page.png")

            html_content = await self.page.content()
            with open("debug_step1_top_page.html", "w", encoding="utf-8") as f:
                f.write(html_content)
            logger.info("HTML保存: debug_step1_top_page.html")

            # ログインボタンを探してクリック
            logger.info("ログインボタンを探しています...")
            logger.info(f"現在のURL: {self.page.url}")

            # 複数のセレクタパターンを試行
            login_selectors = [
                'a:has-text("ログイン")',
                'button:has-text("ログイン")',
                '[data-test="login-button"]',
                '.login-button',
                '#login-link'
            ]

            login_clicked = False
            for selector in login_selectors:
                try:
                    login_button = await self.page.wait_for_selector(
                        selector,
                        timeout=5000,
                        state="visible"
                    )
                    if login_button:
                        await login_button.click()
                        login_clicked = True
                        logger.info(f"ログインボタンをクリック: {selector}")
                        break
                except Exception:
                    continue

            if not login_clicked:
                logger.warning("ログインボタンが見つかりませんでした。既にログイン済みの可能性があります。")
                # 既にログイン済みかチェック
                if await self._is_logged_in():
                    logger.info("既にログイン済みです")
                    return True
                else:
                    logger.error("ログイン画面に遷移できませんでした")
                    return False

            # 楽天ID入力画面が表示されるまで待機
            logger.info("楽天ID入力画面を待機中...")
            await self.page.wait_for_load_state("domcontentloaded")

            # デバッグ用：ログイン画面のスクリーンショットとHTMLを保存
            await self.page.screenshot(path="debug_step1_5_login_form.png")
            logger.info("スクリーンショット保存: debug_step1_5_login_form.png")

            html_content = await self.page.content()
            with open("debug_step1_5_login_form.html", "w", encoding="utf-8") as f:
                f.write(html_content)
            logger.info("HTML保存: debug_step1_5_login_form.html")
            logger.info(f"ログインフォームURL: {self.page.url}")

            # ステップ1: 楽天ID（ユーザー名）を入力
            logger.info("ステップ1: 楽天IDを入力中...")
            username_selectors = [
                'input[name="username"]',
                '#user_id',
                'input[autocomplete="username"]',
                'input[name="u"]',
                '#loginInner_u',
                'input[type="email"]',
                'input[placeholder*="楽天会員ID"]',
                'input[placeholder*="ユーザID"]',
                'input[placeholder*="メールアドレス"]'
            ]

            username_filled = False
            for selector in username_selectors:
                try:
                    username_input = await self.page.wait_for_selector(
                        selector,
                        timeout=5000,
                        state="visible"
                    )
                    if username_input:
                        # クリックしてフォーカスを当てる
                        await username_input.click()
                        # すぐに入力
                        await username_input.fill(rakuten_id)
                        logger.info(f"楽天ID入力完了: {selector}")
                        username_filled = True
                        break
                except Exception as e:
                    logger.debug(f"楽天ID入力欄検索失敗: {selector} - {e}")
                    continue

            if not username_filled:
                logger.error("楽天ID入力欄が見つかりませんでした")
                return False

            # 「次へ」ボタンをクリック（楽天の2段階認証）
            logger.info("「次へ」ボタンを探しています...")
            next_button_selectors = [
                '#cta001',
                'div[role="button"]:has-text("次へ")',
                'button:has-text("次へ")',
                '[id*="cta"]',
                'button[type="submit"]',
                'input[type="submit"]',
                '.submit-button'
            ]

            import asyncio
            next_clicked = False
            for selector in next_button_selectors:
                try:
                    next_button = await self.page.wait_for_selector(
                        selector,
                        timeout=5000,
                        state="visible"
                    )
                    if next_button:
                        await next_button.click()
                        logger.info(f"「次へ」ボタンをクリック: {selector}")
                        next_clicked = True
                        await asyncio.sleep(4)  # ページ遷移を待つ（延長）
                        await self.page.wait_for_load_state("domcontentloaded")
                        break
                except Exception as e:
                    logger.debug(f"「次へ」ボタン検索失敗: {selector} - {e}")
                    continue

            # ステップ2: パスワードを入力
            logger.info("ステップ2: パスワードを入力中...")

            # デバッグ用：パスワード画面のスクリーンショットとHTMLを保存
            await self.page.screenshot(path="debug_step1_7_password_form.png")
            logger.info("スクリーンショット保存: debug_step1_7_password_form.png")

            html_content = await self.page.content()
            with open("debug_step1_7_password_form.html", "w", encoding="utf-8") as f:
                f.write(html_content)
            logger.info("HTML保存: debug_step1_7_password_form.html")
            logger.info(f"パスワードフォームURL: {self.page.url}")

            password_selectors = [
                '#password_current',
                'input[name="password"]',
                'input[autocomplete="current-password"]'
            ]

            password_filled = False
            for selector in password_selectors:
                try:
                    password_input = await self.page.wait_for_selector(
                        selector,
                        timeout=3000,
                        state="visible"
                    )
                    if password_input:
                        # クリックしてフォーカスを当ててすぐに入力
                        await password_input.click()
                        await password_input.fill(password)
                        logger.info(f"パスワード入力完了: {selector}")
                        password_filled = True
                        break
                except Exception as e:
                    logger.debug(f"パスワード入力欄検索失敗: {selector} - {e}")
                    continue

            if not password_filled:
                logger.error("パスワード入力欄が見つかりませんでした")
                return False

            # ログインボタンをクリック
            logger.info("ログインボタンを探しています...")

            # ボタンが有効になるまで少し待つ
            await asyncio.sleep(1)

            login_button_selectors = [
                '#cta011',
                '#cta001',
                'div[role="button"]:has-text("次へ")',
                '[id*="cta"]'
            ]

            login_clicked = False
            for selector in login_button_selectors:
                try:
                    login_button = await self.page.wait_for_selector(
                        selector,
                        timeout=5000,
                        state="visible"
                    )
                    if login_button:
                        await login_button.click()
                        logger.info(f"ログインボタンをクリック: {selector}")
                        login_clicked = True
                        break
                except Exception as e:
                    logger.debug(f"ログインボタン検索失敗: {selector} - {e}")
                    continue

            if not login_clicked:
                logger.error("ログインボタンが見つかりませんでした")
                return False

            # ログイン完了を待機（十分な時間を確保）
            await asyncio.sleep(3)  # 3秒待機
            await self.page.wait_for_load_state("domcontentloaded")
            logger.info("ログイン処理完了")

            # デバッグ用：ログイン後のスクリーンショットとHTMLを保存
            await self.page.screenshot(path="debug_step2_after_login.png")
            logger.info("スクリーンショット保存: debug_step2_after_login.png")

            html_content = await self.page.content()
            with open("debug_step2_after_login.html", "w", encoding="utf-8") as f:
                f.write(html_content)
            logger.info("HTML保存: debug_step2_after_login.html")
            logger.info(f"ログイン後のURL: {self.page.url}")

            # ログイン成功確認
            is_logged_in = await self._is_logged_in()
            logger.info(f"ログイン状態チェック結果: {is_logged_in}")

            if is_logged_in:
                logger.info("✅ ログイン成功")
                return True
            else:
                logger.error("❌ ログイン失敗")
                # エラーメッセージがあれば表示
                try:
                    error_msg = await self.page.query_selector('text=/エラー|ログインに失敗|正しく/')
                    if error_msg:
                        error_text = await error_msg.text_content()
                        logger.error(f"エラーメッセージ: {error_text}")
                except:
                    pass
                return False

        except Exception as e:
            logger.error(f"ログイン処理エラー: {e}", exc_info=True)
            return False

    async def _is_logged_in(self) -> bool:
        """ログイン状態を確認"""
        try:
            # ログイン状態を示す要素をチェック
            # （実際のサイトの構造に応じて調整が必要）
            current_url = self.page.url
            logger.debug(f"ログイン確認中 - URL: {current_url}")

            # ログアウトボタンやマイページリンクがあればログイン済み
            logout_exists = await self.page.query_selector('a:has-text("ログアウト")') is not None
            logger.debug(f"ログアウトボタン存在: {logout_exists}")

            mypage_exists = await self.page.query_selector('a:has-text("マイページ")') is not None
            logger.debug(f"マイページリンク存在: {mypage_exists}")

            # 楽天西友特有の要素をチェック
            cart_exists = await self.page.query_selector('a:has-text("カート")') is not None
            logger.debug(f"カートボタン存在: {cart_exists}")

            # URLチェック
            url_check = "mypage" in current_url.lower() or "member" in current_url.lower()
            logger.debug(f"URL判定: {url_check}")

            result = logout_exists or mypage_exists or cart_exists or url_check
            logger.debug(f"最終判定: {result}")

            return result
        except Exception as e:
            logger.error(f"ログイン確認エラー: {e}")
            return False

    async def set_delivery_area(self, zip_code: str) -> bool:
        """
        配送先エリアを設定

        Args:
            zip_code: 郵便番号（例: "211-0063"）

        Returns:
            成功したらTrue
        """
        try:
            logger.info(f"配送先エリアを設定中: {zip_code}")

            # 配送先設定ページに移動
            # （実際のサイト構造に応じて調整が必要）

            # 郵便番号入力フォームを探す
            zip_code_selectors = [
                'input[name="zip_code"]',
                'input[name="zipcode"]',
                'input[placeholder*="郵便番号"]',
                '#zip-code-input'
            ]

            zip_code_found = False
            for selector in zip_code_selectors:
                try:
                    zip_input = await self.page.wait_for_selector(
                        selector,
                        timeout=5000,
                        state="visible"
                    )
                    if zip_input:
                        await zip_input.fill(zip_code.replace("-", ""))
                        logger.info(f"郵便番号入力完了: {selector}")
                        zip_code_found = True
                        break
                except Exception:
                    continue

            if not zip_code_found:
                logger.warning("郵便番号入力欄が見つかりませんでした")
                # エリア設定が不要な場合もあるのでWarningのみ
                return True

            # 確定ボタンをクリック
            confirm_selectors = [
                'button:has-text("確定")',
                'button:has-text("この住所に配送")',
                'button[type="submit"]'
            ]

            for selector in confirm_selectors:
                try:
                    confirm_button = await self.page.wait_for_selector(
                        selector,
                        timeout=5000
                    )
                    if confirm_button:
                        await confirm_button.click()
                        logger.info("配送先確定ボタンをクリック")
                        break
                except Exception:
                    continue

            await self.page.wait_for_load_state("domcontentloaded")
            logger.info("✅ 配送先エリア設定完了")
            return True

        except Exception as e:
            logger.error(f"配送先エリア設定エラー: {e}", exc_info=True)
            return False

    async def save_cookies(self, file_path: str = "rakuten_seiyu_cookies.json") -> bool:
        """
        セッションCookieをファイルに保存

        Args:
            file_path: 保存先ファイルパス

        Returns:
            成功したらTrue
        """
        try:
            logger.info(f"Cookieを保存中: {file_path}")
            cookies = await self.context.cookies()

            # ファイルに保存
            output_path = Path(file_path)
            with output_path.open("w", encoding="utf-8") as f:
                json.dump(cookies, f, indent=2, ensure_ascii=False)

            logger.info(f"✅ Cookie保存完了: {len(cookies)}個のCookieを保存")
            return True

        except Exception as e:
            logger.error(f"Cookie保存エラー: {e}", exc_info=True)
            return False

    async def close(self):
        """ブラウザを閉じる"""
        try:
            if self.browser:
                await self.browser.close()
                logger.info("ブラウザを閉じました")
            if self.playwright:
                await self.playwright.stop()
        except Exception as e:
            logger.error(f"ブラウザクローズエラー: {e}", exc_info=True)


async def main():
    """テスト実行用のメイン関数"""
    import os
    from dotenv import load_dotenv

    load_dotenv()

    rakuten_id = os.getenv("RAKUTEN_ID")
    password = ***REDACTED***"RAKUTEN_PASSWORD")
    zip_code = os.getenv("DELIVERY_ZIP_CODE", "211-0063")

    if not rakuten_id or not password:
        logger.error("環境変数 RAKUTEN_ID と RAKUTEN_PASSWORD を設定してください")
        return

    async with RakutenSeiyuAuthManager(headless=False) as auth:
        # ログイン
        if await auth.login(rakuten_id, password):
            # 配送先設定
            await auth.set_delivery_area(zip_code)
            # Cookie保存
            await auth.save_cookies("rakuten_seiyu_cookies.json")
            logger.info("✅ 認証処理完了")
        else:
            logger.error("❌ 認証処理失敗")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

### services\data-ingestion\rakuten_seiyu\categories_config.json

```json
{
  "categories": [
    {
      "id": "110001",
      "name": "野菜",
      "enabled": true,
      "description": "野菜全般"
    },
    {
      "id": "110002",
      "name": "果物",
      "enabled": true,
      "description": "果物・フルーツ"
    },
    {
      "id": "110003",
      "name": "肉",
      "enabled": true,
      "description": "肉類（牛肉・豚肉・鶏肉など）"
    },
    {
      "id": "110004",
      "name": "魚介",
      "enabled": true,
      "description": "魚・魚介類"
    },
    {
      "id": "110005",
      "name": "惣菜",
      "enabled": true,
      "description": "惣菜・お弁当"
    },
    {
      "id": "110006",
      "name": "パン",
      "enabled": false,
      "description": "パン・ベーカリー"
    },
    {
      "id": "110007",
      "name": "乳製品",
      "enabled": true,
      "description": "牛乳・乳製品・卵"
    },
    {
      "id": "110008",
      "name": "冷凍食品",
      "enabled": true,
      "description": "冷凍食品"
    },
    {
      "id": "120001",
      "name": "調味料",
      "enabled": false,
      "description": "調味料・油・ドレッシング"
    },
    {
      "id": "120002",
      "name": "飲料",
      "enabled": false,
      "description": "飲料・ドリンク"
    },
    {
      "id": "120003",
      "name": "菓子",
      "enabled": false,
      "description": "お菓子・スナック"
    },
    {
      "id": "120004",
      "name": "日用品",
      "enabled": false,
      "description": "日用品・生活雑貨"
    }
  ],
  "scraping_config": {
    "area_zip_code": "211-0063",
    "access_interval_min": 1.0,
    "access_interval_max": 2.0,
    "max_pages_per_category": 100,
    "notes": [
      "カテゴリーIDは楽天西友のサイト構造を調査して正確な値に修正してください",
      "enabled: false のカテゴリーはスクレイピング対象外です",
      "初回テストでは野菜（110001）のみ有効にして動作確認することを推奨"
    ]
  }
}
```

### services\data-ingestion\rakuten_seiyu\process_with_schedule.py

```py
"""
楽天西友ネットスーパー スケジュール管理対応版

カテゴリーごとの実行スケジュールを管理し、
サーバー負荷を最小限に抑える待機時間を実装します。
"""

import os
import sys
import asyncio
import random
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from common.category_manager_db import CategoryManagerDB
from rakuten_seiyu.product_ingestion import RakutenSeiyuProductIngestionPipeline

# ロガー設定
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class PoliteRakutenSeiyuPipeline:
    """サーバー負荷に配慮したスケジュール管理パイプライン"""

    def __init__(
        self,
        rakuten_id: str,
        password: str,
        headless: bool = True,
        dry_run: bool = False
    ):
        """
        Args:
            rakuten_id: 楽天ID
            password: パスワード
            headless: ヘッドレスモード
            dry_run: Dry Run モード（設定ファイルの初期化のみ）
        """
        self.pipeline = RakutenSeiyuProductIngestionPipeline(
            rakuten_id=rakuten_id,
            password=***REDACTED***
            headless=headless
        )
        self.manager = CategoryManagerDB()
        self.dry_run = dry_run
        self.store_name = "rakuten_seiyu"

    async def polite_wait_between_pages(self):
        """ページ遷移間の待機（4秒〜8秒のランダム）"""
        wait_time = random.uniform(4.0, 8.0)
        logger.info(f"⏳ ページ遷移待機: {wait_time:.1f}秒")
        await asyncio.sleep(wait_time)

    async def polite_wait_between_categories(self):
        """カテゴリー切替時の待機（15秒〜30秒のランダム）"""
        wait_time = random.uniform(15.0, 30.0)
        logger.info(f"⏳ カテゴリー切替待機: {wait_time:.1f}秒")
        await asyncio.sleep(wait_time)

    async def initialize_categories(self):
        """カテゴリーを初期化（初回実行時）"""
        logger.info("📋 カテゴリーを初期化します...")

        # スクレイパー起動
        success = await self.pipeline.start()
        if not success:
            logger.error("❌ スクレイパー起動失敗")
            return False

        try:
            # カテゴリーを動的に取得
            categories = await self.pipeline.discover_categories()

            if not categories:
                logger.warning("カテゴリーが見つかりませんでした")
                return False

            # CategoryManagerに登録
            category_list = [
                {"name": cat["name"], "url": cat["url"]}
                for cat in categories
            ]

            # デフォルト設定で初期化
            # 開始日: 明日、インターバル: 7日
            tomorrow = (datetime.now()).strftime("%Y-%m-%d")
            self.manager.initialize_store_categories(
                self.store_name,
                category_list,
                default_interval_days=7,
                default_start_date=tomorrow
            )

            logger.info(f"✅ {len(categories)}件のカテゴリーを初期化しました")
            logger.info("管理画面で設定を調整してください:")
            logger.info("  streamlit run B_ingestion/netsuper_category_manager_ui.py")

            return True

        finally:
            await self.pipeline.close()

    async def run_scheduled_categories(self, manual_categories: List[str] = None):
        """スケジュールに基づいてカテゴリーを処理

        Args:
            manual_categories: 手動実行時に指定されたカテゴリー名のリスト（Noneの場合はスケジュールに従う）
        """
        if manual_categories:
            logger.info("="*80)
            logger.info("楽天西友ネットスーパー 手動実行開始")
            logger.info(f"対象カテゴリー: {', '.join(manual_categories)}")
            logger.info("="*80)
        else:
            logger.info("="*80)
            logger.info("楽天西友ネットスーパー スケジュール実行開始")
            logger.info("="*80)

        # スクレイパー起動
        success = await self.pipeline.start()
        if not success:
            logger.error("❌ スクレイパー起動失敗")
            return

        try:
            # カテゴリーを動的に取得して更新
            logger.info("🔄 カテゴリーを最新化中...")
            discovered_categories = await self.pipeline.discover_categories()

            if discovered_categories:
                logger.info(f"✅ {len(discovered_categories)}件のカテゴリーを取得")

                # 既存の設定を取得
                existing_categories = self.manager.get_all_categories(self.store_name)
                existing_names = {cat["name"]: cat for cat in existing_categories} if existing_categories else {}

                # 新規カテゴリーを追加
                for cat in discovered_categories:
                    if cat["name"] not in existing_names:
                        logger.info(f"  📝 新規カテゴリー追加: {cat['name']}")
                        self.manager.update_category(
                            self.store_name,
                            cat["name"],
                            {
                                "url": cat["url"],
                                "enabled": True,
                                "interval_days": 7,
                                "start_date": datetime.now().strftime("%Y-%m-%d")
                            }
                        )
                    else:
                        # URLが変更されている場合は更新
                        if existing_names[cat["name"]].get("url") != cat["url"]:
                            logger.info(f"  🔄 URL更新: {cat['name']}")
                            self.manager.update_category(
                                self.store_name,
                                cat["name"],
                                {"url": cat["url"]}
                            )

            # 設定からカテゴリーを取得
            categories = self.manager.get_all_categories(self.store_name)

            if not categories:
                logger.warning("カテゴリーが設定されていません。初回実行してください:")
                logger.warning("  python -m B_ingestion.rakuten_seiyu.process_with_schedule --init")
                return

            # 実行すべきカテゴリーをフィルタリング
            today = datetime.now()
            runnable_categories = []

            if manual_categories:
                # 手動実行時: 指定されたカテゴリーのみ
                for cat in categories:
                    if cat["name"] in manual_categories:
                        runnable_categories.append(cat)
            else:
                # スケジュール実行時: 今日実行すべきカテゴリー
                for cat in categories:
                    if self.manager.should_run_category(self.store_name, cat["name"], today):
                        runnable_categories.append(cat)

            logger.info(f"📊 総カテゴリー数: {len(categories)}件")
            logger.info(f"✅ 本日実行対象: {len(runnable_categories)}件")

            if not runnable_categories:
                logger.info("本日実行するカテゴリーはありません")
                return

            # カテゴリーごとに処理
            for idx, cat in enumerate(runnable_categories, 1):
                logger.info("")
                logger.info("="*80)
                logger.info(f"📦 カテゴリー {idx}/{len(runnable_categories)}: {cat['name']}")
                logger.info(f"   URL: {cat['url']}")
                logger.info("="*80)

                try:
                    # カテゴリーの商品データを取得してSupabaseに保存
                    result = await self.pipeline.process_category_all_pages(
                        category_url=cat['url'],
                        category_name=cat['name']
                    )

                    if result:
                        logger.info(f"✅ カテゴリー {cat['name']} の処理完了")
                        logger.info(f"   商品数: {result.get('total_products', 0)}件")
                        logger.info(f"   新規: {result.get('new_products', 0)}件, 更新: {result.get('updated_products', 0)}件")
                    else:
                        logger.warning(f"⚠️ カテゴリー {cat['name']} の処理に問題がありました")

                except Exception as e:
                    logger.error(f"❌ カテゴリー {cat['name']} 処理エラー: {e}", exc_info=True)

                finally:
                    # 成功・失敗に関わらず実行済みとしてマーク
                    self.manager.mark_as_run(self.store_name, cat["name"], today)

                # カテゴリー間の待機
                if idx < len(runnable_categories):
                    await self.polite_wait_between_categories()

            logger.info("")
            logger.info("="*80)
            logger.info("✅ すべてのカテゴリー処理完了")
            logger.info("="*80)

        finally:
            await self.pipeline.close()


async def main():
    """メイン処理"""
    import argparse

    parser = argparse.ArgumentParser(description="楽天西友スクレイピング（スケジュール管理対応）")
    parser.add_argument("--init", action="store_true", help="カテゴリーを初期化（初回実行時のみ）")
    parser.add_argument("--manual", action="store_true", help="手動実行モード（環境変数MANUAL_CATEGORIESからカテゴリーを取得）")
    parser.add_argument("--headless", action="store_true", default=True, help="ヘッドレスモード")
    args = parser.parse_args()

    rakuten_id = os.getenv("RAKUTEN_ID")
    password = ***REDACTED***"RAKUTEN_PASSWORD")

    if not rakuten_id or not password:
        logger.error("❌ 環境変数 RAKUTEN_ID と RAKUTEN_PASSWORD を設定してください")
        return

    pipeline = PoliteRakutenSeiyuPipeline(
        rakuten_id=rakuten_id,
        password=***REDACTED***
        headless=args.headless
    )

    if args.init:
        # 初期化モード
        await pipeline.initialize_categories()
    elif args.manual:
        # 手動実行モード
        manual_categories_str = os.getenv("MANUAL_CATEGORIES", "")
        if not manual_categories_str:
            logger.error("❌ 環境変数 MANUAL_CATEGORIES が設定されていません")
            return

        manual_categories = [cat.strip() for cat in manual_categories_str.split(",") if cat.strip()]
        if not manual_categories:
            logger.error("❌ カテゴリーが指定されていません")
            return

        await pipeline.run_scheduled_categories(manual_categories=manual_categories)

        # 商品データ取得後、自動的にembedding生成を実行
        await generate_embeddings_if_needed()
    else:
        # 通常実行モード（スケジュール）
        await pipeline.run_scheduled_categories()

        # 商品データ取得後、自動的にembedding生成を実行
        await generate_embeddings_if_needed()


async def generate_embeddings_if_needed():
    """商品データの分類・embedding生成（未生成のものがあれば実行）"""
    try:
        logger.info("")
        logger.info("="*80)
        logger.info("🔄 商品分類・Embedding生成チェック開始")
        logger.info("="*80)

        # ステップ1: Gemini 2.5 Flash で general_name, small_category, keywords を生成
        logger.info("ステップ1: Gemini 2.5 Flash で商品分類生成")
        classification_path = Path(__file__).parent.parent.parent / "L_product_classification"
        sys.path.insert(0, str(classification_path))

        from daily_auto_classifier import DailyAutoClassifier

        classifier = DailyAutoClassifier()
        result = await classifier.process_unclassified_products()
        logger.info(f"✅ 分類完了: {result.get('classified_count', 0)}件")

        # ステップ2: Embedding生成
        logger.info("ステップ2: Embedding生成")
        netsuper_app_path = Path(__file__).parent.parent.parent / "netsuper_search_app"
        sys.path.insert(0, str(netsuper_app_path))

        from generate_multi_embeddings import MultiEmbeddingGenerator

        # embedding生成を実行
        generator = MultiEmbeddingGenerator()
        generator.process_products(delay=0.1)

        logger.info("✅ 商品分類・Embedding生成処理完了")

    except Exception as e:
        logger.error(f"⚠️ 商品分類・Embedding生成エラー（スキップして続行）: {e}")


if __name__ == "__main__":
    asyncio.run(main())
```

### services\data-ingestion\rakuten_seiyu\product_ingestion.py

```py
"""
楽天西友ネットスーパー 商品データ取得パイプライン

商品データを取得してSupabaseに保存します。

処理フロー:
1. Cookieを使用して商品ページにアクセス
2. 商品データを抽出
3. JANコードで既存商品をチェック
4. Supabaseに保存（新規 or 更新）
"""

import os
import sys
import logging
from pathlib import Path
from typing import List, Dict, Any

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

# .envファイルを読み込む
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from common.base_product_ingestion import BaseProductIngestionPipeline
from rakuten_seiyu.rakuten_seiyu_scraper_playwright import RakutenSeiyuScraperPlaywright

# ロガー設定
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class RakutenSeiyuProductIngestionPipeline(BaseProductIngestionPipeline):
    """楽天西友商品データ取得パイプライン（共通基盤クラス継承）"""

    def __init__(self, rakuten_id: str, password: str, headless: bool = True):
        """
        Args:
            rakuten_id: 楽天ID
            password: パスワード
            headless: ヘッドレスモードで実行するか
        """
        super().__init__(organization_name="楽天西友ネットスーパー", headless=headless)
        self.rakuten_id = rakuten_id
        self.password = ***REDACTED***

        logger.info("RakutenSeiyuProductIngestionPipeline初期化完了（Service Role使用）")

    async def start(self) -> bool:
        """
        スクレイパーを起動してログイン（楽天西友固有）

        Returns:
            成功したらTrue
        """
        try:
            self.scraper = RakutenSeiyuScraperPlaywright()
            await self.scraper.start(headless=self.headless)

            # ログイン
            success = await self.scraper.login(self.rakuten_id, self.password)
            if not success:
                logger.error("❌ ログイン失敗")
                await self.scraper.close()
                return False

            logger.info("✅ スクレイパー起動・ログイン完了")
            return True

        except Exception as e:
            logger.error(f"スクレイパー起動エラー: {e}", exc_info=True)
            return False

    async def close(self):
        """スクレイパーを終了"""
        if self.scraper:
            await self.scraper.close()

    async def discover_categories(self) -> List[Dict[str, Any]]:
        """
        実際のカテゴリーを取得（毎回動的に取得）

        Returns:
            カテゴリー情報のリスト
        """
        logger.info("🔍 カテゴリーを探索中...")

        page = self.scraper.page

        # トップページにアクセス
        await page.goto(self.scraper.base_url, wait_until="domcontentloaded")
        await page.wait_for_timeout(2000)

        # カテゴリーリンクを探す
        category_selectors = [
            'a[href*="/search/"]',
            '[class*="category"] a',
        ]

        all_categories = []

        for selector in category_selectors:
            try:
                links = await page.query_selector_all(selector)

                for link in links:
                    try:
                        href = await link.get_attribute('href')
                        text = await link.inner_text()

                        if not href or href.startswith('javascript:') or not text:
                            continue

                        # カテゴリーIDを抽出
                        category_id = None
                        if '/search/' in href:
                            parts = href.split('/search/')
                            if len(parts) > 1:
                                category_id = parts[1].split('?')[0].split('/')[0]

                        # 完全なURLを構築
                        full_url = href if href.startswith('http') else f"https://netsuper.rakuten.co.jp{href}"

                        category_info = {
                            'name': text.strip(),
                            'url': full_url,
                            'category_id': category_id
                        }

                        # 重複チェック
                        if not any(cat['url'] == full_url for cat in all_categories):
                            all_categories.append(category_info)

                    except Exception:
                        continue

            except Exception:
                continue

        logger.info(f"✅ {len(all_categories)}件のカテゴリーを発見")
        return all_categories


async def main():
    """テスト実行用のメイン関数"""
    logger.info("楽天西友商品データ取得パイプライン開始")

    rakuten_id = os.getenv("RAKUTEN_ID")
    password = ***REDACTED***"RAKUTEN_PASSWORD")

    if not rakuten_id or not password:
        logger.error("❌ 環境変数 RAKUTEN_ID と RAKUTEN_PASSWORD を設定してください")
        return

    pipeline = RakutenSeiyuProductIngestionPipeline(
        rakuten_id=rakuten_id,
        password=***REDACTED***
        headless=False
    )

    try:
        # スクレイパー起動
        success = await pipeline.start()
        if not success:
            logger.error("❌ スクレイパー起動失敗")
            return

        # カテゴリーを動的に取得
        categories = await pipeline.discover_categories()
        if categories:
            logger.info(f"発見したカテゴリー: {len(categories)}件")
            for cat in categories[:5]:  # 最初の5件を表示
                logger.info(f"  - {cat['name']}: {cat['url']}")

        logger.info("✅ テスト完了")

    finally:
        await pipeline.close()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

### services\data-ingestion\rakuten_seiyu\rakuten_seiyu_scraper_playwright.py

```py
"""
楽天西友ネットスーパー スクレイピングモジュール (Playwright版)

Playwrightを使用してログイン状態を保持したまま商品データを取得します。
"""

import json
import re
import time
import random
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from playwright.async_api import async_playwright, Browser, Page, BrowserContext

# ロガー設定
logger = logging.getLogger(__name__)


class RakutenSeiyuScraperPlaywright:
    """楽天西友ネットスーパーのスクレイピングクラス (Playwright版)"""

    def __init__(self):
        self.base_url = "https://netsuper.rakuten.co.jp/seiyu"
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None

    async def __aenter__(self):
        """async with構文でのコンテキスト開始"""
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """async with構文でのコンテキスト終了"""
        await self.close()

    async def start(self, headless: bool = True):
        """
        ブラウザを起動

        Args:
            headless: ヘッドレスモードで起動するか
        """
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(headless=headless)
            self.context = await self.browser.new_context(
                viewport={'width': 1280, 'height': 720},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            self.page = await self.context.new_page()
            logger.info("✅ Playwrightブラウザ起動完了")

        except Exception as e:
            logger.error(f"ブラウザ起動エラー: {e}", exc_info=True)
            raise

    async def close(self):
        """ブラウザを閉じる"""
        try:
            if self.page:
                await self.page.close()
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
            logger.info("✅ ブラウザ終了")

        except Exception as e:
            logger.error(f"ブラウザ終了エラー: {e}", exc_info=True)

    async def _is_logged_in(self) -> bool:
        """ログイン状態をチェック"""
        try:
            page_content = await self.page.content()
            current_url = self.page.url

            # URLが楽天西友のホームページであれば、ログイン成功とみなす
            if "netsuper.rakuten.co.jp/seiyu" in current_url and "id.rakuten.co.jp" not in current_url:
                # さらに、ログインページ特有の要素がないことを確認
                if "ログイン" not in page_content or "shopping-cart" in page_content or "minicart" in page_content:
                    return True

            # 従来の方法もチェック
            return "ログアウト" in page_content or "マイページ" in page_content
        except:
            return False

    async def login(self, rakuten_id: str, password: str) -> bool:
        """
        楽天西友にログイン (auth_manager.pyから移植)

        Args:
            rakuten_id: 楽天ID
            password: パスワード

        Returns:
            成功したらTrue
        """
        import asyncio

        try:
            logger.info("🔐 楽天西友にログイン中...")
            await self.page.goto(self.base_url, wait_until="domcontentloaded", timeout=60000)

            # ログインボタンを探してクリック
            login_selectors = [
                'a:has-text("ログイン")',
                'button:has-text("ログイン")',
                '[data-test="login-button"]',
                '.login-button',
                '#login-link'
            ]

            login_clicked = False
            for selector in login_selectors:
                try:
                    login_button = await self.page.wait_for_selector(
                        selector,
                        timeout=5000,
                        state="visible"
                    )
                    if login_button:
                        await login_button.click()
                        login_clicked = True
                        logger.info(f"ログインボタンをクリック: {selector}")
                        break
                except Exception:
                    continue

            if not login_clicked:
                if await self._is_logged_in():
                    logger.info("✅ 既にログイン済みです")
                    return True
                else:
                    logger.error("ログイン画面に遷移できませんでした")
                    return False

            await self.page.wait_for_load_state("domcontentloaded")

            # ステップ1: 楽天ID入力
            logger.info("ステップ1: 楽天IDを入力中...")
            username_selectors = [
                'input[name="username"]',
                '#user_id',
                'input[autocomplete="username"]',
                'input[name="u"]',
                '#loginInner_u',
                'input[type="email"]',
                'input[placeholder*="楽天会員ID"]',
                'input[placeholder*="ユーザID"]',
                'input[placeholder*="メールアドレス"]'
            ]

            username_filled = False
            for selector in username_selectors:
                try:
                    username_input = await self.page.wait_for_selector(
                        selector,
                        timeout=5000,
                        state="visible"
                    )
                    if username_input:
                        await username_input.click()
                        await username_input.fill(rakuten_id)
                        logger.info(f"✅ 楽天ID入力完了: {selector}")
                        username_filled = True
                        break
                except Exception:
                    continue

            if not username_filled:
                logger.error("楽天ID入力欄が見つかりませんでした")
                return False

            # 「次へ」ボタンをクリック
            next_button_selectors = [
                '#cta001',
                'div[role="button"]:has-text("次へ")',
                'button:has-text("次へ")',
                '[id*="cta"]',
                'button[type="submit"]',
                'input[type="submit"]',
                '.submit-button'
            ]

            next_clicked = False
            for selector in next_button_selectors:
                try:
                    next_button = await self.page.wait_for_selector(
                        selector,
                        timeout=5000,
                        state="visible"
                    )
                    if next_button:
                        await next_button.click()
                        logger.info(f"「次へ」ボタンをクリック: {selector}")
                        next_clicked = True
                        await asyncio.sleep(0.5)
                        await self.page.wait_for_load_state("domcontentloaded")
                        break
                except Exception:
                    continue

            # ステップ2: パスワード入力
            logger.info("ステップ2: パスワードを入力中...")
            password_selectors = [
                '#password_current',
                'input[name="password"]',
                'input[autocomplete="current-password"]'
            ]

            password_filled = False
            for selector in password_selectors:
                try:
                    password_input = await self.page.wait_for_selector(
                        selector,
                        timeout=3000,
                        state="visible"
                    )
                    if password_input:
                        await password_input.click()
                        await password_input.fill(password)
                        logger.info(f"✅ パスワード入力完了: {selector}")
                        password_filled = True
                        break
                except Exception:
                    continue

            if not password_filled:
                logger.error("パスワード入力欄が見つかりませんでした")
                return False

            # ログインボタンをクリック
            await asyncio.sleep(0.5)
            login_button_selectors = [
                '#cta011',
                '#cta001',
                'div[role="button"]:has-text("次へ")',
                '[id*="cta"]'
            ]

            login_clicked = False
            for selector in login_button_selectors:
                try:
                    login_button = await self.page.wait_for_selector(
                        selector,
                        timeout=5000,
                        state="visible"
                    )
                    if login_button:
                        await login_button.click()
                        logger.info(f"ログインボタンをクリック: {selector}")
                        login_clicked = True
                        break
                except Exception:
                    continue

            if not login_clicked:
                logger.error("ログインボタンが見つかりませんでした")
                return False

            # ログイン完了を待機
            await asyncio.sleep(2)
            await self.page.wait_for_load_state("domcontentloaded")

            # デバッグ用：ログイン後のページを保存
            await self.page.screenshot(path="after_login.png")
            logger.info("スクリーンショット保存: after_login.png")

            current_url = self.page.url
            logger.info(f"ログイン後のURL: {current_url}")

            page_content = await self.page.content()
            with open("after_login.html", "w", encoding="utf-8") as f:
                f.write(page_content)
            logger.info("HTML保存: after_login.html")

            # ログイン成功確認
            is_logged_in = await self._is_logged_in()

            # デバッグ情報
            has_logout = "ログアウト" in page_content
            has_mypage = "マイページ" in page_content
            logger.info(f"ログイン確認: ログアウト={has_logout}, マイページ={has_mypage}")

            if is_logged_in:
                logger.info("✅ ログイン成功")
                return True
            else:
                logger.error("❌ ログイン失敗")
                return False

        except Exception as e:
            logger.error(f"ログインエラー: {e}", exc_info=True)
            return False

    async def fetch_products_page(
        self,
        category_url: str,
        page: int = 1
    ) -> tuple[List[Dict[str, Any]], Optional[dict]]:
        """
        カテゴリーページの商品データを取得

        Args:
            category_url: カテゴリーの完全URL（例: "https://netsuper.rakuten.co.jp/seiyu/search/110001/"）
            page: ページ番号

        Returns:
            (商品データのリスト, ページネーション情報)、失敗時は(空リスト, None)
        """
        try:
            # URLにページ番号を追加（既にクエリパラメータがある場合は&で追加）
            if '?' in category_url:
                url = f"{category_url}&page={page}"
            else:
                url = f"{category_url}?page={page}"
            logger.info(f"商品ページ取得中: {url}")

            await self.page.goto(url, wait_until="networkidle", timeout=30000)
            await self.page.wait_for_timeout(3000)

            # デバッグ用：商品ページのHTMLを保存
            if page == 1:
                await self.page.screenshot(path="product_page.png")
                logger.info("商品ページのスクリーンショット保存: product_page.png")

                html_debug = await self.page.content()
                with open("product_page.html", "w", encoding="utf-8") as f:
                    f.write(html_debug)
                logger.info("商品ページのHTML保存: product_page.html")

            logger.info(f"ステータスコード: OK (Playwright)")

            # 商品データとページネーション情報を抽出
            products, pagination_info = await self.extract_products_from_page()

            # アクセス間隔制御
            await self.page.wait_for_timeout(random.randint(1000, 2000))

            return products, pagination_info

        except Exception as e:
            logger.error(f"商品ページ取得エラー: {e}", exc_info=True)
            return [], None

    async def extract_products_from_page(self) -> tuple[List[Dict[str, Any]], Optional[dict]]:
        """
        PlaywrightページからHTMLを解析して商品データを抽出

        Returns:
            (商品データのリスト, ページネーション情報)
        """
        try:
            # HTMLから直接商品データを抽出
            extraction_result = await self.page.evaluate("""
                () => {
                    const result = {
                        products: [],
                        pagination: null
                    };

                    // 総商品数を取得（total-hit属性から）
                    const itemListContainer = document.querySelector('[id="item-list"]');
                    if (itemListContainer) {
                        const totalHit = itemListContainer.getAttribute('total-hit');
                        if (totalHit) {
                            const totalItems = parseInt(totalHit);
                            // 1ページあたり48件として計算
                            const itemsPerPage = 48;
                            const totalPages = Math.ceil(totalItems / itemsPerPage);

                            result.pagination = {
                                totalItems: totalItems,
                                itemsPerPage: itemsPerPage,
                                totalPages: totalPages,
                                source: 'html:total-hit'
                            };
                        }
                    }

                    // 商品要素を取得（data-ratid属性を持つ.product-item要素）
                    const productElements = document.querySelectorAll('.product-item[data-ratid][data-ratunit="item"]');

                    productElements.forEach((element) => {
                        try {
                            // 商品ID（data-ratid）
                            const itemId = element.getAttribute('data-ratid');

                            // メーカー/産地
                            const makerElement = element.querySelector('.product-item-info-maker');
                            const manufacturer = makerElement ? makerElement.textContent.trim() : null;

                            // 商品名
                            const nameElement = element.querySelector('.product-item-info-name');
                            const productName = nameElement ? nameElement.textContent.trim() : null;

                            // 数量
                            const amountElement = element.querySelector('.product-item-info-amount');
                            const amount = amountElement ? amountElement.textContent.trim() : null;

                            // 価格（税抜）
                            const priceElement = element.querySelector('.product-item-info-price');
                            let price = null;
                            if (priceElement) {
                                const priceText = priceElement.childNodes[0].textContent.trim();
                                price = parseInt(priceText.replace(/[^0-9]/g, ''));
                            }

                            // 価格（税込）
                            const taxPriceElement = element.querySelector('.product-item-info-tax');
                            let priceTaxIncluded = null;
                            if (taxPriceElement) {
                                const taxPriceText = taxPriceElement.textContent.trim();
                                const match = taxPriceText.match(/(\d+)円/);
                                if (match) {
                                    priceTaxIncluded = parseInt(match[1]);
                                }
                            }

                            // 画像URL
                            const imgElement = element.querySelector('.product-item-img img');
                            let imageUrl = null;
                            if (imgElement) {
                                imageUrl = imgElement.getAttribute('data-src') || imgElement.getAttribute('src');
                                if (imageUrl && imageUrl.startsWith('//')) {
                                    imageUrl = 'https:' + imageUrl;
                                }
                            }

                            // 商品ページURL
                            const linkElement = element.querySelector('a[href*="/item/"]');
                            const productUrl = linkElement ? linkElement.getAttribute('href') : null;

                            const product = {
                                itemId: itemId,
                                productName: productName,
                                manufacturer: manufacturer,
                                amount: amount,
                                price: price,
                                priceTaxIncluded: priceTaxIncluded,
                                imageUrl: imageUrl,
                                productUrl: productUrl
                            };

                            result.products.push(product);
                        } catch (err) {
                            console.error('Error parsing product element:', err);
                        }
                    });

                    return result;
                }
            """)

            products_data = extraction_result.get('products', [])
            pagination_info = extraction_result.get('pagination')

            logger.info(f"✅ HTML解析結果:")
            logger.info(f"  - 商品数: {len(products_data)}件")

            if pagination_info:
                logger.info(f"  - 総商品数: {pagination_info.get('totalItems')}件")
                logger.info(f"  - 総ページ数: {pagination_info.get('totalPages')}ページ")
                logger.info(f"  - 1ページあたり: {pagination_info.get('itemsPerPage')}件")
            else:
                logger.warning("  - ⚠️ ページネーション情報が見つかりませんでした")

            if not products_data:
                logger.warning("商品データが見つかりませんでした")
                return [], pagination_info

            # 商品データを整形
            products = []
            for item in products_data:
                try:
                    # 商品URLを完全なURLに変換
                    product_url = item.get('productUrl')
                    if product_url:
                        if product_url.startswith('http'):
                            pass  # 既に完全なURL
                        elif product_url.startswith('/'):
                            product_url = f"https://netsuper.rakuten.co.jp{product_url}"
                        else:
                            product_url = f"https://netsuper.rakuten.co.jp/{product_url}"

                    product = {
                        "product_name": f"{item.get('manufacturer', '')} {item.get('productName', '')} {item.get('amount', '')}".strip(),
                        "price": item.get('price'),
                        "price_tax_included": item.get('priceTaxIncluded'),
                        "jan_code": item.get('itemId'),  # 商品IDをJANコードとして使用
                        "image_url": item.get('imageUrl'),
                        "url": product_url,  # URLを追加
                        "manufacturer": item.get('manufacturer'),
                        "category": None,  # HTMLからは取得できないため
                        "in_stock": True,  # リストにあるものは在庫ありと仮定
                        "raw_data": item
                    }
                    products.append(product)
                except Exception as e:
                    logger.error(f"商品データ整形エラー: {e}")
                    continue

            logger.info(f"✅ {len(products)}件の商品を抽出完了")
            return products, pagination_info

        except Exception as e:
            logger.error(f"商品抽出エラー: {e}", exc_info=True)
            return [], None

    def _find_products_in_nuxt(self, nuxt_data: dict) -> Optional[list]:
        """
        NUXT データ内から商品リストを探す

        Args:
            nuxt_data: window.__NUXT__ のJSONデータ

        Returns:
            商品リスト、見つからない場合はNone
        """
        possible_paths = [
            ["data", 0, "itemList"],
            ["data", 0, "products"],
            ["data", "items"],
            ["data", "products"],
            ["state", "items"],
            ["state", "products"],
        ]

        for path in possible_paths:
            try:
                current = nuxt_data
                for key in path:
                    current = current[key]

                if isinstance(current, list) and len(current) > 0:
                    logger.info(f"商品データ発見: {'.'.join(map(str, path))}")
                    return current
            except (KeyError, IndexError, TypeError):
                continue

        logger.debug(f"NUXT データ構造: {json.dumps(nuxt_data, indent=2, ensure_ascii=False)[:1000]}")
        return None

    def _parse_product_item(self, item: dict) -> Optional[Dict[str, Any]]:
        """
        商品アイテムをパースして統一フォーマットに変換

        Args:
            item: 商品データ（辞書）

        Returns:
            整形された商品データ
        """
        try:
            product_name = item.get("name") or item.get("productName") or item.get("title")
            if not product_name:
                logger.warning("商品名が見つかりません")
                return None

            # 価格
            price = None
            price_data = item.get("price") or item.get("priceInfo") or {}
            if isinstance(price_data, dict):
                price = price_data.get("value") or price_data.get("price")
            elif isinstance(price_data, (int, float)):
                price = price_data

            # JANコード
            jan_code = item.get("janCode") or item.get("jan") or item.get("barcode")

            # 画像URL
            image_url = item.get("imageUrl") or item.get("image") or item.get("thumbnailUrl")
            if image_url:
                image_url = self._fix_image_url(image_url)

            # メーカー
            manufacturer = item.get("manufacturer") or item.get("brand") or item.get("maker")

            # カテゴリー
            category = item.get("category") or item.get("categoryName")

            # 在庫状況
            in_stock = item.get("inStock", True)
            is_available = item.get("isAvailable", True)

            # 商品URL（複数のフィールド名をチェック）
            product_url = item.get("url") or item.get("productUrl") or item.get("itemUrl") or item.get("link")

            # URLがない場合、商品IDから構築を試みる
            if not product_url:
                item_id = item.get("itemId") or item.get("id") or item.get("productId")
                if item_id:
                    product_url = f"https://netsuper.rakuten.co.jp/seiyu/product/{item_id}/"

            product = {
                "product_name": product_name,
                "price": price,
                "jan_code": jan_code,
                "image_url": image_url,
                "manufacturer": manufacturer,
                "category": category,
                "in_stock": in_stock,
                "is_available": is_available,
                "url": product_url,  # URLを追加
                "raw_data": item
            }

            return product

        except Exception as e:
            logger.error(f"商品パースエラー: {e}", exc_info=True)
            return None

    def _fix_image_url(self, url: str) -> str:
        """
        画像URLを修正（プロトコル補完など）

        Args:
            url: 元のURL

        Returns:
            修正されたURL
        """
        if url.startswith("//"):
            return f"https:{url}"
        elif not url.startswith("http"):
            return f"{self.base_url}{url}"
        return url

    async def scrape_category_all_pages(
        self,
        category_url: str,
        category_name: str,
        max_pages: int = 100
    ) -> List[Dict[str, Any]]:
        """
        カテゴリーの全ページから商品を取得

        Args:
            category_url: カテゴリーの完全URL
            category_name: カテゴリー名
            max_pages: 最大ページ数（安全装置）

        Returns:
            全商品のリスト
        """
        all_products = []
        page = 1
        total_pages = None

        logger.info(f"カテゴリー '{category_name}' の全ページスクレイピング開始")

        while page <= max_pages:
            logger.info(f"ページ {page} を処理中..." + (f"（全{total_pages}ページ）" if total_pages else ""))

            products, pagination_info = await self.fetch_products_page(category_url, page)

            # 初回のみページネーション情報を取得
            if page == 1 and pagination_info:
                total_pages = pagination_info.get('totalPages')
                total_items = pagination_info.get('totalItems')
                items_per_page = pagination_info.get('itemsPerPage')

                if total_pages:
                    logger.info(f"📄 総ページ数: {total_pages}ページ")
                if total_items:
                    logger.info(f"📦 総商品数: {total_items}件")
                if items_per_page:
                    logger.info(f"📝 1ページあたり: {items_per_page}件")

                # 総ページ数がわかっている場合はmax_pagesを更新
                if total_pages and total_pages < max_pages:
                    max_pages = total_pages
                    logger.info(f"✅ 最大ページ数を {total_pages} に設定")

            if not products:
                logger.info(f"ページ {page} に商品なし、終了します")
                break

            all_products.extend(products)
            logger.info(f"ページ {page}: {len(products)}件取得（累計: {len(all_products)}件）")

            # ページネーション情報から総ページ数がわかっている場合、それを超えたら終了
            if total_pages and page >= total_pages:
                logger.info(f"✅ 全{total_pages}ページの処理完了")
                break

            page += 1

            # ページ間待機
            await self.page.wait_for_timeout(random.randint(1000, 2000))

        logger.info(f"✅ カテゴリー '{category_name}' 完了: 合計 {len(all_products)}件")
        return all_products


async def main():
    """テスト実行用のメイン関数"""
    import os
    from dotenv import load_dotenv
    load_dotenv()

    rakuten_id = os.getenv("RAKUTEN_ID")
    password = ***REDACTED***"RAKUTEN_PASSWORD")

    if not rakuten_id or not password:
        logger.error("❌ 環境変数 RAKUTEN_ID と RAKUTEN_PASSWORD を設定してください")
        return

    async with RakutenSeiyuScraperPlaywright() as scraper:
        # ログイン
        success = await scraper.login(rakuten_id, password)
        if not success:
            logger.error("❌ ログイン失敗")
            return

        # テスト: 1ページ取得
        html = await scraper.fetch_products_page(category_id="110001", page=1)
        if html:
            products = scraper.extract_products_from_html(html)
            logger.info(f"取得した商品数: {len(products)}")

            if products:
                logger.info(f"最初の商品: {json.dumps(products[0], indent=2, ensure_ascii=False)}")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

### services\data-ingestion\requirements.txt

```txt
# Streamlit Cloud用 requirements.txt
# services/data-ingestion の依存パッケージ

# Google API
google-api-python-client==2.108.0
google-auth==2.25.2
google-auth-oauthlib==1.2.0
google-auth-httplib2==0.2.0
google-generativeai>=0.8.0

# AI APIs
anthropic==0.39.0
openai==1.54.0

# Database
supabase==2.10.0
pgvector==0.2.4

# Data Processing
numpy>=1.26.2,<2.0.0
pandas>=2.1.4,<3.0.0

# Utilities
python-dotenv==1.0.0
loguru==0.7.2
pydantic
pydantic-settings
tenacity>=8.2.0
jsonschema>=4.20.0
pyyaml>=6.0

# Streamlit
streamlit>=1.29.0,<2.0.0
```

### services\data-ingestion\tokubai\__init__.py

```py
"""
トクバイチラシ取得モジュール

トクバイのウェブサイトからチラシ画像を取得し、
Google DriveおよびSupabaseに登録する。
"""

from .flyer_ingestion import TokubaiFlyerIngestionPipeline

__all__ = ['TokubaiFlyerIngestionPipeline']
```

### services\data-ingestion\tokubai\flyer_ingestion.py

```py
"""
トクバイチラシ取得パイプライン

トクバイのウェブサイトからチラシ画像を取得し、Google DriveとSupabaseに登録する。

処理フロー:
1. トクバイの店舗ページからチラシ一覧を取得
2. Supabaseで既存データをチェックして新着チラシを抽出
3. チラシ画像をダウンロードしてGoogle Driveに保存
4. Supabaseに基本情報を登録（processing_status='pending'）
5. 別途 process_queued_documents.py で処理（画像抽出、Stage A/B/C）
"""
import os
import sys
import hashlib
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from loguru import logger

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

# .envファイルを読み込む
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from shared.common.connectors.google_drive import GoogleDriveConnector
from shared.common.database.client import DatabaseClient
from tokubai.tokubai_scraper import TokubaiScraper


class TokubaiFlyerIngestionPipeline:
    """トクバイチラシ取得パイプライン"""

    def __init__(
        self,
        store_url: Optional[str] = None,
        flyer_folder_id: Optional[str] = None,
        store_name: Optional[str] = None
    ):
        """
        Args:
            store_url: トクバイの店舗URL（Noneの場合は環境変数から取得）
            flyer_folder_id: チラシ保存先のDriveフォルダID（Noneの場合は環境変数から取得）
            store_name: 店舗名（Noneの場合は環境変数から取得）
        """
        self.store_url = store_url or os.getenv("TOKUBAI_STORE_URL")
        self.flyer_folder_id = flyer_folder_id or os.getenv("TOKUBAI_FLYER_FOLDER_ID")
        self.store_name = store_name or os.getenv("TOKUBAI_STORE_NAME", "トクバイ")

        if not self.store_url:
            raise ValueError("店舗URLが指定されていません。環境変数 TOKUBAI_STORE_URL を設定してください。")

        if not self.flyer_folder_id:
            raise ValueError("フォルダIDが指定されていません。環境変数 TOKUBAI_FLYER_FOLDER_ID を設定してください。")

        # コネクタの初期化
        self.scraper = TokubaiScraper(self.store_url)
        self.drive = GoogleDriveConnector()
        self.db = DatabaseClient()

        logger.info(f"TokubaiFlyerIngestionPipeline初期化完了")
        logger.info(f"  - Store name: {self.store_name}")
        logger.info(f"  - Store URL: {self.store_url}")
        logger.info(f"  - Flyer folder: {self.flyer_folder_id}")

    async def check_existing_flyers(self, flyer_ids: List[str]) -> set:
        """
        Supabaseで既存のチラシIDをチェック

        Args:
            flyer_ids: チェックするチラシIDのリスト

        Returns:
            既に存在するチラシIDのセット
        """
        try:
            # Rawdata_FLYER_shops テーブルで既存のチラシIDを取得
            result = self.db.client.table('Rawdata_FLYER_shops').select('flyer_id').in_(
                'flyer_id', flyer_ids
            ).execute()

            # flyer_id を抽出
            existing_ids = set()
            if result.data:
                for doc in result.data:
                    flyer_id = doc.get('flyer_id')
                    if flyer_id:
                        existing_ids.add(flyer_id)

            logger.info(f"既存のチラシ: {len(existing_ids)}件")
            return existing_ids

        except Exception as e:
            logger.error(f"Supabase検索エラー: {e}")
            return set()

    def save_image_to_drive(
        self,
        image_data: bytes,
        flyer_id: str,
        page_num: int,
        flyer_title: str
    ) -> Optional[tuple]:
        """
        チラシ画像をGoogle Driveに保存

        Args:
            image_data: 画像のバイトデータ
            flyer_id: チラシID
            page_num: ページ番号
            flyer_title: チラシのタイトル

        Returns:
            (DriveのファイルID, ファイル名)のタプル、失敗時はNone
        """
        # 安全なファイル名を生成
        safe_title = "".join(c for c in flyer_title if c.isalnum() or c in (' ', '-', '_', '　')).strip()
        if not safe_title:
            safe_title = "tokubai_flyer"

        # ファイル名生成
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_name = f"{timestamp}_{safe_title}_{flyer_id}_p{page_num}.webp"

        # 画像形式を判定（簡易版）
        # 実際のContent-Typeやファイルヘッダーから判定する方が正確
        if image_data.startswith(b'\xff\xd8\xff'):
            file_name = file_name.replace('.webp', '.jpg')
            mime_type = 'image/jpeg'
        elif image_data.startswith(b'\x89PNG'):
            file_name = file_name.replace('.webp', '.png')
            mime_type = 'image/png'
        elif image_data.startswith(b'RIFF') and b'WEBP' in image_data[:20]:
            mime_type = 'image/webp'
        else:
            # デフォルトはwebp
            mime_type = 'image/webp'

        # Driveにアップロード
        file_id = self.drive.upload_file(
            file_content=image_data,
            file_name=file_name,
            mime_type=mime_type,
            folder_id=self.flyer_folder_id
        )

        if file_id:
            logger.info(f"チラシ画像をDriveに保存: {file_name}")
        else:
            logger.error(f"チラシ画像の保存に失敗: {file_name}")

        return (file_id, file_name) if file_id else None

    async def process_single_flyer(
        self,
        flyer_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        1件のチラシを処理（画像ダウンロード→Drive保存→Supabase登録）

        Args:
            flyer_info: チラシ情報
                {'title': 'タイトル', 'url': '/チラシURL', 'flyer_id': 'xxx', 'period': '期間'}

        Returns:
            処理結果の辞書
        """
        result = {
            'flyer_id': flyer_info.get('flyer_id'),
            'success': False,
            'image_file_ids': [],
            'document_ids': [],
            'error': None
        }

        try:
            flyer_id = flyer_info['flyer_id']
            title = flyer_info.get('title', 'タイトルなし')
            period = flyer_info.get('period', '')
            flyer_url = flyer_info.get('url', '')

            logger.info(f"チラシ処理開始: {title} (ID: {flyer_id})")

            # 1. チラシページから画像URLを取得
            images = self.scraper.get_flyer_images(flyer_info)

            if not images:
                logger.warning(f"画像が見つかりません、スキップ: {title}")
                result['success'] = True
                return result

            logger.info(f"画像を{len(images)}件取得しました")

            # 2. 各画像を処理
            for img_info in images:
                img_url = img_info['url']
                page_num = img_info['page']

                # 画像をダウンロード
                image_data = self.scraper.download_image(img_url)
                if not image_data:
                    logger.warning(f"画像のダウンロードに失敗（スキップ）: {img_url}")
                    continue

                # Google Driveに保存
                drive_result = self.save_image_to_drive(image_data, flyer_id, page_num, title)
                if not drive_result:
                    logger.error(f"画像の保存に失敗: page {page_num}")
                    continue

                file_id, actual_file_name = drive_result
                result['image_file_ids'].append(file_id)

                # 3. メタデータ準備
                full_flyer_url = f"https://tokubai.co.jp{flyer_url}" if flyer_url.startswith('/') else flyer_url

                # 4. Supabaseに基本情報のみ保存（Rawdata_FLYER_shopsテーブル）
                doc_data = {
                    # 基本情報
                    'source_type': 'flyer',
                    'workspace': 'shopping',
                    'doc_type': 'physical shop',
                    'organization': self.store_name,  # 店舗名

                    # チラシ固有情報
                    'flyer_id': f"{flyer_id}_p{page_num}",  # ページごとにユニークなID
                    'flyer_title': title,
                    'flyer_period': period,
                    'flyer_url': full_flyer_url,
                    'page_number': page_num,

                    # ファイル情報
                    'source_id': file_id,
                    'source_url': f"https://drive.google.com/file/d/{file_id}/view",
                    'file_name': actual_file_name,
                    'file_type': 'image',
                    'content_hash': hashlib.sha256(image_data).hexdigest(),

                    # OCR・テキスト情報（後で処理）
                    'attachment_text': '',
                    'summary': '',

                    # 分類・タグ
                    'tags': ['チラシ', '買い物'],

                    # 日付
                    'document_date': datetime.now().date().isoformat(),

                    # 処理ステータス
                    'processing_status': 'pending',  # 画像処理待ち
                    'processing_stage': 'tokubai_flyer_downloaded',

                    # 表示用フィールド
                    'display_subject': f"{title} (ページ {page_num})",
                    'display_sent_at': datetime.now().isoformat(),
                    'display_sender': 'トクバイ',
                    'display_post_text': period,

                    # メタデータ
                    'metadata': {
                        'image_url': img_url,
                        'store_url': self.store_url,
                        'original_flyer_id': flyer_id
                    },

                    # その他
                    'person': '共有'
                }

                try:
                    # Supabaseに保存
                    doc_result = await self.db.insert_document('Rawdata_FLYER_shops', doc_data)
                    if doc_result:
                        doc_id = doc_result.get('id')
                        result['document_ids'].append(doc_id)
                        logger.info(f"Supabase保存完了（pending状態）: {doc_id}")

                except Exception as db_error:
                    logger.error(f"Supabase保存エラー: {db_error}")
                    result['error'] = str(db_error)

            result['success'] = True
            logger.info(f"チラシ処理完了: {title} ({len(result['image_file_ids'])} images)")

        except Exception as e:
            logger.error(f"チラシ処理エラー: {e}", exc_info=True)
            result['error'] = str(e)

        return result


def load_stores_config(config_path: Optional[Path] = None) -> Dict[str, Any]:
    """
    店舗設定ファイルを読み込む

    Args:
        config_path: 設定ファイルのパス（Noneの場合はデフォルトパス）

    Returns:
        設定データ
    """
    if config_path is None:
        config_path = Path(__file__).parent / "stores_config.json"

    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        logger.info(f"店舗設定ファイルを読み込みました: {config_path}")
        return config
    except Exception as e:
        logger.error(f"店舗設定ファイルの読み込みエラー: {e}")
        return {"stores": [], "default_folder_id": None}


async def process_store(store_config: Dict[str, str], folder_id: str) -> Dict[str, Any]:
    """
    1店舗のチラシを処理

    Args:
        store_config: 店舗設定 {'name': '店舗名', 'url': 'URL', 'enabled': True}
        folder_id: Google DriveフォルダID

    Returns:
        処理結果のサマリー
    """
    store_name = store_config['name']
    store_url = store_config['url']

    logger.info("=" * 60)
    logger.info(f"店舗処理開始: {store_name}")
    logger.info("=" * 60)

    # パイプラインの初期化
    try:
        pipeline = TokubaiFlyerIngestionPipeline(
            store_url=store_url,
            flyer_folder_id=folder_id,
            store_name=store_name
        )
    except ValueError as e:
        logger.error(f"初期化エラー ({store_name}): {e}")
        return {
            'store_name': store_name,
            'success': False,
            'error': str(e),
            'results': []
        }

    # 1. 店舗ページから全チラシ情報を取得
    logger.info("店舗ページからチラシ一覧を取得中...")
    all_flyers = pipeline.scraper.get_all_flyers()

    if not all_flyers:
        logger.warning("チラシが取得できませんでした")
        return {
            'store_name': store_name,
            'success': True,
            'new_flyers': 0,
            'results': []
        }

    logger.info(f"チラシを{len(all_flyers)}件取得しました")

    # 2. 既存のチラシIDをSupabaseから取得
    flyer_ids = [f.get('flyer_id') for f in all_flyers if f.get('flyer_id')]
    existing_ids = await pipeline.check_existing_flyers(flyer_ids)

    # 3. 新着チラシを抽出
    new_flyers = [f for f in all_flyers if f.get('flyer_id') not in existing_ids]

    logger.info(f"現在のチラシ: {len(all_flyers)}件")
    logger.info(f"既存のチラシ: {len(existing_ids)}件")
    logger.info(f"新着チラシ: {len(new_flyers)}件")

    if not new_flyers:
        logger.info("新着チラシはありません")
        return {
            'store_name': store_name,
            'success': True,
            'new_flyers': 0,
            'results': []
        }

    # 4. 新着チラシを処理
    results = []
    for i, flyer in enumerate(new_flyers, 1):
        logger.info(f"[{i}/{len(new_flyers)}] 処理中: {flyer.get('title', '無題')}")
        result = await pipeline.process_single_flyer(flyer)
        results.append(result)

    # 5. サマリー
    success_count = sum(1 for r in results if r['success'])
    total_images = sum(len(r['image_file_ids']) for r in results)
    total_docs = sum(len(r['document_ids']) for r in results)

    logger.info("=" * 60)
    logger.info(f"{store_name} の処理完了")
    logger.info(f"  成功: {success_count}/{len(results)}")
    logger.info(f"  失敗: {len(results) - success_count}/{len(results)}")
    logger.info(f"  処理した画像: {total_images}件")
    logger.info(f"  登録したドキュメント: {total_docs}件（pending状態）")
    logger.info("=" * 60)

    return {
        'store_name': store_name,
        'success': True,
        'new_flyers': len(new_flyers),
        'success_count': success_count,
        'total_images': total_images,
        'total_docs': total_docs,
        'results': results
    }


async def main():
    """メインエントリーポイント"""
    logger.info("=" * 60)
    logger.info("トクバイチラシ取得パイプライン開始（複数店舗対応）")
    logger.info("=" * 60)

    # 設定ファイルを読み込む
    config = load_stores_config()

    if not config.get('stores'):
        logger.error("店舗設定が見つかりません")
        logger.info("stores_config.json を確認してください")
        return

    # 有効な店舗のみを処理
    enabled_stores = [s for s in config['stores'] if s.get('enabled', True)]
    logger.info(f"処理対象店舗: {len(enabled_stores)}件")

    # フォルダIDを取得（環境変数または設定ファイルから）
    folder_id = os.getenv("TOKUBAI_FLYER_FOLDER_ID") or config.get('default_folder_id')

    if not folder_id:
        logger.error("フォルダIDが設定されていません")
        logger.info("環境変数 TOKUBAI_FLYER_FOLDER_ID または stores_config.json で設定してください")
        return

    # 各店舗を処理
    all_store_results = []
    for i, store in enumerate(enabled_stores, 1):
        logger.info(f"\n[{i}/{len(enabled_stores)}] 店舗処理開始: {store['name']}")
        store_result = await process_store(store, folder_id)
        all_store_results.append(store_result)

    # 全体のサマリー
    logger.info("\n" + "=" * 60)
    logger.info("全店舗の処理完了")
    logger.info("=" * 60)

    total_new_flyers = sum(r.get('new_flyers', 0) for r in all_store_results)
    total_images = sum(r.get('total_images', 0) for r in all_store_results)
    total_docs = sum(r.get('total_docs', 0) for r in all_store_results)

    logger.info(f"  処理した店舗: {len(all_store_results)}件")
    logger.info(f"  新着チラシ: {total_new_flyers}件")
    logger.info(f"  処理した画像: {total_images}件")
    logger.info(f"  登録したドキュメント: {total_docs}件（pending状態）")
    logger.info("=" * 60)

    # 結果を表示
    print("\n" + "=" * 80)
    print("🛒 トクバイチラシ取得結果（複数店舗）")
    print("=" * 80)

    for store_result in all_store_results:
        print(f"\n店舗: {store_result['store_name']}")
        print(f"  新着チラシ: {store_result.get('new_flyers', 0)}件")
        if store_result.get('results'):
            print(f"  成功: {store_result.get('success_count', 0)}")
            print(f"  画像: {store_result.get('total_images', 0)}件")
            print(f"  ドキュメント: {store_result.get('total_docs', 0)}件")

    print("\n" + "=" * 80)
    print(f"合計: 新着チラシ {total_new_flyers}件、画像 {total_images}件、ドキュメント {total_docs}件")
    print("=" * 80)
    print("\n次のステップ:")
    print("  python process_queued_documents.py --workspace=household")
    print("=" * 80)


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

### services\data-ingestion\tokubai\flyer_processor.py

```py
"""
チラシ画像処理パイプライン

Gemini 2.5 Pro Visionを使用してチラシ画像から商品情報を抽出し、
Rawdata_FLYER_itemsテーブルに保存する。

処理フロー:
1. Rawdata_FLYER_shops から processing_status='pending' のチラシを取得
2. Gemini 2.5 Pro Vision でチラシ画像から商品情報を抽出
3. Rawdata_FLYER_items テーブルに商品データを保存
4. Rawdata_FLYER_shops の processing_status を 'completed' に更新
"""
import os
import sys
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from loguru import logger
import traceback

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from shared.common.connectors.google_drive import GoogleDriveConnector
from shared.common.database.client import DatabaseClient
from shared.ai.llm_client.llm_client import LLMClient


# 商品抽出用のプロンプトスキーマ
PRODUCT_EXTRACTION_PROMPT = """
あなたはスーパーマーケットのチラシから商品情報を抽出する専門家です。

チラシ画像から以下の情報を含む商品リストをJSON形式で抽出してください：

- product_name: 商品名（必須）
- price: 価格（数値、単位なし）
- original_price: 元の価格（割引前、ある場合のみ）
- discount_rate: 割引率（%、ある場合のみ）
- price_unit: 価格の単位（例: "円", "円/100g"）
- price_text: 価格の元のテキスト（例: "298円", "特価"）
- category: カテゴリ（野菜、肉、魚、日用品、飲料、冷凍食品、菓子、調味料、その他）
- brand: ブランド名（ある場合のみ）
- quantity: 数量・容量（例: "100g", "1パック", "500ml"）
- origin: 産地（ある場合のみ）
- is_special_offer: 特売品かどうか（true/false）
- offer_type: 特売タイプ（タイムセール、日替わり、週末限定など、ある場合のみ）
- extracted_text: この商品に関する元のテキスト
- confidence: 抽出の信頼度（0.0〜1.0）

**重要な注意事項:**
1. すべての商品を漏れなく抽出してください
2. 価格は数値のみ抽出（例: "298円" → 298）
3. カテゴリは上記のいずれかに分類
4. 商品名は正確に抽出（ブランド名を含む）
5. 特売・セール品は is_special_offer を true に設定
6. 情報が不明な場合は null を設定

**出力形式:**
```json
{
  "products": [
    {
      "product_name": "国産キャベツ",
      "price": 98,
      "price_unit": "円",
      "price_text": "98円",
      "category": "野菜",
      "quantity": "1玉",
      "origin": "国産",
      "is_special_offer": true,
      "offer_type": "日替わり",
      "extracted_text": "国産キャベツ 1玉 98円 日替わり特価",
      "confidence": 0.95
    }
  ],
  "total_products": 1
}
```

チラシ情報:
- 店舗: {store_name}
- タイトル: {flyer_title}
- 期間: {flyer_period}
- ページ: {page_number}

それでは、画像から商品情報を抽出してください。
"""


class FlyerProcessor:
    """チラシ画像処理プロセッサー"""

    def __init__(self, temp_dir: str = "./temp"):
        """
        Args:
            temp_dir: 一時ファイル保存ディレクトリ
        """
        self.llm_client = LLMClient()
        self.drive = GoogleDriveConnector()
        self.db = DatabaseClient()

        self.temp_dir = Path(temp_dir)
        self.temp_dir.mkdir(parents=True, exist_ok=True)

        logger.info("FlyerProcessor初期化完了")

    async def get_pending_flyers(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        処理待ちのチラシを取得

        Args:
            limit: 取得件数

        Returns:
            チラシ情報のリスト
        """
        try:
            result = self.db.client.table('Rawdata_FLYER_shops').select('*').eq(
                'processing_status', 'pending'
            ).limit(limit).execute()

            if result.data:
                logger.info(f"処理待ちチラシ: {len(result.data)}件")
                return result.data

            return []

        except Exception as e:
            logger.error(f"チラシ取得エラー: {e}")
            return []

    async def extract_products_from_image(
        self,
        flyer_doc: Dict[str, Any],
        image_path: str
    ) -> Optional[Dict[str, Any]]:
        """
        Gemini 2.5 Pro Visionでチラシ画像から商品情報を抽出

        Args:
            flyer_doc: チラシドキュメント情報
            image_path: ローカル画像パス

        Returns:
            抽出結果 {'products': [...], 'total_products': N}
        """
        try:
            # プロンプト生成
            prompt = PRODUCT_EXTRACTION_PROMPT.format(
                store_name=flyer_doc.get('organization', '不明'),
                flyer_title=flyer_doc.get('flyer_title', '不明'),
                flyer_period=flyer_doc.get('flyer_period', '不明'),
                page_number=flyer_doc.get('page_number', 1)
            )

            logger.info(f"Gemini Vision で商品抽出開始: {flyer_doc.get('file_name')}")

            # Gemini 2.5 Pro Vision で画像を処理
            result = await self.llm_client.generate_with_vision(
                prompt=prompt,
                image_path=image_path,
                model="gemini-2.0-flash-exp",  # Gemini 2.5 Pro Vision
                response_format="json"
            )

            # JSONパース
            try:
                products_data = json.loads(result)
                logger.info(f"商品抽出完了: {products_data.get('total_products', 0)}件")
                return products_data
            except json.JSONDecodeError as json_err:
                logger.error(f"JSON パースエラー: {json_err}")
                logger.debug(f"レスポンス: {result[:500]}")
                return None

        except Exception as e:
            logger.error(f"商品抽出エラー: {e}", exc_info=True)
            return None

    async def save_products_to_db(
        self,
        flyer_doc_id: str,
        products: List[Dict[str, Any]],
        page_number: int
    ) -> int:
        """
        商品情報をRawdata_FLYER_itemsテーブルに保存

        Args:
            flyer_doc_id: チラシドキュメントID
            products: 商品リスト
            page_number: ページ番号

        Returns:
            保存成功件数
        """
        success_count = 0

        for product in products:
            try:
                # 商品名を取得
                product_name = product.get('product_name', '')

                # カテゴリの正規化
                category_map = {
                    '野菜': '野菜',
                    '果物': '果物',
                    '肉': '肉',
                    '魚': '魚',
                    '日用品': '日用品',
                    '飲料': '飲料',
                    '冷凍食品': '冷凍食品',
                    '菓子': '菓子',
                    '調味料': '調味料',
                }
                category = category_map.get(product.get('category', 'その他'), 'その他')

                product_data = {
                    'flyer_document_id': flyer_doc_id,
                    'product_name': product_name,
                    'price': product.get('price'),
                    'original_price': product.get('original_price'),
                    'discount_rate': product.get('discount_rate'),
                    'price_unit': product.get('price_unit', '円'),
                    'price_text': product.get('price_text'),
                    'category': category,
                    'subcategory': product.get('subcategory'),
                    'brand': product.get('brand'),
                    'quantity': product.get('quantity'),
                    'origin': product.get('origin'),
                    'is_special_offer': product.get('is_special_offer', False),
                    'offer_type': product.get('offer_type'),
                    'page_number': page_number,
                    'extracted_text': product.get('extracted_text'),
                    'confidence': product.get('confidence', 0.5),
                    'metadata': {
                        'extraction_date': datetime.now().isoformat(),
                        'extraction_model': 'gemini-2.5-pro-vision'
                    }
                }

                result = await self.db.insert_document('Rawdata_FLYER_items', product_data)
                if result:
                    success_count += 1
                    logger.debug(f"商品保存成功: {product_name}")

            except Exception as e:
                logger.error(f"商品保存エラー: {e}")
                logger.debug(f"商品データ: {product}")

        logger.info(f"商品保存完了: {success_count}/{len(products)}件")
        return success_count

    async def update_flyer_status(
        self,
        flyer_doc_id: str,
        status: str,
        attachment_text: str = None,
        error: str = None
    ):
        """
        チラシの処理ステータスを更新

        Args:
            flyer_doc_id: チラシドキュメントID
            status: ステータス（completed, failed）
            attachment_text: 抽出したテキスト
            error: エラーメッセージ
        """
        try:
            update_data = {
                'processing_status': status,
                'updated_at': datetime.now().isoformat()
            }

            if attachment_text:
                update_data['attachment_text'] = attachment_text

            if error:
                update_data['processing_error'] = error

            if status == 'completed':
                update_data['processing_stage'] = 'products_extracted'

            self.db.client.table('Rawdata_FLYER_shops').update(update_data).eq(
                'id', flyer_doc_id
            ).execute()

            logger.info(f"チラシステータス更新: {status}")

        except Exception as e:
            logger.error(f"ステータス更新エラー: {e}")

    async def process_single_flyer(self, flyer_doc: Dict[str, Any]) -> Dict[str, Any]:
        """
        1件のチラシを処理

        Args:
            flyer_doc: チラシドキュメント情報

        Returns:
            処理結果
        """
        flyer_doc_id = flyer_doc['id']
        file_name = flyer_doc.get('file_name', '不明')
        source_id = flyer_doc.get('source_id')  # Google Drive ID

        logger.info(f"=== チラシ処理開始: {file_name} ===")

        result = {
            'flyer_doc_id': flyer_doc_id,
            'file_name': file_name,
            'success': False,
            'products_count': 0,
            'error': None
        }

        local_path = None

        try:
            # 1. Google Driveから画像をダウンロード
            logger.info("画像ダウンロード中...")
            local_path = self.drive.download_file(source_id, file_name, self.temp_dir)

            if not local_path or not Path(local_path).exists():
                raise Exception("画像ダウンロード失敗")

            # 2. Gemini Vision で商品情報を抽出
            products_data = await self.extract_products_from_image(flyer_doc, local_path)

            if not products_data or not products_data.get('products'):
                logger.warning("商品が抽出できませんでした")
                await self.update_flyer_status(flyer_doc_id, 'completed', attachment_text="商品情報なし")
                result['success'] = True
                return result

            # 3. 商品をDBに保存
            products = products_data['products']
            page_number = flyer_doc.get('page_number', 1)

            saved_count = await self.save_products_to_db(flyer_doc_id, products, page_number)

            # 4. チラシのステータスを更新
            # 抽出したテキストをまとめる
            all_texts = [p.get('extracted_text', '') for p in products]
            attachment_text = '\n'.join(filter(None, all_texts))

            await self.update_flyer_status(flyer_doc_id, 'completed', attachment_text=attachment_text)

            result['success'] = True
            result['products_count'] = saved_count
            logger.info(f"=== チラシ処理完了: {file_name} ({saved_count}件の商品) ===")

        except Exception as e:
            error_msg = str(e)
            logger.error(f"チラシ処理エラー: {error_msg}", exc_info=True)

            await self.update_flyer_status(flyer_doc_id, 'failed', error=error_msg)
            result['error'] = error_msg

        finally:
            # 一時ファイル削除
            if local_path and Path(local_path).exists():
                Path(local_path).unlink()
                logger.debug(f"一時ファイル削除: {local_path}")

        return result

    async def process_pending_flyers(self, limit: int = 10) -> Dict[str, Any]:
        """
        処理待ちのチラシを一括処理

        Args:
            limit: 処理件数

        Returns:
            処理結果のサマリー
        """
        logger.info("=" * 60)
        logger.info("チラシ処理パイプライン開始")
        logger.info("=" * 60)

        # 処理待ちチラシを取得
        pending_flyers = await self.get_pending_flyers(limit)

        if not pending_flyers:
            logger.info("処理待ちのチラシはありません")
            return {'total': 0, 'success': 0, 'failed': 0}

        logger.info(f"処理対象: {len(pending_flyers)}件")

        results = []
        for i, flyer in enumerate(pending_flyers, 1):
            logger.info(f"[{i}/{len(pending_flyers)}] 処理中: {flyer.get('file_name')}")
            result = await self.process_single_flyer(flyer)
            results.append(result)

        # サマリー
        success_count = sum(1 for r in results if r['success'])
        failed_count = len(results) - success_count
        total_products = sum(r.get('products_count', 0) for r in results)

        logger.info("=" * 60)
        logger.info("処理完了")
        logger.info(f"  成功: {success_count}/{len(results)}")
        logger.info(f"  失敗: {failed_count}/{len(results)}")
        logger.info(f"  抽出商品数: {total_products}件")
        logger.info("=" * 60)

        return {
            'total': len(results),
            'success': success_count,
            'failed': failed_count,
            'total_products': total_products,
            'results': results
        }


async def main():
    """メインエントリーポイント"""
    processor = FlyerProcessor()
    result = await processor.process_pending_flyers(limit=100)

    # 結果を表示
    print("\n" + "=" * 80)
    print("🛒 チラシ商品抽出結果")
    print("=" * 80)
    print(f"処理件数: {result['total']}")
    print(f"成功: {result['success']}")
    print(f"失敗: {result['failed']}")
    print(f"抽出商品数: {result['total_products']}")
    print("=" * 80)


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

### services\data-ingestion\tokubai\stores_config.json

```json
{
  "stores": [
    {
      "name": "フーディアム 武蔵小杉",
      "url": "https://tokubai.co.jp/フーディアム/7978",
      "enabled": true
    },
    {
      "name": "マルエツ 武蔵小杉駅前店",
      "url": "https://tokubai.co.jp/マルエツ/9809",
      "enabled": true
    },
    {
      "name": "イトーヨーカドー グランツリー武蔵小杉店",
      "url": "https://tokubai.co.jp/イトーヨーカドー/15009",
      "enabled": true
    },
    {
      "name": "成城石井 ららテラス武蔵小杉店",
      "url": "https://tokubai.co.jp/成城石井/13372",
      "enabled": true
    },
    {
      "name": "イトーヨーカドー 武蔵小杉駅前店",
      "url": "https://tokubai.co.jp/イトーヨーカドー/8943",
      "enabled": true
    },
    {
      "name": "東急ストア 武蔵小杉店",
      "url": "https://tokubai.co.jp/東急ストア/5735",
      "enabled": true
    },
    {
      "name": "ユニクロ 武蔵小杉東急スクエア店",
      "url": "https://tokubai.co.jp/ユニクロ/87223",
      "enabled": true
    },
    {
      "name": "どらっぐぱぱす 武蔵小杉店",
      "url": "https://tokubai.co.jp/どらっぐぱぱす/27325",
      "enabled": true
    },
    {
      "name": "トモズ 武蔵小杉店",
      "url": "https://tokubai.co.jp/トモズ/35716",
      "enabled": true
    },
    {
      "name": "ハックドラッグ 武蔵小杉東急スクエア店",
      "url": "https://tokubai.co.jp/prefectures/14/areas/360/leaflet?page=3",
      "enabled": true
    }
  ],
  "default_folder_id": "1uQEJbV94mBC2y0D0FQztDGrzy6UNgEhv"
}
```

### services\data-ingestion\tokubai\tokubai_scraper.py

```py
"""
トクバイチラシスクレイピングモジュール

トクバイのウェブサイトからチラシ情報を取得する。
"""
import re
import time
from typing import List, Dict, Any, Optional
from loguru import logger
import requests
from bs4 import BeautifulSoup
from io import BytesIO
from PIL import Image


class TokubaiScraper:
    """トクバイスクレイピングクラス"""

    def __init__(self, store_url: str):
        """
        Args:
            store_url: 店舗のトクバイURL
                例: https://tokubai.co.jp/フーディアム/7978
        """
        self.store_url = store_url
        self.base_url = "https://tokubai.co.jp"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }

    def fetch_store_page(self) -> Optional[str]:
        """
        店舗ページのHTMLを取得

        Returns:
            HTMLコンテンツ、失敗時はNone
        """
        try:
            logger.info(f"店舗ページを取得中: {self.store_url}")
            response = requests.get(self.store_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            logger.info(f"ステータスコード: {response.status_code}")
            return response.text
        except Exception as e:
            logger.error(f"店舗ページの取得に失敗: {e}")
            return None

    def extract_flyer_links(self, html_content: str) -> List[Dict[str, str]]:
        """
        HTMLからチラシのリンク情報を抽出

        Args:
            html_content: 店舗ページのHTML

        Returns:
            チラシ情報のリスト
            [{'title': 'タイトル', 'url': '/チラシURL', 'flyer_id': 'xxx', 'period': '期間'}, ...]
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            flyer_links = []

            # チラシへのリンクを抽出（aタグのhref="/店舗名/数字/leaflets/数字"のパターン）
            for link in soup.find_all('a', href=True):
                href = link['href']
                # チラシページのURLパターンをマッチ（例: /フーディアム/7978/leaflets/96703723）
                match = re.match(r'^/[^/]+/\d+/leaflets/(\d+)$', href)
                if match:
                    flyer_id = match.group(1)

                    # タイトルを取得（リンク内のテキストまたは画像のalt）
                    title = link.get_text(strip=True)
                    if not title:
                        img = link.find('img')
                        if img and img.get('alt'):
                            title = img['alt']

                    if not title:
                        title = f"チラシ_{flyer_id}"

                    # 期間情報を取得（親要素から探す）
                    period = ""
                    parent = link.find_parent()
                    if parent:
                        period_elem = parent.find(text=re.compile(r'\d{4}[./]\d{1,2}[./]\d{1,2}'))
                        if period_elem:
                            period = period_elem.strip()

                    flyer_info = {
                        'title': title,
                        'url': href,
                        'flyer_id': flyer_id,
                        'period': period
                    }

                    flyer_links.append(flyer_info)
                    logger.debug(f"チラシ発見: {title} ({flyer_id})")

            # 重複を除去（flyer_idでユニーク化）
            unique_flyers = {}
            for flyer in flyer_links:
                flyer_id = flyer['flyer_id']
                if flyer_id not in unique_flyers:
                    unique_flyers[flyer_id] = flyer

            result = list(unique_flyers.values())
            logger.info(f"チラシリンクを{len(result)}件抽出しました")
            return result

        except Exception as e:
            logger.error(f"チラシリンク抽出エラー: {e}", exc_info=True)
            return []

    def fetch_flyer_page(self, flyer_url: str) -> Optional[str]:
        """
        チラシ詳細ページのHTMLを取得

        Args:
            flyer_url: チラシの相対URL（例: /フーディアム/7978/1234567）

        Returns:
            HTMLコンテンツ、失敗時はNone
        """
        try:
            full_url = f"{self.base_url}{flyer_url}"
            logger.info(f"チラシページを取得中: {full_url}")
            response = requests.get(full_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.error(f"チラシページの取得に失敗: {e}")
            return None

    def extract_image_urls(self, html_content: str, flyer_id: str) -> List[Dict[str, Any]]:
        """
        チラシページから画像URLを抽出

        Args:
            html_content: チラシページのHTML
            flyer_id: チラシID

        Returns:
            画像情報のリスト
            [{'url': '画像URL', 'page': ページ番号, 'flyer_id': 'xxx'}, ...]
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            images = []

            # 画像を抽出（imgタグのsrcまたはdata-src属性）
            img_tags = soup.find_all('img')
            page_num = 1

            for img in img_tags:
                # src または data-src から画像URLを取得
                img_url = img.get('data-src') or img.get('src')

                if not img_url:
                    continue

                # チラシ画像のパターンにマッチするか確認
                # 例: https://cdn-ak.f.st-hatena.com/images/fotolife/...
                # または相対パスの場合は絶対パスに変換
                if img_url.startswith('//'):
                    img_url = 'https:' + img_url
                elif img_url.startswith('/'):
                    img_url = self.base_url + img_url

                # 画像URLが有効かチェック（最小限のフィルタ）
                if not img_url.startswith('http'):
                    continue

                # アイコンやロゴ、サムネイルを除外（URLのキーワードでフィルタ）
                if any(keyword in img_url.lower() for keyword in ['logo', 'icon', 'banner', 'ad', 'thumb', 'thumbnail']):
                    continue

                # URLに小さいサイズの寸法が含まれている場合は除外
                # 例: "100x100", "150x150", "200x200" など
                size_pattern = re.search(r'(\d+)x(\d+)', img_url)
                if size_pattern:
                    width = int(size_pattern.group(1))
                    height = int(size_pattern.group(2))
                    if width < 400 or height < 400:
                        logger.debug(f"URLにサムネイルサイズが含まれているため除外: {width}x{height} ({img_url})")
                        continue

                images.append({
                    'url': img_url,
                    'page': page_num,
                    'flyer_id': flyer_id
                })

                page_num += 1

            logger.info(f"チラシ画像を{len(images)}件抽出しました (flyer_id: {flyer_id})")
            return images

        except Exception as e:
            logger.error(f"画像URL抽出エラー: {e}", exc_info=True)
            return []

    def download_image(self, image_url: str, min_width: int = 400, min_height: int = 400) -> Optional[bytes]:
        """
        画像をダウンロード（サムネイルサイズは除外）

        Args:
            image_url: 画像のURL
            min_width: 最小幅（ピクセル）。これ以下はサムネイルとして除外
            min_height: 最小高さ（ピクセル）。これ以下はサムネイルとして除外

        Returns:
            画像データ（バイト列）、失敗時はNone
        """
        try:
            logger.debug(f"画像ダウンロード中: {image_url}")
            response = requests.get(image_url, headers=self.headers, timeout=30)
            response.raise_for_status()

            # Content-Typeをチェック
            content_type = response.headers.get('Content-Type', '')
            if not content_type.startswith('image/'):
                logger.warning(f"画像ではないコンテンツ: {content_type}")
                return None

            image_data = response.content

            # 画像サイズをチェック（サムネイルを除外）
            try:
                img = Image.open(BytesIO(image_data))
                width, height = img.size
                logger.debug(f"画像サイズ: {width}x{height}")

                if width < min_width or height < min_height:
                    logger.info(f"サムネイルサイズのため除外: {width}x{height} (URL: {image_url})")
                    return None

            except Exception as img_error:
                logger.warning(f"画像サイズチェックエラー: {img_error}")
                # サイズチェックに失敗した場合は、画像データをそのまま返す

            logger.debug(f"画像ダウンロード成功 ({len(image_data)} bytes, {width}x{height})")
            return image_data

        except Exception as e:
            logger.error(f"画像ダウンロードエラー: {e}")
            return None

    def get_all_flyers(self) -> List[Dict[str, Any]]:
        """
        店舗の全チラシ情報を取得（リンクのみ、画像は未ダウンロード）

        Returns:
            チラシ情報のリスト
        """
        html_content = self.fetch_store_page()
        if not html_content:
            return []

        flyer_links = self.extract_flyer_links(html_content)
        return flyer_links

    def get_flyer_images(self, flyer_info: Dict[str, str]) -> List[Dict[str, Any]]:
        """
        特定のチラシの画像情報を取得

        Args:
            flyer_info: get_all_flyers()で取得したチラシ情報

        Returns:
            画像情報のリスト
        """
        flyer_url = flyer_info['url']
        flyer_id = flyer_info['flyer_id']

        html_content = self.fetch_flyer_page(flyer_url)
        if not html_content:
            return []

        # ページ間で負荷をかけないよう少し待機
        time.sleep(1)

        images = self.extract_image_urls(html_content, flyer_id)
        return images
```

### services\data-ingestion\tokyu_store\__init__.py

```py
"""
東急ストア ネットスーパー データ取得モジュール
"""
```

### services\data-ingestion\tokyu_store\process_with_schedule.py

```py
"""
東急ストアネットスーパー スケジュール管理対応版

カテゴリーごとの実行スケジュールを管理し、
サーバー負荷を最小限に抑える待機時間を実装します。
"""

import os
import sys
import asyncio
import random
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from common.category_manager_db import CategoryManagerDB
from tokyu_store.product_ingestion import TokyuStoreProductIngestionPipeline

# ロガー設定
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class PoliteTokyuStorePipeline:
    """サーバー負荷に配慮したスケジュール管理パイプライン"""

    def __init__(
        self,
        login_id: str,
        password: str,
        zip_code: str = "158-0094",
        headless: bool = True,
        dry_run: bool = False
    ):
        """
        Args:
            login_id: 東急ストアログインID
            password: パスワード
            zip_code: 配達エリア郵便番号
            headless: ヘッドレスモード
            dry_run: Dry Run モード（設定ファイルの初期化のみ）
        """
        self.pipeline = TokyuStoreProductIngestionPipeline(
            login_id=login_id,
            password=***REDACTED***
            zip_code=zip_code,
            headless=headless
        )
        self.manager = CategoryManagerDB()
        self.dry_run = dry_run
        self.store_name = "tokyu_store"

    async def polite_wait_between_pages(self):
        """ページ遷移間の待機（4秒〜8秒のランダム）"""
        wait_time = random.uniform(4.0, 8.0)
        logger.info(f"⏳ ページ遷移待機: {wait_time:.1f}秒")
        await asyncio.sleep(wait_time)

    async def polite_wait_between_categories(self):
        """カテゴリー切替時の待機（15秒〜30秒のランダム）"""
        wait_time = random.uniform(15.0, 30.0)
        logger.info(f"⏳ カテゴリー切替待機: {wait_time:.1f}秒")
        await asyncio.sleep(wait_time)

    async def initialize_categories(self):
        """カテゴリーを初期化（初回実行時）"""
        logger.info("📋 カテゴリーを初期化します...")

        # スクレイパー起動
        success = await self.pipeline.start()
        if not success:
            logger.error("❌ スクレイパー起動失敗")
            return False

        try:
            # カテゴリーを動的に取得
            categories = await self.pipeline.discover_categories()

            if not categories:
                logger.warning("カテゴリーが見つかりませんでした")
                return False

            # CategoryManagerに登録
            category_list = [
                {"name": cat["name"], "url": cat["url"]}
                for cat in categories
            ]

            # デフォルト設定で初期化
            # 開始日: 明日、インターバル: 7日
            tomorrow = datetime.now().strftime("%Y-%m-%d")
            self.manager.initialize_store_categories(
                self.store_name,
                category_list,
                default_interval_days=7,
                default_start_date=tomorrow
            )

            logger.info(f"✅ {len(categories)}件のカテゴリーを初期化しました")
            logger.info("管理画面で設定を調整してください:")
            logger.info("  streamlit run B_ingestion/netsuper_category_manager_ui.py")

            return True

        finally:
            await self.pipeline.close()

    async def run_scheduled_categories(self, manual_categories: List[str] = None):
        """スケジュールに基づいてカテゴリーを処理

        Args:
            manual_categories: 手動実行時に指定されたカテゴリー名のリスト（Noneの場合はスケジュールに従う）
        """
        if manual_categories:
            logger.info("="*80)
            logger.info("東急ストアネットスーパー 手動実行開始")
            logger.info(f"対象カテゴリー: {', '.join(manual_categories)}")
            logger.info("="*80)
        else:
            logger.info("="*80)
            logger.info("東急ストアネットスーパー スケジュール実行開始")
            logger.info("="*80)

        # スクレイパー起動
        success = await self.pipeline.start()
        if not success:
            logger.error("❌ スクレイパー起動失敗")
            return

        try:
            # カテゴリーを動的に取得して更新
            logger.info("🔄 カテゴリーを最新化中...")
            discovered_categories = await self.pipeline.discover_categories()

            if discovered_categories:
                logger.info(f"✅ {len(discovered_categories)}件のカテゴリーを取得")

                # 既存の設定を取得
                existing_categories = self.manager.get_all_categories(self.store_name)
                existing_names = {cat["name"]: cat for cat in existing_categories} if existing_categories else {}

                # 新規カテゴリーを追加
                for cat in discovered_categories:
                    if cat["name"] not in existing_names:
                        logger.info(f"  📝 新規カテゴリー追加: {cat['name']}")
                        self.manager.update_category(
                            self.store_name,
                            cat["name"],
                            {
                                "url": cat["url"],
                                "enabled": True,
                                "interval_days": 7,
                                "start_date": datetime.now().strftime("%Y-%m-%d")
                            }
                        )
                    else:
                        # URLが変更されている場合は更新
                        if existing_names[cat["name"]].get("url") != cat["url"]:
                            logger.info(f"  🔄 URL更新: {cat['name']}")
                            self.manager.update_category(
                                self.store_name,
                                cat["name"],
                                {"url": cat["url"]}
                            )

            # 設定からカテゴリーを取得
            categories = self.manager.get_all_categories(self.store_name)

            if not categories:
                logger.warning("カテゴリーが設定されていません。初回実行してください:")
                logger.warning("  python -m B_ingestion.tokyu_store.process_with_schedule --init")
                return

            # 実行すべきカテゴリーをフィルタリング
            today = datetime.now()
            runnable_categories = []

            if manual_categories:
                # 手動実行時: 指定されたカテゴリーのみ
                for cat in categories:
                    if cat["name"] in manual_categories:
                        runnable_categories.append(cat)
            else:
                # スケジュール実行時: 今日実行すべきカテゴリー
                for cat in categories:
                    if self.manager.should_run_category(self.store_name, cat["name"], today):
                        runnable_categories.append(cat)

            logger.info(f"📊 総カテゴリー数: {len(categories)}件")
            logger.info(f"✅ 本日実行対象: {len(runnable_categories)}件")

            if not runnable_categories:
                logger.info("本日実行するカテゴリーはありません")
                return

            # カテゴリーごとに処理
            for idx, cat in enumerate(runnable_categories, 1):
                logger.info("")
                logger.info("="*80)
                logger.info(f"📦 カテゴリー {idx}/{len(runnable_categories)}: {cat['name']}")
                logger.info(f"   URL: {cat['url']}")
                logger.info("="*80)

                try:
                    # カテゴリーの商品データを取得してSupabaseに保存
                    result = await self.pipeline.process_category_all_pages(
                        category_url=cat['url'],
                        category_name=cat['name']
                    )

                    if result:
                        logger.info(f"✅ カテゴリー {cat['name']} の処理完了")
                        logger.info(f"   商品数: {result.get('total_products', 0)}件")
                        logger.info(f"   新規: {result.get('new_products', 0)}件, 更新: {result.get('updated_products', 0)}件")
                    else:
                        logger.warning(f"⚠️ カテゴリー {cat['name']} の処理に問題がありました")

                except Exception as e:
                    logger.error(f"❌ カテゴリー {cat['name']} 処理エラー: {e}", exc_info=True)

                finally:
                    # 成功・失敗に関わらず実行済みとしてマーク
                    self.manager.mark_as_run(self.store_name, cat["name"], today)

                # カテゴリー間の待機
                if idx < len(runnable_categories):
                    await self.polite_wait_between_categories()

            logger.info("")
            logger.info("="*80)
            logger.info("✅ すべてのカテゴリー処理完了")
            logger.info("="*80)

        finally:
            await self.pipeline.close()


async def main():
    """メイン処理"""
    import argparse

    parser = argparse.ArgumentParser(description="東急ストアスクレイピング（スケジュール管理対応）")
    parser.add_argument("--init", action="store_true", help="カテゴリーを初期化（初回実行時のみ）")
    parser.add_argument("--manual", action="store_true", help="手動実行モード（環境変数MANUAL_CATEGORIESからカテゴリーを取得）")
    parser.add_argument("--headless", action="store_true", default=True, help="ヘッドレスモード")
    parser.add_argument("--zip-code", default="158-0094", help="配達エリア郵便番号")
    args = parser.parse_args()

    login_id = os.getenv("TOKYU_STORE_LOGIN_ID")
    password = ***REDACTED***"TOKYU_STORE_PASSWORD")

    if not login_id or not password:
        logger.error("❌ 環境変数 TOKYU_STORE_LOGIN_ID と TOKYU_STORE_PASSWORD を設定してください")
        return

    pipeline = PoliteTokyuStorePipeline(
        login_id=login_id,
        password=***REDACTED***
        zip_code=args.zip_code,
        headless=args.headless
    )

    if args.init:
        # 初期化モード
        await pipeline.initialize_categories()
    elif args.manual:
        # 手動実行モード
        manual_categories_str = os.getenv("MANUAL_CATEGORIES", "")
        if not manual_categories_str:
            logger.error("❌ 環境変数 MANUAL_CATEGORIES が設定されていません")
            return

        manual_categories = [cat.strip() for cat in manual_categories_str.split(",") if cat.strip()]
        if not manual_categories:
            logger.error("❌ カテゴリーが指定されていません")
            return

        await pipeline.run_scheduled_categories(manual_categories=manual_categories)

        # 商品データ取得後、自動的にembedding生成を実行
        await generate_embeddings_if_needed()
    else:
        # 通常実行モード（スケジュール）
        await pipeline.run_scheduled_categories()

        # 商品データ取得後、自動的にembedding生成を実行
        await generate_embeddings_if_needed()


async def generate_embeddings_if_needed():
    """商品データの分類・embedding生成（未生成のものがあれば実行）"""
    try:
        logger.info("")
        logger.info("="*80)
        logger.info("🔄 商品分類・Embedding生成チェック開始")
        logger.info("="*80)

        # ステップ1: Gemini 2.5 Flash で general_name, small_category, keywords を生成
        logger.info("ステップ1: Gemini 2.5 Flash で商品分類生成")
        classification_path = Path(__file__).parent.parent.parent / "L_product_classification"
        sys.path.insert(0, str(classification_path))

        from daily_auto_classifier import DailyAutoClassifier

        classifier = DailyAutoClassifier()
        result = await classifier.process_unclassified_products()
        logger.info(f"✅ 分類完了: {result.get('classified_count', 0)}件")

        # ステップ2: Embedding生成
        logger.info("ステップ2: Embedding生成")
        netsuper_app_path = Path(__file__).parent.parent.parent / "netsuper_search_app"
        sys.path.insert(0, str(netsuper_app_path))

        from generate_multi_embeddings import MultiEmbeddingGenerator

        # embedding生成を実行
        generator = MultiEmbeddingGenerator()
        generator.process_products(delay=0.1)

        logger.info("✅ 商品分類・Embedding生成処理完了")

    except Exception as e:
        logger.error(f"⚠️ 商品分類・Embedding生成エラー（スキップして続行）: {e}")


if __name__ == "__main__":
    asyncio.run(main())
```

### services\data-ingestion\tokyu_store\product_ingestion.py

```py
"""
東急ストア ネットスーパー 商品データ取得パイプライン

商品データを取得してSupabaseに保存します。

処理フロー:
1. ログインして配達エリアを選択
2. カテゴリーページの商品データを取得
3. JANコードで既存商品をチェック
4. Supabaseに保存（新規 or 更新）
"""

import os
import sys
import logging
from pathlib import Path
from typing import List, Dict, Any

# プロジェクトルートをパスに追加
root_dir = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(root_dir))

# .envファイルを読み込む
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from common.base_product_ingestion import BaseProductIngestionPipeline
from tokyu_store.tokyu_store_scraper_playwright import TokyuStoreScraperPlaywright

# ロガー設定
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class TokyuStoreProductIngestionPipeline(BaseProductIngestionPipeline):
    """東急ストア商品データ取得パイプライン（共通基盤クラス継承）"""

    def __init__(self, login_id: str, password: str, zip_code: str = "158-0094", headless: bool = True):
        """
        Args:
            login_id: 東急ストアログインID（メールアドレス）
            password: パスワード
            zip_code: 配達エリア郵便番号
            headless: ヘッドレスモードで実行するか
        """
        super().__init__(organization_name="東急ストア ネットスーパー", headless=headless)
        self.login_id = login_id
        self.password = ***REDACTED***
        self.zip_code = zip_code

        logger.info("TokyuStoreProductIngestionPipeline初期化完了（Service Role使用）")

    async def start(self) -> bool:
        """
        スクレイパーを起動してログイン（東急ストア固有）

        Returns:
            成功したらTrue
        """
        try:
            self.scraper = TokyuStoreScraperPlaywright()
            await self.scraper.start(headless=self.headless)

            # ログイン
            success = await self.scraper.login(self.login_id, self.password)
            if not success:
                logger.error("❌ ログイン失敗")
                await self.scraper.close()
                return False

            # 配達エリア選択
            success = await self.scraper.select_delivery_area(self.zip_code)
            if not success:
                logger.warning("⚠️ 配達エリア選択に失敗しましたが続行します")

            logger.info("✅ スクレイパー起動・ログイン・配達エリア選択完了")
            return True

        except Exception as e:
            logger.error(f"スクレイパー起動エラー: {e}", exc_info=True)
            return False

    async def close(self):
        """スクレイパーを終了"""
        if self.scraper:
            await self.scraper.close()

    async def discover_categories(self) -> List[Dict[str, str]]:
        """
        トップページからカテゴリーを動的に取得

        Returns:
            カテゴリー情報のリスト [{"name": "カテゴリー名", "url": "URL"}]
        """
        try:
            logger.info("📂 カテゴリーを取得中...")

            # トップページまたはカテゴリー一覧ページにアクセス
            await self.scraper.page.goto(
                f"{self.scraper.base_url}/shop/default.aspx",
                wait_until="domcontentloaded",
                timeout=60000
            )
            await self.scraper.page.wait_for_timeout(2000)

            # カテゴリーモーダルを開く
            try:
                category_modal_button = await self.scraper.page.query_selector('.category-modal-open, a:has-text("カテゴリ")')
                if category_modal_button:
                    await category_modal_button.click()
                    await self.scraper.page.wait_for_timeout(1000)
                    logger.info("カテゴリーモーダルを開きました")
            except Exception as e:
                logger.warning(f"カテゴリーモーダルを開けませんでした: {e}")

            # カテゴリーリンクを取得（カテゴリーモーダル内の主要カテゴリー）
            category_links = await self.scraper.page.query_selector_all(
                'h3.category-name a, .category-name a'
            )

            categories = []
            for link in category_links:
                try:
                    name = await link.inner_text()
                    href = await link.get_attribute('href')

                    if href and name:
                        # 相対URLを絶対URLに変換
                        if not href.startswith('http'):
                            href = f"{self.scraper.base_url}{href}" if href.startswith('/') else f"{self.scraper.base_url}/{href}"

                        categories.append({
                            "name": name.strip(),
                            "url": href
                        })
                except Exception as e:
                    logger.warning(f"カテゴリーリンク処理エラー: {e}")
                    continue

            logger.info(f"✅ {len(categories)}件のカテゴリーを発見")
            return categories

        except Exception as e:
            logger.error(f"カテゴリー取得エラー: {e}", exc_info=True)
            return []


async def main():
    """テスト実行用のメイン関数"""
    logger.info("東急ストア商品データ取得パイプライン開始")

    login_id = os.getenv("TOKYU_STORE_LOGIN_ID")
    password = ***REDACTED***"TOKYU_STORE_PASSWORD")
    zip_code = os.getenv("DELIVERY_ZIP_CODE", "158-0094")

    if not login_id or not password:
        logger.error("❌ 環境変数 TOKYU_STORE_LOGIN_ID と TOKYU_STORE_PASSWORD を設定してください")
        return

    pipeline = TokyuStoreProductIngestionPipeline(
        login_id=login_id,
        password=***REDACTED***
        zip_code=zip_code,
        headless=False
    )

    try:
        # スクレイパー起動
        success = await pipeline.start()
        if not success:
            logger.error("❌ スクレイパー起動失敗")
            return

        # カテゴリーを動的に取得
        categories = await pipeline.discover_categories()
        if categories:
            logger.info(f"発見したカテゴリー: {len(categories)}件")
            for cat in categories[:5]:  # 最初の5件を表示
                logger.info(f"  - {cat['name']}: {cat['url']}")

        logger.info("✅ テスト完了")

    finally:
        await pipeline.close()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

### services\data-ingestion\tokyu_store\tokyu_store_scraper_playwright.py

```py
"""
東急ストア ネットスーパー スクレイピングモジュール (Playwright版)

Playwrightを使用してログイン状態を保持したまま商品データを取得します。
"""

import json
import re
import time
import random
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from playwright.async_api import async_playwright, Browser, Page, BrowserContext

# ロガー設定
logger = logging.getLogger(__name__)


class TokyuStoreScraperPlaywright:
    """東急ストア ネットスーパーのスクレイピングクラス (Playwright版)"""

    def __init__(self):
        self.base_url = "https://ns.tokyu-bell.jp"
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None

    async def __aenter__(self):
        """async with構文でのコンテキスト開始"""
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """async with構文でのコンテキスト終了"""
        await self.close()

    async def start(self, headless: bool = True):
        """
        ブラウザを起動

        Args:
            headless: ヘッドレスモードで起動するか
        """
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(headless=headless)
            self.context = await self.browser.new_context(
                viewport={'width': 1280, 'height': 800},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            self.page = await self.context.new_page()
            logger.info("✅ Playwrightブラウザ起動完了")

        except Exception as e:
            logger.error(f"ブラウザ起動エラー: {e}", exc_info=True)
            raise

    async def close(self):
        """ブラウザを閉じる"""
        try:
            if self.page:
                await self.page.close()
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
            logger.info("✅ ブラウザ終了")

        except Exception as e:
            logger.error(f"ブラウザ終了エラー: {e}", exc_info=True)

    async def _is_logged_in(self) -> bool:
        """ログイン状態をチェック"""
        try:
            # ログインフォームが表示されているかチェック
            login_form = await self.page.query_selector('input[name="LoginID"]')
            if login_form:
                # ログインフォームが見つかった = ログインしていない
                return False

            # ログアウトリンクまたはマイページリンクがあればログイン済み
            logout_link = await self.page.query_selector('a[href*="logout"]')
            mypage_link = await self.page.query_selector('a[href*="mypage"]')

            return bool(logout_link or mypage_link)
        except:
            return False

    async def login(self, login_id: str, password: str) -> bool:
        """
        東急ストア ネットスーパーにログイン

        Args:
            login_id: ログインID（メールアドレス）
            password: パスワード

        Returns:
            成功したらTrue
        """
        import asyncio

        try:
            logger.info("🔐 東急ストア ネットスーパーにログイン中...")

            # 直接ログイン/会員メニューページにアクセス
            logger.info("ステップ1: ログインページに遷移中...")
            await self.page.goto(f"{self.base_url}/shop/customer/menu.aspx", wait_until="domcontentloaded", timeout=30000)
            await self.page.wait_for_timeout(2000)

            # 現在のページを確認
            current_url = self.page.url
            logger.info(f"📍 現在のURL: {current_url}")

            # デバッグ: ログインページのHTML/スクリーンショット保存
            logger.info("📸 ログインページのHTML/スクリーンショット保存中...")
            await self.page.screenshot(path="tokyu_store_login_page.png")
            login_page_html = await self.page.content()
            with open("tokyu_store_login_page.html", "w", encoding="utf-8") as f:
                f.write(login_page_html)
            logger.info("✅ 保存完了: tokyu_store_login_page.png, tokyu_store_login_page.html")

            # メールアドレスを入力
            logger.info("ステップ2: メールアドレスを入力中...")
            login_id_input = await self.page.wait_for_selector(
                'input[name="uid"], input[id="login_uid"]',
                timeout=10000,
                state="visible"
            )
            await login_id_input.click()
            await login_id_input.fill(login_id)
            logger.info("✅ メールアドレス入力完了")

            # パスワードを入力
            logger.info("ステップ3: パスワードを入力中...")
            password_input = await self.page.wait_for_selector(
                'input[name="pwd"], input[id="login_pwd"]',
                timeout=5000,
                state="visible"
            )
            await password_input.click()
            await password_input.fill(password)
            logger.info("✅ パスワード入力完了")

            # ログインボタンをクリック
            await asyncio.sleep(0.5)
            login_button = await self.page.wait_for_selector(
                'input[type="submit"][name="order"]',
                timeout=5000,
                state="visible"
            )
            await login_button.click()
            logger.info("ログインボタンをクリック")

            # ログイン完了を待機
            await asyncio.sleep(3)
            await self.page.wait_for_load_state("domcontentloaded")

            current_url = self.page.url
            logger.info(f"ログイン後のURL: {current_url}")

            # ログイン成功確認
            if await self._is_logged_in():
                logger.info("✅ ログイン成功")
                return True
            else:
                logger.error("❌ ログイン失敗")
                return False

        except Exception as e:
            logger.error(f"ログインエラー: {e}", exc_info=True)
            return False

    async def select_delivery_area(self, zip_code: str = "158-0094") -> bool:
        """
        配達エリア（郵便番号）を選択

        Args:
            zip_code: 郵便番号（デフォルト: 158-0094 世田谷区）

        Returns:
            成功したらTrue
        """
        import asyncio

        try:
            logger.info("📍 配達エリアを選択中...")

            # 郵便番号入力フォームを探す
            zip_input = await self.page.query_selector('input[name*="zip"], input[id*="txtZip"]')

            if zip_input:
                await zip_input.click()
                await zip_input.fill(zip_code.replace("-", ""))
                logger.info(f"✅ 郵便番号入力: {zip_code}")

                # 検索ボタンをクリック
                search_button = await self.page.query_selector('input[type="submit"], button[type="submit"]')
                if search_button:
                    await search_button.click()
                    await asyncio.sleep(2)
                    await self.page.wait_for_load_state("domcontentloaded")
                    logger.info("✅ 配達エリア選択完了")
                    return True
            else:
                # 郵便番号選択が不要な場合
                logger.info("✅ 配達エリア選択は不要です")
                return True

        except Exception as e:
            logger.error(f"配達エリア選択エラー: {e}", exc_info=True)
            return False

    async def fetch_products_page(
        self,
        category_url: str,
        page: int = 1
    ) -> tuple[List[Dict[str, Any]], Optional[dict]]:
        """
        カテゴリーページの商品データを取得

        Args:
            category_url: カテゴリーの完全URL
            page: ページ番号（1始まり）

        Returns:
            (商品データのリスト, ページネーション情報)
        """
        try:
            # URLにページ番号を追加（東急ストアは _p2/ 形式）
            if page == 1:
                url = category_url
            else:
                # 末尾の / を削除してから _p{page}/ を追加
                base_url = category_url.rstrip('/')
                url = f"{base_url}_p{page}/"

            logger.info(f"商品ページ取得中 (page={page}): {url}")

            await self.page.goto(url, wait_until="networkidle", timeout=60000)
            await self.page.wait_for_timeout(3000)

            # 商品データを抽出（HTMLベース）
            products, pagination_info = await self._extract_products_from_html()

            # アクセス間隔制御
            await self.page.wait_for_timeout(random.randint(1000, 2000))

            return products, pagination_info

        except Exception as e:
            logger.error(f"商品ページ取得エラー: {e}", exc_info=True)
            return [], None

    async def _extract_products_from_html(self) -> tuple[List[Dict[str, Any]], Optional[dict]]:
        """
        HTMLから商品データを抽出

        Returns:
            (商品データのリスト, ページネーション情報)
        """
        try:
            logger.info("✅ HTML解析開始")

            # デバッグ用にスクリーンショット・HTML保存
            await self.page.screenshot(path="tokyu_store_product_page.png")
            logger.info("スクリーンショット保存: tokyu_store_product_page.png")

            html_content = await self.page.content()
            with open("tokyu_store_product_page.html", "w", encoding="utf-8") as f:
                f.write(html_content)
            logger.info("HTML保存: tokyu_store_product_page.html")

            # 商品コンテナを取得（東急ストアの実際のHTML構造）
            product_containers = await self.page.query_selector_all(
                'div.block-pickup-list-p--item-body, li:has(div.block-pickup-list-p--item-body)'
            )
            logger.info(f"商品コンテナ数: {len(product_containers)}")

            products = []

            for container in product_containers:
                try:
                    # 商品名を取得
                    name_elem = await container.query_selector('.block-pickup-list-p--goods-name a')
                    product_name = await name_elem.inner_text() if name_elem else None

                    # 商品画像URLを取得
                    img_elem = await container.query_selector('.block-pickup-list-p--image img')
                    if img_elem:
                        img_src = await img_elem.get_attribute('data-src')  # lazyload用
                        if not img_src:
                            img_src = await img_elem.get_attribute('src')
                        if img_src and not img_src.startswith('http'):
                            img_src = f"{self.base_url}{img_src}"
                    else:
                        img_src = None

                    # 価格を取得（税抜価格）
                    price_elem = await container.query_selector('.block-pickup-list-p--net-price')
                    price_text = await price_elem.inner_text() if price_elem else None
                    price = None
                    if price_text:
                        # 価格から数字を抽出（￥を除去）
                        price_cleaned = price_text.replace('￥', '').replace(',', '').strip()
                        match = re.search(r'(\d+\.?\d*)', price_cleaned)
                        if match:
                            price = float(match.group(1))

                    # 税込価格を取得
                    price_tax_elem = await container.query_selector('.block-pickup-list-p--price.reference-price')
                    price_tax_text = await price_tax_elem.inner_text() if price_tax_elem else None
                    price_tax_included = None
                    if price_tax_text:
                        price_tax_cleaned = price_tax_text.replace('参考税込', '').replace('￥', '').replace(',', '').strip()
                        match = re.search(r'(\d+\.?\d*)', price_tax_cleaned)
                        if match:
                            price_tax_included = float(match.group(1))

                    # 商品IDとURLを取得（リンクから）
                    product_id = None
                    product_url = None
                    link = await container.query_selector('.block-pickup-list-p--goods-name a')
                    if link:
                        href = await link.get_attribute('href')
                        if href:
                            # /shop/g/g01087086/ のような形式からIDを抽出
                            id_match = re.search(r'/g/g(\d+)', href)
                            if id_match:
                                product_id = id_match.group(1)
                            # 完全なURLを構築
                            if href.startswith('http'):
                                product_url = href
                            elif href.startswith('/'):
                                product_url = f"https://www.tokyu-store.co.jp{href}"
                            else:
                                product_url = f"https://www.tokyu-store.co.jp/{href}"

                    if product_name:  # 商品名がある場合のみ追加
                        product = {
                            "product_id": product_id,
                            "product_name": product_name.strip() if product_name else None,
                            "price": price,
                            "price_tax_included": price_tax_included if price_tax_included else price,
                            "image_url": img_src,
                            "url": product_url,  # URLを追加
                            "in_stock": True,  # ページに表示されている = 在庫あり
                            "is_available": True,
                            "raw_data": {
                                "product_id": product_id,
                                "price_text": price_text,
                                "price_tax_text": price_tax_text,
                                "url": product_url  # raw_dataにも追加
                            }
                        }

                        products.append(product)

                except Exception as e:
                    logger.warning(f"商品データ抽出エラー（スキップ）: {e}")
                    continue

            # ページネーション情報を取得
            pagination_info = None
            try:
                # ページング情報を探す（一般的なパターン）
                pager = await self.page.query_selector('div.pager, div.pagination, ul.pagination')
                if pager:
                    page_links = await pager.query_selector_all('a, span')
                    total_pages = len([p for p in page_links if (await p.inner_text()).strip().isdigit()])

                    pagination_info = {
                        "totalItems": len(products) * total_pages,  # 推定
                        "currentPage": 1,  # URLから取得が必要
                        "totalPages": total_pages,
                        "itemsPerPage": len(products),
                        "source": "html:pagination"
                    }
            except Exception as e:
                logger.warning(f"ページネーション情報取得エラー: {e}")

            logger.info(f"✅ 商品抽出完了: {len(products)}件")

            return products, pagination_info

        except Exception as e:
            logger.error(f"商品抽出エラー: {e}", exc_info=True)
            return [], None


async def main():
    """テスト実行用のメイン関数"""
    import os
    from dotenv import load_dotenv
    load_dotenv()

    login_id = os.getenv("TOKYU_STORE_LOGIN_ID")
    password = ***REDACTED***"TOKYU_STORE_PASSWORD")
    zip_code = os.getenv("DELIVERY_ZIP_CODE", "158-0094")

    if not login_id or not password:
        logger.error("❌ 環境変数 TOKYU_STORE_LOGIN_ID と TOKYU_STORE_PASSWORD を設定してください")
        return

    scraper = TokyuStoreScraperPlaywright()

    try:
        # ヘッドレスモードをオフにしてブラウザを表示
        await scraper.start(headless=False)

        # ログイン
        success = await scraper.login(login_id, password)
        if not success:
            logger.error("❌ ログイン失敗")
            return

        # 配達エリア選択
        success = await scraper.select_delivery_area(zip_code)
        if not success:
            logger.warning("⚠️ 配達エリア選択に失敗しましたが続行します")

        # トップページのスクリーンショット・HTML保存
        await scraper.page.screenshot(path="tokyu_store_top.png")
        html_content = await scraper.page.content()
        with open("tokyu_store_top.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        logger.info("📸 トップページのスクリーンショット・HTML保存完了")

        # テスト: カテゴリーページにアクセス
        # 実際のカテゴリーURLを確認してから設定
        test_category_url = f"{scraper.base_url}/shop/default.aspx"
        logger.info(f"📦 テストページにアクセス: {test_category_url}")

        products, pagination = await scraper.fetch_products_page(test_category_url, page=1)
        logger.info(f"✅ 商品取得完了: {len(products)}件")
        if pagination:
            logger.info(f"ページネーション情報: {pagination}")

        logger.info("✅ テスト完了")

        # 画面を確認できるように5秒待つ
        await scraper.page.wait_for_timeout(5000)

    finally:
        await scraper.close()


if __name__ == "__main__":
    import asyncio
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    asyncio.run(main())
```

### services\data-ingestion\waseda_academy\__init__.py

```py
"""
早稲田アカデミーOnlineのお知らせ取得モジュール
"""
```

### services\data-ingestion\waseda_academy\browser_automation.py

```py
"""
早稲田アカデミーOnlineブラウザ自動化

Playwrightを使用してログイン・PDF取得を自動化
"""
import os
import asyncio
from pathlib import Path
from typing import Optional, Dict, List, Tuple
from loguru import logger
from playwright.async_api import async_playwright, Browser, Page, TimeoutError as PlaywrightTimeout


class WasedaAcademyBrowser:
    """早稲田アカデミーOnlineのブラウザ自動化"""

    def __init__(
        self,
        login_id: Optional[str] = None,
        password: Optional[str] = None,
        headless: bool = True
    ):
        """
        Args:
            login_id: ログインID（Noneの場合は環境変数から取得）
            password: パスワード（Noneの場合は環境変数から取得）
            headless: ヘッドレスモードで実行するか
        """
        self.login_id = login_id or os.getenv('WASEDA_LOGIN_ID')
        self.password = ***REDACTED*** or os.getenv('WASEDA_PASSWORD')
        self.headless = headless
        self.base_url = "https://online.waseda-ac.co.jp"

        if not self.login_id or not self.password:
            raise ValueError(
                "ログイン情報が設定されていません。"
                "環境変数 WASEDA_LOGIN_ID と WASEDA_PASSWORD を設定してください。"
            )

    async def login(self, page: Page) -> bool:
        """
        早稲田アカデミーOnlineにログイン

        Args:
            page: Playwrightのページオブジェクト

        Returns:
            ログイン成功時True
        """
        try:
            logger.info("ログインページにアクセス中...")
            await page.goto(f"{self.base_url}/login", wait_until="networkidle")

            # ログインフォームが表示されるまで待機
            await page.wait_for_selector('input[name="email"], input[name="username"], input[type="email"]', timeout=10000)

            # ログインID入力（複数のセレクタを試行）
            login_selectors = [
                'input[name="email"]',
                'input[name="username"]',
                'input[type="email"]',
                'input[placeholder*="メール"]',
                'input[placeholder*="ID"]'
            ]

            login_input_found = False
            for selector in login_selectors:
                try:
                    login_input = page.locator(selector).first
                    if await login_input.count() > 0:
                        await login_input.fill(self.login_id)
                        logger.info(f"ログインID入力完了（selector: {selector}）")
                        login_input_found = True
                        break
                except Exception:
                    continue

            if not login_input_found:
                logger.error("ログインID入力フィールドが見つかりません")
                return False

            # パスワード入力
            password_selectors = [
                'input[name="password"]',
                'input[type="password"]'
            ]

            password_input_found = False
            for selector in password_selectors:
                try:
                    password_input = page.locator(selector).first
                    if await password_input.count() > 0:
                        await password_input.fill(self.password)
                        logger.info(f"パスワード入力完了（selector: {selector}）")
                        password_input_found = True
                        break
                except Exception:
                    continue

            if not password_input_found:
                logger.error("パスワード入力フィールドが見つかりません")
                return False

            # ログインボタンをクリック
            login_button_selectors = [
                'button[type="submit"]',
                'input[type="submit"]',
                'button:has-text("ログイン")',
                'input[value*="ログイン"]'
            ]

            login_button_found = False
            for selector in login_button_selectors:
                try:
                    login_button = page.locator(selector).first
                    if await login_button.count() > 0:
                        logger.info("ログインボタンをクリック中...")
                        await login_button.click()
                        login_button_found = True
                        break
                except Exception:
                    continue

            if not login_button_found:
                logger.error("ログインボタンが見つかりません")
                return False

            # ページ遷移を待機（ログイン後のページ）
            try:
                await page.wait_for_url(f"{self.base_url}/home", timeout=10000)
                logger.info("✓ ログイン成功")
                return True
            except PlaywrightTimeout:
                # URLが変わらない場合、エラーメッセージをチェック
                error_element = page.locator('.error, .alert, [role="alert"]')
                if await error_element.count() > 0:
                    error_text = await error_element.first.text_content()
                    logger.error(f"ログインエラー: {error_text}")
                else:
                    logger.warning("ログイン後のページ遷移を確認できませんでしたが、続行します")
                    return True

        except Exception as e:
            logger.error(f"ログインエラー: {e}", exc_info=True)
            return False

    async def get_notice_page_html(self, page: Page) -> Optional[str]:
        """
        お知らせページのHTMLを取得

        Args:
            page: Playwrightのページオブジェクト

        Returns:
            HTMLコンテンツ、失敗時はNone
        """
        try:
            logger.info("お知らせページにアクセス中...")
            await page.goto(f"{self.base_url}/notice", wait_until="networkidle")

            # window.appPropsが存在するまで待機
            await page.wait_for_function(
                "typeof window.appProps !== 'undefined'",
                timeout=10000
            )

            # HTMLを取得
            html_content = await page.content()
            logger.info(f"HTMLを取得しました（{len(html_content)} bytes）")
            return html_content

        except Exception as e:
            logger.error(f"お知らせページ取得エラー: {e}", exc_info=True)
            return None

    async def download_pdf(
        self,
        page: Page,
        pdf_url: str,
        pdf_title: str
    ) -> Optional[bytes]:
        """
        PDFをダウンロード

        Args:
            page: Playwrightのページオブジェクト
            pdf_url: PDFのURL（相対パス可）
            pdf_title: PDFのタイトル

        Returns:
            PDFのバイトデータ、失敗時はNone
        """
        try:
            # 完全なURLを構築
            if pdf_url.startswith('/'):
                full_url = f"{self.base_url}{pdf_url}"
            elif pdf_url.startswith('http'):
                full_url = pdf_url
            else:
                full_url = f"{self.base_url}/{pdf_url}"

            logger.info(f"PDFダウンロード開始: {pdf_title}")
            logger.debug(f"  URL: {full_url}")

            # ページのコンテキストを使用してAPIリクエストでPDFを取得
            # これにより認証済みセッションでPDFをダウンロードできる
            response = await page.request.get(full_url)

            if response.status != 200:
                logger.error(f"PDFダウンロード失敗: HTTP {response.status}")
                return None

            # PDFデータを取得
            pdf_data = await response.body()

            # Content-Typeを確認
            content_type = response.headers.get('content-type', '')
            if 'application/pdf' not in content_type.lower():
                logger.warning(f"PDFではないコンテンツ: {content_type}")
                # それでもPDFかもしれないので続行

            logger.info(f"PDFダウンロード完了: {len(pdf_data)} bytes")
            return pdf_data

        except Exception as e:
            logger.error(f"PDFダウンロードエラー ({pdf_title}): {e}")
            return None

    async def run_automated_session(
        self
    ) -> Tuple[Optional[str], Dict[str, bytes]]:
        """
        自動化セッションを実行（ログイン → HTML取得 → PDF取得）

        Returns:
            (HTMLコンテンツ, {notice_id: pdf_data}の辞書)
        """
        html_content = None
        pdfs = {}

        async with async_playwright() as p:
            # ブラウザを起動
            logger.info(f"ブラウザ起動中（headless={self.headless}）...")
            browser = await p.chromium.launch(headless=self.headless)

            try:
                # 新しいコンテキストを作成
                context = await browser.new_context(
                    viewport={'width': 1280, 'height': 720},
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                )
                page = await context.new_page()

                # ログイン
                if not await self.login(page):
                    logger.error("ログインに失敗しました")
                    return None, {}

                # お知らせページのHTMLを取得
                html_content = await self.get_notice_page_html(page)
                if not html_content:
                    logger.error("HTMLの取得に失敗しました")
                    return None, {}

                logger.info("✓ 自動化セッション完了")

            finally:
                await browser.close()

        return html_content, pdfs

    async def download_pdfs_batch(
        self,
        pdf_info_list: List[Dict[str, str]]
    ) -> Dict[str, bytes]:
        """
        複数のPDFを一括ダウンロード

        Args:
            pdf_info_list: [{
                'notice_id': 'xxx',
                'pdf_url': '/notice/xxx/pdf/0',
                'pdf_title': 'タイトル'
            }, ...]

        Returns:
            {notice_id: pdf_data}の辞書
        """
        pdfs = {}

        async with async_playwright() as p:
            logger.info(f"ブラウザ起動中（PDF一括ダウンロード）...")
            browser = await p.chromium.launch(headless=self.headless)

            try:
                context = await browser.new_context()
                page = await context.new_page()

                # ログイン
                if not await self.login(page):
                    logger.error("ログインに失敗しました")
                    return {}

                # 各PDFをダウンロード
                for i, pdf_info in enumerate(pdf_info_list, 1):
                    notice_id = pdf_info['notice_id']
                    pdf_url = pdf_info['pdf_url']
                    pdf_title = pdf_info['pdf_title']

                    logger.info(f"[{i}/{len(pdf_info_list)}] {pdf_title}")

                    pdf_data = await self.download_pdf(page, pdf_url, pdf_title)
                    if pdf_data:
                        pdfs[notice_id] = pdf_data

                    # レート制限対策
                    await asyncio.sleep(1)

            finally:
                await browser.close()

        logger.info(f"PDFダウンロード完了: {len(pdfs)}/{len(pdf_info_list)}件")
        return pdfs


async def test_browser_automation():
    """ブラウザ自動化のテスト"""
    browser = WasedaAcademyBrowser(headless=False)  # デバッグ用にヘッドレスオフ

    html_content, pdfs = await browser.run_automated_session()

    if html_content:
        logger.info(f"HTML取得成功: {len(html_content)} bytes")
        # HTMLをファイルに保存
        with open('waseda_notice_page.html', 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.info("HTMLをwaseda_notice_page.htmlに保存しました")
    else:
        logger.error("HTML取得失敗")


if __name__ == "__main__":
    asyncio.run(test_browser_automation())
```

### services\data-ingestion\waseda_academy\known_waseda_notice_ids.json

```json
[
    "7486553B-0DBC-4421-B3F0-1C17F169BBA1",
    "4AEBB284-A343-44FB-9B52-70B6513B1D43",
    "9B0B1F9F-0BA0-4305-8F0E-89C0DE241EDC",
    "C8318E39-A5D0-431B-A971-50CE3E681103",
    "B95C7D70-72FF-4424-99C3-0439C7A8534C",
    "9F096DCA-91DF-4A37-95F5-80366821358A",
    "1D14E5A3-22F5-4410-885D-0549EEF07D5E",
    "1885C35F-893A-40FF-8453-5445708373ED",
    "A6331350-B437-4590-8B26-B9E21C19EA19",
    "F2E9401B-4D23-474A-8A27-A843CF1458FA"
]
```

### services\data-ingestion\waseda_academy\notice_ingestion.py

```py
"""
早稲田アカデミーOnlineお知らせ取得パイプライン

HTML → PDF抽出・ダウンロード → Google Drive → Supabase (pending)

処理フロー:
1. HTMLファイル（window.appPropsのJSON）からお知らせ一覧を取得
2. Supabaseで既存データをチェックして新着お知らせを抽出
3. PDFリンクからPDFをダウンロードしてGoogle Driveに保存
4. Supabaseに基本情報を登録（processing_status='pending'）
5. 別途 process_queued_documents.py で処理（PDF抽出、Stage A/B/C）
"""
import os
import sys
import re
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from loguru import logger

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

# .envファイルを読み込む
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from shared.common.connectors.google_drive import GoogleDriveConnector
from shared.common.database.client import DatabaseClient
from waseda_academy.browser_automation import WasedaAcademyBrowser


class WasedaNoticeIngestionPipeline:
    """早稲田アカデミーOnlineお知らせ取得パイプライン"""

    def __init__(
        self,
        pdf_folder_id: Optional[str] = None,
        session_cookies: Optional[Dict[str, str]] = None
    ):
        """
        Args:
            pdf_folder_id: PDF保存先のDriveフォルダID（Noneの場合は環境変数から取得）
            session_cookies: 早稲田アカデミーOnlineのセッションクッキー（PDF取得用）
        """
        self.pdf_folder_id = pdf_folder_id or os.getenv("WASEDA_PDF_FOLDER_ID")
        self.session_cookies = session_cookies or {}
        self.base_url = "https://online.waseda-ac.co.jp"

        # コネクタの初期化
        self.drive = GoogleDriveConnector()
        self.db = DatabaseClient()

        logger.info(f"WasedaNoticeIngestionPipeline初期化完了")
        logger.info(f"  - PDF folder: {self.pdf_folder_id}")

    async def fetch_html_with_browser(self) -> Optional[str]:
        """
        ブラウザ自動化を使用してHTMLを取得

        Returns:
            HTMLコンテンツ、失敗時はNone
        """
        try:
            browser = WasedaAcademyBrowser(headless=True)
            html_content, _ = await browser.run_automated_session()
            return html_content
        except Exception as e:
            logger.error(f"ブラウザ自動化エラー: {e}", exc_info=True)
            return None

    def extract_notice_data(self, html_content: str) -> List[Dict[str, Any]]:
        """
        HTMLコンテンツからお知らせデータを抽出する
        データはwindow.appPropsというJavaScript変数内のJSONとして埋め込まれている

        Args:
            html_content: HTMLコンテンツ全体

        Returns:
            お知らせデータのリスト
        """
        match = re.search(r'window\.appProps\s*=\s*(\{.*?\});', html_content, re.DOTALL)

        if not match:
            logger.warning("HTMLからwindow.appPropsが見つかりませんでした")
            return []

        json_string = match.group(1)

        try:
            app_props = json.loads(json_string)
            notices = app_props['page']['noticeList']['_0']['notices']
            logger.info(f"お知らせを{len(notices)}件抽出しました")
            return notices
        except (json.JSONDecodeError, KeyError) as e:
            logger.error(f"JSONパースエラー: {e}")
            return []

    async def check_existing_notices(self, notice_ids: List[str]) -> set:
        """
        Supabaseで既存のお知らせIDをチェック

        Args:
            notice_ids: チェックするお知らせIDのリスト

        Returns:
            既に存在するお知らせIDのセット
        """
        try:
            # Rawdata_FILE_AND_MAIL テーブルで source_type='waseda_academy_online' のドキュメントを取得
            result = self.db.client.table('Rawdata_FILE_AND_MAIL').select('metadata').eq(
                'source_type', 'waseda_academy_online'
            ).execute()

            # metadata->notice_id を抽出
            existing_ids = set()
            if result.data:
                for doc in result.data:
                    metadata = doc.get('metadata', {})
                    if isinstance(metadata, dict):
                        notice_id = metadata.get('notice_id')
                        if notice_id:
                            existing_ids.add(notice_id)

            logger.info(f"既存のお知らせ: {len(existing_ids)}件")
            return existing_ids

        except Exception as e:
            logger.error(f"Supabase検索エラー: {e}")
            return set()

    async def download_pdfs_with_browser(
        self,
        pdf_info_list: List[Dict[str, str]]
    ) -> Dict[str, bytes]:
        """
        ブラウザ自動化を使用して複数のPDFを一括ダウンロード

        Args:
            pdf_info_list: [{'notice_id': 'xxx', 'pdf_url': '/notice/xxx/pdf/0', 'pdf_title': 'タイトル'}, ...]

        Returns:
            {notice_id: pdf_data}の辞書
        """
        try:
            browser = WasedaAcademyBrowser(headless=True)
            pdfs = await browser.download_pdfs_batch(pdf_info_list)
            return pdfs
        except Exception as e:
            logger.error(f"PDFバッチダウンロードエラー: {e}", exc_info=True)
            return {}

    def save_pdf_to_drive(
        self,
        pdf_data: bytes,
        pdf_title: str,
        notice_id: str
    ) -> Optional[str]:
        """
        PDFをGoogle Driveに保存

        Args:
            pdf_data: PDFのバイトデータ
            pdf_title: PDFのタイトル
            notice_id: お知らせID

        Returns:
            DriveのファイルID、失敗時はNone
        """
        # 安全なファイル名を生成
        safe_title = "".join(c for c in pdf_title if c.isalnum() or c in (' ', '-', '_', '　')).strip()
        if not safe_title:
            safe_title = "waseda_notice"

        # タイムスタンプ付きファイル名
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_name = f"{timestamp}_{safe_title}_{notice_id[:8]}.pdf"

        # Driveにアップロード
        file_id = self.drive.upload_file(
            file_content=pdf_data,
            file_name=file_name,
            mime_type='application/pdf',
            folder_id=self.pdf_folder_id
        )

        if file_id:
            logger.info(f"PDFをDriveに保存: {file_name}")
        else:
            logger.error(f"PDFの保存に失敗: {file_name}")

        return file_id, file_name

    async def process_single_notice(
        self,
        notice: Dict[str, Any],
        pdf_data_dict: Dict[str, bytes]
    ) -> Dict[str, Any]:
        """
        1件のお知らせを処理（PDFのみ）

        Args:
            notice: お知らせデータ
            pdf_data_dict: {notice_id: pdf_data}の辞書（事前ダウンロード済み）

        Returns:
            処理結果の辞書
        """
        result = {
            'notice_id': notice.get('id'),
            'success': False,
            'pdf_file_ids': [],
            'document_ids': [],
            'error': None
        }

        try:
            notice_id = notice.get('id')
            title = notice.get('title', 'タイトルなし')
            date = notice.get('date', '')
            message = notice.get('message', '')
            source = notice.get('source', {})
            category = notice.get('category', {})

            logger.info(f"お知らせ処理開始: {title}")

            # PDFリンクがあるか確認
            pdfs = notice.get('pdfs', [])
            if not pdfs:
                logger.info(f"PDFリンクなし、スキップ: {title}")
                result['success'] = True
                return result

            # 各PDFを処理
            for pdf in pdfs:
                pdf_title = pdf.get('title', 'untitled')
                pdf_url = pdf.get('url', '')

                if not pdf_url:
                    logger.warning(f"PDFのURLが空: {pdf_title}")
                    continue

                # 1. 事前ダウンロード済みのPDFデータを取得
                pdf_data = pdf_data_dict.get(notice_id)
                if not pdf_data:
                    logger.warning(f"PDFデータが見つかりません（スキップ）: {pdf_title}")
                    continue

                # 2. PDFをGoogle Driveに保存
                file_id, actual_file_name = self.save_pdf_to_drive(pdf_data, pdf_title, notice_id)
                if not file_id:
                    logger.error(f"PDFの保存に失敗: {pdf_title}")
                    continue

                result['pdf_file_ids'].append(file_id)

                # 3. 日付を datetime に変換（フォーマット: 2025.12.16）
                sent_at = None
                if date:
                    try:
                        sent_at = datetime.strptime(date, '%Y.%m.%d').isoformat()
                    except ValueError:
                        logger.warning(f"日付のパースに失敗: {date}")

                # 4. メタデータ準備
                # 完全なPDF URLを構築
                if pdf_url.startswith('http'):
                    full_pdf_url = pdf_url
                elif pdf_url.startswith('/'):
                    full_pdf_url = f"{self.base_url}{pdf_url}"
                else:
                    full_pdf_url = f"{self.base_url}/{pdf_url}"

                metadata = {
                    'notice_id': notice_id,
                    'notice_title': title,
                    'notice_date': date,
                    'notice_source': source.get('label', '不明'),
                    'notice_category': category.get('label', 'その他'),
                    'notice_message': message,
                    'pdf_url': full_pdf_url,
                    'pdf_title': pdf_title
                }

                # 5. Supabaseに基本情報のみ保存
                doc_data = {
                    'source_type': 'waseda_academy_online',
                    'source_id': file_id,
                    'source_url': f"https://drive.google.com/file/d/{file_id}/view",
                    'file_name': actual_file_name,  # 拡張子付きのファイル名を使用
                    'file_type': 'pdf',
                    'doc_type': '早稲アカオンライン',  # 固定値
                    'workspace': 'waseda_academy',
                    'person': ['育哉'],  # 担当者（配列形式）
                    'organization': ['早稲田アカデミー'],  # 組織（配列形式）
                    'attachment_text': '',  # 空（process_queued_documents.py で抽出）
                    'summary': '',  # 空（process_queued_documents.py で生成）
                    'tags': [category.get('label', 'その他')],
                    'document_date': date,
                    'metadata': metadata,
                    'content_hash': hashlib.sha256(pdf_data).hexdigest(),
                    'processing_status': 'pending',  # PDF処理待ち
                    'processing_stage': 'waseda_notice_downloaded',
                    # 表示用フィールド
                    'display_subject': title,
                    'display_sent_at': sent_at,
                    'display_sender': source.get('label', '不明'),
                    'display_post_text': message
                }

                try:
                    # Supabaseに保存
                    doc_result = await self.db.insert_document('Rawdata_FILE_AND_MAIL', doc_data)
                    if doc_result:
                        doc_id = doc_result.get('id')
                        result['document_ids'].append(doc_id)
                        logger.info(f"Supabase保存完了（pending状態）: {doc_id}")
                        logger.info(f"  → process_queued_documents.py で処理してください")

                except Exception as db_error:
                    logger.error(f"Supabase保存エラー: {db_error}")
                    result['error'] = str(db_error)

            result['success'] = True
            logger.info(f"お知らせ処理完了: {title} ({len(result['pdf_file_ids'])} PDFs)")

        except Exception as e:
            logger.error(f"お知らせ処理エラー: {e}", exc_info=True)
            result['error'] = str(e)

        return result



async def main():
    """メインエントリーポイント"""
    import sys

    # パイプラインの初期化
    pipeline = WasedaNoticeIngestionPipeline()

    # コマンドライン引数でモードを選択
    use_browser = "--browser" in sys.argv or "--auto" in sys.argv

    html_content = None

    if use_browser:
        # ブラウザ自動化でHTMLを取得
        logger.info("ブラウザ自動化モード: ログイン → HTML取得")
        html_content = await pipeline.fetch_html_with_browser()

        if not html_content:
            logger.error("HTMLの取得に失敗しました")
            return

        # HTMLを一時ファイルに保存（デバッグ用）
        temp_html_file = Path(__file__).parent.parent.parent / "waseda_notice_page.html"
        with open(temp_html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.info(f"取得したHTMLを保存: {temp_html_file}")
    else:
        # ローカルHTMLファイルから読み込み（デバッグ用）
        html_file = Path(__file__).parent.parent.parent / "pasted_content.txt"

        if not html_file.exists():
            logger.error(f"HTMLファイルが見つかりません: {html_file}")
            logger.info("ヒント: --browser オプションでブラウザ自動化を使用できます")
            logger.info("  python -m B_ingestion.waseda_academy.notice_ingestion --browser")
            return

        with open(html_file, 'r', encoding='utf-8') as f:
            html_content = f.read()

    # お知らせデータを抽出
    current_notices = pipeline.extract_notice_data(html_content)
    if not current_notices:
        logger.warning("お知らせが抽出できませんでした")
        return

    # 既存のお知らせIDをSupabaseから取得
    notice_ids = [n.get('id') for n in current_notices if n.get('id')]
    existing_ids = await pipeline.check_existing_notices(notice_ids)

    # 新着お知らせを抽出
    new_notices = [n for n in current_notices if n.get('id') not in existing_ids]

    logger.info(f"現在のお知らせ: {len(current_notices)}件")
    logger.info(f"既存のお知らせ: {len(existing_ids)}件")
    logger.info(f"新着お知らせ: {len(new_notices)}件")

    if not new_notices:
        logger.info("新着お知らせはありません")
        return

    # 新着お知らせからPDF情報を収集
    pdf_info_list = []
    for notice in new_notices:
        notice_id = notice.get('id')
        pdfs = notice.get('pdfs', [])
        for pdf in pdfs:
            pdf_title = pdf.get('title', 'untitled')
            pdf_url = pdf.get('url', '')
            if pdf_url:
                pdf_info_list.append({
                    'notice_id': notice_id,
                    'pdf_url': pdf_url,
                    'pdf_title': pdf_title
                })

    logger.info(f"ダウンロード対象のPDF: {len(pdf_info_list)}件")

    # PDFを一括ダウンロード（ブラウザ自動化）
    pdf_data_dict = {}
    if pdf_info_list:
        logger.info("ブラウザ自動化でPDFを一括ダウンロード中...")
        pdf_data_dict = await pipeline.download_pdfs_with_browser(pdf_info_list)
        logger.info(f"ダウンロード完了: {len(pdf_data_dict)}/{len(pdf_info_list)}件")

    # 新着お知らせを処理（PDFデータは既にダウンロード済み）
    results = []
    for i, notice in enumerate(new_notices, 1):
        logger.info(f"[{i}/{len(new_notices)}] 処理中...")
        result = await pipeline.process_single_notice(notice, pdf_data_dict)
        results.append(result)

    # サマリー
    success_count = sum(1 for r in results if r['success'])
    total_pdfs = sum(len(r['pdf_file_ids']) for r in results)
    total_docs = sum(len(r['document_ids']) for r in results)

    logger.info("=" * 60)
    logger.info("処理完了")
    logger.info(f"  成功: {success_count}/{len(results)}")
    logger.info(f"  失敗: {len(results) - success_count}/{len(results)}")
    logger.info(f"  処理したPDF: {total_pdfs}件")
    logger.info(f"  登録したドキュメント: {total_docs}件（pending状態）")
    logger.info("=" * 60)
    logger.info("")
    logger.info("次のステップ:")
    logger.info("  python process_queued_documents.py --workspace=waseda_academy")
    logger.info("=" * 60)

    # 結果を表示
    print("\n" + "=" * 80)
    print("📢 早稲田アカデミーお知らせ取得結果")
    print("=" * 80)

    for result in results:
        print(f"\nNotice ID: {result['notice_id']}")
        print(f"  Success: {result['success']}")
        print(f"  PDFs: {len(result['pdf_file_ids'])}")
        for file_id in result['pdf_file_ids']:
            print(f"    - https://drive.google.com/file/d/{file_id}/view")
        print(f"  Documents: {len(result['document_ids'])} (pending)")
        if result['error']:
            print(f"  ❌ Error: {result['error']}")

    print("\n" + "=" * 80)
    print("次のステップ:")
    print("  python process_queued_documents.py --workspace=waseda_academy")
    print("=" * 80)


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

### services\doc-processor\app.py

```py
"""
Flask Web Application - Document Processing System
ドキュメント処理システムのWebインターフェース（処理専用）
"""
import os
import sys
from pathlib import Path

# プロジェクトルートをPythonパスに追加（ローカル実行時用）
# Docker環境では PYTHONPATH=/app が設定済みなので、parent.parentが/になる場合はスキップ
# ローカル実行時: doc-processor -> services -> document-management-system
project_root = Path(__file__).resolve().parent.parent.parent
services_root = Path(__file__).resolve().parent.parent
scripts_processing = project_root / 'scripts' / 'processing'
for p in [str(project_root), str(services_root), str(scripts_processing)]:
    if p != '/' and p not in sys.path:
        sys.path.insert(0, p)

from datetime import datetime, timezone
from flask import Flask, render_template, request, jsonify
from flask_cors import CORS
from loguru import logger
import psutil
import time

# ========== 定数定義 ==========
# ロック関連
LOCK_TIMEOUT_SECONDS = 300  # ロックの有効期限（5分）
LOCK_RETRY_COUNT = 3  # ロック設定時の最大リトライ回数
LOCK_RETRY_DELAY = 1.0  # リトライ間隔（秒）

# リソース監視関連
RESOURCE_UPDATE_INTERVAL = 5.0  # リソース情報更新間隔（秒）
MAX_LOG_ENTRIES = 300  # 保持するログの最大件数
MAX_LOG_ENTRIES_SUPABASE = 150  # Supabaseに保存するログの最大件数

# 並列処理関連
DEFAULT_MAX_PARALLEL = 1  # デフォルト並列数
MIN_PARALLEL = 1  # 最小並列数
MAX_PARALLEL_LIMIT = 100  # 最大並列数上限

# メモリ閾値（16GB環境用デフォルト）
MEMORY_LOW_THRESHOLD = 60.0  # 余裕あり（並列数増加可）
MEMORY_HIGH_THRESHOLD = 85.0  # 逼迫（減速開始）
MEMORY_CRITICAL_THRESHOLD = 90.0  # 危険（並列数削減）
MEMORY_RECOVER_THRESHOLD = 70.0  # 回復（減速緩和）

# スロットル関連
THROTTLE_STEP = 0.5  # スロットル調整ステップ（秒）
MAX_THROTTLE_DELAY = 3.0  # 最大スロットル遅延（秒）

# タイムアウト関連
DOCUMENT_PROCESS_TIMEOUT = int(os.getenv('DOC_PROCESS_TIMEOUT', '1800'))  # ドキュメント処理タイムアウト（秒）

app = Flask(__name__)

# CORS設定: 環境変数で許可オリジンを指定（デフォルトは本番環境のみ）
ALLOWED_ORIGINS = os.getenv('ALLOWED_ORIGINS', 'https://doc-processor-*.run.app,https://docs.ookubotechnologies.com').split(',')
# 開発環境では全許可
if os.getenv('FLASK_ENV') == 'development' or os.getenv('DEBUG') == 'true':
    CORS(app)
else:
    CORS(app, origins=ALLOWED_ORIGINS, supports_credentials=True)

# ========== 認証設定 ==========
API_KEY = ***REDACTED***'DOC_PROCESSOR_API_KEY', '')
REQUIRE_AUTH = os.getenv('REQUIRE_AUTH', 'true').lower() == 'true'

def require_api_key(f):
    """APIキー認証デコレーター"""
    from functools import wraps
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not REQUIRE_AUTH:
            return f(*args, **kwargs)

        # APIキーが設定されていない場合は認証をスキップ（開発環境用）
        if not API_KEY:
            logger.warning("DOC_PROCESSOR_API_KEY is not set. Skipping authentication.")
            return f(*args, **kwargs)

        # ヘッダーまたはクエリパラメータからAPIキーを取得
        provided_key = request.headers.get('X-API-Key') or request.args.get('api_key')
        if provided_key != API_KEY:
            return jsonify({'success': False, 'error': 'Unauthorized'}), 401

        return f(*args, **kwargs)
    return decorated_function

def safe_error_response(error: Exception, status_code: int = 500):
    """安全なエラーレスポンスを生成（本番環境ではスタックトレースを隠す）"""
    import traceback
    tb = traceback.format_exc()
    print(f"ERROR: {error}\n{tb}")
    return jsonify({'success': False, 'error': str(error), 'traceback': tb}), status_code

# Supabaseクライアント（処理ロック用）
_supabase_client = None

def get_supabase_client():
    """Supabaseクライアントを取得（シングルトン）"""
    global _supabase_client
    if _supabase_client is None:
        from shared.common.database.client import DatabaseClient
        db = DatabaseClient(use_service_role=True)
        _supabase_client = db.client
    return _supabase_client


def get_processing_lock():
    """Supabaseから処理ロック状態を取得"""
    try:
        client = get_supabase_client()
        result = client.table('processing_lock').select('*').eq('id', 1).execute()
        if result.data:
            lock = result.data[0]
            # LOCK_TIMEOUT_SECONDS以上更新がなければ期限切れとみなす
            if lock.get('is_processing') and lock.get('updated_at'):
                from datetime import datetime, timezone
                updated_at = datetime.fromisoformat(lock['updated_at'].replace('Z', '+00:00'))
                now = datetime.now(timezone.utc)
                if (now - updated_at).total_seconds() > LOCK_TIMEOUT_SECONDS:
                    logger.warning(f"処理ロックが期限切れ（{(now - updated_at).total_seconds():.0f}秒経過）。自動リセット。")
                    set_processing_lock(False)
                    return False
            return lock.get('is_processing', False)
        return False
    except Exception as e:
        logger.error(f"処理ロック取得エラー: {e}")
        return False


def set_processing_lock(is_processing: bool, max_retries: int = 3, timeout_sec: float = 10.0):
    """Supabaseに処理ロック状態を設定（タイムアウトとリトライ付き）

    Args:
        is_processing: ロック状態
        max_retries: 最大リトライ回数（デフォルト: 3）
        timeout_sec: 1回あたりのタイムアウト秒数（デフォルト: 10秒）

    Returns:
        bool: 成功した場合True
    """
    import time as time_module

    for attempt in range(max_retries):
        try:
            start_time = time_module.time()
            client = get_supabase_client()

            data = {
                'is_processing': is_processing,
                'updated_at': datetime.now(timezone.utc).isoformat()
            }
            if is_processing:
                data['started_at'] = datetime.now(timezone.utc).isoformat()
                # 処理開始時: processing状態のまま残っているドキュメントをpendingにリセット
                reset_stuck_documents()
            else:
                # 処理終了時: 全ワーカーをクリア & processing状態のドキュメントをリセット
                clear_all_workers()
                reset_stuck_documents()

            # タイムアウトチェック
            elapsed = time_module.time() - start_time
            if elapsed > timeout_sec:
                raise TimeoutError(f"Processing lock operation timed out after {elapsed:.1f}s")

            client.table('processing_lock').update(data).eq('id', 1).execute()
            logger.info(f"処理ロック設定: {is_processing}")
            return True

        except Exception as e:
            logger.warning(f"処理ロック設定エラー (attempt {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time_module.sleep(1.0)  # リトライ前に1秒待機
            else:
                logger.error(f"処理ロック設定に失敗しました（{max_retries}回試行）: {e}")
                return False

    return False


def update_processing_lock():
    """処理中のロックタイムスタンプを更新（ハートビート）"""
    try:
        client = get_supabase_client()
        client.table('processing_lock').update({
            'updated_at': datetime.now(timezone.utc).isoformat()
        }).eq('id', 1).execute()
        return True
    except Exception as e:
        logger.error(f"ロック更新エラー: {e}")
        return False


# ========== ワーカー管理（複数インスタンス対応） ==========

import uuid
_instance_id = str(uuid.uuid4())[:8]  # このインスタンスのID


def register_worker(doc_id: str, doc_title: str) -> bool:
    """ワーカーを登録（処理開始時）- active_tasksのみ使用"""
    # active_tasksへの追加は呼び出し元で実施済み
    # ここでは何もしない（互換性のため関数は残す）
    return True


def unregister_worker(doc_id: str) -> bool:
    """ワーカーを解除（処理終了時）- active_tasksのみ使用"""
    # active_tasksからの削除は呼び出し元で実施済み
    # ここでは何もしない（互換性のため関数は残す）
    return True


def clear_all_workers() -> bool:
    """全ワーカーをクリア（処理終了時）- active_tasksのみ使用"""
    global active_tasks
    # active_tasksをクリア
    count = len(active_tasks)
    active_tasks.clear()
    logger.info(f"全ワーカーをクリアしました（削除件数: {count}件）")
    return True




def reset_stuck_documents() -> int:
    """
    processing状態でスタックしているドキュメントをpendingにリセット

    スタック判定:
    - processing_status='processing'
    - かつ active_tasks に存在しない

    Returns:
        リセットした件数
    """
    global active_tasks
    try:
        from shared.common.database.client import DatabaseClient
        db = DatabaseClient(use_service_role=True)

        # processing状態のドキュメントを取得
        result = db.client.table('Rawdata_FILE_AND_MAIL').select('id').eq('processing_status', 'processing').execute()
        processing_ids = [row['id'] for row in result.data] if result.data else []

        if not processing_ids:
            return 0

        # active_tasksから実際に処理中のdoc_idを取得
        active_doc_ids = list(active_tasks.keys())

        # スタックしているドキュメント = processingだがactive_tasksに存在しない
        stuck_ids = [doc_id for doc_id in processing_ids if doc_id not in active_doc_ids]

        if stuck_ids:
            # pendingにリセット
            db.client.table('Rawdata_FILE_AND_MAIL').update({
                'processing_status': 'pending'
            }).in_('id', stuck_ids).execute()
            logger.info(f"スタック状態のドキュメントをリセットしました（{len(stuck_ids)}件）")
            return len(stuck_ids)

        return 0
    except Exception as e:
        logger.error(f"reset_stuck_documents エラー: {e}")
        return 0

def update_worker_count() -> int:
    """現在のワーカー数を返す - active_tasksのみ使用"""
    global active_tasks
    # active_tasksから直接カウント（DB更新不要）
    return len(active_tasks)


def update_progress_to_supabase(current_index: int, total_count: int, current_file: str,
                                 success_count: int, error_count: int, logs: list):
    """進捗情報をSupabaseに保存（複数インスタンス共有用）"""
    global resource_manager, active_tasks
    try:
        client = get_supabase_client()
        # 最新のログのみ保存
        latest_logs = logs[-MAX_LOG_ENTRIES_SUPABASE:] if len(logs) > MAX_LOG_ENTRIES_SUPABASE else logs

        # システムリソース情報も含める（Cloud Run環境対応）
        cpu_percent = get_cgroup_cpu()
        memory_info = get_cgroup_memory()

        # 実際のワーカー数をactive_tasksから取得
        actual_workers = len(active_tasks)
        logger.info(f"[UPDATE_SUPABASE] active_tasks count: {actual_workers}, keys: {list(active_tasks.keys())[:5]}")

        # グローバルなresource_managerが存在すれば、リソース調整を実行
        if resource_manager is not None:
            # メモリ使用率に基づいてリソース調整（実際のワーカー数を使用）
            adjust_result = resource_manager.adjust_resources(memory_info['percent'], actual_workers)
            # 調整結果をprocessing_statusに反映（whileループで参照される）
            processing_status['resource_control']['throttle_delay'] = adjust_result['throttle_delay']
            processing_status['resource_control']['adjustment_count'] = resource_manager.adjustment_count

        # resource_managerから最新値を取得（存在する場合）
        if resource_manager is not None:
            throttle_delay = resource_manager.throttle_delay
            max_parallel = resource_manager.max_parallel
            adjustment_count = resource_manager.adjustment_count
        else:
            resource_control = processing_status.get('resource_control', {})
            throttle_delay = resource_control.get('throttle_delay', 0.0)
            max_parallel = resource_control.get('max_parallel', 1)
            adjustment_count = resource_control.get('adjustment_count', 0)

        client.table('processing_lock').update({
            'current_index': current_index,
            'total_count': total_count,
            'current_file': current_file,
            'success_count': success_count,
            'error_count': error_count,
            'logs': latest_logs,
            'cpu_percent': cpu_percent,
            'memory_percent': memory_info['percent'],
            'memory_used_gb': memory_info['used_gb'],
            'memory_total_gb': memory_info['total_gb'],
            'throttle_delay': throttle_delay,
            'max_parallel': max_parallel,
            'current_workers': actual_workers,  # active_tasksから取得した実際のワーカー数
            'adjustment_count': adjustment_count,
            'updated_at': datetime.now(timezone.utc).isoformat()
        }).eq('id', 1).execute()
        return True
    except Exception as e:
        logger.error(f"進捗更新エラー: {e}")
        return False


def get_progress_from_supabase() -> dict:
    """Supabaseから進捗情報を取得（複数インスタンス共有用）"""
    try:
        client = get_supabase_client()
        result = client.table('processing_lock').select('*').eq('id', 1).execute()
        if result.data:
            data = result.data[0]
            return {
                'current_index': data.get('current_index', 0),
                'total_count': data.get('total_count', 0),
                'current_file': data.get('current_file', ''),
                'success_count': data.get('success_count', 0),
                'error_count': data.get('error_count', 0),
                'logs': data.get('logs', []),
                # バックグラウンド処理インスタンスのリソース情報
                'cpu_percent': data.get('cpu_percent', 0.0),
                'memory_percent': data.get('memory_percent', 0.0),
                'memory_used_gb': data.get('memory_used_gb', 0.0),
                'memory_total_gb': data.get('memory_total_gb', 0.0),
                'throttle_delay': data.get('throttle_delay', 0.0),
                'adjustment_count': data.get('adjustment_count', 0)
            }
        return {
            'current_index': 0,
            'total_count': 0,
            'current_file': '',
            'success_count': 0,
            'error_count': 0,
            'logs': [],
            'cpu_percent': 0.0,
            'memory_percent': 0.0,
            'memory_used_gb': 0.0,
            'memory_total_gb': 0.0,
            'throttle_delay': 0.0,
            'adjustment_count': 0
        }
    except Exception as e:
        logger.error(f"進捗取得エラー: {e}")
        return {
            'current_index': 0,
            'total_count': 0,
            'current_file': '',
            'success_count': 0,
            'error_count': 0,
            'logs': [],
            'cpu_percent': 0.0,
            'memory_percent': 0.0,
            'memory_used_gb': 0.0,
            'memory_total_gb': 0.0,
            'throttle_delay': 0.0,
            'adjustment_count': 0
        }


def get_worker_status() -> dict:
    """現在のワーカー状況を取得 - Supabaseから取得（複数インスタンス対応）"""
    global resource_manager, active_tasks
    try:
        # Supabaseからワーカー情報を取得（処理インスタンスが更新した値）
        client = get_supabase_client()
        lock_result = client.table('processing_lock').select(
            'is_processing, current_workers, max_parallel'
        ).eq('id', 1).execute()
        lock_data = lock_result.data[0] if lock_result.data else {}

        # Supabaseから取得（処理インスタンスが更新した値を優先）
        current_workers = lock_data.get('current_workers', 0)
        max_parallel = lock_data.get('max_parallel', 1)

        # ローカルのactive_tasksにデータがある場合はそちらを優先（同一インスタンスの場合）
        if len(active_tasks) > 0:
            current_workers = len(active_tasks)
            if resource_manager:
                max_parallel = resource_manager.max_parallel

        # active_tasksの内容をworkers形式に変換（同一インスタンスの場合のみ）
        workers = [
            {
                'doc_id': doc_id,
                'doc_title': task_info.get('title', ''),
                'started_at': task_info.get('started_at', '')
            }
            for doc_id, task_info in active_tasks.items()
        ]

        return {
            'max_parallel': max_parallel,
            'current_workers': current_workers,
            'is_processing': lock_data.get('is_processing', False),
            'workers': workers
        }
    except Exception as e:
        logger.error(f"ワーカー状況取得エラー: {e}")
        return {'max_parallel': 1, 'current_workers': 0, 'is_processing': False, 'workers': []}


def adjust_max_parallel(memory_percent: float) -> int:
    """メモリ使用率に基づいてmax_parallelを調整

    重要: 実行数が上限に達している場合のみ並列数を増やす
    （実際にフル稼働している状態で余裕があることを確認してから増やす）
    """
    global resource_manager, active_tasks
    try:
        if not resource_manager:
            return 3

        current_max = resource_manager.max_parallel
        current_workers = len(active_tasks)

        new_max = current_max

        # メモリに余裕があり、ワーカーが上限に達している場合のみ増加
        # current_workers >= current_max - 1 で、ほぼフル稼働していることを確認
        if memory_percent < 60 and current_workers >= current_max - 1:
            new_max = min(current_max + 1, 20)  # 最大20
        # メモリが逼迫している場合は減少
        elif memory_percent > 85:
            new_max = max(current_max - 1, 1)  # 最小1

        if new_max != current_max:
            resource_manager.max_parallel = new_max
            logger.info(f"max_parallel調整: {current_max} → {new_max} (メモリ: {memory_percent:.1f}%)")

        return new_max
    except Exception as e:
        logger.error(f"max_parallel調整エラー: {e}")
        return 3


def can_start_new_worker() -> bool:
    """新しいワーカーを開始できるか確認 - active_tasksのみ使用"""
    global resource_manager, active_tasks
    current_workers = len(active_tasks)
    max_parallel = resource_manager.max_parallel if resource_manager else 1
    return current_workers < max_parallel


# ========== 処理進捗の管理（ローカルキャッシュ、表示用） ==========
processing_status = {
    'is_processing': False,
    'current_index': 0,
    'total_count': 0,
    'current_file': '',
    'success_count': 0,
    'error_count': 0,
    'logs': [],
    # ステージ進捗（ドキュメント内の進捗）
    'current_stage': '',
    'stage_progress': 0.0,  # 0.0 - 1.0
    # アダプティブリソース制御情報
    'resource_control': {
        'throttle_delay': 0.0,
        'adjustment_count': 0
    }
}

# CPU使用率計算用の前回の値
_last_cpu_stats = {'usage_usec': 0, 'timestamp': time.time()}

# グローバルなリソースマネージャー（update_progress_to_supabaseからアクセス用）
resource_manager = None

# アクティブタスク管理（辞書型: {doc_id: {'title': str, 'started_at': str}}）
active_tasks = {}


def get_cgroup_memory():
    """cgroupからメモリ使用率を取得（Cloud Run Gen2 cgroup v2対応）"""
    import os

    try:
        current = None
        max_mem = None
        inactive_file = 0

        # cgroup v2 パスを優先的に試行（Cloud Run Gen2）
        v2_current_path = '/sys/fs/cgroup/memory.current'
        v2_max_path = '/sys/fs/cgroup/memory.max'
        v2_stat_path = '/sys/fs/cgroup/memory.stat'

        # cgroup v1 パス（フォールバック用）
        v1_current_path = '/sys/fs/cgroup/memory/memory.usage_in_bytes'
        v1_max_path = '/sys/fs/cgroup/memory/memory.limit_in_bytes'
        v1_stat_path = '/sys/fs/cgroup/memory/memory.stat'

        # cgroup v2 を試行
        if os.path.exists(v2_current_path):
            logger.debug("[MEMORY] Using cgroup v2")
            with open(v2_current_path, 'r') as f:
                current = int(f.read().strip())
            with open(v2_max_path, 'r') as f:
                max_val = f.read().strip()
                # "max" は無制限を意味する
                if max_val == 'max':
                    max_mem = psutil.virtual_memory().total
                else:
                    max_mem = int(max_val)

            # cgroup v2 の memory.stat から inactive_file を取得
            if os.path.exists(v2_stat_path):
                with open(v2_stat_path, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) == 2:
                            key, value = parts
                            if key == 'inactive_file':
                                inactive_file = int(value)

        # cgroup v1 にフォールバック
        elif os.path.exists(v1_current_path):
            logger.debug("[MEMORY] Using cgroup v1")
            with open(v1_current_path, 'r') as f:
                current = int(f.read().strip())
            with open(v1_max_path, 'r') as f:
                max_mem = int(f.read().strip())
                # 非常に大きい値の場合は無制限
                if max_mem > 1e15:
                    max_mem = psutil.virtual_memory().total

            # cgroup v1 の memory.stat から inactive_file を取得
            if os.path.exists(v1_stat_path):
                with open(v1_stat_path, 'r') as f:
                    stat_content = f.read()
                    for line in stat_content.split('\n'):
                        parts = line.strip().split()
                        if len(parts) == 2:
                            key, value = parts
                            # total_inactive_file または inactive_file を探す
                            if key in ('total_inactive_file', 'inactive_file'):
                                inactive_file = int(value)
                                logger.debug(f"[MEMORY] Found {key}={value}")
                                break
                    # デバッグ: 見つからなかった場合、最初の10行をログ
                    if inactive_file == 0:
                        lines = stat_content.split('\n')[:10]
                        logger.warning(f"[MEMORY] inactive_file not found. First 10 lines: {lines}")

        if current is None or max_mem is None:
            raise FileNotFoundError("cgroup memory files not found")

        # キャッシュを除いた実際の使用量（Kubernetes標準）
        actual_used = current - inactive_file
        percent = (actual_used / max_mem) * 100
        used_gb = actual_used / (1024 ** 3)
        total_gb = max_mem / (1024 ** 3)

        logger.debug(f"[MEMORY] current={current/(1024**3):.2f}GB, inactive={inactive_file/(1024**3):.2f}GB, max={max_mem/(1024**3):.2f}GB, percent={percent:.1f}%")

        return {
            'percent': round(percent, 1),
            'used_gb': round(used_gb, 2),
            'total_gb': round(total_gb, 2)
        }
    except Exception as e:
        # cgroupが使えない場合はpsutilにフォールバック
        logger.warning(f"cgroup memory読み取り失敗、psutilを使用: {e}")
        try:
            memory = psutil.virtual_memory()
            return {
                'percent': round(memory.percent, 1),
                'used_gb': round(memory.used / (1024 ** 3), 2),
                'total_gb': round(memory.total / (1024 ** 3), 2)
            }
        except Exception as e2:
            # psutilも失敗した場合はデフォルト値を返す
            logger.error(f"psutil memory読み取りも失敗、デフォルト値を使用: {e2}")
            return {
                'percent': 50.0,  # 安全側のデフォルト値
                'used_gb': 8.0,
                'total_gb': 16.0
            }


def get_cgroup_cpu():
    """CPU使用率を取得（Cloud Run対応）"""
    global _last_cpu_stats

    try:
        # Cloud Run Gen2: cgroup v2 または v1 のパスを試行
        usage_nsec = None
        cpu_usage_paths = [
            '/sys/fs/cgroup/cpu.stat',  # cgroup v2 (Gen2)
            '/sys/fs/cgroup/cpuacct/cpuacct.usage'  # cgroup v1 (Gen1)
        ]

        for path in cpu_usage_paths:
            try:
                if path.endswith('cpu.stat'):
                    # cgroup v2: cpu.statから usage_usec を読み取る
                    with open(path, 'r') as f:
                        for line in f:
                            if line.startswith('usage_usec'):
                                usage_nsec = int(line.split()[1]) * 1000  # マイクロ秒→ナノ秒
                                break
                else:
                    # cgroup v1: cpuacct.usageから直接読み取る
                    with open(path, 'r') as f:
                        usage_nsec = int(f.read().strip())

                if usage_nsec is not None:
                    break
            except Exception:
                continue

        if usage_nsec is None:
            # cgroupから読み取れない場合はpsutilにフォールバック
            return round(psutil.cpu_percent(interval=0.1), 1)

        current_time = time.time()

        # 初回呼び出しの場合は初期化してpsutilで取得
        if _last_cpu_stats['usage_usec'] == 0:
            _last_cpu_stats = {'usage_usec': usage_nsec, 'timestamp': current_time}
            # psutilで即座の値を返す
            return round(psutil.cpu_percent(interval=0.1), 1)

        # 前回との差分から使用率を計算
        time_delta = current_time - _last_cpu_stats['timestamp']
        usage_delta = usage_nsec - _last_cpu_stats['usage_usec']

        if time_delta > 0:
            # usage_deltaはナノ秒、time_deltaは秒
            # CPU使用率 = (使用時間の増加 / 経過時間) / CPU数
            cpu_count = psutil.cpu_count() or 4
            cpu_percent = (usage_delta / (time_delta * 1_000_000_000)) * 100 / cpu_count
            cpu_percent = max(0.0, min(cpu_percent, 100.0))  # 0-100%の範囲
        else:
            cpu_percent = 0.0

        # 次回のために保存
        _last_cpu_stats = {'usage_usec': usage_nsec, 'timestamp': current_time}

        return round(cpu_percent, 1)
    except Exception as e:
        # 失敗時はpsutilにフォールバック
        logger.debug(f"cgroup CPU読み取り失敗、psutilを使用: {e}")
        return round(psutil.cpu_percent(interval=0.1), 1)


class AdaptiveResourceManager:
    """アダプティブリソース制御マネージャー

    メモリ使用率に基づいて並列数とスロットル遅延を動的に調整します。

    制御ロジック（総メモリ量に応じて閾値を自動調整）:

    16GB環境（保守的）:
    - 並列数制御: memory < 60% → 増加, > 90% → 削減
    - スロットル: > 85% → 減速開始, < 70% → 減速緩和

    32GB環境（攻めの設定）:
    - 並列数制御: memory < 70% → 増加, > 95% → 削減
    - スロットル: > 90% → 減速開始, < 80% → 減速緩和

    32GBの利点:
    - 95%まで使っても残り1.6GB（16GBの90%と同等）
    - 並列数を最大100まで増やせる可能性
    - 処理時間短縮でコストメリット
    """

    def __init__(self, initial_max_parallel=1, min_parallel=1, max_parallel=100, history_size=3):
        """初期化

        Args:
            initial_max_parallel: 初期並列数（デフォルト: 1）
            min_parallel: 最小並列数（デフォルト: 1）
            max_parallel: 最大並列数（デフォルト: 100）
            history_size: 移動平均用の履歴サイズ（デフォルト: 3）
        """
        self.max_parallel = initial_max_parallel
        self.min_parallel = min_parallel
        self.max_parallel_limit = max_parallel
        self.throttle_delay = 0.0  # スロットル遅延（秒）

        # 総メモリ量を取得して閾値を動的に設定
        total_memory_gb = 16.0  # デフォルト
        try:
            mem_info = get_cgroup_memory()
            total_memory_gb = mem_info['total_gb']
        except:
            pass

        # しきい値をメモリサイズに応じて調整
        if total_memory_gb >= 32:
            # 32GB以上: 攻めの設定（残り1.6GB = 95%まで許容）
            self.memory_low = 70.0   # 余裕あり → 並列数増加
            self.memory_high = 90.0  # 逼迫 → 減速開始
            self.memory_critical = 95.0  # 危険 → 並列数削減
            self.memory_recover = 80.0  # 回復 → 減速緩和
        elif total_memory_gb >= 24:
            # 24GB: 中間設定
            self.memory_low = 65.0
            self.memory_high = 87.0
            self.memory_critical = 92.0
            self.memory_recover = 75.0
        else:
            # 16GB以下: 保守的設定（デフォルト）
            self.memory_low = 60.0   # 余裕あり → 並列数増加
            self.memory_high = 85.0  # 逼迫 → 減速開始
            self.memory_critical = 90.0  # 危険 → 並列数削減
            self.memory_recover = 70.0  # 回復 → 減速緩和

        from loguru import logger
        self.logger = logger
        self.logger.info(f"[INIT] AdaptiveResourceManager: total_memory={total_memory_gb:.1f}GB, thresholds=[{self.memory_low}%, {self.memory_high}%, {self.memory_critical}%]")

        # 調整ステップ
        self.parallel_step = 1
        self.throttle_step = 0.5  # 秒
        self.max_throttle = 3.0   # 最大スロットル遅延

        # 移動平均用の履歴
        self.history_size = history_size
        self.memory_history = []

        # 統計
        self.adjustment_count = 0
        self.last_adjustment_time = time.time()

        from loguru import logger
        self.logger = logger

    def adjust_resources(self, memory_percent, current_workers=0):
        """リソース使用率に基づいて並列数とスロットルを調整

        Args:
            memory_percent: 現在のメモリ使用率（%）
            current_workers: 現在の実行中ワーカー数

        Returns:
            dict: 調整情報 {'max_parallel': int, 'throttle_delay': float, 'adjusted': bool}
        """
        # 履歴に追加
        self.memory_history.append(memory_percent)
        if len(self.memory_history) > self.history_size:
            self.memory_history.pop(0)

        # 移動平均を計算（直近3回の平均）
        memory_avg = sum(self.memory_history) / len(self.memory_history)

        # 移動平均で判断（瞬間値ではなく）
        memory_percent = memory_avg

        original_parallel = self.max_parallel
        original_throttle = self.throttle_delay
        adjusted = False

        # フェーズ3: 並列数削減（緊急時）- 最優先
        if memory_percent > self.memory_critical:
            if self.max_parallel > self.min_parallel:
                self.max_parallel = max(self.max_parallel - self.parallel_step, self.min_parallel)
                adjusted = True
                self.logger.warning(
                    f"[リソース制御] 🚨 メモリ逼迫 ({memory_percent:.1f}%) → 並列数削減: {original_parallel} → {self.max_parallel}"
                )

        # フェーズ2: 減速制御
        if memory_percent > self.memory_high:
            # 逼迫 → 減速
            if self.throttle_delay < self.max_throttle:
                self.throttle_delay = min(self.throttle_delay + self.throttle_step, self.max_throttle)
                adjusted = True
                self.logger.info(
                    f"[リソース制御] ⚠️ メモリ高使用 ({memory_percent:.1f}%) → 減速: {original_throttle:.1f}秒 → {self.throttle_delay:.1f}秒"
                )
        elif memory_percent < self.memory_recover and self.throttle_delay > 0:
            # 回復 → 減速緩和
            self.throttle_delay = max(self.throttle_delay - self.throttle_step, 0.0)
            adjusted = True
            self.logger.info(
                f"[リソース制御] ✅ メモリ回復 ({memory_percent:.1f}%) → 減速緩和: {original_throttle:.1f}秒 → {self.throttle_delay:.1f}秒"
            )

        # max_parallel = current_workers + 1 (常に実行数+1が上限)
        # 実行数が減ればmax_parallelも減る、増えれば増える
        new_max_parallel = min(current_workers + 1, self.max_parallel_limit)
        new_max_parallel = max(new_max_parallel, self.min_parallel)  # 最小値を保証

        if new_max_parallel != self.max_parallel:
            old_max = self.max_parallel
            self.max_parallel = new_max_parallel
            adjusted = True
            self.logger.info(
                f"[リソース制御] max_parallel調整: {old_max} → {self.max_parallel} (実行数: {current_workers})"
            )

        if adjusted:
            self.adjustment_count += 1
            self.last_adjustment_time = time.time()

        return {
            'max_parallel': self.max_parallel,
            'throttle_delay': self.throttle_delay,
            'adjusted': adjusted,
            'memory_percent': memory_percent
        }

    async def apply_throttle(self):
        """スロットル遅延を適用"""
        if self.throttle_delay > 0:
            import asyncio
            await asyncio.sleep(self.throttle_delay)

    def get_status(self):
        """現在の状態を取得"""
        return {
            'max_parallel': self.max_parallel,
            'throttle_delay': self.throttle_delay,
            'adjustment_count': self.adjustment_count,
            'last_adjustment': self.last_adjustment_time
        }


# loguruのカスタムハンドラー：ログをprocessing_statusに送信
def log_to_processing_status(message):
    """loguruのログをprocessing_statusに追加 + ステージ検出"""
    log_record = message.record
    level = log_record['level'].name
    msg = log_record['message']

    # ログレベルに応じてフィルタリング（INFOのみ表示）
    if level in ['INFO', 'WARNING', 'ERROR']:
        timestamp = datetime.now().strftime('%H:%M:%S')
        formatted_msg = f"[{timestamp}] {msg}"
        processing_status['logs'].append(formatted_msg)

        # ステージ検出（ログメッセージから現在のステージを推測）
        msg_lower = msg.lower()
        if 'stage h' in msg_lower or '構造化' in msg_lower:
            processing_status['current_stage'] = 'Stage H: 構造化'
            processing_status['stage_progress'] = 0.3
        elif 'stage j' in msg_lower or 'チャンク' in msg_lower:
            processing_status['current_stage'] = 'Stage J: チャンク化'
            processing_status['stage_progress'] = 0.5
        elif 'stage k' in msg_lower or 'embedding' in msg_lower or 'embed' in msg_lower:
            processing_status['current_stage'] = 'Stage K: Embedding'
            processing_status['stage_progress'] = 0.7
        elif '成功' in msg or '✅' in msg:
            processing_status['current_stage'] = '完了'
            processing_status['stage_progress'] = 1.0
        elif 'エラー' in msg or '❌' in msg:
            processing_status['current_stage'] = 'エラー'
            processing_status['stage_progress'] = 1.0
        elif 'ダウンロード' in msg_lower or 'download' in msg_lower:
            processing_status['current_stage'] = 'ダウンロード中'
            processing_status['stage_progress'] = 0.1

        # ログは最大件数まで保持
        if len(processing_status['logs']) > MAX_LOG_ENTRIES:
            processing_status['logs'] = processing_status['logs'][-MAX_LOG_ENTRIES:]


# loguruにカスタムハンドラーを追加（スレッド内で個別に追加するためここでは追加しない）
# logger.add(log_to_processing_status, format="{message}")


@app.route('/')
def index():
    """メインページ - 処理画面にリダイレクト"""
    return render_template('processing.html',
        supabase_url=os.getenv('SUPABASE_URL', ''),
        supabase_anon_key=os.getenv('SUPABASE_KEY', ''))


@app.route('/processing')
def processing():
    """ドキュメント処理システムのメインページ"""
    return render_template('processing.html',
        supabase_url=os.getenv('SUPABASE_URL', ''),
        supabase_anon_key=os.getenv('SUPABASE_KEY', ''))


@app.route('/api/health', methods=['GET'])
def health_check():
    """ヘルスチェックエンドポイント"""
    return jsonify({
        'status': 'ok',
        'message': 'Document Processing System is running',
        'version': '2025-01-11-v2'  # デプロイ確認用
    })


@app.route('/api/process/progress', methods=['GET'])
def get_process_progress():
    """
    処理進捗とシステムリソースを取得（Supabaseから共有状態を取得）
    全ての情報はバックグラウンド処理インスタンスがSupabaseに保存した値を使用
    """
    try:
        # Supabaseからワーカー状況を取得（複数インスタンス対応）
        worker_status = get_worker_status()

        # Supabaseから進捗情報を取得（複数インスタンス共有）
        # CPU/メモリ/スロットル等もバックグラウンド処理インスタンスの値を取得
        progress = get_progress_from_supabase()

        # 現在処理中のドキュメントの進捗を取得
        current_stage = ''
        stage_progress = 0.0
        try:
            # processing中のドキュメントを取得（最新1件）
            client = get_supabase_client()
            processing_doc = client.table('Rawdata_FILE_AND_MAIL').select('processing_stage, processing_progress').eq('processing_status', 'processing').order('created_at', desc=False).limit(1).execute()
            if processing_doc.data and len(processing_doc.data) > 0:
                current_stage = processing_doc.data[0].get('processing_stage', '')
                stage_progress = processing_doc.data[0].get('processing_progress', 0.0)
        except Exception as e:
            logger.error(f"進捗取得エラー: {e}")

        return jsonify({
            'success': True,
            'processing': worker_status['is_processing'],
            'current_index': progress['current_index'],
            'total_count': progress['total_count'],
            'current_file': progress['current_file'],
            'success_count': progress['success_count'],
            'error_count': progress['error_count'],
            'logs': progress['logs'],  # 最新150件
            # ステージ進捗（Supabaseから取得）
            'current_stage': current_stage,
            'stage_progress': stage_progress,
            # バックグラウンド処理インスタンスのリソース情報（Supabaseから取得）
            'system': {
                'cpu_percent': progress['cpu_percent'],
                'memory_percent': progress['memory_percent'],
                'memory_used_gb': progress['memory_used_gb'],
                'memory_total_gb': progress['memory_total_gb']
            },
            'resource_control': {
                'current_workers': worker_status['current_workers'],
                'max_parallel': worker_status['max_parallel'],
                'throttle_delay': progress['throttle_delay'],
                'adjustment_count': progress['adjustment_count']
            },
            'workers': worker_status['workers']  # 処理中のドキュメント一覧
        })
    except Exception as e:
        return safe_error_response(e)


@app.route('/api/workspaces', methods=['GET'])
def get_workspaces():
    """
    ワークスペース一覧を取得
    """
    try:
        from shared.common.database.client import DatabaseClient
        db = DatabaseClient()

        # ワークスペース一覧を取得
        query = db.client.table('Rawdata_FILE_AND_MAIL').select('workspace').execute()

        # ユニークなワークスペースを抽出
        workspaces = set()
        for row in query.data:
            workspace = row.get('workspace')
            if workspace:
                workspaces.add(workspace)

        # ソートしてリスト化
        workspace_list = sorted(list(workspaces))

        return jsonify({
            'success': True,
            'workspaces': workspace_list
        })

    except Exception as e:
        logger.error(f"ワークスペース取得エラー: {e}")
        return safe_error_response(e)


@app.route('/api/process/stats', methods=['GET'])
def get_process_stats():
    """
    処理キューの統計情報を取得

    ステータスの意味:
    - pending: 未処理（まだバッチに選ばれていない）
    - processing: このバッチの処理対象として選択済み（重複防止マーク）
    - completed: 処理完了
    - error: 処理エラー
    """
    try:
        from shared.common.database.client import DatabaseClient
        db = DatabaseClient()

        workspace = request.args.get('workspace', 'all')

        query = db.client.table('Rawdata_FILE_AND_MAIL').select('processing_status, workspace')

        if workspace != 'all':
            query = query.eq('workspace', workspace)

        response = query.execute()

        stats = {
            'pending': 0,
            'processing': 0,  # バッチに選ばれた件数
            'completed': 0,
            'error': 0  # フロントエンド表示用は 'error'
        }

        for doc in response.data:
            status = doc.get('processing_status')
            # nullは無視（pending/processing/completed/failedのみカウント）
            # データベースのステータスは 'failed' のまま、フロントには 'error' として送る
            if status == 'failed':
                stats['error'] = stats.get('error', 0) + 1
            elif status in stats:
                stats[status] = stats.get(status, 0) + 1

        return jsonify({
            'success': True,
            'stats': stats
        })

    except Exception as e:
        logger.error(f"統計取得エラー: {e}")
        return safe_error_response(e)


@app.route('/api/process/start', methods=['POST'])
@require_api_key
def start_processing():
    """
    ドキュメント処理を開始（バックグラウンド実行）
    """
    global processing_status

    # Supabaseでロック状態をチェック（複数インスタンス対応）
    if get_processing_lock():
        return jsonify({
            'success': False,
            'error': '既に処理が実行中です'
        }), 400

    try:
        import threading
        import asyncio

        data = request.get_json()
        workspace = data.get('workspace', 'all')
        limit = data.get('limit', 100)
        preserve_workspace = data.get('preserve_workspace', True)

        # Supabaseにロックを設定
        set_processing_lock(True)

        # loguruハンドラーを追加（process_queued_documents.pyのログをキャッチ）
        handler_id = logger.add(log_to_processing_status, format="{message}")

        # 進捗状況を初期化（処理開始をすぐに表示）
        processing_status['is_processing'] = True
        processing_status['current_index'] = 0
        processing_status['total_count'] = 0
        processing_status['current_file'] = '初期化中...'
        processing_status['success_count'] = 0
        processing_status['error_count'] = 0
        processing_status['current_stage'] = ''
        processing_status['stage_progress'] = 0.0
        processing_status['logs'] = [
            f"[{datetime.now().strftime('%H:%M:%S')}] 処理開始準備中...",
            f"[{datetime.now().strftime('%H:%M:%S')}] ワークスペース: {workspace}, 制限: {limit}件"
        ]
        # リソース制御を初期化
        processing_status['resource_control'] = {
            'throttle_delay': 0.0,
            'adjustment_count': 0
        }

        # Supabaseに初期状態を保存
        update_progress_to_supabase(0, 0, '初期化中...', 0, 0, processing_status['logs'])

        # DocumentProcessorをインポート
        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] モジュール読み込み中...")
        update_progress_to_supabase(0, 0, 'モジュール読み込み中...', 0, 0, processing_status['logs'])
        from process_queued_documents import DocumentProcessor

        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] プロセッサ初期化中...")
        update_progress_to_supabase(0, 0, 'プロセッサ初期化中...', 0, 0, processing_status['logs'])
        processor = DocumentProcessor()

        # pending ドキュメントを取得
        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] ドキュメント取得中...")
        update_progress_to_supabase(0, 0, 'ドキュメント取得中...', 0, 0, processing_status['logs'])
        docs = processor.get_pending_documents(workspace, limit)

        if not docs:
            processing_status['is_processing'] = False
            processing_status['current_file'] = ''
            processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] 処理対象のドキュメントがありません")
            set_processing_lock(False)  # ロック解放
            return jsonify({
                'success': True,
                'message': '処理対象のドキュメントがありません',
                'processed': 0
            })

        # ドキュメント数を更新
        processing_status['total_count'] = len(docs)
        processing_status['current_file'] = ''
        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] {len(docs)}件のドキュメントを取得しました")
        processing_status['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] バックグラウンド処理を開始します...")
        update_progress_to_supabase(0, len(docs), 'バックグラウンド処理開始', 0, 0, processing_status['logs'])

        # バックグラウンド処理関数
        def background_processing():
            global processing_status
            print("[DEBUG] background_processing() 開始")
            logger.info("[DEBUG] background_processing() 開始")

            # リソース監視用のTimer
            update_timer = None

            def periodic_resource_update():
                """5秒ごとにリソース情報を更新（別スレッドで実行）"""
                nonlocal update_timer
                if not processing_status['is_processing']:
                    logger.info("[PERIODIC_UPDATE] 処理終了につきタイマー停止")
                    return

                try:
                    logger.debug("[PERIODIC_UPDATE] リソース情報更新開始")
                    # update_progress_to_supabaseを呼び出し
                    update_progress_to_supabase(
                        processing_status['current_index'],
                        processing_status['total_count'],
                        processing_status['current_file'],
                        processing_status['success_count'],
                        processing_status['error_count'],
                        processing_status['logs']
                    )
                    logger.debug("[PERIODIC_UPDATE] リソース情報更新完了")
                except Exception as e:
                    logger.error(f"[PERIODIC_UPDATE] エラー: {e}", exc_info=True)
                finally:
                    # エラーが発生しても次のタイマーを確実に設定
                    if processing_status['is_processing']:
                        update_timer = threading.Timer(5.0, periodic_resource_update)
                        update_timer.daemon = True
                        update_timer.start()

            # 定期更新を開始（5秒間隔）
            update_timer = threading.Timer(5.0, periodic_resource_update)
            update_timer.daemon = True
            update_timer.start()
            logger.info("[PERIODIC_UPDATE] タイマー開始（5秒間隔）")

            async def process_all():
                print("[DEBUG] process_all() 開始")
                logger.info("[DEBUG] process_all() 開始")
                # アダプティブリソースマネージャーを初期化（グローバル変数として）
                # リソース適応型並列制御
                # - 初期並列数: 1（最小同時実行数）
                # - 最小並列数: 1（リソース逼迫時も1は維持）
                # - 最大並列数: 100（余裕がある場合はここまで増やす）
                # update_progress_to_supabaseからアクセスできるようにグローバル変数として初期化
                global resource_manager
                resource_manager = AdaptiveResourceManager(initial_max_parallel=1, min_parallel=1, max_parallel=100)

                # 並列数制御用のセマフォ（動的に調整）
                # NOTE: セマフォは固定値なので、タスク内で制御ロジックを実装
                # グローバルのactive_tasks（辞書）とは別に、asyncio.Taskのリストを管理
                pending_async_tasks = []
                processed_count = 0

                # 個別ドキュメント処理タスク
                async def process_single_document(doc, index):
                    """個別ドキュメントを処理"""
                    global active_tasks
                    nonlocal processed_count

                    # 停止フラグをチェック
                    if not processing_status['is_processing']:
                        return False

                    file_name = doc.get('file_name', 'unknown')
                    title = doc.get('title', '') or '(タイトル未生成)'
                    doc_id = doc.get('id', str(index))

                    # active_tasksにワーカー情報を追加
                    active_tasks[doc_id] = {
                        'title': title,
                        'started_at': datetime.now(timezone.utc).isoformat()
                    }
                    logger.info(f"[WORKER_ADD] doc_id={doc_id}, active_tasks count={len(active_tasks)}")
                    # Supabaseにワーカー登録
                    register_worker(doc_id, title)

                    try:
                        # 処理前のメモリを記録
                        mem_before = get_cgroup_memory()

                        # 進捗を更新
                        processing_status['current_index'] = index
                        processing_status['current_file'] = title
                        processing_status['logs'].append(
                            f"[{datetime.now().strftime('%H:%M:%S')}] [{index}/{len(docs)}] 処理中: {title}"
                        )

                        # ログは最大件数まで保持
                        if len(processing_status['logs']) > MAX_LOG_ENTRIES:
                            processing_status['logs'] = processing_status['logs'][-MAX_LOG_ENTRIES:]

                        # リソース情報を取得してリソース調整
                        memory_info = get_cgroup_memory()
                        memory_percent = memory_info['percent']
                        worker_status = get_worker_status()
                        current_workers = worker_status['current_workers']

                        # リソース調整（並列数を動的に調整）
                        status = resource_manager.adjust_resources(memory_percent, current_workers)
                        processing_status['resource_control']['throttle_delay'] = status['throttle_delay']
                        processing_status['resource_control']['adjustment_count'] = resource_manager.adjustment_count

                        # 進捗コールバック関数を定義
                        def progress_callback(stage):
                            """各ステージ開始時にSupabaseを更新"""
                            logger.info(f"[PROGRESS] Stage {stage} 開始")
                            processing_status['current_file'] = f"{title} (Stage {stage})"
                            update_progress_to_supabase(
                                processing_status['current_index'],
                                processing_status['total_count'],
                                processing_status['current_file'],
                                processing_status['success_count'],
                                processing_status['error_count'],
                                processing_status['logs']
                            )

                        # ドキュメント処理（進捗コールバックを渡す）
                        success = await processor.process_document(doc, preserve_workspace, progress_callback=progress_callback)

                        # 処理後のメモリを記録（最初の3件のみログ出力）
                        if index <= 3:
                            mem_after = get_cgroup_memory()
                            mem_delta = mem_after['used_gb'] - mem_before['used_gb']
                            logger.info(f"[MEMORY PER DOC] Doc#{index}: before={mem_before['used_gb']:.2f}GB, after={mem_after['used_gb']:.2f}GB, delta={mem_delta:.2f}GB, parallel={len(active_tasks)}")

                        # スロットル遅延を適用
                        await resource_manager.apply_throttle()

                        processed_count += 1

                        if success:
                            processing_status['success_count'] += 1
                            processing_status['logs'].append(
                                f"[{datetime.now().strftime('%H:%M:%S')}] ✅ 成功: {title}"
                            )
                        else:
                            processing_status['error_count'] += 1
                            processing_status['logs'].append(
                                f"[{datetime.now().strftime('%H:%M:%S')}] ❌ エラー: {title}"
                            )

                        # Supabaseに結果を保存
                        update_progress_to_supabase(
                            index, len(docs), title,
                            processing_status['success_count'],
                            processing_status['error_count'],
                            processing_status['logs']
                        )

                        return success
                    finally:
                        # active_tasksからワーカー情報を削除
                        if doc_id in active_tasks:
                            del active_tasks[doc_id]
                        # Supabaseからワーカー解除
                        unregister_worker(doc_id)

                # リソース監視はthreading.Timerで定期実行されているため、ここでは不要

                try:
                    # ドキュメントを処理
                    for i, doc in enumerate(docs, 1):
                        logger.info(f"[FOR_LOOP] イテレーション {i}/{len(docs)} 開始, pending_async_tasks={len(pending_async_tasks)}, max_parallel={resource_manager.max_parallel}")

                        # 停止フラグをチェック
                        if not processing_status['is_processing']:
                            processing_status['logs'].append(
                                f"[{datetime.now().strftime('%H:%M:%S')}] ⚠️ 処理が中断されました"
                            )
                            break

                        # 並列数制御：pending_async_tasksが max_parallel 未満になるまで待機
                        # 動的に調整される max_parallel を使用（リソースに応じて1～100で変動）
                        if len(pending_async_tasks) >= resource_manager.max_parallel:
                            logger.info(f"[WHILE_ENTER] pending_async_tasks={len(pending_async_tasks)} >= max_parallel={resource_manager.max_parallel}, 待機開始")
                        while len(pending_async_tasks) >= resource_manager.max_parallel:
                            # 完了したタスクを削除
                            done_tasks = [t for t in pending_async_tasks if t.done()]
                            for t in done_tasks:
                                pending_async_tasks.remove(t)

                            if len(pending_async_tasks) >= resource_manager.max_parallel:
                                # まだ並列数が上限に達している場合は少し待機
                                await asyncio.sleep(0.1)

                        # 新しいタスクを開始
                        task = asyncio.create_task(process_single_document(doc, i))
                        pending_async_tasks.append(task)

                        # イベントループに制御を渡す
                        await asyncio.sleep(0)

                    # すべてのタスクが完了するまで待機
                    if pending_async_tasks:
                        await asyncio.gather(*pending_async_tasks, return_exceptions=True)

                finally:
                    # 処理完了（Supabaseも更新）
                    processing_status['is_processing'] = False
                    set_processing_lock(False)
                    logger.info("[PROCESS] 全ドキュメント処理完了")

            try:
                asyncio.run(process_all())

                # 処理完了
                processing_status['is_processing'] = False
                processing_status['current_file'] = ''
                processing_status['logs'].append(
                    f"[{datetime.now().strftime('%H:%M:%S')}] 処理完了: 成功={processing_status['success_count']}, エラー={processing_status['error_count']}"
                )

                # Supabaseに完了状態を保存
                update_progress_to_supabase(
                    processing_status['current_index'],
                    processing_status['total_count'],
                    '',
                    processing_status['success_count'],
                    processing_status['error_count'],
                    processing_status['logs']
                )
            except Exception as e:
                processing_status['is_processing'] = False
                processing_status['logs'].append(
                    f"[{datetime.now().strftime('%H:%M:%S')}] ❌ エラー: {str(e)}"
                )
                print(f"[ERROR] バックグラウンド処理エラー: {e}")
            finally:
                # 定期更新タイマーを停止
                processing_status['is_processing'] = False  # タイマーのループ条件をFalseに
                set_processing_lock(False)  # Supabaseも更新
                if update_timer is not None:
                    update_timer.cancel()
                    logger.info("[PERIODIC_UPDATE] タイマーをキャンセルしました")

                # グローバルなresource_managerをリセット
                global resource_manager
                resource_manager = None
                logger.info("[RESOURCE] resource_managerをリセットしました")

                # ハンドラーを削除
                logger.remove(handler_id)
                # Supabaseロック解放
                set_processing_lock(False)

        # 別スレッドで処理を開始
        # daemon=False: Cloud Runがインスタンスを落としても、処理完了まで待機
        thread = threading.Thread(target=background_processing, daemon=False)
        thread.start()

        # すぐにレスポンスを返す
        return jsonify({
            'success': True,
            'message': '処理を開始しました',
            'total_count': len(docs)
        })

    except Exception as e:
        processing_status['is_processing'] = False
        set_processing_lock(False)  # ロック解放
        logger.error(f"処理開始エラー: {e}")
        return safe_error_response(e)


@app.route('/api/process/stop', methods=['POST'])
@require_api_key
def stop_processing():
    """
    処理を停止
    """
    global processing_status

    # Supabaseのロックをチェック（ローカルは別インスタンスの可能性があるので見ない）
    if not get_processing_lock():
        return jsonify({
            'success': False,
            'error': '実行中の処理がありません'
        }), 400

    # 停止フラグを立てる（ローカル＋Supabase両方）
    processing_status['is_processing'] = False
    set_processing_lock(False)  # Supabaseロック解放 + ワーカークリア
    processing_status['logs'].append(
        f"[{datetime.now().strftime('%H:%M:%S')}] ⚠️ ユーザーによって停止されました"
    )

    return jsonify({
        'success': True,
        'message': '処理を停止しました'
    })


@app.route('/api/process/reset', methods=['POST'])
def reset_processing():
    """
    処理フラグを強制リセット（Supabase + ローカル両方）
    注意: 緊急用のため認証なしでアクセス可能
    """
    global processing_status, active_tasks, resource_manager

    # active_tasksをクリア
    active_tasks.clear()
    logger.info("active_tasksをクリアしました")

    # resource_managerをリセット
    resource_manager = None

    # Supabaseロック解放
    set_processing_lock(False)

    # Supabaseの全状態をリセット
    try:
        client = get_supabase_client()
        client.table('processing_lock').update({
            'is_processing': False,
            'current_index': 0,
            'total_count': 0,
            'success_count': 0,
            'error_count': 0,
            'current_file': '',
            'current_workers': 0,
            'max_parallel': 1,
            'throttle_delay': 0.0,
            'adjustment_count': 0,
            'cpu_percent': 0.0,
            'memory_percent': 0.0,
            'memory_used_gb': 0.0,
            'memory_total_gb': 0.0,
            'logs': []
        }).eq('id', 1).execute()
        logger.info("Supabase processing_lockをリセットしました")
    except Exception as e:
        logger.error(f"Supabaseリセットエラー: {e}")

    # ローカル状態もリセット
    processing_status['is_processing'] = False
    processing_status['current_index'] = 0
    processing_status['total_count'] = 0
    processing_status['current_file'] = ''
    processing_status['success_count'] = 0
    processing_status['error_count'] = 0
    processing_status['current_stage'] = ''
    processing_status['stage_progress'] = 0.0
    processing_status['logs'] = [
        f"[{datetime.now().strftime('%H:%M:%S')}] 🔄 処理フラグを強制リセットしました（Supabase + ローカル + active_tasks）"
    ]
    processing_status['resource_control'] = {
        'throttle_delay': 0.0,
        'adjustment_count': 0
    }

    return jsonify({
        'success': True,
        'message': '処理フラグをリセットしました'
    })


if __name__ == '__main__':
    # 開発環境での実行
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
```

### services\doc-processor\cloudbuild.yaml

```yaml
steps:
  # Dockerイメージをビルド
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'build'
      - '-t'
      - 'asia-northeast1-docker.pkg.dev/$PROJECT_ID/cloud-run-source-deploy/doc-processor:${_TAG}'
      - '-f'
      - 'services/doc-processor/Dockerfile'
      - '.'

  # イメージをArtifact Registryにプッシュ
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - 'asia-northeast1-docker.pkg.dev/$PROJECT_ID/cloud-run-source-deploy/doc-processor:${_TAG}'

  # Cloud Runにデプロイ
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: gcloud
    args:
      - 'run'
      - 'deploy'
      - 'doc-processor'
      - '--image'
      - 'asia-northeast1-docker.pkg.dev/$PROJECT_ID/cloud-run-source-deploy/doc-processor:${_TAG}'
      - '--region'
      - 'asia-northeast1'
      - '--platform'
      - 'managed'
      - '--memory'
      - '12Gi'
      - '--cpu'
      - '2'
      - '--timeout'
      - '3600'
      - '--allow-unauthenticated'
      - '--min-instances'
      - '0'
      - '--max-instances'
      - '10'
      - '--concurrency'
      - '1'
      - '--execution-environment'
      - 'gen1'

images:
  - 'asia-northeast1-docker.pkg.dev/$PROJECT_ID/cloud-run-source-deploy/doc-processor:${_TAG}'

substitutions:
  _TAG: 'latest'

options:
  machineType: 'E2_HIGHCPU_8'
```

### services\doc-processor\deploy.sh

```sh
#!/bin/bash
  set -e
  PROJECT_ID="consummate-yew-479020-u2"
  SERVICE_NAME="doc-processor"
  REGION="asia-northeast1"
  cd ~/document-management-system
  docker build -t asia-northeast1-docker.pkg.dev/${PROJECT_ID}/cloud-run-source-deploy/${SERVICE_NAME}:latest -f services/doc-processor/Dockerfile .
  docker push asia-northeast1-docker.pkg.dev/${PROJECT_ID}/cloud-run-source-deploy/${SERVICE_NAME}:latest
  gcloud run deploy ${SERVICE_NAME} --image asia-northeast1-docker.pkg.dev/${PROJECT_ID}/cloud-run-source-deploy/${SERVICE_NAME}:latest --region ${REGION} --memory 16Gi --cpu 4 --timeout 3600 --allow-unauthenticated --min-instances 0 --max-instances 10 --concurrency 1 --execution-environment gen1
  echo "✅ ${SERVICE_NAME} deployed!"
```

### services\doc-processor\deploy_to_cloud_run.sh

```sh
#!/bin/bash
# Cloud Runへのデプロイスクリプト（doc-processor専用）

set -e  # エラーが発生したら終了

echo "================================"
echo "doc-processor のデプロイを開始"
echo "================================"

# スクリプトのディレクトリを取得
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
PROJECT_ROOT="$( cd "$SCRIPT_DIR/../.." && pwd )"

# .envファイルから環境変数を読み込む
ENV_FILE="$PROJECT_ROOT/.env"
if [ ! -f "$ENV_FILE" ]; then
    echo "エラー: .envファイルが見つかりません ($ENV_FILE)"
    exit 1
fi

# .envファイルから必要な環境変数を抽出
echo "環境変数を読み込んでいます..."
set -a  # 自動的にexport
source "$ENV_FILE" 2>/dev/null || true  # エラーを無視
set +a

# 必須環境変数のチェック
if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_KEY" ]; then
    echo "エラー: SUPABASE_URLまたはSUPABASE_KEYが設定されていません"
    exit 1
fi

echo "✓ 環境変数の読み込み完了"

# Cloud Runにデプロイ
echo ""
echo "Cloud Runにデプロイしています..."
# doc-processorディレクトリからデプロイ
cd "$SCRIPT_DIR"
gcloud run deploy doc-processor \
  --source . \
  --region asia-northeast1 \
  --allow-unauthenticated \
  --timeout 3600 \
  --memory 16Gi \
  --cpu 4 \
  --set-env-vars "SUPABASE_URL=$SUPABASE_URL" \
  --set-env-vars "SUPABASE_KEY=***REDACTED***" \
  --set-env-vars "GOOGLE_AI_API_KEY=***REDACTED***" \
  --set-env-vars "OPENAI_API_KEY=***REDACTED***" \
  --set-env-vars "ANTHROPIC_API_KEY=***REDACTED***" \
  --set-env-vars "LOG_LEVEL=${LOG_LEVEL:-INFO}" \
  --set-env-vars "RERANK_ENABLED=${RERANK_ENABLED:-true}"

echo ""
echo "================================"
echo "✓ デプロイが完了しました！"
echo "================================"
echo ""
echo "確認コマンド:"
echo "  curl https://doc-processor-983922127476.asia-northeast1.run.app/processing"
echo "  curl https://doc-processor-983922127476.asia-northeast1.run.app/api/health"
echo ""
```

### services\doc-processor\Dockerfile

```text
# ベースイメージを使用（依存関係インストール済み）
FROM asia-northeast1-docker.pkg.dev/consummate-yew-479020-u2/cloud-run-source-deploy/doc-processor-base:latest

WORKDIR /app

# アプリケーションファイルをコピー（プロジェクトルートからビルドする場合）
COPY services/doc-processor/app.py .
COPY services/doc-processor/templates/ ./templates/
COPY services/doc-processor/start.sh .

# プロジェクトルートからの共通モジュール
COPY shared/__init__.py ./shared/
COPY shared/common/ ./shared/common/
COPY shared/ai/ ./shared/ai/
COPY shared/pipeline/ ./shared/pipeline/
COPY shared/kakeibo/ ./shared/kakeibo/

# ドキュメント処理用のファイル
COPY scripts/processing/ ./scripts/processing/
# app.py から直接インポートできるように /app/ 直下にもコピー
COPY scripts/processing/process_queued_documents.py .

# 環境変数を設定
ENV PORT=8080
ENV PYTHONPATH=/app

# API Keys - 本番環境では Cloud Run の環境変数で上書き推奨
ARG GOOGLE_AI_API_KEY
ARG ANTHROPIC_API_KEY
ARG OPENAI_API_KEY
ARG SUPABASE_URL
ARG SUPABASE_KEY
ARG SUPABASE_SERVICE_ROLE_KEY

ENV GOOGLE_AI_API_KEY=***REDACTED***
ENV ANTHROPIC_API_KEY=***REDACTED***
ENV OPENAI_API_KEY=***REDACTED***
ENV SUPABASE_URL=${SUPABASE_URL}
ENV SUPABASE_KEY=***REDACTED***
ENV SUPABASE_SERVICE_ROLE_KEY=***REDACTED***

# 起動スクリプトに実行権限を付与
RUN chmod +x start.sh

# サーバーを起動
CMD ["./start.sh"]
```

### services\doc-processor\requirements.txt

```txt
google-api-python-client==2.108.0
google-auth==2.25.2
google-auth-oauthlib==1.2.0
google-auth-httplib2==0.2.0
google-generativeai==0.8.5

anthropic==0.39.0

openai==1.54.0

supabase==2.10.0

pdfplumber==0.10.3
PyMuPDF>=1.24.0
python-docx==1.1.0
python-pptx==0.6.21
openpyxl==3.1.2
Pillow>=10.2.0,<11.0.0
pytesseract==0.3.10
pypdf==3.17.4
pdf2image==1.17.0
opencv-python-headless==4.11.0.86
paddlex>=3.3.0
paddleocr==3.3.2
surya-ocr==0.17.0

numpy>=1.26.2
sentence-transformers==5.1.2
torch>=2.7.0,<3.0.0
pyarrow==22.0.0
pgvector==0.2.4

python-dotenv==1.0.0
loguru==0.7.2
pydantic
pydantic-settings
tenacity>=8.2.0
jsonschema>=4.20.0
pyyaml>=6.0

streamlit>=1.29.0,<2.0.0
pandas>=2.1.4,<3.0.0
streamlit-pdf-viewer

gunicorn==23.0.0
Flask==3.1.0
Flask-CORS==5.0.0
psutil==5.9.8
json_repair
beautifulsoup4==4.12.3
lxml>=5.1.0
# playwright==1.40.0  # Gmail取り込み専用（Cloud Runでは不要）

```

### services\doc-processor\run_server.py

```py
#!/usr/bin/env python
"""パスを正しく設定してからサーバーを起動"""
import os
import sys
from pathlib import Path

# プロジェクトルートをパスに追加
project_root = Path(__file__).resolve().parent.parent.parent
services_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(services_root))

print(f"project_root: {project_root}")
print(f"sys.path[:3]: {sys.path[:3]}")

# appをインポートして起動
from app import app

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
```

### services\doc-processor\start.sh

```sh
#!/bin/bash

# Flask アプリのみを起動（処理はAPIリクエストで開始）
exec gunicorn --bind :$PORT --workers 1 --threads 8 --timeout 0 app:app
```

### services\doc-processor\templates\processing.html

```html
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ドキュメント処理システム</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #2563eb 0%, #7c3aed 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #2563eb 0%, #7c3aed 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }

        .header h1 {
            font-size: 2em;
            margin-bottom: 10px;
        }

        .header p {
            opacity: 0.9;
            font-size: 1em;
        }

        .main-content {
            padding: 30px;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }

        .stat-card {
            background: linear-gradient(135deg, #f3f4f6 0%, #e5e7eb 100%);
            border-radius: 10px;
            padding: 20px;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        .stat-card .label {
            font-size: 0.9em;
            color: #6b7280;
            margin-bottom: 8px;
        }

        .stat-card .value {
            font-size: 2em;
            font-weight: bold;
            color: #1f2937;
        }

        .stat-card.pending { background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%); }
        .stat-card.processing { background: linear-gradient(135deg, #dbeafe 0%, #bfdbfe 100%); }
        .stat-card.completed { background: linear-gradient(135deg, #d1fae5 0%, #a7f3d0 100%); }
        .stat-card.error { background: linear-gradient(135deg, #fee2e2 0%, #fecaca 100%); }

        .control-panel {
            background: #f9fafb;
            border-radius: 10px;
            padding: 25px;
            margin-bottom: 30px;
        }

        .control-row {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-bottom: 20px;
        }

        .control-group {
            display: flex;
            flex-direction: column;
        }

        .control-group label {
            font-weight: 600;
            color: #374151;
            margin-bottom: 8px;
            font-size: 0.9em;
        }

        .control-group input,
        .control-group select {
            padding: 10px;
            border: 2px solid #e5e7eb;
            border-radius: 8px;
            font-size: 1em;
            transition: border-color 0.3s;
        }

        .control-group input:focus,
        .control-group select:focus {
            outline: none;
            border-color: #2563eb;
        }

        .btn {
            padding: 12px 30px;
            border: none;
            border-radius: 8px;
            font-size: 1em;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
            width: 100%;
        }

        .btn-primary {
            background: linear-gradient(135deg, #2563eb 0%, #7c3aed 100%);
            color: white;
        }

        .btn-primary:hover:not(:disabled) {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(37, 99, 235, 0.4);
        }

        .btn-primary:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .btn-secondary {
            background: #6b7280;
            color: white;
        }

        .btn-secondary:hover {
            background: #4b5563;
        }

        .status-message {
            padding: 15px;
            border-radius: 8px;
            margin-top: 20px;
            display: none;
        }

        .status-message.success {
            background: #d1fae5;
            color: #065f46;
        }

        .status-message.error {
            background: #fee2e2;
            color: #991b1b;
        }

        .status-message.info {
            background: #dbeafe;
            color: #1e40af;
        }

        .loading {
            display: none;
            text-align: center;
            padding: 20px;
        }

        .loading.active {
            display: block;
        }

        .spinner {
            border: 4px solid #f3f4f6;
            border-top: 4px solid #2563eb;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
            margin: 0 auto 10px;
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .info-box {
            background: #eff6ff;
            border-left: 4px solid #2563eb;
            padding: 15px;
            border-radius: 8px;
            margin-top: 20px;
        }

        .info-box p {
            color: #1e40af;
            font-size: 0.9em;
            line-height: 1.6;
        }

        /* 進捗バー */
        .progress-section {
            background: #f9fafb;
            border-radius: 10px;
            padding: 20px;
            margin-bottom: 20px;
            display: block;
        }

        .progress-bar-container {
            background: #e5e7eb;
            border-radius: 10px;
            height: 30px;
            overflow: hidden;
            margin-bottom: 15px;
        }

        .progress-bar {
            background: linear-gradient(135deg, #2563eb 0%, #7c3aed 100%);
            height: 100%;
            width: 0%;
            transition: width 0.3s;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
        }

        .current-file {
            font-size: 0.9em;
            color: #6b7280;
            margin-bottom: 10px;
        }

        /* システムリソース */
        .resource-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 15px;
            margin-top: 15px;
        }

        .resource-card {
            background: white;
            border-radius: 8px;
            padding: 12px;
            border: 2px solid #e5e7eb;
        }

        .resource-card .label {
            font-size: 0.8em;
            color: #6b7280;
            margin-bottom: 5px;
        }

        .resource-card .value {
            font-size: 1.3em;
            font-weight: bold;
            color: #1f2937;
        }

        /* ログウィンドウ */
        .log-window {
            background: #1f2937;
            border-radius: 10px;
            padding: 15px;
            margin-top: 20px;
            height: 1200px;
            overflow-y: auto;
            font-family: 'Courier New', monospace;
            font-size: 0.85em;
            display: block;
        }

        .log-window .log-entry {
            color: #d1d5db;
            margin-bottom: 5px;
            line-height: 1.4;
        }

        .log-window .log-entry.success {
            color: #10b981;
        }

        .log-window .log-entry.error {
            color: #ef4444;
        }

        .log-window::-webkit-scrollbar {
            width: 8px;
        }

        .log-window::-webkit-scrollbar-track {
            background: #374151;
            border-radius: 4px;
        }

        .log-window::-webkit-scrollbar-thumb {
            background: #6b7280;
            border-radius: 4px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>📄 ドキュメント処理システム</h1>
            <p>待機中のドキュメントをOCR処理してデータベースに登録</p>
        </div>

        <div class="main-content">
            <!-- 統計情報 -->
            <h2 style="margin-bottom: 20px; color: #1f2937;">📊 処理キューの状態</h2>
            <div class="stats-grid">
                <div class="stat-card pending">
                    <div class="label">⏳ 処理待ち</div>
                    <div class="value" id="stat-pending">-</div>
                </div>
                <div class="stat-card completed">
                    <div class="label">✅ 処理済み</div>
                    <div class="value" id="stat-completed">-</div>
                </div>
                <div class="stat-card error">
                    <div class="label">❌ エラー</div>
                    <div class="value" id="stat-error">-</div>
                </div>
            </div>

            <!-- コントロールパネル -->
            <h2 style="margin-bottom: 20px; color: #1f2937;">⚙️ 処理設定</h2>
            <div class="control-panel">
                <div class="control-row">
                    <div class="control-group">
                        <label for="workspace">ワークスペース</label>
                        <select id="workspace" onchange="refreshStats()">
                            <option value="all">all（全て）</option>
                        </select>
                    </div>
                    <div class="control-group">
                        <label for="limit">処理件数上限</label>
                        <input type="number" id="limit" value="100" min="1" max="1000">
                    </div>
                </div>
                <div class="control-group" style="margin-bottom: 20px;">
                    <label>
                        <input type="checkbox" id="preserve-workspace" checked style="width: auto; margin-right: 5px;">
                        ワークスペースを保持
                    </label>
                </div>

                <button class="btn btn-primary" id="start-btn" onclick="confirmStart()">
                    ➕ キューに追加
                </button>

                <button class="btn btn-secondary" id="stop-btn" onclick="stopProcessing()" style="margin-top: 10px; background: #dc2626; display: none;">
                    ⏹️ 処理を停止
                </button>

                <button class="btn btn-primary" id="reprocess-btn" onclick="startProcessing()" style="margin-top: 10px; background: #10b981;">
                    🔄 再処理
                </button>

                <button class="btn btn-secondary" onclick="resetProcessing()" style="margin-top: 10px; background: #f59e0b;">
                    🔄 強制リセット
                </button>

                <button class="btn btn-secondary" onclick="refreshStats()" style="margin-top: 10px;">
                    📊 統計情報を更新
                </button>
            </div>

            <!-- 進捗セクション -->
            <div class="progress-section" id="progress-section">
                <h3 style="margin-bottom: 15px; color: #1f2937;">📊 処理進捗</h3>
                <div class="progress-bar-container">
                    <div class="progress-bar" id="progress-bar">0%</div>
                </div>
                <div class="current-file" id="current-file">準備中...</div>

                <!-- システムリソース -->
                <div class="resource-grid">
                    <div class="resource-card">
                        <div class="label">💻 CPU使用率</div>
                        <div class="value" id="cpu-usage">-</div>
                    </div>
                    <div class="resource-card">
                        <div class="label">🧠 メモリ使用率</div>
                        <div class="value" id="memory-usage">-</div>
                    </div>
                    <div class="resource-card">
                        <div class="label">⚡ 実行中</div>
                        <div class="value" id="active-workers">-</div>
                    </div>
                    <div class="resource-card">
                        <div class="label">⏱️ スロットル</div>
                        <div class="value" id="throttle-info">-</div>
                    </div>
                </div>
            </div>

            <!-- ローディング表示 -->
            <div class="loading" id="loading">
                <div class="spinner"></div>
                <p>処理中... しばらくお待ちください</p>
            </div>

            <!-- ステータスメッセージ -->
            <div class="status-message" id="status-message"></div>

            <!-- ログウィンドウ -->
            <div class="log-window" id="log-window"></div>

            <!-- 情報ボックス -->
            <div class="info-box">
                <p><strong>注意:</strong> 処理には時間がかかる場合があります。ブラウザを閉じても処理は継続されます。</p>
                <p><strong>ヒント:</strong> 「キューに追加」ボタンを押すと、未処理のドキュメントがバッチ処理対象としてマークされ、システムが自動的に並列処理を開始します。</p>
            </div>
        </div>
    </div>

    <script>
        // Supabase設定（モニタリング用）
        const SUPABASE_URL = '{{ supabase_url }}';
        const SUPABASE_ANON_KEY = '{{ supabase_anon_key }}';

        let progressInterval = null;
        let isProcessing = false;  // 処理中フラグ

        // ページ読み込み時
        window.addEventListener('load', function() {
            loadWorkspaces();
            refreshStats();
        });

        // ポーリング開始（処理中のみ）
        function startPolling() {
            if (!progressInterval) {
                progressInterval = setInterval(updateProgress, 1000);
            }
        }

        // ポーリング停止
        function stopPolling() {
            if (progressInterval) {
                clearInterval(progressInterval);
                progressInterval = null;
            }
        }

        // ワークスペース一覧を読み込む
        async function loadWorkspaces() {
            try {
                const response = await fetch('/api/workspaces');
                const data = await response.json();

                if (data.success && data.workspaces) {
                    const select = document.getElementById('workspace');

                    // 既存のオプション（all）はそのまま
                    // 各ワークスペースを追加
                    data.workspaces.forEach(workspace => {
                        const option = document.createElement('option');
                        option.value = workspace;
                        option.textContent = workspace;
                        select.appendChild(option);
                    });
                }
            } catch (error) {
                console.error('ワークスペース読み込みエラー:', error);
            }
        }

        // 進捗状況を取得（Supabaseから直接取得 - Cloud Runにリクエストしない）
        async function updateProgress() {
            try {
                // processing_lockとprocessing_workersを並列で取得
                const [lockResponse, workersResponse] = await Promise.all([
                    fetch(`${SUPABASE_URL}/rest/v1/processing_lock?id=eq.1&select=*`, {
                        headers: {
                            'apikey': SUPABASE_ANON_KEY,
                            'Authorization': `Bearer ${SUPABASE_ANON_KEY}`
                        }
                    }),
                    fetch(`${SUPABASE_URL}/rest/v1/processing_workers?select=*`, {
                        headers: {
                            'apikey': SUPABASE_ANON_KEY,
                            'Authorization': `Bearer ${SUPABASE_ANON_KEY}`
                        }
                    })
                ]);

                const lockResult = await lockResponse.json();
                const workersResult = await workersResponse.json();
                const data = lockResult[0];
                const currentWorkers = Array.isArray(workersResult) ? workersResult.length : 0;

                if (!data) {
                    console.error('processing_lockデータが見つかりません');
                    return;
                }

                const maxParallel = data.max_parallel || 10;

                // デバッグ
                console.log('[Polling from Supabase]', new Date().toLocaleTimeString(),
                    'is_processing:', data.is_processing,
                    'workers:', currentWorkers + '/' + maxParallel);

                const progressBar = document.getElementById('progress-bar');
                const currentFile = document.getElementById('current-file');
                const logWindow = document.getElementById('log-window');
                const cpuUsage = document.getElementById('cpu-usage');
                const memoryUsage = document.getElementById('memory-usage');
                const activeWorkers = document.getElementById('active-workers');
                const throttleInfo = document.getElementById('throttle-info');

                // システムリソースを更新
                if (data.cpu_percent !== null) {
                    cpuUsage.textContent = data.cpu_percent + '%';
                }
                if (data.memory_percent !== null) {
                    memoryUsage.textContent = data.memory_percent + '% (' +
                        (data.memory_used_gb || 0) + '/' +
                        (data.memory_total_gb || 0) + ' GB)';
                }

                // ワーカー情報（実行数/実行可能数）
                activeWorkers.textContent = currentWorkers + '/' + maxParallel;

                // スロットル表示
                const delay = data.throttle_delay || 0;
                if (delay === 0) {
                    throttleInfo.textContent = '🟢 フルスピード';
                } else if (delay < 1.5) {
                    throttleInfo.textContent = '🟡 減速中（' + delay.toFixed(1) + '秒）';
                } else {
                    throttleInfo.textContent = '🔴 減速中（' + delay.toFixed(1) + '秒）';
                }

                // ログを更新
                if (data.logs && data.logs.length > 0) {
                    try {
                        logWindow.innerHTML = data.logs.map(log => {
                            let className = 'log-entry';
                            if (log.includes('✅') || log.includes('成功')) className += ' success';
                            if (log.includes('❌') || log.includes('エラー')) className += ' error';
                            return `<div class="${className}">${escapeHtml(log)}</div>`;
                        }).join('');
                        logWindow.scrollTop = logWindow.scrollHeight;
                    } catch (err) {
                        console.error('ログ表示エラー:', err);
                    }
                }

                if (data.is_processing) {
                    isProcessing = true;

                    // 進捗バーを更新（処理中ドキュメントのステージ進捗の平均）
                    const stageWeights = {
                        'pending': 0,
                        'stage_e': 20,
                        'stage_f': 40,
                        'stage_h': 60,
                        'stage_j': 80,
                        'stage_k': 90,
                        'completed': 100,
                        'error': 100
                    };

                    // 処理中のドキュメントの進捗を計算
                    let totalProgress = 0;
                    if (Array.isArray(workersResult) && workersResult.length > 0) {
                        workersResult.forEach(worker => {
                            const stage = worker.current_stage || 'pending';
                            totalProgress += stageWeights[stage] || 0;
                        });
                        const percent = Math.round(totalProgress / (workersResult.length * 100) * 100);
                        progressBar.style.width = percent + '%';
                        progressBar.textContent = `${percent}%`;
                    } else {
                        // 処理中ワーカーがいない場合
                        progressBar.style.width = '0%';
                        progressBar.textContent = '0%';
                    }

                    // 現在のファイル名を更新
                    const fileName = data.current_file || '処理中...';
                    const successCount = data.success_count || 0;
                    const errorCount = data.error_count || 0;
                    const completedDocs = successCount + errorCount;
                    const totalCount = data.total_count || 0;
                    if (totalCount > 0) {
                        currentFile.textContent = `[${completedDocs + 1}/${totalCount}件目] ${fileName}`;
                    } else {
                        currentFile.textContent = fileName;
                    }

                    // ボタン状態を更新
                    document.getElementById('start-btn').disabled = true;
                    document.getElementById('reprocess-btn').disabled = true;
                    document.getElementById('stop-btn').style.display = 'block';
                } else {
                    // 処理完了 → ポーリング停止

                    stopPolling(); // 無条件で停止

                    if (isProcessing) {

                        isProcessing = false;

                        console.log('処理完了 - ポーリング停止');

                    }

                    currentFile.textContent = '待機中';
                    document.getElementById('start-btn').disabled = false;
                    document.getElementById('reprocess-btn').disabled = false;
                    document.getElementById('stop-btn').style.display = 'none';
                }
            } catch (error) {
                console.error('進捗取得エラー:', error);
            }
        }

        // HTMLエスケープ
        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }

        // 統計情報を更新
        async function refreshStats() {
            try {
                const workspace = document.getElementById('workspace').value || 'all';
                const response = await fetch(`/api/process/stats?workspace=${encodeURIComponent(workspace)}`);
                const data = await response.json();

                if (data.success) {
                    const stats = data.stats;
                    document.getElementById('stat-pending').textContent = stats.pending || 0;
                    document.getElementById('stat-completed').textContent = stats.completed || 0;
                    document.getElementById('stat-error').textContent = stats.error || 0;

                    // 処理開始ボタンの状態を更新
                    const startBtn = document.getElementById('start-btn');
                    if (stats.pending > 0) {
                        startBtn.disabled = false;
                    } else {
                        startBtn.disabled = true;
                    }
                } else {
                    showMessage('error', '統計情報の取得に失敗しました: ' + data.error);
                }
            } catch (error) {
                showMessage('error', '統計情報の取得中にエラーが発生しました: ' + error.message);
            }
        }

        // 確認ダイアログ
        function confirmStart() {
            const workspace = document.getElementById('workspace').value || 'all';
            const limit = parseInt(document.getElementById('limit').value) || 100;

            const message = `${limit}件のドキュメントをキューに追加しますか？\n\nワークスペース: ${workspace}\n処理件数: ${limit}件\n\nキューに追加後、システムが自動的に並列処理を開始します。`;

            if (confirm(message)) {
                startProcessing();
            }
        }

        // 処理を停止
        async function stopProcessing() {
            if (!confirm('処理を停止しますか？\n\n現在処理中のドキュメントまで完了し、次のドキュメントから停止します。')) {
                return;
            }

            try {
                const response = await fetch('/api/process/stop', {
                    method: 'POST'
                });

                const data = await response.json();

                if (response.ok && data.success) {
                    showMessage('info', '⚠️ ' + data.message);
                    document.getElementById('stop-btn').style.display = 'none';
                    document.getElementById('start-btn').disabled = false;
                } else {
                    showMessage('error', '❌ ' + (data.error || '停止に失敗しました'));
                }
            } catch (error) {
                showMessage('error', '❌ 停止中にエラーが発生しました: ' + error.message);
            }
        }

        // 強制リセット
        async function resetProcessing() {
            if (!confirm('処理フラグを強制リセットしますか？\n\n「既に処理が実行中です」エラーが解消されない場合に使用してください。')) {
                return;
            }

            try {
                const response = await fetch('/api/process/reset', {
                    method: 'POST'
                });

                const data = await response.json();

                if (response.ok && data.success) {
                    showMessage('info', '🔄 ' + data.message);
                    document.getElementById('stop-btn').style.display = 'none';
                    document.getElementById('start-btn').disabled = false;
                    updateProgress();
                } else {
                    showMessage('error', '❌ ' + (data.error || 'リセットに失敗しました'));
                }
            } catch (error) {
                showMessage('error', '❌ リセット中にエラーが発生しました: ' + error.message);
            }
        }

        // 処理を開始
        async function startProcessing() {
            const workspace = document.getElementById('workspace').value || 'all';
            const limit = parseInt(document.getElementById('limit').value) || 100;
            const preserveWorkspace = document.getElementById('preserve-workspace').checked;

            // UIを更新
            document.getElementById('loading').classList.add('active');
            document.getElementById('start-btn').disabled = true;
            document.getElementById('status-message').style.display = 'none';

            // 注: ポーリングはAPI成功後にupdateProgress()内で開始される

            try {
                const response = await fetch('/api/process/start', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        workspace: workspace,
                        limit: limit,
                        preserve_workspace: preserveWorkspace
                    })
                });

                const data = await response.json();

                if (response.ok && data.success) {
                    showMessage('info', '▶️ 処理を開始しました（' + data.total_count + '件）');
                    document.getElementById('loading').classList.remove('active');
                    document.getElementById('stop-btn').style.display = 'block';
                    document.getElementById('reprocess-btn').disabled = true;
                    // 処理開始 → ポーリング開始
                    isProcessing = true;
                    startPolling();
                } else {
                    showMessage('error', '❌ エラー: ' + (data.error || '不明なエラー'));
                    document.getElementById('loading').classList.remove('active');
                    document.getElementById('start-btn').disabled = false;
                }
            } catch (error) {
                showMessage('error', '❌ 処理中にエラーが発生しました: ' + error.message);
                document.getElementById('loading').classList.remove('active');
                document.getElementById('start-btn').disabled = false;
            }
        }

        // メッセージを表示
        function showMessage(type, message) {
            const messageBox = document.getElementById('status-message');
            messageBox.className = 'status-message ' + type;
            messageBox.textContent = message;
            messageBox.style.display = 'block';
        }

        // ページ読み込み時に初回更新
        document.addEventListener('DOMContentLoaded', function() {
            updateProgress(); // 初回チェックのみ
            // startPolling(); // 削除 - 処理中の場合のみ自動開始
        });
    </script>
</body>
</html>
```

### services\doc-search\app.py

```py
"""
Flask Web Application
質問・回答システムのWebインターフェース
"""
import os
import sys
from pathlib import Path

# プロジェクトルートをPythonパスに追加（ローカル実行時用）
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from typing import Dict, List, Any, Optional
from datetime import datetime
from flask import Flask, render_template, request, jsonify
from flask_cors import CORS

app = Flask(__name__)
CORS(app)

# クライアントの遅延初期化（Cloud Run起動高速化）
db_client = None
llm_client = None
query_expander = None


def get_clients():
    """クライアントを初回アクセス時に初期化（遅延読み込み）"""
    global db_client, llm_client, query_expander

    if db_client is None:
        print("[INFO] クライアントを初期化中...")
        # 遅延import（起動高速化）
        from shared.common.database.client import DatabaseClient
        from shared.ai.llm_client.llm_client import LLMClient
        from shared.common.utils.query_expansion import QueryExpander

        db_client = DatabaseClient()
        llm_client = LLMClient()
        query_expander = QueryExpander(llm_client=llm_client)
        print("[INFO] クライアント初期化完了")

    return db_client, llm_client, query_expander


@app.route('/')
def index():
    """メインページ"""
    return render_template('index.html')


@app.route('/api/filters', methods=['GET'])
def get_filters():
    """
    フィルタオプション取得API（階層構造対応）
    workspace（親）→ doc_type（子）の階層データを返す
    """
    try:
        # クライアント取得（遅延初期化）
        db_client, _, _ = get_clients()

        # workspace別のdoc_type階層構造を取得
        hierarchy = db_client.get_workspace_hierarchy()

        # 階層構造をリスト形式に変換（フロントエンド用）
        workspace_list = []
        for workspace, doc_types in hierarchy.items():
            workspace_list.append({
                'name': workspace,
                'doc_types': doc_types
            })

        print(f"[DEBUG] フィルタ取得: {len(workspace_list)} workspaces（階層構造）")

        return jsonify({
            'success': True,
            'hierarchy': workspace_list
        })
    except Exception as e:
        print(f"[ERROR] フィルタ取得エラー: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/search', methods=['POST'])
def search_documents():
    """
    ベクトル検索API（クエリ拡張対応 + 複数フィルタ対応）
    ユーザーの質問から関連文書を検索
    """
    try:
        # クライアント取得（遅延初期化）
        db_client, llm_client, query_expander = get_clients()

        data = request.get_json()
        query = data.get('query', '')
        # リランク機能のため、フロントエンドの指定を尊重（最大50件まで）
        requested_limit = data.get('limit', 3)
        limit = min(requested_limit, 50)  # 50件取得→高精度な5件にリランク可能

        # ✅ 配列で受け取る（後方互換性のため単一値もサポート）
        workspaces = data.get('workspaces', [])
        doc_types = data.get('doc_types', [])

        # 後方互換性: 単一のworkspaceパラメータもサポート
        if not workspaces and data.get('workspace'):
            workspaces = [data.get('workspace')]

        enable_query_expansion = data.get('enable_query_expansion', False)  # デフォルトで無効

        print(f"[DEBUG] 検索リクエスト: query='{query}', limit={limit}, workspaces={workspaces}, doc_types={doc_types}")

        if not query:
            return jsonify({'success': False, 'error': 'クエリが空です'}), 400

        # ユーザーコンテキストを読み込み、関連情報を抽出（検索用：軽量）
        from shared.common.utils.context_extractor import ContextExtractor
        from shared.common.config.yaml_loader import load_user_context

        user_context = load_user_context()
        context_extractor = ContextExtractor(user_context)
        extracted_context = context_extractor.extract_relevant_context(
            query,
            include_schedules=False  # 検索時はスケジュール不要
        )
        context_string = context_extractor.build_search_context_string(extracted_context)

        # クエリ拡張を適用（有効な場合）
        expanded_query = query
        expansion_info = None

        # ユーザーコンテキストがあればクエリに追加
        if context_string:
            expanded_query = f"{query} {context_string}"
            print(f"[DEBUG] コンテキスト追加: '{query}' → '{expanded_query}'")

        if enable_query_expansion:
            expansion_result = query_expander.expand_query(expanded_query)
            if expansion_result.get('expansion_applied'):
                expanded_query = expansion_result.get('expanded_query', expanded_query)
                expansion_info = {
                    'original': query,
                    'expanded': expanded_query,
                    'keywords': expansion_result.get('keywords', [])
                }
                print(f"[DEBUG] クエリ拡張適用: '{query}' → '{expanded_query}'")
            else:
                print(f"[DEBUG] クエリ拡張スキップ: '{expanded_query}'")

        # Embeddingを生成（拡張されたクエリを使用）
        embedding = llm_client.generate_embedding(expanded_query)

        # ✅ 時系列フィルタを検出
        date_filter = _detect_date_filter(query)
        print(f"[DEBUG] 時系列フィルタ検出: {date_filter}")

        # ✅ クエリタイプを検出
        query_type_info = _detect_query_type(query)
        print(f"[DEBUG] クエリタイプ検出: {query_type_info['type']} (focus: {query_type_info['focus']})")

        # ✅ クロスリファレンスを検出
        referenced_file = _detect_cross_reference(query)
        cross_reference_results = []
        if referenced_file:
            print(f"[DEBUG] クロスリファレンス検出: {referenced_file}")
            # 参照されたファイルを検索
            try:
                cross_ref_response = db_client.client.table('Rawdata_FILE_AND_MAIL').select('*').ilike('file_name', f'%{referenced_file}%').limit(3).execute()
                if cross_ref_response.data:
                    # 検索結果の形式に変換
                    for doc in cross_ref_response.data:
                        cross_reference_results.append({
                            'id': doc.get('id'),
                            'file_name': doc.get('file_name'),
                            'doc_type': doc.get('doc_type'),
                            'workspace': doc.get('workspace'),
                            'document_date': doc.get('document_date'),
                            'metadata': doc.get('metadata', {}),
                            'summary': doc.get('summary'),
                            'content': doc.get('attachment_text', ''),
                            'similarity': 1.0,  # 最高スコア（参照されたファイル）
                            'is_cross_reference': True  # クロスリファレンスフラグ
                        })
                    print(f"[DEBUG] クロスリファレンス結果: {len(cross_reference_results)} 件")
            except Exception as e:
                print(f"[WARNING] クロスリファレンス検索エラー: {e}")

        # ベクトル検索を実行（同期ラッパーを使用）
        # 拡張されたクエリをテキスト検索にも使用
        results = db_client.search_documents_sync(
            expanded_query,
            embedding,
            limit,
            doc_types if doc_types else None,
            date_filter=date_filter  # 時系列フィルタを渡す
        )

        # ✅ クロスリファレンス結果を先頭に追加
        if cross_reference_results:
            # 重複を避ける：クロスリファレンス結果と同じIDのものは除外
            cross_ref_ids = {doc['id'] for doc in cross_reference_results}
            results = [doc for doc in results if doc.get('id') not in cross_ref_ids]
            # クロスリファレンス結果を先頭に
            results = cross_reference_results + results
            print(f"[DEBUG] クロスリファレンス結果を先頭に追加: 合計 {len(results)} 件")

        print(f"[DEBUG] 検索結果: {len(results)} 件（doc_types={doc_types}）")

        print(f"[DEBUG] 最終検索結果: {len(results)} 件返却")

        response_data = {
            'success': True,
            'results': results,
            'count': len(results),
            'query_type': query_type_info  # クエリタイプ情報を含める
        }

        # クエリ拡張情報を含める（デバッグ用）
        if expansion_info:
            response_data['query_expansion'] = expansion_info

        return jsonify(response_data)

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/answer', methods=['POST'])
def generate_answer():
    """
    回答生成API（構成フロー対応）
    検索結果を元にAIで自然な回答を生成
    """
    try:
        # クライアント取得（遅延初期化）
        db_client, llm_client, _ = get_clients()

        data = request.get_json()
        query = data.get('query', '')
        documents = data.get('documents', [])
        flow_id = data.get('flow', 'flash-x1')  # ユーザーが選択した構成フロー

        if not query:
            return jsonify({'success': False, 'error': 'クエリが空です'}), 400

        # ✅ 構成フローを取得
        from shared.common.config.model_tiers import ResearchFlow
        flow_config = ResearchFlow.get_flow(flow_id)
        steps = flow_config.get('steps', ['gemini-2.5-flash'])

        print(f"[INFO] 構成フロー実行: {flow_id} ({len(steps)}ステップ)")

        # ✅ 構成フローの実行
        current_query = query
        current_documents = documents

        for step_idx, model_name in enumerate(steps, 1):
            print(f"[INFO] ステップ {step_idx}/{len(steps)}: {model_name}")

            # 2ステップ目以降は、前の回答を使ってクエリを改善
            if step_idx > 1 and 'answer' in locals():
                # クエリ改善プロンプト
                refinement_prompt = f"""以下のユーザーの質問と、これまでの回答を踏まえて、より的確な検索クエリを生成してください。

【元の質問】
{query}

【これまでの回答】
{answer}

【指示】
- 元の質問の意図を保ちつつ、より具体的で詳細な検索キーワードを含むクエリを生成してください
- 回答から得られた重要なキーワードを追加してください
- 改善されたクエリのみを出力してください（説明は不要）

【改善されたクエリ】"""

                refinement_response = llm_client.call_model(
                    tier="ui_response",
                    prompt=refinement_prompt,
                    model_name=model_name
                )

                if refinement_response.get('success'):
                    current_query = refinement_response.get('content', current_query).strip()
                    print(f"[INFO] クエリ改善: '{query}' → '{current_query}'")

                    # 改善されたクエリで再検索
                    embedding = llm_client.generate_embedding(current_query)
                    current_documents = db_client.search_documents_sync(
                        current_query,
                        embedding,
                        limit=5,
                        doc_types=None
                    )
                    print(f"[INFO] 再検索結果: {len(current_documents)} 件")

            # 現在のステップで回答生成
            answer, model_used = _generate_answer_with_model(
                llm_client,
                model_name,
                query,
                current_documents,
                is_final_step=(step_idx == len(steps))
            )

            if not answer:
                return jsonify({
                    'success': False,
                    'error': f'ステップ{step_idx}で回答生成に失敗しました'
                }), 500

        # 最終回答を返す
        return jsonify({
            'success': True,
            'answer': answer,
            'model': model_used,
            'provider': 'gemini',
            'flow': flow_id,
            'steps': len(steps)
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


def _generate_answer_with_model(
    llm_client,
    model_name: str,
    query: str,
    documents: List[Dict[str, Any]],
    is_final_step: bool = True
) -> tuple:
    """
    指定されたモデルで回答を生成

    Args:
        llm_client: LLMクライアント
        model_name: 使用するモデル名
        query: ユーザーの質問
        documents: 検索結果
        is_final_step: 最終ステップかどうか

    Returns:
        (回答テキスト, モデル名)
    """
    try:
        # ユーザーコンテキストを読み込み、関連情報を抽出（回答生成用：詳細）
        from shared.common.utils.context_extractor import ContextExtractor
        from shared.common.config.yaml_loader import load_user_context

        user_context = load_user_context()
        context_extractor = ContextExtractor(user_context)
        extracted_context = context_extractor.extract_relevant_context(
            query,
            include_schedules=True  # 回答生成時はスケジュールも含める
        )
        user_context_prompt = context_extractor.build_answer_context_string(extracted_context)

        # ドキュメントコンテキストを構築
        context = _build_context(documents)

        import sys
        print(f"[DEBUG] 文書数: {len(documents)}件", flush=True, file=sys.stderr)
        print(f"[DEBUG] コンテキスト文字数: {len(context)}文字", flush=True, file=sys.stderr)

        # ✅ コンテキストの文字数制限（2025年モデル対応: Gemini 2.5 Flashは100万トークン対応）
        MAX_CONTEXT_LENGTH = 500000  # 約50万文字まで（Gemini 2.5 Flashの100万トークン性能をフル活用）
        if len(context) > MAX_CONTEXT_LENGTH:
            context = context[:MAX_CONTEXT_LENGTH] + "\n\n[... 以降は省略されました ...]"
            print(f"[WARNING] コンテキストを切り詰めました: {len(context)} → {MAX_CONTEXT_LENGTH} 文字")

        # プロンプトを作成（Phase 2.2.3: 構造的クエリ対応 + ユーザーコンテキスト追加）
        prompt_parts = []

        prompt_parts.append("以下の文書情報を参考に、ユーザーの質問に日本語で回答してください。")

        # ユーザーコンテキストがあれば追加
        if user_context_prompt:
            prompt_parts.append(f"\n{user_context_prompt}\n")

        prompt_parts.append(f"""
【質問】
{query}

【参考文書】
{context}

【回答の条件】
- **最重要：質問「{query}」に直接関連する情報は、具体的かつ詳細に回答してください**
  * 課題名、ファイル名、期限、提出方法、解答の場所など、実行に必要な全ての詳細情報を含めてください
  * ただし、質問に無関係な背景情報（生活態度、安全な生活などの一般的注意事項）は省略してください
  * 例：「宿題をリストにして」→宿題の詳細情報は全て記載、生活態度の一般的注意事項は省略

- **読みやすさを最優先してください**
  * **見出し**を使って科目やカテゴリーごとに明確に区切る（例：### 理科、### 英語）
  * **箇条書きとインデント**を活用して階層構造を明確にする
  * **重要な情報**（期限、提出方法など）は太字で強調する
  * 適切に空白行を入れて視覚的に見やすくする
  * **長くなっても構いません**。詳細かつ読みやすい回答を心がけてください

- **具体的な記載例：**
  * ✅ 良い例：「**理科：** 「植物が生きるしくみ 演習問題(自習課題)」を解く。解答は「【解答】植物がいきるしくみ（自習課題）.pdf」で確認し、丸付けと直しを丁寧に行うこと」
  * ❌ 悪い例：「理科：演習問題を解く」

- 参考文書の情報を基に、正確に回答してください
- **ユーザーの前提情報を考慮してください**（上記に記載された子供の情報、学校や塾のスケジュールなど）
- **重要：ファイル名も重視してください**
  * ユーザーが特定のファイル名を質問している場合（例：「学年通信（29）」）、そのファイル名と完全一致または部分一致する文書を優先的に参照してください
  * ファイル名が一致する文書があれば、必ずその内容を回答に含めてください
- **【表データ】**が含まれている場合、表形式の情報を積極的に活用してください
  * 時間割やスケジュールに関する質問には、表データから該当する科目や予定を抽出して回答してください
  * 議事録の質問には、議題グループや担当者・期限情報を参照してください
  * 複数のクラスやグループがある場合、質問に該当するものを絞り込んで回答してください
- 情報が不足している場合は、その旨を伝えてください
- **参考文書の記載方法：**
  * 回答の最後に「参考文書：」として、実際に使用した全ての文書のファイル名を列挙してください
  * 根拠確認のため、参照した文書は全て記載してください
  * ファイル名のみを記載し、文書番号は不要です

【回答】
""")

        # プロンプトを結合
        prompt = "\n".join(prompt_parts)

        print(f"[DEBUG] 最終プロンプト文字数: {len(prompt)}文字", flush=True, file=sys.stderr)
        print(f"[DEBUG] 推定トークン数: {len(prompt) // 4}トークン（概算）", flush=True, file=sys.stderr)

        # AIモデルで回答生成
        response = llm_client.call_model(
            tier="ui_response",
            prompt=prompt,
            model_name=model_name
        )

        if not response.get('success'):
            error_msg = response.get('error', 'Unknown error')
            print(f"[ERROR] LLM呼び出し失敗: {error_msg}", flush=True, file=sys.stderr)
            print(f"[ERROR] finish_reason: {response.get('error_details', {}).get('finish_reason_name', 'N/A')}", flush=True, file=sys.stderr)
            print(f"[ERROR] Full response: {response}", flush=True, file=sys.stderr)
            return None, None

        return response.get('content', ''), model_name

    except Exception as e:
        print(f"[ERROR] 回答生成エラー: {e}")
        return None, None


def _format_table_to_markdown(table_data: Dict[str, Any]) -> str:
    """
    表データをMarkdown形式のテーブルに変換（Phase 2.2.3 構造的クエリ対応）

    Args:
        table_data: 表データ（table_type, headers, rows などを含む）

    Returns:
        Markdown形式のテーブル文字列
    """
    try:
        table_type = table_data.get("table_type", "table")
        headers = table_data.get("headers", [])

        # ヘッダー行の構築
        if isinstance(headers, list) and headers:
            # シンプルなリスト形式のヘッダー
            header_line = "| " + " | ".join(str(h) for h in headers) + " |"
            separator_line = "|" + "|".join(["---" for _ in headers]) + "|"
            markdown_lines = [f"\n**表形式データ ({table_type})**\n", header_line, separator_line]
        elif isinstance(headers, dict):
            # 複雑なヘッダー構造（例: class_timetable の classes）
            classes = headers.get("classes", [])
            if classes:
                header_line = "| 日 | " + " | ".join(str(c) for c in classes) + " |"
                separator_line = "|" + "|".join(["---" for _ in range(len(classes) + 1)]) + "|"
                markdown_lines = [f"\n**クラス別時間割 ({table_type})**\n", header_line, separator_line]
            else:
                markdown_lines = [f"\n**表形式データ ({table_type})**\n"]
        else:
            markdown_lines = [f"\n**表形式データ ({table_type})**\n"]

        # 行データの処理
        rows = table_data.get("rows", [])
        if rows:
            for row in rows:
                # 行が辞書形式の場合
                if isinstance(row, dict):
                    # cells フィールドがある場合
                    if "cells" in row:
                        cells = row["cells"]
                        cell_values = []
                        for cell in cells:
                            if isinstance(cell, dict):
                                value = cell.get("value", "")
                                cell_values.append(str(value))
                            else:
                                cell_values.append(str(cell))
                        row_line = "| " + " | ".join(cell_values) + " |"
                        markdown_lines.append(row_line)
                    else:
                        # 通常の辞書行（キー: 値）
                        values = [str(v) for v in row.values()]
                        row_line = "| " + " | ".join(values) + " |"
                        markdown_lines.append(row_line)

        # daily_schedule や agenda_groups などの特殊構造
        if "daily_schedule" in table_data:
            markdown_lines.append("\n**日別スケジュール:**")
            for schedule in table_data["daily_schedule"]:
                day = schedule.get("day", "")
                markdown_lines.append(f"\n- **{day}曜日:**")

                if "class_schedules" in schedule:
                    for class_schedule in schedule["class_schedules"]:
                        class_name = class_schedule.get("class", "")
                        subjects = class_schedule.get("subjects", []) or class_schedule.get("periods", [])
                        markdown_lines.append(f"  - {class_name}: {', '.join(str(s) for s in subjects)}")

        if "agenda_groups" in table_data:
            markdown_lines.append("\n**議題グループ:**")
            for group in table_data["agenda_groups"]:
                topic = group.get("topic", "")
                markdown_lines.append(f"\n- **{topic}:**")
                for item in group.get("items", []):
                    decision = item.get("decision", "")
                    assignee = item.get("assignee", "")
                    deadline = item.get("deadline", "")
                    markdown_lines.append(f"  - {decision} (担当: {assignee}, 期限: {deadline})")

        return "\n".join(markdown_lines)

    except Exception as e:
        return f"\n[表データの変換エラー: {str(e)}]\n"


def _format_metadata(metadata: Dict[str, Any], indent: int = 0) -> str:
    """
    メタデータを見やすく整形（Phase 2.2.3: tables フィールド対応）

    Args:
        metadata: メタデータ辞書
        indent: インデントレベル

    Returns:
        整形された文字列
    """
    if not metadata:
        return ""

    lines = []
    prefix = "  " * indent

    for key, value in metadata.items():
        # Phase 2.2.3: tables フィールドを特別に処理
        if key == "tables" and isinstance(value, list):
            if not value:
                continue
            lines.append(f"{prefix}【表データ】")
            for idx, table in enumerate(value, 1):
                if isinstance(table, dict):
                    # 表をMarkdown形式に変換
                    markdown_table = _format_table_to_markdown(table)
                    lines.append(markdown_table)
            continue

        # 通常のメタデータ処理
        if isinstance(value, dict):
            lines.append(f"{prefix}{key}:")
            lines.append(_format_metadata(value, indent + 1))
        elif isinstance(value, list):
            if not value:
                continue
            lines.append(f"{prefix}{key}:")
            for item in value:
                if isinstance(item, dict):
                    # 辞書のリストの場合、各アイテムを整形
                    for sub_key, sub_value in item.items():
                        lines.append(f"{prefix}  - {sub_key}: {sub_value}")
                else:
                    lines.append(f"{prefix}  - {item}")
        else:
            lines.append(f"{prefix}{key}: {value}")

    return "\n".join(lines)


def _group_documents_by_file(documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    同じドキュメントIDのチャンクをグルーピングし、最高スコアのチャンクを代表として返す

    Args:
        documents: 検索結果のリスト

    Returns:
        ドキュメントIDでグルーピングされた結果（最高スコア順）
    """
    from collections import defaultdict

    # ドキュメントIDでグルーピング
    grouped = defaultdict(list)
    for doc in documents:
        doc_id = doc.get('id')
        if doc_id:
            grouped[doc_id].append(doc)

    # 各ドキュメントグループから最高スコアのチャンクを選択
    result = []
    for doc_id, chunks in grouped.items():
        # 類似度が最も高いチャンクを選択
        best_chunk = max(chunks, key=lambda x: x.get('similarity', 0))

        # 同じドキュメントの全チャンクの内容を結合（重複排除）
        all_contents = []
        seen_contents = set()
        for chunk in sorted(chunks, key=lambda x: x.get('similarity', 0), reverse=True):
            content = chunk.get('content') or chunk.get('summary') or chunk.get('attachment_text', '')
            if content and content not in seen_contents:
                all_contents.append(content)
                seen_contents.add(content)

        # 最高スコアのチャンクに統合された内容を設定
        if all_contents:
            best_chunk['content'] = '\n\n'.join(all_contents[:3])  # 最大3チャンクまで

        result.append(best_chunk)

    # 類似度順にソート
    result.sort(key=lambda x: x.get('similarity', 0), reverse=True)

    return result


def _detect_date_filter(query: str) -> Optional[str]:
    """
    クエリから時系列フィルタを検出

    Args:
        query: ユーザーのクエリ

    Returns:
        'recent': 最近1週間
        'this_week': 今週
        'this_month': 今月
        'today': 今日
        None: フィルタなし
    """
    import re
    from datetime import datetime, timedelta

    query_lower = query.lower()

    # 今日
    if re.search(r'(今日|きょう|本日)', query):
        return 'today'

    # 今週
    if re.search(r'(今週|こんしゅう|this week)', query):
        return 'this_week'

    # 今月
    if re.search(r'(今月|こんげつ|this month)', query):
        return 'this_month'

    # 最新・最近
    if re.search(r'(最新|最近|さいきん|さいしん|new|latest|recent)', query):
        return 'recent'

    return None


def _detect_cross_reference(query: str) -> Optional[str]:
    """
    クエリからクロスリファレンス（他の文書への参照）を検出

    Args:
        query: ユーザーのクエリ

    Returns:
        参照されているファイル名、またはNone
    """
    import re

    # パターン1: "○○.pdfを参照", "○○.docxを参照" など
    match = re.search(r'([^\s]+?\.(pdf|docx?|xlsx?|pptx?|txt|png|jpe?g))\s*(を|の)?参照', query, re.IGNORECASE)
    if match:
        return match.group(1)

    # パターン2: "参照: ○○", "参照：○○"
    match = re.search(r'参照[:：]\s*([^\s]+)', query)
    if match:
        return match.group(1)

    # パターン3: "see ○○.pdf", "refer to ○○.pdf"
    match = re.search(r'(?:see|refer to|reference)\s+([^\s]+?\.(pdf|docx?|xlsx?|pptx?|txt|png|jpe?g))', query, re.IGNORECASE)
    if match:
        return match.group(1)

    # パターン4: "○○という文書", "○○というファイル"
    match = re.search(r'([^\s]+)\s*という(?:文書|ファイル|ドキュメント)', query)
    if match:
        return match.group(1)

    return None


def _detect_query_type(query: str) -> Dict[str, Any]:
    """
    クエリのタイプを検出

    Args:
        query: ユーザーのクエリ

    Returns:
        {
            'type': str,  # 'who', 'when', 'what', 'where', 'how', 'why', 'general'
            'focus': str,  # 検出されたフォーカス
            'keywords': List[str]  # 検出されたキーワード
        }
    """
    import re

    query_lower = query.lower()

    # 優先順位順に検出

    # When: 時間に関する質問
    if re.search(r'(いつ|何時|何日|何月|何年|when|期限|締切|締め切り|デッドライン|予定|スケジュール)', query):
        return {
            'type': 'when',
            'focus': 'time_date',
            'keywords': ['document_date', 'deadline', 'schedule', 'weekly_schedule']
        }

    # Who: 人に関する質問
    if re.search(r'(誰|だれ|who|先生|teacher|from|送信者|差出人)', query):
        return {
            'type': 'who',
            'focus': 'person',
            'keywords': ['sender', 'teacher', 'author', 'display_sender']
        }

    # Where: 場所に関する質問
    if re.search(r'(どこ|where|場所|教室|クラス|classroom)', query):
        return {
            'type': 'where',
            'focus': 'location',
            'keywords': ['location', 'classroom', 'place']
        }

    # How: 方法・手順に関する質問
    if re.search(r'(どうやって|どのように|how|方法|手順|やり方)', query):
        return {
            'type': 'how',
            'focus': 'method',
            'keywords': ['procedure', 'method', 'steps']
        }

    # Why: 理由に関する質問
    if re.search(r'(なぜ|why|理由|原因)', query):
        return {
            'type': 'why',
            'focus': 'reason',
            'keywords': ['reason', 'cause', 'purpose']
        }

    # What: 物事・内容に関する質問（デフォルト）
    if re.search(r'(何|なに|what|内容|詳細)', query):
        return {
            'type': 'what',
            'focus': 'content',
            'keywords': ['content', 'subject', 'topic']
        }

    # General: 一般的な質問
    return {
        'type': 'general',
        'focus': 'general',
        'keywords': []
    }


def _build_context(documents: List[Dict[str, Any]]) -> str:
    """
    検索結果からコンテキストを構築（チャンクベース）

    Args:
        documents: 検索結果のリスト

    Returns:
        フォーマットされたコンテキスト文字列
    """
    if not documents:
        return "関連する文書が見つかりませんでした。"

    import json

    context_parts = []
    total_chunks = 0

    for doc_idx, doc in enumerate(documents, 1):
        file_name = doc.get('file_name', '無題')
        doc_type = doc.get('doc_type', '不明')
        similarity = doc.get('similarity', 0)
        all_chunks = doc.get('all_chunks', [])

        # 基本情報
        context_part = f"""
【文書{doc_idx}】
ファイル名: {file_name}
文書タイプ: {doc_type}
類似度: {similarity:.2f}
チャンク数: {len(all_chunks)}個
"""

        # ✅ ヒットした全チャンクを追加
        if all_chunks:
            context_part += "\n【ヒットしたチャンク】"
            for chunk_idx, chunk in enumerate(all_chunks, 1):
                chunk_type = chunk.get('chunk_type', 'unknown')
                chunk_content = chunk.get('chunk_content', '')
                chunk_metadata = chunk.get('chunk_metadata', {})
                search_weight = chunk.get('search_weight', 1.0)

                context_part += f"\n\n  チャンク{chunk_idx} (タイプ: {chunk_type}, 重み: {search_weight})"
                context_part += f"\n  内容: {chunk_content}"

                # ✅ chunk_metadataに構造化データがある場合は追加
                if chunk_metadata:
                    original_structure = chunk_metadata.get('original_structure')
                    if original_structure:
                        context_part += f"\n\n  【構造化データ（JSON）】"
                        context_part += f"\n  ```json\n{json.dumps(original_structure, ensure_ascii=False, indent=2)}\n  ```"
                        context_part += "\n  ※上記の構造化データを参照して、表の行・列の関係や階層構造を正確に把握してください"

                total_chunks += 1
        else:
            # フォールバック: all_chunksがない場合（後方互換性）
            summary = doc.get('summary', '')
            if summary:
                context_part += f"\n\n要約: {summary}"

        context_parts.append(context_part)

    # ✅ デバッグ用: コンテキストの文字数をログ出力
    final_context = "\n".join(context_parts)
    print(f"[DEBUG] コンテキスト文字数: {len(final_context)} 文字")
    print(f"[DEBUG] 文書数: {len(documents)} 件")
    print(f"[DEBUG] 総チャンク数: {total_chunks} 個")

    return final_context


@app.route('/api/extract_schedules', methods=['POST'])
def extract_schedules():
    """
    スケジュール抽出API
    指定された条件でドキュメントからスケジュール情報を抽出して返す
    """
    try:
        # クライアント取得
        db_client, _, _ = get_clients()

        data = request.get_json()
        workspace = data.get('workspace')
        doc_types = data.get('doc_types', [])
        start_date = data.get('start_date')  # YYYY-MM-DD形式
        end_date = data.get('end_date')  # YYYY-MM-DD形式
        limit = data.get('limit', 100)

        print(f"[DEBUG] スケジュール抽出リクエスト: workspace={workspace}, doc_types={doc_types}, date_range={start_date}~{end_date}")

        # データベースクエリを構築
        query = db_client.client.table('Rawdata_FILE_AND_MAIL').select('*')

        # フィルタを適用
        if workspace:
            query = query.eq('workspace', workspace)

        if doc_types:
            query = query.in_('doc_type', doc_types)

        # 日付範囲でフィルタ
        if start_date:
            query = query.gte('document_date', start_date)
        if end_date:
            query = query.lte('document_date', end_date)

        # 実行
        response = query.limit(limit).execute()
        documents = response.data if response.data else []

        print(f"[DEBUG] 検索結果: {len(documents)} 件")

        # スケジュール情報を抽出
        schedules = []
        for doc in documents:
            metadata = doc.get('metadata', {})

            # weekly_scheduleを抽出
            weekly_schedule = metadata.get('weekly_schedule', [])
            if weekly_schedule:
                for schedule_item in weekly_schedule:
                    schedules.append({
                        'document_id': doc.get('id'),
                        'file_name': doc.get('file_name'),
                        'doc_type': doc.get('doc_type'),
                        'workspace': doc.get('workspace'),
                        'document_date': doc.get('document_date'),
                        'schedule_type': 'weekly',
                        'schedule_data': schedule_item
                    })

            # text_blocksから日付・時間情報を抽出（オプション）
            text_blocks = metadata.get('text_blocks', [])
            for block in text_blocks:
                title = block.get('title', '')
                content = block.get('content', '')

                # タイトルや内容に日付・時間のキーワードが含まれる場合
                import re
                if re.search(r'(予定|スケジュール|日程|期限|締切|締め切り|\d{1,2}月\d{1,2}日|\d{4}-\d{2}-\d{2})', title + content):
                    schedules.append({
                        'document_id': doc.get('id'),
                        'file_name': doc.get('file_name'),
                        'doc_type': doc.get('doc_type'),
                        'workspace': doc.get('workspace'),
                        'document_date': doc.get('document_date'),
                        'schedule_type': 'text_block',
                        'schedule_data': {
                            'title': title,
                            'content': content
                        }
                    })

        print(f"[DEBUG] 抽出されたスケジュール: {len(schedules)} 件")

        # 日付順にソート
        schedules_sorted = sorted(
            schedules,
            key=lambda x: x.get('document_date') or '9999-12-31'
        )

        return jsonify({
            'success': True,
            'schedules': schedules_sorted,
            'count': len(schedules_sorted)
        })

    except Exception as e:
        print(f"[ERROR] スケジュール抽出エラー: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/health', methods=['GET'])
def health_check():
    """ヘルスチェックエンドポイント"""
    return jsonify({
        'status': 'ok',
        'message': 'Document Q&A System is running'
    })


@app.route('/api/debug/database', methods=['GET'])
def debug_database():
    """データベース接続とワークスペース情報のデバッグエンドポイント"""
    try:
        # クライアント取得
        db_client, _, _ = get_clients()

        # Rawdata_FILE_AND_MAILテーブルの件数を確認
        count_response = db_client.client.table('Rawdata_FILE_AND_MAIL').select('id', count='exact').limit(1).execute()
        total_count = count_response.count if hasattr(count_response, 'count') else 'unknown'

        # サンプルデータを取得
        sample_response = db_client.client.table('Rawdata_FILE_AND_MAIL').select('workspace, doc_type').limit(10).execute()
        samples = sample_response.data if sample_response.data else []

        # get_workspace_hierarchy()を実行
        hierarchy = db_client.get_workspace_hierarchy()

        # 環境変数の確認（セキュリティのため一部のみ）
        supabase_url = os.getenv('SUPABASE_URL', 'NOT_SET')
        supabase_key_set = 'YES' if os.getenv('SUPABASE_KEY') else 'NO'

        return jsonify({
            'success': True,
            'database_info': {
                'total_documents': total_count,
                'sample_count': len(samples),
                'samples': samples[:5],  # 最初の5件のみ
                'workspace_count': len(hierarchy),
                'workspaces': list(hierarchy.keys()),
                'hierarchy_sample': {k: v for k, v in list(hierarchy.items())[:2]}  # 最初の2つのみ
            },
            'env_check': {
                'supabase_url_set': supabase_url[:30] + '...' if supabase_url != 'NOT_SET' else 'NOT_SET',
                'supabase_key_set': supabase_key_set
            }
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e),
            'error_type': type(e).__name__
        }), 500


@app.route('/api/workspaces', methods=['GET'])
def get_workspaces():
    """
    ワークスペース一覧を取得
    """
    try:
        from shared.common.database.client import DatabaseClient
        db = DatabaseClient()

        # ワークスペース一覧を取得
        query = db.client.table('Rawdata_FILE_AND_MAIL').select('workspace').execute()

        # ユニークなワークスペースを抽出
        workspaces = set()
        for row in query.data:
            workspace = row.get('workspace')
            if workspace:
                workspaces.add(workspace)

        # ソートしてリスト化
        workspace_list = sorted(list(workspaces))

        return jsonify({
            'success': True,
            'workspaces': workspace_list
        })

    except Exception as e:
        print(f"[ERROR] ワークスペース取得エラー: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


if __name__ == '__main__':
    # 開発環境での実行
    port = int(os.environ.get('PORT', 5001))
    app.run(host='0.0.0.0', port=port, debug=False)
```

### services\doc-search\cloudbuild.yaml

```yaml
steps:
  # Build the container image
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'build'
      - '-t'
      - 'asia-northeast1-docker.pkg.dev/$PROJECT_ID/cloud-run-repo/mail-doc-search-system:latest'
      - '-f'
      - 'services/doc-search/Dockerfile'
      - '.'
    dir: '.'

  # Push the container image to Artifact Registry
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - 'asia-northeast1-docker.pkg.dev/$PROJECT_ID/cloud-run-repo/mail-doc-search-system:latest'

  # Deploy container image to Cloud Run
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: gcloud
    args:
      - 'run'
      - 'deploy'
      - 'mail-doc-search-system'
      - '--image'
      - 'asia-northeast1-docker.pkg.dev/$PROJECT_ID/cloud-run-repo/mail-doc-search-system:latest'
      - '--region'
      - 'asia-northeast1'
      - '--platform'
      - 'managed'
      - '--allow-unauthenticated'
      - '--timeout'
      - '3600'
      - '--memory'
      - '4Gi'
      - '--cpu'
      - '2'

images:
  - 'asia-northeast1-docker.pkg.dev/$PROJECT_ID/cloud-run-repo/mail-doc-search-system:latest'

timeout: 1200s
```

### services\doc-search\deploy.sh

```sh
#!/bin/bash
  set -e
  PROJECT_ID="consummate-yew-479020-u2"
  SERVICE_NAME="mail-doc-search-system"
  REGION="asia-northeast1"
  cd ~/document-management-system
  docker build -t asia-northeast1-docker.pkg.dev/${PROJECT_ID}/cloud-run-source-deploy/${SERVICE_NAME}:latest -f services/doc-search/Dockerfile .
  docker push asia-northeast1-docker.pkg.dev/${PROJECT_ID}/cloud-run-source-deploy/${SERVICE_NAME}:latest
  gcloud run deploy ${SERVICE_NAME} --image asia-northeast1-docker.pkg.dev/${PROJECT_ID}/cloud-run-source-deploy/${SERVICE_NAME}:latest --region ${REGION} --memory 4Gi --cpu 2 --timeout 300 --allow-unauthenticated
  echo "✅ ${SERVICE_NAME} deployed!"
```

### services\doc-search\deploy_to_cloud_run.sh

```sh
#!/bin/bash
# Cloud Runへのデプロイスクリプト（環境変数設定含む）

set -e  # エラーが発生したら終了

echo "================================"
echo "Cloud Runへのデプロイを開始します"
echo "================================"

# スクリプトのディレクトリを取得
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
PROJECT_ROOT="$( cd "$SCRIPT_DIR/.." && pwd )"

# .envファイルから環境変数を読み込む
ENV_FILE="$PROJECT_ROOT/.env"
if [ ! -f "$ENV_FILE" ]; then
    echo "エラー: .envファイルが見つかりません ($ENV_FILE)"
    exit 1
fi

# .envファイルから必要な環境変数を抽出
echo "環境変数を読み込んでいます..."
set -a  # 自動的にexport
source "$ENV_FILE" 2>/dev/null || true  # エラーを無視
set +a

# 必須環境変数のチェック
if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_KEY" ]; then
    echo "エラー: SUPABASE_URLまたはSUPABASE_KEYが設定されていません"
    exit 1
fi

echo "✓ 環境変数の読み込み完了"

# Cloud Runにデプロイ
echo ""
echo "Cloud Runにデプロイしています..."
# プロジェクトルートに移動（Dockerfileがプロジェクトルートからのパスを使用するため）
cd "$PROJECT_ROOT"
gcloud run deploy mail-doc-search-system \
  --source . \
  --region asia-northeast1 \
  --allow-unauthenticated \
  --timeout 3600 \
  --memory 4Gi \
  --cpu 2 \
  --set-env-vars "SUPABASE_URL=$SUPABASE_URL" \
  --set-env-vars "SUPABASE_KEY=***REDACTED***" \
  --set-env-vars "GOOGLE_AI_API_KEY=***REDACTED***" \
  --set-env-vars "ANTHROPIC_API_KEY=***REDACTED***" \
  --set-env-vars "OPENAI_API_KEY=***REDACTED***" \
  --set-env-vars "LOG_LEVEL=${LOG_LEVEL:-INFO}" \
  --set-env-vars "RERANK_ENABLED=${RERANK_ENABLED:-true}"

echo ""
echo "================================"
echo "✓ デプロイが完了しました！"
echo "================================"
echo ""
echo "確認コマンド:"
echo "  curl https://mail-doc-search-system-983922127476.asia-northeast1.run.app/api/health"
echo "  curl https://mail-doc-search-system-983922127476.asia-northeast1.run.app/api/filters"
echo ""
```

### services\doc-search\Dockerfile

```text
# 1. Python のベースイメージを使う
FROM python:3.12-slim

# 2. 必要なシステムツール（Tesseract, Popplerなど）をインストール
# ここが Buildpacks ではできなかった部分です！
# 修正点: libgl1-mesa-glx は古いので libgl1 に変更しました
RUN apt-get update && apt-get install -y \
    tesseract-ocr \
    tesseract-ocr-jpn \
    libtesseract-dev \
    poppler-utils \
    libgl1 \
    && rm -rf /var/lib/apt/lists/*

# 3. 作業ディレクトリを設定
WORKDIR /app

# 4. プロジェクトルート全体をコピー（親ディレクトリから）
# ビルドコンテキストはプロジェクトルート (document_management_system/) です
# まずrequirements.txtだけコピーしてインストール（キャッシュ効率化）
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 5. 必要なモジュールとアプリケーションファイルをコピー
COPY services/doc-search/app.py .
COPY services/doc-search/templates/ ./templates/
# プロジェクトルートからの共通モジュール
COPY shared/common/ ./shared/common/
COPY shared/ai/ ./shared/ai/
# ドキュメント処理用のファイル
COPY scripts/processing/process_queued_documents.py .
COPY shared/pipeline/ ./shared/pipeline/

# Note: PlaywrightはローカルのGmail取り込みのみで使用。Cloud Runでは不要。

# 6. ポート環境変数を設定（念のため）
ENV PORT=8080

# 7. サーバーを起動（gunicornを使用）
CMD exec gunicorn --bind :$PORT --workers 1 --threads 8 --timeout 0 app:app
```

### services\doc-search\requirements.txt

```txt
google-api-python-client==2.108.0
google-auth==2.25.2
google-auth-oauthlib==1.2.0
google-auth-httplib2==0.2.0
google-generativeai==0.3.0

anthropic==0.39.0

openai==1.54.0

supabase==2.10.0

pdfplumber==0.10.3
python-docx==1.1.0
python-pptx==0.6.21
openpyxl==3.1.2
Pillow
pytesseract==0.3.10
pypdf==3.17.4
pdf2image==1.17.0
opencv-python-headless==4.10.0.84

numpy>=1.26.2,<2.0.0
pyarrow==22.0.0
pgvector==0.2.4

python-dotenv==1.0.0
loguru==0.7.2
pydantic
pydantic-settings
tenacity>=8.2.0
jsonschema>=4.20.0
pyyaml>=6.0

streamlit>=1.29.0,<2.0.0
pandas>=2.1.4,<3.0.0
streamlit-pdf-viewer

gunicorn==23.0.0
Flask==3.1.0
Flask-CORS==5.0.0
psutil==5.9.8
json_repair
beautifulsoup4==4.12.3
lxml>=5.1.0
# playwright==1.40.0  # Gmail取り込み専用（Cloud Runでは不要）

```

### services\doc-search\templates\index.html

```html
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>文書検索・質問回答システム</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }

        .header h1 {
            font-size: 2em;
            margin-bottom: 10px;
        }

        .header p {
            opacity: 0.9;
            font-size: 1em;
        }

        .main-content {
            padding: 30px;
        }

        .search-section {
            margin-bottom: 30px;
        }

        .input-group {
            display: flex;
            gap: 10px;
            margin-bottom: 15px;
        }

        #queryInput {
            flex: 1;
            padding: 15px;
            border: 2px solid #e0e0e0;
            border-radius: 8px;
            font-size: 16px;
            transition: border-color 0.3s;
        }

        #queryInput:focus {
            outline: none;
            border-color: #667eea;
        }

        .btn {
            padding: 15px 30px;
            border: none;
            border-radius: 8px;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
        }

        .btn-primary {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }

        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
        }

        .btn-primary:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }

        .btn-secondary {
            background: #f0f0f0;
            color: #333;
        }

        .btn-secondary:hover {
            background: #e0e0e0;
        }

        .btn-link {
            display: inline-block;
            padding: 8px 16px;
            background: #667eea;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-size: 14px;
            font-weight: 500;
            margin-top: 10px;
            transition: all 0.3s;
        }

        .btn-link:hover {
            background: #5568d3;
            transform: translateY(-1px);
            box-shadow: 0 3px 10px rgba(102, 126, 234, 0.3);
        }

        .options {
            display: flex;
            gap: 15px;
            align-items: center;
        }

        .options label {
            display: flex;
            align-items: center;
            gap: 5px;
            font-size: 14px;
        }

        .options input[type="number"] {
            width: 60px;
            padding: 5px;
            border: 1px solid #e0e0e0;
            border-radius: 4px;
        }

        .loading {
            display: none;
            text-align: center;
            padding: 20px;
            color: #667eea;
            font-size: 16px;
        }

        .loading.active {
            display: block;
        }

        .spinner {
            border: 3px solid #f3f3f3;
            border-top: 3px solid #667eea;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
            margin: 0 auto 10px;
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .results-section {
            display: none;
        }

        .results-section.active {
            display: block;
        }

        .section-title {
            font-size: 1.5em;
            color: #333;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid #667eea;
        }

        .answer-box {
            background: #f8f9ff;
            border-left: 4px solid #667eea;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            line-height: 1.6;
        }

        .answer-box .model-info {
            font-size: 12px;
            color: #999;
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #e0e0e0;
        }

        .document-list {
            display: grid;
            gap: 15px;
        }

        .document-card {
            background: #fff;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 15px;
            transition: all 0.3s;
        }

        .document-card:hover {
            border-color: #667eea;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .document-title {
            font-weight: 600;
            color: #333;
            margin-bottom: 8px;
            font-size: 1.1em;
        }

        .document-meta {
            display: flex;
            gap: 15px;
            font-size: 0.85em;
            color: #666;
            margin-bottom: 10px;
        }

        .document-content {
            color: #555;
            line-height: 1.5;
            font-size: 0.95em;
        }

        .similarity-badge {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 3px 8px;
            border-radius: 12px;
            font-size: 0.8em;
        }

        /* ✅ Classroom投稿専用スタイル */
        .classroom-post {
            background: #f8f9ff;
            border-left: 4px solid #1565c0;
            margin-top: 10px;
            padding: 12px;
            border-radius: 6px;
        }

        .classroom-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 8px;
            font-size: 0.85em;
            color: #666;
        }

        .classroom-sender {
            font-weight: 600;
            color: #1565c0;
        }

        .classroom-date {
            color: #999;
        }

        .classroom-body {
            color: #333;
            line-height: 1.6;
            margin-bottom: 10px;
            white-space: pre-wrap;
            font-size: 0.9em;
        }

        .classroom-attachments {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-top: 10px;
        }

        .attachment-link {
            display: inline-flex;
            align-items: center;
            gap: 5px;
            padding: 6px 12px;
            background: #1565c0;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85em;
            transition: all 0.3s;
        }

        .attachment-link:hover {
            background: #0d47a1;
            transform: translateY(-1px);
            box-shadow: 0 3px 8px rgba(66, 133, 244, 0.3);
        }

        .error-message {
            background: #fee;
            border-left: 4px solid #f44;
            padding: 15px;
            border-radius: 8px;
            color: #c00;
            margin-bottom: 20px;
        }

        .empty-state {
            text-align: center;
            padding: 40px;
            color: #999;
        }

        /* ✅ フィルターチップ */
        .filter-chips {
            display: flex;
            gap: 10px;
            align-items: center;
            flex-wrap: wrap;
            margin-bottom: 15px;
        }

        .chip {
            padding: 8px 15px;
            border: 1px solid #e0e0e0;
            border-radius: 20px;
            background: white;
            font-size: 14px;
            cursor: pointer;
            transition: all 0.3s;
            display: inline-flex;
            align-items: center;
            gap: 5px;
        }

        .chip-filter {
            background: #667eea;
            color: white;
            border-color: #667eea;
        }

        .chip-filter:hover {
            background: #5568d3;
            transform: translateY(-1px);
        }

        .chip-active {
            background: #f0f0f0;
            border-color: #667eea;
            color: #667eea;
            padding-right: 8px;
        }

        .chip-active .remove-chip {
            margin-left: 5px;
            font-size: 16px;
            font-weight: bold;
            cursor: pointer;
        }

        .active-filters {
            display: flex;
            gap: 8px;
            flex-wrap: wrap;
        }

        /* ✅ Bottom Sheet */
        .bottom-sheet-overlay {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: rgba(0, 0, 0, 0.5);
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s;
            z-index: 999;
        }

        .bottom-sheet-overlay.active {
            opacity: 1;
            visibility: visible;
        }

        .bottom-sheet {
            position: fixed;
            bottom: -100%;
            left: 0;
            right: 0;
            background: white;
            border-radius: 20px 20px 0 0;
            box-shadow: 0 -5px 20px rgba(0, 0, 0, 0.3);
            transition: bottom 0.3s ease;
            z-index: 1000;
            max-height: 80vh;
            display: flex;
            flex-direction: column;
        }

        .bottom-sheet.active {
            bottom: 0;
        }

        .sheet-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 20px 20px 15px;
            border-bottom: 1px solid #e0e0e0;
        }

        .sheet-header h3 {
            margin: 0;
            font-size: 1.2em;
            color: #333;
        }

        .btn-close {
            background: none;
            border: none;
            font-size: 28px;
            color: #999;
            cursor: pointer;
            padding: 0;
            width: 30px;
            height: 30px;
            line-height: 1;
        }

        .sheet-content {
            flex: 1;
            overflow-y: auto;
            padding: 20px;
        }

        .filter-section {
            margin-bottom: 25px;
        }

        .filter-section-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 10px;
        }

        .filter-section h4 {
            margin: 0;
            font-size: 1em;
            color: #555;
        }

        .btn-clear {
            background: none;
            border: none;
            color: #667eea;
            font-size: 0.85em;
            cursor: pointer;
            text-decoration: underline;
        }

        .filter-checkboxes {
            display: grid;
            gap: 8px;
        }

        .filter-checkbox {
            display: flex;
            align-items: center;
            gap: 10px;
            padding: 10px;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.2s;
        }

        .filter-checkbox:hover {
            background: #f8f9ff;
            border-color: #667eea;
        }

        .filter-checkbox input[type="checkbox"] {
            width: 18px;
            height: 18px;
            cursor: pointer;
        }

        .filter-checkbox label {
            flex: 1;
            cursor: pointer;
            margin: 0;
        }

        /* ✅ 階層構造（アコーディオン）用CSS */
        .hierarchy-container {
            display: flex;
            flex-direction: column;
            gap: 10px;
        }

        .workspace-item {
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            overflow: hidden;
        }

        .workspace-header {
            display: flex;
            align-items: center;
            gap: 10px;
            padding: 12px;
            background: #f8f9ff;
            cursor: pointer;
            transition: background 0.2s;
        }

        .workspace-header:hover {
            background: #eef1ff;
        }

        .workspace-toggle {
            width: 20px;
            height: 20px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 14px;
            transition: transform 0.3s;
            color: #667eea;
        }

        .workspace-toggle.expanded {
            transform: rotate(90deg);
        }

        .workspace-checkbox {
            width: 18px;
            height: 18px;
            cursor: pointer;
        }

        .workspace-label {
            flex: 1;
            font-weight: 600;
            color: #333;
            cursor: pointer;
        }

        .workspace-count {
            font-size: 0.85em;
            color: #999;
        }

        .workspace-children {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease;
        }

        .workspace-children.expanded {
            max-height: 1000px; /* 十分に大きな値 */
        }

        .doc-type-item {
            display: flex;
            align-items: center;
            gap: 10px;
            padding: 10px 12px 10px 42px; /* 左にインデント */
            border-top: 1px solid #f0f0f0;
            cursor: pointer;
            transition: background 0.2s;
        }

        .doc-type-item:hover {
            background: #f8f9ff;
        }

        .doc-type-checkbox {
            width: 16px;
            height: 16px;
            cursor: pointer;
        }

        .doc-type-label {
            flex: 1;
            cursor: pointer;
            font-size: 0.95em;
            color: #555;
        }

        .btn-apply {
            margin: 15px 20px 20px;
            width: calc(100% - 40px);
        }

        /* スマホ対応 */
        @media (max-width: 768px) {
            .bottom-sheet {
                max-height: 90vh;
            }

            .filter-chips {
                font-size: 13px;
            }

            .chip {
                padding: 6px 12px;
                font-size: 13px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>文書検索・質問回答システム</h1>
            <p>ベクトル検索とAIで、文書から必要な情報を素早く取得</p>
        </div>

        <div class="main-content">
            <!-- 検索セクション -->
            <div class="search-section">
                <div class="input-group">
                    <input
                        type="text"
                        id="queryInput"
                        placeholder="質問を入力してください（例: プロジェクトの納期はいつですか？）"
                        onkeypress="handleKeyPress(event)"
                    >
                    <button class="btn btn-primary" onclick="performSearch()" id="searchBtn">
                        検索・回答
                    </button>
                </div>

                <!-- ✅ フィルターチップ -->
                <div class="filter-chips" id="filterChips">
                    <button class="chip chip-filter" onclick="openFilterSheet()">
                        ⚙️ 絞り込み
                    </button>
                    <div id="activeFilters" class="active-filters"></div>
                </div>

                <div class="options">
                    <label>
                        検索件数:
                        <input type="number" id="limitInput" value="50" min="1" max="50">
                    </label>
                    <label>
                        構成フロー:
                        <select id="flow-select" style="padding: 5px; border: 1px solid #e0e0e0; border-radius: 4px;">
                            <option value="flash-x1" selected>Flash×1 (標準・高速)</option>
                            <option value="lite-lite-pro">Lite→Lite→Pro (コスパ最強)</option>
                            <option value="lite-flash-pro">Lite→Flash→Pro (黄金比)</option>
                            <option value="flash-flash-pro">Flash→Flash→Pro (3回ループの王)</option>
                            <option value="lite-flash-flash-pro">Lite→Flash→Flash→Pro (賢い節約術)</option>
                            <option value="flash-flash-flash-pro">Flash→Flash→Flash→Pro (Deep Research推奨)</option>
                            <option value="lite-lite-flash-flash">Lite→Lite→Flash×Flash (粘りの凡人)</option>
                            <option value="flash-flash-flash">Flash→Flash→Flash (ザ・標準)</option>
                            <option value="lite-lite-lite-flash">Lite→Lite→Lite→Flash (人海戦術)</option>
                            <option value="flash-flash-flash-flash">Flash→Flash→Flash→Flash (優等生の限界)</option>
                            <option value="lite-lite-pro">Lite→Lite→Pro (コスパ最強)</option>
                            <option value="lite-flash-pro">Lite→Flash→Pro (黄金比・標準)</option>
                            <option value="lite-lite-flash-pro">Lite→Lite→Flash→Pro (泥臭い名探偵)</option>
                            <option value="lite-lite-lite-pro">Lite→Lite→Lite→Pro (質より量の極み)</option>
                            <option value="flash-pro-pro">Flash→Pro→Pro (精鋭部隊)</option>
                            <option value="lite-pro-pro">Lite→Pro→Pro (一点突破・改)</option>
                            <option value="lite-lite-pro-pro">Lite→Lite→Pro→Pro (素材重視のプロ)</option>
                            <option value="lite-flash-pro-pro">Lite→Flash→Pro→Pro (超・黄金比)</option>
                            <option value="flash-flash-pro-pro">Flash→Flash→Pro→Pro (編集長決済)</option>
                            <option value="pro-pro-pro">Pro→Pro→Pro (富豪の鉄板)</option>
                            <option value="lite-pro-pro-pro">Lite→Pro→Pro→Pro (成り上がり)</option>
                            <option value="flash-pro-pro-pro">Flash→Pro→Pro→Pro (天才集団)</option>
                            <option value="pro-pro-pro-pro">Pro→Pro→Pro→Pro (全知全能)</option>
                        </select>
                    </label>
                </div>
            </div>

            <!-- ✅ Bottom Sheet（フィルター選択・階層構造） -->
            <div class="bottom-sheet-overlay" id="filterSheetOverlay" onclick="closeFilterSheet()"></div>
            <div class="bottom-sheet" id="filterSheet">
                <div class="sheet-header">
                    <h3>検索条件を選択</h3>
                    <button class="btn-close" onclick="closeFilterSheet()">×</button>
                </div>
                <div class="sheet-content">
                    <div class="filter-section">
                        <div class="filter-section-header">
                            <h4>ワークスペース別フィルタ</h4>
                            <button class="btn-clear" onclick="clearAllFilters()">すべて解除</button>
                        </div>
                        <!-- 階層構造（アコーディオン）が動的に生成される -->
                        <div id="hierarchyFilters" class="hierarchy-container"></div>
                    </div>
                </div>
                <button class="btn btn-primary btn-apply" onclick="applyFilters()">
                    決定して検索
                </button>
            </div>

            <!-- ローディング -->
            <div class="loading" id="loading">
                <div class="spinner"></div>
                <div>処理中...</div>
            </div>

            <!-- エラーメッセージ -->
            <div id="errorContainer"></div>

            <!-- 結果セクション -->
            <div class="results-section" id="resultsSection">
                <!-- 回答 -->
                <div id="answerSection" style="display: none;">
                    <h2 class="section-title">回答</h2>
                    <div class="answer-box" id="answerBox"></div>
                </div>

                <!-- 検索結果 -->
                <div id="documentsSection" style="display: none;">
                    <h2 class="section-title">関連文書</h2>
                    <div class="document-list" id="documentList"></div>
                </div>
            </div>
        </div>
    </div>

    <script>
        let currentDocuments = [];
        let selectedWorkspaces = [];
        let selectedDocTypes = [];
        let hierarchyData = []; // {name: 'workspace', doc_types: [...]}

        // ✅ ページ読み込み時に階層構造を取得
        window.onload = async function() {
            try {
                const response = await fetch('/api/filters');
                const data = await response.json();

                if (data.success) {
                    hierarchyData = data.hierarchy || [];
                    renderHierarchyFilters();
                    console.log(`[フィルタ読み込み] ${hierarchyData.length} workspaces（階層構造）`);
                }
            } catch (error) {
                console.error('フィルタ取得エラー:', error);
            }
        };

        function renderHierarchyFilters() {
            const container = document.getElementById('hierarchyFilters');

            container.innerHTML = hierarchyData.map(workspace => {
                const wsId = `ws-${workspace.name}`;
                const docTypeCount = workspace.doc_types.length;

                return `
                    <div class="workspace-item">
                        <div class="workspace-header" onclick="toggleWorkspaceExpansion('${workspace.name}')">
                            <span class="workspace-toggle" id="toggle-${wsId}">▶</span>
                            <input type="checkbox"
                                   class="workspace-checkbox"
                                   id="${wsId}"
                                   value="${workspace.name}"
                                   onclick="event.stopPropagation()"
                                   onchange="onWorkspaceChange('${workspace.name}')">
                            <label class="workspace-label" for="${wsId}" onclick="event.stopPropagation()">
                                ${escapeHtml(workspace.name)}
                            </label>
                            <span class="workspace-count">${docTypeCount}件</span>
                        </div>
                        <div class="workspace-children" id="children-${wsId}">
                            ${workspace.doc_types.map(dt => {
                                const dtId = `dt-${workspace.name}-${dt}`;
                                return `
                                    <div class="doc-type-item" onclick="toggleCheckbox('${dtId}')">
                                        <input type="checkbox"
                                               class="doc-type-checkbox"
                                               id="${dtId}"
                                               value="${dt}"
                                               data-workspace="${workspace.name}"
                                               onclick="event.stopPropagation()"
                                               onchange="onDocTypeChange('${workspace.name}')">
                                        <label class="doc-type-label" for="${dtId}" onclick="event.stopPropagation()">
                                            ${escapeHtml(dt)}
                                        </label>
                                    </div>
                                `;
                            }).join('')}
                        </div>
                    </div>
                `;
            }).join('');
        }

        function toggleCheckbox(id) {
            const checkbox = document.getElementById(id);
            if (checkbox) {
                checkbox.checked = !checkbox.checked;
                // 親のworkspaceを取得して変更イベントをトリガー
                const workspace = checkbox.dataset.workspace;
                if (workspace) {
                    onDocTypeChange(workspace);
                }
            }
        }

        function toggleWorkspaceExpansion(workspaceName) {
            const wsId = `ws-${workspaceName}`;
            const toggle = document.getElementById(`toggle-${wsId}`);
            const children = document.getElementById(`children-${wsId}`);

            if (children.classList.contains('expanded')) {
                children.classList.remove('expanded');
                toggle.classList.remove('expanded');
            } else {
                children.classList.add('expanded');
                toggle.classList.add('expanded');
            }
        }

        // ✅ 親チェックボックスの変更時：子を連動
        function onWorkspaceChange(workspaceName) {
            const wsId = `ws-${workspaceName}`;
            const wsCheckbox = document.getElementById(wsId);
            const isChecked = wsCheckbox.checked;

            // 配下の子（doc_type）をすべて同じ状態にする
            const workspace = hierarchyData.find(w => w.name === workspaceName);
            if (workspace) {
                workspace.doc_types.forEach(dt => {
                    const dtCheckbox = document.getElementById(`dt-${workspaceName}-${dt}`);
                    if (dtCheckbox) {
                        dtCheckbox.checked = isChecked;
                    }
                });
            }

            console.log(`[親チェック] ${workspaceName}: ${isChecked ? 'ON' : 'OFF'} → 子を連動`);
        }

        // ✅ 子チェックボックスの変更時：親の状態を更新
        function onDocTypeChange(workspaceName) {
            const wsId = `ws-${workspaceName}`;
            const wsCheckbox = document.getElementById(wsId);

            // 配下の子の状態を確認
            const workspace = hierarchyData.find(w => w.name === workspaceName);
            if (!workspace) return;

            let allChecked = true;
            let noneChecked = true;

            workspace.doc_types.forEach(dt => {
                const dtCheckbox = document.getElementById(`dt-${workspaceName}-${dt}`);
                if (dtCheckbox) {
                    if (dtCheckbox.checked) {
                        noneChecked = false;
                    } else {
                        allChecked = false;
                    }
                }
            });

            // 親の状態を更新
            if (allChecked) {
                wsCheckbox.checked = true;
                wsCheckbox.indeterminate = false;
            } else if (noneChecked) {
                wsCheckbox.checked = false;
                wsCheckbox.indeterminate = false;
            } else {
                // 一部だけチェックされている場合はindeterminate
                wsCheckbox.checked = false;
                wsCheckbox.indeterminate = true;
            }
        }

        function openFilterSheet() {
            document.getElementById('filterSheet').classList.add('active');
            document.getElementById('filterSheetOverlay').classList.add('active');
            document.body.style.overflow = 'hidden'; // スクロール無効化
        }

        function closeFilterSheet() {
            document.getElementById('filterSheet').classList.remove('active');
            document.getElementById('filterSheetOverlay').classList.remove('active');
            document.body.style.overflow = ''; // スクロール有効化
        }

        function clearAllFilters() {
            hierarchyData.forEach(workspace => {
                // 親チェックボックスをクリア
                const wsCheckbox = document.getElementById(`ws-${workspace.name}`);
                if (wsCheckbox) {
                    wsCheckbox.checked = false;
                    wsCheckbox.indeterminate = false;
                }

                // 子チェックボックスをクリア
                workspace.doc_types.forEach(dt => {
                    const dtCheckbox = document.getElementById(`dt-${workspace.name}-${dt}`);
                    if (dtCheckbox) dtCheckbox.checked = false;
                });
            });

            console.log('[すべて解除]');
        }

        function applyFilters() {
            // チェックされたworkspaceとdoc_typeを収集
            selectedWorkspaces = [];
            selectedDocTypes = [];

            hierarchyData.forEach(workspace => {
                const wsName = workspace.name;

                // 子（doc_type）のチェック状態を確認
                workspace.doc_types.forEach(dt => {
                    const dtCheckbox = document.getElementById(`dt-${wsName}-${dt}`);
                    if (dtCheckbox && dtCheckbox.checked) {
                        // workspaceを追加（重複回避）
                        if (!selectedWorkspaces.includes(wsName)) {
                            selectedWorkspaces.push(wsName);
                        }
                        // doc_typeを追加
                        selectedDocTypes.push(dt);
                    }
                });
            });

            console.log('[フィルタ適用]', { selectedWorkspaces, selectedDocTypes });

            // フィルタチップを更新
            updateFilterChips();

            // Bottom Sheetを閉じる
            closeFilterSheet();

            // 検索を実行（クエリがある場合のみ）
            const query = document.getElementById('queryInput').value.trim();
            if (query) {
                performSearch();
            }
        }

        function updateFilterChips() {
            const container = document.getElementById('activeFilters');

            // workspace別にdoc_typeをグループ化して表示
            const chipsByWorkspace = {};

            hierarchyData.forEach(workspace => {
                const wsName = workspace.name;
                const checkedTypes = [];

                workspace.doc_types.forEach(dt => {
                    const dtCheckbox = document.getElementById(`dt-${wsName}-${dt}`);
                    if (dtCheckbox && dtCheckbox.checked) {
                        checkedTypes.push(dt);
                    }
                });

                if (checkedTypes.length > 0) {
                    chipsByWorkspace[wsName] = checkedTypes;
                }
            });

            if (Object.keys(chipsByWorkspace).length === 0) {
                container.innerHTML = '';
                return;
            }

            // チップを生成
            const chips = [];
            for (const [wsName, types] of Object.entries(chipsByWorkspace)) {
                types.forEach(dt => {
                    chips.push(`
                        <span class="chip chip-active">
                            ${escapeHtml(wsName)}: ${escapeHtml(dt)}
                            <span class="remove-chip" onclick="removeFilter('${escapeHtml(wsName)}', '${escapeHtml(dt)}')">×</span>
                        </span>
                    `);
                });
            }

            container.innerHTML = chips.join('');
        }

        function removeFilter(workspaceName, docType) {
            // 該当するdoc_typeのチェックを外す
            const dtCheckbox = document.getElementById(`dt-${workspaceName}-${docType}`);
            if (dtCheckbox) {
                dtCheckbox.checked = false;
                // 親の状態を更新
                onDocTypeChange(workspaceName);
            }

            // フィルタチップを更新
            updateFilterChips();

            // 検索を再実行
            const query = document.getElementById('queryInput').value.trim();
            if (query) {
                performSearch();
            }
        }

        function handleKeyPress(event) {
            if (event.key === 'Enter') {
                performSearch();
            }
        }

        async function performSearch() {
            const query = document.getElementById('queryInput').value.trim();
            const limit = parseInt(document.getElementById('limitInput').value);

            if (!query) {
                showError('質問を入力してください');
                return;
            }

            // UI状態更新
            setLoading(true);
            clearError();
            hideResults();

            try {
                // ステップ1: ベクトル検索（✅ フィルタ配列を送信）
                const searchPayload = {
                    query,
                    limit,
                    workspaces: selectedWorkspaces,
                    doc_types: selectedDocTypes
                };

                console.log('[検索実行]', searchPayload);

                const searchResponse = await fetch('/api/search', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify(searchPayload)
                });

                const searchData = await searchResponse.json();

                if (!searchData.success) {
                    throw new Error(searchData.error || '検索に失敗しました');
                }

                currentDocuments = searchData.results;

                // ステップ2: 回答生成
                const selectedFlow = document.getElementById('flow-select').value;
                const answerResponse = await fetch('/api/answer', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        query,
                        documents: currentDocuments,
                        flow: selectedFlow
                    })
                });

                const answerData = await answerResponse.json();

                if (!answerData.success) {
                    throw new Error(answerData.error || '回答生成に失敗しました');
                }

                // 結果表示
                displayAnswer(answerData);
                displayDocuments(currentDocuments);
                showResults();

            } catch (error) {
                showError(error.message);
            } finally {
                setLoading(false);
            }
        }

        function displayAnswer(data) {
            const answerBox = document.getElementById('answerBox');
            const answerSection = document.getElementById('answerSection');

            const modelInfo = `モデル: ${data.provider} / ${data.model}`;

            answerBox.innerHTML = `
                <div>${data.answer.replace(/\n/g, '<br>')}</div>
                <div class="model-info">${modelInfo}</div>
            `;

            answerSection.style.display = 'block';
        }

        function displayDocuments(documents) {
            const documentList = document.getElementById('documentList');
            const documentsSection = document.getElementById('documentsSection');

            if (documents.length === 0) {
                documentList.innerHTML = '<div class="empty-state">関連する文書が見つかりませんでした</div>';
            } else {
                documentList.innerHTML = documents.map(doc => {
                    try {
                        // ✅ Classroom投稿かどうかを判定
                        const sourceType = doc.source_type || '';
                        const metadata = doc.metadata || {};

                        // 複数の条件でClassroom投稿を判定
                        const hasClassroomSubject = doc.display_subject && doc.display_subject.trim() !== '';
                        const hasClassroomSender = doc.display_sender && doc.display_sender.trim() !== '';
                        const hasClassroomMetadata = metadata.course_name || metadata.course_id;
                        const isClassroomRoute = doc.ingestion_route === 'classroom';

                        const isClassroom = sourceType === 'classroom' ||
                                          sourceType === 'classroom_text' ||
                                          hasClassroomSubject ||
                                          hasClassroomSender ||
                                          hasClassroomMetadata ||
                                          isClassroomRoute;

                        if (isClassroom) {
                            return renderClassroomPost(doc);
                        } else {
                            return renderRegularDocument(doc);
                        }
                    } catch (error) {
                        console.error('Document rendering error:', error, doc);
                        return renderRegularDocument(doc);
                    }
                }).join('');
            }

            documentsSection.style.display = 'block';
        }

        function renderClassroomPost(doc) {
            // デバッグ: どのデータが来ているか確認
            console.log('[DEBUG] Classroom投稿データ:', {
                display_subject: doc.display_subject,
                display_post_text: doc.display_post_text,
                chunk_content: doc.chunk_content,
                file_name: doc.file_name
            });

            // ✅ タイトルと本文を正しく取得
            // display_subject: タイトル（課題/資料名、またはお知らせの件名）
            // display_post_text: 投稿本文（classroom_textの場合のみ）
            // full_text: ドキュメント本体（PDFの内容など）

            // タイトルは最初の50文字に制限（長すぎる場合）
            let title = doc.display_subject || '';
            if (title && title.length > 50) {
                title = title.substring(0, 50) + '...';
            }

            // 投稿本文はdisplay_post_textから取得、なければ検索でヒットしたチャンクを表示
            const bodyText = (doc.display_post_text && doc.display_post_text.trim())
                ? doc.display_post_text
                : (doc.chunk_content || '');

            // ✅ 送信者情報（display_senderを優先）
            const metadata = doc.metadata || {};
            const sender = doc.display_sender || metadata.author_name || metadata.sender || metadata.sender_name || '不明';

            // ✅ 送信日時（display_sent_atを優先）
            const dateStr = doc.display_sent_at || doc.created_at || metadata.created_time || '';
            const formattedDate = dateStr ? formatDate(dateStr) : '日時不明';

            // ✅ 投稿種別（display_type）
            const postType = doc.display_type || metadata.post_type || 'Classroom';

            // 添付ファイル（source_urlまたはmetadataのmaterialsから）
            const attachments = [];

            // source_urlがある場合（Driveファイルへのリンク）
            if (doc.source_url) {
                const fileName = doc.file_name || 'ファイル';
                attachments.push({
                    url: doc.source_url,
                    name: fileName,
                    type: 'drive'
                });
            }

            // metadataに追加の添付ファイル情報がある場合
            if (metadata.materials && Array.isArray(metadata.materials)) {
                metadata.materials.forEach(material => {
                    if (material.driveFile && material.driveFile.url) {
                        attachments.push({
                            url: material.driveFile.url,
                            name: material.driveFile.title || 'ファイル',
                            type: 'material'
                        });
                    }
                });
            }

            // 添付ファイルのHTML
            const attachmentsHtml = attachments.length > 0 ? `
                <div class="classroom-attachments">
                    ${attachments.map(att => `
                        <a href="${escapeHtml(att.url)}" target="_blank" class="attachment-link">
                            📎 ${escapeHtml(att.name)}
                        </a>
                    `).join('')}
                </div>
            ` : '';

            return `
                <div class="document-card">
                    ${title ? `<div class="document-title">📘 ${escapeHtml(title)}</div>` : ''}
                    <div class="document-meta">
                        <span class="similarity-badge">類似度: ${(doc.similarity || 0).toFixed(2)}</span>
                        <span style="color: #1565c0; font-weight: 500;">📚 ${escapeHtml(postType)}</span>
                    </div>
                    <div class="classroom-post">
                        ${(sender && sender !== '不明') || formattedDate ? `
                        <div class="classroom-header">
                            ${(sender && sender !== '不明') ? `<span class="classroom-sender">👤 ${escapeHtml(sender)}</span>` : ''}
                            ${formattedDate ? `<span class="classroom-date">🕒 ${escapeHtml(formattedDate)}</span>` : ''}
                        </div>` : ''}
                        ${bodyText ? `<div class="classroom-body">${escapeHtml(bodyText)}</div>` : ''}
                        ${attachmentsHtml}
                    </div>
                </div>
            `;
        }

        function renderRegularDocument(doc) {
            // ✅ タイトル：summaryの最初の100文字を使用（ファイル名は使わない）
            let title = 'ドキュメント';
            if (doc.summary && doc.summary.trim() !== '') {
                // summaryの最初の100文字をタイトルに使用
                title = doc.summary.substring(0, 100);
                if (doc.summary.length > 100) {
                    title += '...';
                }
            }

            // ✅ 要約があればそれを優先、なければ本文をトリミング
            const displayText = doc.summary || truncateText(doc.content || '', 200);

            // ✅ source_urlがあればリンクボタンを表示（ファイル名をリンクテキストに使用）
            const sourceUrl = doc.source_url || '';
            const fileName = doc.file_name || 'ファイルを開く';
            const linkButton = sourceUrl ?
                `<a href="${escapeHtml(sourceUrl)}" target="_blank" class="btn-link">📄 ${escapeHtml(fileName)}</a>` :
                '';

            return `
                <div class="document-card">
                    <div class="document-title">${escapeHtml(title)}</div>
                    <div class="document-meta">
                        <span class="similarity-badge">類似度: ${(doc.similarity || 0).toFixed(2)}</span>
                    </div>
                    <div class="document-content">
                        ${escapeHtml(displayText)}
                    </div>
                    ${linkButton}
                </div>
            `;
        }

        function formatDate(dateStr) {
            try {
                const date = new Date(dateStr);
                const year = date.getFullYear();
                const month = String(date.getMonth() + 1).padStart(2, '0');
                const day = String(date.getDate()).padStart(2, '0');
                const hours = String(date.getHours()).padStart(2, '0');
                const minutes = String(date.getMinutes()).padStart(2, '0');
                return `${year}/${month}/${day} ${hours}:${minutes}`;
            } catch (e) {
                return dateStr;
            }
        }

        function showResults() {
            document.getElementById('resultsSection').classList.add('active');
        }

        function hideResults() {
            document.getElementById('resultsSection').classList.remove('active');
            document.getElementById('answerSection').style.display = 'none';
            document.getElementById('documentsSection').style.display = 'none';
        }

        function setLoading(isLoading) {
            const loading = document.getElementById('loading');
            const searchBtn = document.getElementById('searchBtn');

            if (isLoading) {
                loading.classList.add('active');
                searchBtn.disabled = true;
            } else {
                loading.classList.remove('active');
                searchBtn.disabled = false;
            }
        }

        function showError(message) {
            const errorContainer = document.getElementById('errorContainer');
            errorContainer.innerHTML = `<div class="error-message">${escapeHtml(message)}</div>`;
        }

        function clearError() {
            document.getElementById('errorContainer').innerHTML = '';
        }

        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }

        function truncateText(text, maxLength) {
            if (text.length <= maxLength) return text;
            return text.substring(0, maxLength) + '...';
        }
    </script>
</body>
</html>
```

### services\netsuper-search\app.py

```py
"""
3ネットスーパー横断商品検索アプリ

楽天西友、東急ストア、ダイエーの商品を横断検索
ベクトル検索で意味的に類似した商品を検索
安い順に表示
"""

import streamlit as st
import os
from supabase import create_client
from openai import OpenAI

# ページ設定
st.set_page_config(
    page_title="ネットスーパー横断検索",
    page_icon="🛒",
    layout="wide"
)

# Supabase接続
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = ***REDACTED***"SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    st.error("環境変数 SUPABASE_URL と SUPABASE_KEY を設定してください")
    st.stop()

db = create_client(SUPABASE_URL, SUPABASE_KEY)

# OpenAI接続
OPENAI_API_KEY = ***REDACTED***"OPENAI_API_KEY")

if not OPENAI_API_KEY:
    st.error("環境変数 OPENAI_API_KEY を設定してください")
    st.stop()

openai_client = OpenAI(api_key=***REDACTED***

# タイトル
st.title("🛒 ネットスーパー横断検索")
st.markdown("**楽天西友・東急ストア・ダイエー**の商品を一括検索！類似度の高い順に表示します")

# 検索欄
st.subheader("🔍 商品を検索")
col1, col2 = st.columns([4, 1])
with col1:
    search_input = st.text_input("商品名", placeholder="例: 牛乳、卵、パン", label_visibility="collapsed")
with col2:
    search_button = st.button("検索", type="primary", use_container_width=True)

# ボタンクリック時のみ、その場の入力値で検索（キャッシュ一切なし）
search_query = None
if search_button and search_input:
    search_query = search_input
    st.query_params["q"] = search_input

def generate_query_embedding(query: str) -> list:
    """検索クエリをベクトル化"""
    response = openai_client.embeddings.create(
        model="text-embedding-3-small",
        input=query
    )
    return response.data[0].embedding


if search_query:
    # 複数キーワード検索対応
    keywords = search_query.split()

    # ベクトル検索
    try:
        with st.spinner("検索中..."):
            # 各キーワードで個別に検索してスコアを合算
            all_results = {}  # product_id -> {product_data, total_score}

            for keyword in keywords:
                # 各キーワードをベクトル化
                query_embedding = generate_query_embedding(keyword)
                embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'

                # ハイブリッド検索
                result = db.rpc('hybrid_search', {
                    'query_embedding': embedding_str,
                    'query_text': keyword,
                    'match_count': 200
                }).execute()

                # 結果を集計
                for product in result.data:
                    product_id = product['id']
                    score = float(product.get('final_score', 0))

                    if product_id in all_results:
                        # 既存の商品：スコアを加算
                        all_results[product_id]['total_score'] += score
                    else:
                        # 新規の商品：データとスコアを保存
                        product['total_score'] = score
                        all_results[product_id] = product

            # 全キーワードが商品名に含まれる場合、大幅ボーナス
            if len(keywords) > 1:
                for product in all_results.values():
                    product_name_lower = product.get('product_name', '').lower()
                    all_match = all(kw.lower() in product_name_lower for kw in keywords)
                    if all_match:
                        product['total_score'] += 0.5  # 大幅ボーナス

            # 辞書から商品リストに変換
            products = list(all_results.values())

            # 合算スコアでソート（関連度の高い順）
            products.sort(key=lambda x: float(x.get('total_score', 0)), reverse=True)

        # 上位20件を取得
        top_products = products[:20]

        # 上位20件を価格の安い順に並べ替え
        display_products = sorted(
            top_products,
            key=lambda x: float(x.get('current_price_tax_included') or 999999)
        )

        if display_products:
            st.success(f"✅ {len(display_products)}件の商品を表示中（検索結果: {len(products)}件）")

            # 商品一覧表示
            for i, product in enumerate(display_products, 1):
                # コンテナキー（商品IDとインデックスのみ、キャッシュなし）
                product_id = product.get('id', i)
                with st.container(key=f"p_{product_id}_{i}"):
                    col1, col2 = st.columns([1, 4])

                    with col1:
                        # 商品画像
                        if product.get('image_url'):
                            st.image(product['image_url'], width=150)
                        else:
                            st.image("https://via.placeholder.com/150?text=No+Image", width=150)

                    with col2:
                        # 商品リンク（metadataから取得）
                        metadata = product.get('metadata', {})
                        product_url = None
                        if isinstance(metadata, dict):
                            product_url = metadata.get('raw_data', {}).get('url')

                        # 商品名（URLがある場合はリンク化）
                        product_name = product['product_name']
                        if product_url:
                            st.markdown(f"### {i}. [{product_name}]({product_url}) 🔗", unsafe_allow_html=True)
                        else:
                            st.markdown(f"### {i}. {product_name}")

                        # 価格（税込と本体を並記）
                        price_tax_included = product.get('current_price_tax_included', 0)
                        price_base = product.get('current_price', 0)
                        if price_base and price_base != price_tax_included:
                            st.markdown(f"## ¥{price_tax_included:,.0f} <small style='font-size:0.6em; color:#666;'>（本体 ¥{price_base:,.0f}）</small>", unsafe_allow_html=True)
                        else:
                            st.markdown(f"## ¥{price_tax_included:,.0f}")

                        # 店舗名
                        organization = product.get('organization', '不明')
                        if organization == '楽天西友ネットスーパー':
                            st.markdown(f"🏪 **{organization}** 🟢")
                        elif organization == '東急ストア ネットスーパー':
                            st.markdown(f"🏪 **{organization}** 🔵")
                        elif organization == 'ダイエーネットスーパー':
                            st.markdown(f"🏪 **{organization}** 🔴")
                        else:
                            st.markdown(f"🏪 **{organization}**")

                        # 商品ページへのボタン（URLがある場合）
                        if product_url:
                            st.markdown(f"""
                            <a href="{product_url}" target="_blank" style="
                                display: inline-block;
                                padding: 0.5em 1em;
                                background-color: #FF4B4B;
                                color: white;
                                text-decoration: none;
                                border-radius: 5px;
                                font-weight: bold;
                                margin-top: 0.5em;
                            ">🛒 商品ページで購入</a>
                            """, unsafe_allow_html=True)

                        # 検索スコア（複数キーワードの場合は合算スコア）
                        score = product.get('total_score') or product.get('final_score', 0)
                        if score:
                            st.caption(f"スコア: {score:.3f}")

                    st.divider()
        else:
            st.warning(f"「{search_query}」に該当する商品が見つかりませんでした")
            st.info("💡 ヒント: 別のキーワードを試してみてください")

    except Exception as e:
        st.error(f"❌ 検索エラー: {e}")
        st.exception(e)

else:
    # 初期画面
    st.info("👆 上の検索欄に商品名を入力してください")

    # サンプル検索
    st.markdown("### 💡 試してみる")
    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("🥛 牛乳"):
            st.query_params["q"] = "牛乳"
            st.rerun()

    with col2:
        if st.button("🥚 卵"):
            st.query_params["q"] = "卵"
            st.rerun()

    with col3:
        if st.button("🍞 パン"):
            st.query_params["q"] = "パン"
            st.rerun()

# フッター
st.markdown("---")
st.markdown("**対象ストア:** 楽天西友ネットスーパー / 東急ストア ネットスーパー / ダイエーネットスーパー")
```

### services\netsuper-search\deploy.ps1

```ps1
# ネットスーパー横断検索アプリをCloud Runにデプロイ (PowerShell版)

$ErrorActionPreference = "Stop"

# プロジェクトルートの.envファイルから必要な環境変数を読み込む
$envPath = "..\\.env"
if (Test-Path $envPath) {
    Get-Content $envPath | ForEach-Object {
        if ($_ -match '^SUPABASE_URL\s*=\s*(.+)$') {
            $env:SUPABASE_URL = $matches[1].Trim()
        }
        if ($_ -match '^SUPABASE_KEY\s*=\s*(.+)$') {
            $env:SUPABASE_KEY = ***REDACTED***
        }
        if ($_ -match '^OPENAI_API_KEY\s*=\s*(.+)$') {
            $env:OPENAI_API_KEY = ***REDACTED***
        }
    }
} else {
    Write-Host "エラー: .env ファイルが見つかりません" -ForegroundColor Red
    exit 1
}

# 環境変数の確認（デバッグ用）
Write-Host "SUPABASE_URL長: $($env:SUPABASE_URL.Length)文字"
Write-Host "SUPABASE_KEY長: $($env:SUPABASE_KEY.Length)文字"
Write-Host "OPENAI_API_KEY長: $($env:OPENAI_API_KEY.Length)文字"

if ([string]::IsNullOrEmpty($env:SUPABASE_URL) -or
    [string]::IsNullOrEmpty($env:SUPABASE_KEY) -or
    [string]::IsNullOrEmpty($env:OPENAI_API_KEY)) {
    Write-Host "エラー: SUPABASE_URL, SUPABASE_KEY, または OPENAI_API_KEY が設定されていません" -ForegroundColor Red
    exit 1
}

$PROJECT_ID = "consummate-yew-479020-u2"
$SERVICE_NAME = "netsuper-search"
$REGION = "asia-northeast1"

Write-Host "==================================" -ForegroundColor Green
Write-Host "ネットスーパー検索アプリデプロイ" -ForegroundColor Green
Write-Host "==================================" -ForegroundColor Green
Write-Host ""

# Cloud Buildでビルド＆デプロイ
Write-Host "Cloud Buildでビルド＆デプロイ中..." -ForegroundColor Yellow

gcloud run deploy $SERVICE_NAME `
  --source . `
  --platform managed `
  --region $REGION `
  --project $PROJECT_ID `
  --allow-unauthenticated `
  --memory 512Mi `
  --cpu 1 `
  --max-instances 10 `
  --set-env-vars "SUPABASE_URL=$($env:SUPABASE_URL),SUPABASE_KEY=***REDACTED***"

Write-Host ""
Write-Host "==================================" -ForegroundColor Green
Write-Host "✅ デプロイ完了！" -ForegroundColor Green
Write-Host "==================================" -ForegroundColor Green
Write-Host ""
Write-Host "アプリURL:"

gcloud run services describe $SERVICE_NAME `
  --region $REGION `
  --project $PROJECT_ID `
  --format 'value(status.url)'
```

### services\netsuper-search\deploy.sh

```sh
#!/bin/bash

# ネットスーパー横断検索アプリをCloud Runにデプロイ

set -e

# プロジェクトルートの.envファイルから必要な環境変数を読み込む
if [ -f "../.env" ]; then
    # 空白を削除して変数を抽出（改行文字も削除）
    export SUPABASE_URL=$(grep "^SUPABASE_URL" ../.env | sed 's/.*=[ ]*//' | tr -d ' \r\n')
    export SUPABASE_KEY=***REDACTED*** "^SUPABASE_KEY" ../.env | sed 's/.*=[ ]*//' | tr -d ' \r\n')
    export OPENAI_API_KEY=***REDACTED*** "^OPENAI_API_KEY" ../.env | sed 's/.*=[ ]*//' | tr -d ' \r\n')
else
    echo "エラー: ../.env ファイルが見つかりません"
    exit 1
fi

# 環境変数の確認（デバッグ用）
echo "SUPABASE_URL長: ${#SUPABASE_URL}文字"
echo "SUPABASE_KEY長: ${#SUPABASE_KEY}文字"
echo "OPENAI_API_KEY長: ${#OPENAI_API_KEY}文字"
echo "SUPABASE_URL: ${SUPABASE_URL:0:40}..."
echo "SUPABASE_KEY: ${SUPABASE_KEY:0:40}..."
echo "OPENAI_API_KEY: ${OPENAI_API_KEY:0:40}..."

if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_KEY" ] || [ -z "$OPENAI_API_KEY" ]; then
    echo "エラー: SUPABASE_URL, SUPABASE_KEY, または OPENAI_API_KEY が設定されていません"
    exit 1
fi

PROJECT_ID="consummate-yew-479020-u2"
SERVICE_NAME="netsuper-search"
REGION="asia-northeast1"
IMAGE_NAME="gcr.io/${PROJECT_ID}/${SERVICE_NAME}"

echo "=================================="
echo "ネットスーパー検索アプリデプロイ"
echo "=================================="

# Cloud Buildでビルド＆デプロイ（ローカルにDockerは不要）
echo ""
echo "Cloud Buildでビルド＆デプロイ中..."
gcloud run deploy ${SERVICE_NAME} \
  --source . \
  --platform managed \
  --region ${REGION} \
  --project ${PROJECT_ID} \
  --allow-unauthenticated \
  --memory 512Mi \
  --cpu 1 \
  --max-instances 10 \
  --set-env-vars "SUPABASE_URL=${SUPABASE_URL},SUPABASE_KEY=***REDACTED***"

echo ""
echo "=================================="
echo "✅ デプロイ完了！"
echo "=================================="
echo ""
echo "アプリURL:"
gcloud run services describe ${SERVICE_NAME} \
  --region ${REGION} \
  --project ${PROJECT_ID} \
  --format 'value(status.url)'
```

### services\netsuper-search\Dockerfile

```text
# Python 3.12ベースイメージ
FROM python:3.12-slim

# 作業ディレクトリ
WORKDIR /app

# 依存パッケージをインストール
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# プロジェクトファイルをコピー
COPY app.py .

# ポート設定
ENV PORT=8080
EXPOSE 8080

# Streamlitを起動
CMD streamlit run app.py \
    --server.port=$PORT \
    --server.address=0.0.0.0 \
    --server.headless=true \
    --server.enableCORS=false \
    --server.enableXsrfProtection=false
```

### services\netsuper-search\generate_embeddings.py

```py
"""
既存商品データをOpenAIでベクトル化

Rawdata_NETSUPER_itemsテーブルの商品名をOpenAI APIでベクトル化し、
embeddingカラムに保存します。

使用モデル: text-embedding-3-small (1536次元)
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
import time
from typing import List, Dict
from openai import OpenAI

# Windows環境でのUnicode出力設定
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

load_dotenv(root_dir / ".env")

from supabase import create_client
from shared.common.database.client import DatabaseClient

# ロギング設定
try:
    from loguru import logger
    logger.remove()
    logger.add(sys.stdout, format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>")
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)-8s | %(message)s', datefmt='%H:%M:%S')
    logger = logging.getLogger(__name__)


class ProductEmbeddingGenerator:
    """商品データのベクトル化"""

    def __init__(self):
        # Supabase接続（service roleキーを使用）
        self.db = DatabaseClient(use_service_role=True)

        # OpenAI接続
        self.openai_api_key = ***REDACTED***"OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("環境変数 OPENAI_API_KEY を設定してください")

        self.client = OpenAI(api_key=***REDACTED***
        self.model = "text-embedding-3-small"  # 1536次元

    def fetch_products_without_embedding(self, limit: int = None) -> List[Dict]:
        """
        embeddingがない商品を取得

        Args:
            limit: 取得する最大件数（Noneの場合は全件）

        Returns:
            商品データのリスト
        """
        logger.info("embeddingがない商品を取得中...")

        query = self.db.client.table('Rawdata_NETSUPER_items').select('id, product_name, general_name, small_category, keywords').is_('embedding', 'null')

        if limit:
            query = query.limit(limit)

        result = query.execute()

        logger.info(f"取得完了: {len(result.data)}件の商品データ")
        return result.data

    def generate_embedding(self, text: str) -> List[float]:
        """
        テキストからembeddingを生成

        Args:
            text: 商品名

        Returns:
            1536次元のベクトル
        """
        response = self.client.embeddings.create(
            model=self.model,
            input=text
        )
        return response.data[0].embedding

    def update_product_embedding(self, product_id: str, embedding: List[float]):
        """
        商品のembeddingを更新

        Args:
            product_id: 商品ID
            embedding: 埋め込みベクトル
        """
        # vector型として保存するために文字列形式に変換
        embedding_str = '[' + ','.join(map(str, embedding)) + ']'
        self.db.client.table('Rawdata_NETSUPER_items').update({
            'embedding': embedding_str
        }).eq('id', product_id).execute()

    def process_products(self, batch_size: int = 100, limit: int = None, delay: float = 0.1):
        """
        商品データをバッチ処理でベクトル化

        Args:
            batch_size: バッチサイズ
            limit: 処理する最大件数（Noneの場合は全件）
            delay: API呼び出し間の待機時間（秒）
        """
        logger.info("="*80)
        logger.info("商品データのベクトル化開始")
        logger.info("="*80)

        # embeddingがない商品を取得
        products = self.fetch_products_without_embedding(limit=limit)

        if not products:
            logger.info("ベクトル化が必要な商品がありません")
            return

        stats = {
            'total': len(products),
            'processed': 0,
            'error': 0
        }

        # バッチ処理
        for i, product in enumerate(products, 1):
            try:
                product_id = product['id']
                product_name = product['product_name']

                if not product_name:
                    logger.warning(f"[{i}/{stats['total']}] 商品名が空のためスキップ: ID={product_id}")
                    stats['error'] += 1
                    continue

                # embeddingを生成
                embedding = self.generate_embedding(product_name)

                # データベースに保存
                self.update_product_embedding(product_id, embedding)

                logger.info(f"[{i}/{stats['total']}] 完了: {product_name}")
                stats['processed'] += 1

                # 進捗表示
                if i % 50 == 0:
                    logger.info(f"進捗: {i}/{stats['total']} ({i/stats['total']*100:.1f}%)")

                # レート制限対策
                time.sleep(delay)

            except Exception as e:
                logger.error(f"[{i}/{stats['total']}] エラー: {product.get('product_name', '不明')} - {e}")
                stats['error'] += 1
                time.sleep(1)  # エラー時は少し長めに待機

        # 結果サマリー
        logger.info("="*80)
        logger.info("ベクトル化完了")
        logger.info("="*80)
        logger.info(f"処理件数: {stats['total']}件")
        logger.info(f"成功:     {stats['processed']}件")
        logger.info(f"エラー:   {stats['error']}件")
        logger.info("="*80)


def main():
    """メイン処理"""
    import argparse

    parser = argparse.ArgumentParser(description='商品データをOpenAIでベクトル化')
    parser.add_argument('--limit', type=int, help='処理する最大件数', default=None)
    parser.add_argument('--batch-size', type=int, help='バッチサイズ', default=100)
    parser.add_argument('--delay', type=float, help='API呼び出し間の待機時間（秒）', default=0.1)
    args = parser.parse_args()

    generator = ProductEmbeddingGenerator()
    generator.process_products(
        batch_size=args.batch_size,
        limit=args.limit,
        delay=args.delay
    )


if __name__ == "__main__":
    main()
```

### services\netsuper-search\generate_multi_embeddings.py

```py
"""
複数embeddingの生成（ハイブリッド検索用）

Rawdata_NETSUPER_itemsの各フィールドを個別にベクトル化:
1. general_name_embedding (重め)
2. small_category_embedding (重め)
3. keywords_embedding (軽め)

使用モデル: text-embedding-3-small (1536次元)
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
import time
from typing import List, Dict, Optional
from openai import OpenAI
import json

# Windows環境でのUnicode出力設定
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

load_dotenv(root_dir / ".env")

from shared.common.database.client import DatabaseClient

# ロギング設定
try:
    from loguru import logger
    logger.remove()
    logger.add(sys.stdout, format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>")
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)-8s | %(message)s', datefmt='%H:%M:%S')
    logger = logging.getLogger(__name__)


class MultiEmbeddingGenerator:
    """ハイブリッド検索用の複数embedding生成"""

    def __init__(self):
        # Supabase接続（service roleキーを使用）
        self.db = DatabaseClient(use_service_role=True)

        # OpenAI接続
        self.openai_api_key = ***REDACTED***"OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("環境変数 OPENAI_API_KEY を設定してください")

        self.client = OpenAI(api_key=***REDACTED***
        self.model = "text-embedding-3-small"  # 1536次元

    def fetch_products_without_embeddings(self, limit: int = None) -> List[Dict]:
        """
        embeddingがない商品を取得

        Args:
            limit: 取得する最大件数（Noneの場合は全件）

        Returns:
            商品データのリスト
        """
        logger.info("embeddingが未生成の商品を取得中...")

        query = self.db.client.table('Rawdata_NETSUPER_items').select(
            'id, product_name, general_name, small_category, keywords'
        ).is_('general_name_embedding', 'null')

        if limit:
            query = query.limit(limit)

        result = query.execute()

        logger.info(f"取得完了: {len(result.data)}件の商品データ")
        return result.data

    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """
        テキストからembeddingを生成

        Args:
            text: 入力テキスト

        Returns:
            1536次元のベクトル（テキストが空の場合はNone）
        """
        if not text or not text.strip():
            return None

        response = self.client.embeddings.create(
            model=self.model,
            input=text.strip()
        )
        return response.data[0].embedding

    def generate_keywords_embedding(self, keywords) -> Optional[List[float]]:
        """
        keywords配列からembeddingを生成

        Args:
            keywords: キーワード（配列またはJSON文字列）

        Returns:
            1536次元のベクトル（キーワードが空の場合はNone）
        """
        # キーワードの解析
        keyword_list = []

        if isinstance(keywords, str):
            try:
                keyword_list = json.loads(keywords)
            except:
                # JSON解析失敗時はスペース区切りとして扱う
                keyword_list = keywords.split()
        elif isinstance(keywords, list):
            keyword_list = keywords
        else:
            return None

        if not keyword_list:
            return None

        # キーワードを結合してembedding生成
        keywords_text = " ".join(str(k) for k in keyword_list if k)
        return self.generate_embedding(keywords_text)

    def update_product_embeddings(
        self,
        product_id: str,
        general_name_embedding: Optional[List[float]],
        small_category_embedding: Optional[List[float]],
        keywords_embedding: Optional[List[float]]
    ):
        """
        商品の3つのembeddingを更新

        Args:
            product_id: 商品ID
            general_name_embedding: general_name用embedding
            small_category_embedding: small_category用embedding
            keywords_embedding: keywords用embedding
        """
        update_data = {}

        # vector型として保存するために文字列形式に変換
        if general_name_embedding:
            update_data['general_name_embedding'] = '[' + ','.join(map(str, general_name_embedding)) + ']'

        if small_category_embedding:
            update_data['small_category_embedding'] = '[' + ','.join(map(str, small_category_embedding)) + ']'

        if keywords_embedding:
            update_data['keywords_embedding'] = '[' + ','.join(map(str, keywords_embedding)) + ']'

        if update_data:
            self.db.client.table('Rawdata_NETSUPER_items').update(
                update_data
            ).eq('id', product_id).execute()

    def process_products(self, batch_size: int = 100, limit: int = None, delay: float = 0.1):
        """
        商品データをバッチ処理で複数embeddingを生成

        Args:
            batch_size: バッチサイズ
            limit: 処理する最大件数（Noneの場合は全件）
            delay: API呼び出し間の待機時間（秒）
        """
        logger.info("="*80)
        logger.info("複数embedding生成開始（ハイブリッド検索用）")
        logger.info("="*80)

        # embeddingがない商品を取得
        products = self.fetch_products_without_embeddings(limit=limit)

        if not products:
            logger.info("ベクトル化が必要な商品がありません")
            return

        stats = {
            'total': len(products),
            'processed': 0,
            'error': 0,
            'general_name_count': 0,
            'small_category_count': 0,
            'keywords_count': 0
        }

        # バッチ処理
        for i, product in enumerate(products, 1):
            try:
                product_id = product['id']
                product_name = product.get('product_name', '')
                general_name = product.get('general_name', '')
                small_category = product.get('small_category', '')
                keywords = product.get('keywords')

                logger.info(f"[{i}/{stats['total']}] 処理中: {product_name[:40]}")

                # 1. general_name embedding（重め）
                general_name_emb = None
                if general_name:
                    general_name_emb = self.generate_embedding(general_name)
                    if general_name_emb:
                        stats['general_name_count'] += 1
                        logger.info(f"  ✓ general_name: {general_name}")

                # 2. small_category embedding（重め）
                small_category_emb = None
                if small_category:
                    small_category_emb = self.generate_embedding(small_category)
                    if small_category_emb:
                        stats['small_category_count'] += 1
                        logger.info(f"  ✓ small_category: {small_category}")

                # 3. keywords embedding（軽め）
                keywords_emb = None
                if keywords:
                    keywords_emb = self.generate_keywords_embedding(keywords)
                    if keywords_emb:
                        stats['keywords_count'] += 1
                        logger.info(f"  ✓ keywords: {keywords}")

                # データベースに保存
                self.update_product_embeddings(
                    product_id,
                    general_name_emb,
                    small_category_emb,
                    keywords_emb
                )

                stats['processed'] += 1

                # 進捗表示
                if i % 50 == 0:
                    logger.info(f"進捗: {i}/{stats['total']} ({i/stats['total']*100:.1f}%)")

                # レート制限対策
                time.sleep(delay)

            except Exception as e:
                logger.error(f"[{i}/{stats['total']}] エラー: {product.get('product_name', '不明')} - {e}")
                stats['error'] += 1
                time.sleep(1)  # エラー時は少し長めに待機

        # 結果サマリー
        logger.info("="*80)
        logger.info("複数embedding生成完了")
        logger.info("="*80)
        logger.info(f"処理件数:              {stats['total']}件")
        logger.info(f"成功:                  {stats['processed']}件")
        logger.info(f"エラー:                {stats['error']}件")
        logger.info(f"general_name生成:      {stats['general_name_count']}件")
        logger.info(f"small_category生成:    {stats['small_category_count']}件")
        logger.info(f"keywords生成:          {stats['keywords_count']}件")
        logger.info("="*80)


def main():
    """メイン処理"""
    import argparse

    parser = argparse.ArgumentParser(description='複数embeddingを生成（ハイブリッド検索用）')
    parser.add_argument('--limit', type=int, help='処理する最大件数', default=None)
    parser.add_argument('--batch-size', type=int, help='バッチサイズ', default=100)
    parser.add_argument('--delay', type=float, help='API呼び出し間の待機時間（秒）', default=0.1)
    args = parser.parse_args()

    generator = MultiEmbeddingGenerator()
    generator.process_products(
        batch_size=args.batch_size,
        limit=args.limit,
        delay=args.delay
    )


if __name__ == "__main__":
    main()
```

### services\netsuper-search\hybrid_search.py

```py
"""
ハイブリッド検索（複数embedding + SQL検索）

検索方式:
1. general_name_embedding による類似度検索（重め: weight=0.4）
2. small_category_embedding による類似度検索（重め: weight=0.3）
3. keywords_embedding による類似度検索（軽め: weight=0.2）
4. SQL テキスト検索（LIKE/trigram）（weight=0.1）

最終スコア = 各検索結果の重み付き合計
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
from typing import List, Dict, Optional
from openai import OpenAI
import json

# Windows環境でのUnicode出力設定
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# プロジェクトルートをパスに追加
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

load_dotenv(root_dir / ".env")

from shared.common.database.client import DatabaseClient


class HybridSearch:
    """ハイブリッド検索エンジン"""

    def __init__(self):
        # Supabase接続
        self.db = DatabaseClient(use_service_role=True)

        # OpenAI接続（クエリのembedding生成用）
        self.openai_api_key = ***REDACTED***"OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("環境変数 OPENAI_API_KEY を設定してください")

        self.client = OpenAI(api_key=***REDACTED***
        self.model = "text-embedding-3-small"

        # 重み設定（合計1.0）
        self.weights = {
            'general_name': 0.4,      # 重め
            'small_category': 0.3,     # 重め
            'keywords': 0.2,           # 軽め
            'text_search': 0.1         # SQL検索
        }

    def generate_query_embedding(self, query: str) -> List[float]:
        """
        検索クエリからembeddingを生成

        Args:
            query: 検索クエリ

        Returns:
            1536次元のベクトル
        """
        response = self.client.embeddings.create(
            model=self.model,
            input=query
        )
        return response.data[0].embedding

    def vector_search(
        self,
        query_embedding: List[float],
        embedding_column: str,
        limit: int = 100
    ) -> Dict[str, float]:
        """
        ベクトル類似度検索

        Args:
            query_embedding: クエリのembedding
            embedding_column: 検索対象のカラム名
            limit: 取得件数

        Returns:
            {product_id: similarity_score} の辞書
        """
        # embedding を文字列形式に変換
        embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'

        # ベクトル類似度検索（コサイン類似度）
        # Supabaseのベクトル検索クエリ
        # 1 - (embedding <=> query) でコサイン類似度を計算（値が大きいほど類似）
        query = f"""
        SELECT
            id,
            1 - ({embedding_column} <=> '{embedding_str}'::vector) as similarity
        FROM "Rawdata_NETSUPER_items"
        WHERE {embedding_column} IS NOT NULL
        ORDER BY {embedding_column} <=> '{embedding_str}'::vector
        LIMIT {limit}
        """

        result = self.db.client.rpc('exec_sql', {'query': query}).execute()

        # 結果を辞書に変換
        scores = {}
        if result.data:
            for row in result.data:
                scores[row['id']] = float(row['similarity'])

        return scores

    def text_search(self, query: str, limit: int = 100) -> Dict[str, float]:
        """
        SQL テキスト検索（LIKE + trigram）

        Args:
            query: 検索クエリ
            limit: 取得件数

        Returns:
            {product_id: match_score} の辞書
        """
        # トライグラム類似度 + LIKE検索
        # similarity() は pg_trgm の関数（0〜1の類似度）
        sql_query = f"""
        SELECT
            id,
            GREATEST(
                similarity(product_name, '{query}'),
                similarity(general_name, '{query}'),
                CASE WHEN product_name ILIKE '%{query}%' THEN 0.5 ELSE 0 END,
                CASE WHEN general_name ILIKE '%{query}%' THEN 0.5 ELSE 0 END
            ) as match_score
        FROM "Rawdata_NETSUPER_items"
        WHERE
            product_name % '{query}'
            OR general_name % '{query}'
            OR product_name ILIKE '%{query}%'
            OR general_name ILIKE '%{query}%'
        ORDER BY match_score DESC
        LIMIT {limit}
        """

        try:
            result = self.db.client.rpc('exec_sql', {'query': sql_query}).execute()

            scores = {}
            if result.data:
                for row in result.data:
                    scores[row['id']] = float(row['match_score'])

            return scores
        except Exception as e:
            print(f"テキスト検索エラー: {e}")
            return {}

    def combine_scores(self, score_dicts: List[Dict[str, float]]) -> Dict[str, float]:
        """
        複数の検索結果を重み付けして統合

        Args:
            score_dicts: [
                {'weight': 0.4, 'scores': {id: score, ...}},
                {'weight': 0.3, 'scores': {id: score, ...}},
                ...
            ]

        Returns:
            {product_id: final_score} の辞書
        """
        combined = {}

        for item in score_dicts:
            weight = item['weight']
            scores = item['scores']

            for product_id, score in scores.items():
                if product_id not in combined:
                    combined[product_id] = 0.0
                combined[product_id] += weight * score

        return combined

    def search(
        self,
        query: str,
        top_k: int = 20,
        general_weight: float = None,
        category_weight: float = None,
        keywords_weight: float = None,
        text_weight: float = None
    ) -> List[Dict]:
        """
        ハイブリッド検索を実行（SQL関数を使用）

        Args:
            query: 検索クエリ
            top_k: 返す結果の件数
            general_weight: general_name重み（Noneの場合はデフォルト0.4）
            category_weight: small_category重み（Noneの場合はデフォルト0.3）
            keywords_weight: keywords重み（Noneの場合はデフォルト0.2）
            text_weight: text検索重み（Noneの場合はデフォルト0.1）

        Returns:
            検索結果のリスト（スコア順）
        """
        print(f"検索クエリ: {query}")
        print("="*80)

        # デフォルト重みを使用
        g_weight = general_weight if general_weight is not None else self.weights['general_name']
        c_weight = category_weight if category_weight is not None else self.weights['small_category']
        k_weight = keywords_weight if keywords_weight is not None else self.weights['keywords']
        t_weight = text_weight if text_weight is not None else self.weights['text_search']

        # クエリのembeddingを生成
        print("クエリのembedding生成中...")
        query_embedding = self.generate_query_embedding(query)

        # embedding を PostgreSQL vector 形式に変換
        embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'

        # ハイブリッド検索SQL関数を呼び出し
        print("ハイブリッド検索実行中...")
        print(f"  重み: general={g_weight}, category={c_weight}, keywords={k_weight}, text={t_weight}")

        try:
            result = self.db.client.rpc('hybrid_search', {
                'query_embedding': embedding_str,
                'query_text': query,
                'match_count': top_k,
                'general_weight': g_weight,
                'category_weight': c_weight,
                'keywords_weight': k_weight,
                'text_weight': t_weight
            }).execute()

            if not result.data:
                print("検索結果: 0件")
                return []

            # 結果を整形
            products = []
            for row in result.data:
                product = {
                    'id': row['id'],
                    'product_name': row['product_name'],
                    'general_name': row['general_name'],
                    'small_category': row['small_category'],
                    'keywords': row['keywords'],
                    'organization': row['organization'],
                    'current_price': row['current_price'],
                    'search_score': row['final_score'],
                    'score_breakdown': {
                        'general': row['general_score'],
                        'category': row['category_score'],
                        'keywords': row['keywords_score'],
                        'text': row['text_score']
                    }
                }
                products.append(product)

            print(f"検索結果: {len(products)}件")
            return products

        except Exception as e:
            print(f"エラー: {e}")
            print("SQL関数 hybrid_search() が存在するか確認してください")
            return []


def main():
    """テスト実行"""
    import argparse

    parser = argparse.ArgumentParser(description='ハイブリッド検索テスト')
    parser.add_argument('query', type=str, help='検索クエリ')
    parser.add_argument('--top-k', type=int, default=10, help='表示件数')
    args = parser.parse_args()

    # 検索実行
    searcher = HybridSearch()
    results = searcher.search(args.query, top_k=args.top_k)

    # 結果表示
    print("\n" + "="*80)
    print(f"検索結果: {len(results)}件")
    print("="*80)

    for i, product in enumerate(results, 1):
        print(f"\n{i}. {product['product_name']}")
        print(f"   一般名: {product.get('general_name', 'N/A')}")
        print(f"   小分類: {product.get('small_category', 'N/A')}")
        print(f"   キーワード: {product.get('keywords', 'N/A')}")
        print(f"   価格: {product.get('current_price', 'N/A')}")
        print(f"   スコア: {product['search_score']:.4f}")

    print("\n" + "="*80)


if __name__ == "__main__":
    main()
```

### services\netsuper-search\inspect_db_triggers.py

```py
"""
データベースのトリガーとFunction定義を確認
"""
import os
from supabase import create_client
from dotenv import load_dotenv

# .envファイルを読み込む
env_path = os.path.join(os.path.dirname(__file__), '..', '.env')
load_dotenv(env_path)

# Supabase接続（SERVICE ROLE KEYを使用）
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

if not SUPABASE_SERVICE_KEY:
    print("警告: SUPABASE_SERVICE_ROLE_KEY が設定されていません。通常のKEYを使用します。")
    SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_KEY")

db = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

print("=" * 80)
print("Rawdata_NETSUPER_items テーブルのトリガーとFunction定義を確認")
print("=" * 80)

# PostgreSQLのシステムカタログを直接クエリ
queries = [
    # トリガー一覧
    """
    SELECT
        trigger_name,
        event_manipulation,
        event_object_table,
        action_statement
    FROM information_schema.triggers
    WHERE event_object_table = 'Rawdata_NETSUPER_items'
    ORDER BY trigger_name;
    """,

    # Function定義を取得
    """
    SELECT
        p.proname as function_name,
        pg_get_functiondef(p.oid) as function_definition
    FROM pg_proc p
    JOIN pg_namespace n ON p.pronamespace = n.oid
    WHERE n.nspname = 'public'
        AND (
            p.proname LIKE '%embedding%'
            OR p.proname LIKE '%product%'
            OR pg_get_functiondef(p.oid) LIKE '%Rawdata_NETSUPER_items%'
        )
    ORDER BY p.proname;
    """
]

try:
    print("\n[1] Rawdata_NETSUPER_items のトリガー一覧:")
    print("-" * 80)
    result = db.rpc('exec_sql', {'query': queries[0]}).execute()
    if hasattr(result, 'data') and result.data:
        for row in result.data:
            print(f"トリガー名: {row.get('trigger_name')}")
            print(f"  イベント: {row.get('event_manipulation')}")
            print(f"  アクション: {row.get('action_statement')}")
            print()
    else:
        # RPCが使えない場合は直接情報を取得できないので、別の方法を試す
        print("RPC経由でのクエリができませんでした。")
        print("Supabaseダッシュボードで以下を確認してください:")
        print("Database > Functions / Triggers")

except Exception as e:
    print(f"エラー: {e}")
    print("\n代替方法:")
    print("1. Supabaseダッシュボード → Database → Triggers")
    print("2. SQL Editorで以下を実行:")
    print(queries[0])

print("\n" + "=" * 80)
print("推測: embeddingはデータベーストリガーで自動生成されている可能性")
print("=" * 80)
```

### services\netsuper-search\inspect_embedding_content.py

```py
"""
embeddingの内容を詳しく確認
"""
import os
import json
from supabase import create_client
from dotenv import load_dotenv

# .envファイルを読み込む
env_path = os.path.join(os.path.dirname(__file__), '..', '.env')
load_dotenv(env_path)

# Supabase接続
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = ***REDACTED***"SUPABASE_KEY")

db = create_client(SUPABASE_URL, SUPABASE_KEY)

print("=" * 60)
print("embeddingの内容を確認")
print("=" * 60)

# サンプルデータを1件取得
result = db.table('Rawdata_NETSUPER_items').select(
    'id, product_name, product_name_normalized, general_name, category, manufacturer, embedding'
).limit(3).execute()

for idx, product in enumerate(result.data, 1):
    print(f"\n--- 商品 {idx} ---")
    print(f"商品名: {product.get('product_name')}")
    print(f"正規化商品名: {product.get('product_name_normalized')}")
    print(f"一般名: {product.get('general_name')}")
    print(f"カテゴリ: {product.get('category')}")
    print(f"メーカー: {product.get('manufacturer')}")

    embedding = product.get('embedding')
    if embedding:
        print(f"embedding型: {type(embedding)}")
        if isinstance(embedding, str):
            print(f"embedding長さ（文字列）: {len(embedding)}文字")
            print(f"embedding先頭: {embedding[:100]}...")
            # JSON形式かどうか確認
            try:
                parsed = json.loads(embedding)
                print(f"JSON解析成功: {type(parsed)}, 長さ: {len(parsed)}")
            except:
                print("JSON解析失敗")
        elif isinstance(embedding, list):
            print(f"embedding次元数: {len(embedding)}")
            print(f"embedding先頭5要素: {embedding[:5]}")
    else:
        print("embedding: None")

print("\n" + "=" * 60)
print("embeddingが何から生成されているか推測")
print("=" * 60)
print("""
可能性:
1. product_name のみ
2. product_name + category
3. product_name_normalized + general_name
4. product_name + manufacturer
5. その他の組み合わせ

データベーストリガーまたはアプリケーション側で生成されている可能性があります。
""")
```

### services\netsuper-search\list_60_tables.py

```py
"""
60番台のテーブル一覧を確認
"""
import os
from supabase import create_client
from dotenv import load_dotenv

load_dotenv(os.path.join(os.path.dirname(__file__), '..', '.env'))

db = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

print("=" * 80)
print("60番台のテーブル一覧")
print("=" * 80)

# PostgreSQLのシステムカタログから60番台のテーブルを取得
# Supabaseの制限により、直接クエリできないので、既知のテーブルをチェック
known_tables = [
    'Rawdata_RECEIPT_items',
    'Rawdata_RECEIPT_shops',
    '60_ms_categories',
    '60_ms_situations',
    '60_ms_product_dict',
    '60_ms_ocr_aliases',
    '60_ag_daily_summary',
    '60_ag_monthly_summary',
]

tables_60 = []

for table_name in known_tables:
    try:
        result = db.table(table_name).select('*').limit(1).execute()
        # エラーが出なければテーブル存在
        row_count_result = db.table(table_name).select('id', count='exact').limit(1).execute()
        count = row_count_result.count if hasattr(row_count_result, 'count') else 'N/A'
        tables_60.append((table_name, count))
        print(f"✅ {table_name:40s} ({count} 件)")
    except Exception as e:
        if 'PGRST205' not in str(e):  # テーブル存在しないエラー以外
            print(f"⚠️  {table_name:40s} (エラー: {str(e)[:30]})")

# 他にも60番台があるか確認するため、パターンでチェック
print("\n" + "=" * 80)
print("その他の60番台テーブルを探索中...")
print("=" * 80)

# よくあるパターン（60_rd_standardized_itemsは削除済み）
other_patterns = [
    '60_rd_items',
    '60_rd_raw_items',
    '60_ms_stores',
    '60_ms_payment_methods',
    '60_ix_',
    '60_lg_',
]

for pattern in other_patterns:
    for suffix in ['', '_backup', '_old', '_new']:
        table_name = pattern + suffix
        try:
            result = db.table(table_name).select('*').limit(1).execute()
            row_count_result = db.table(table_name).select('id', count='exact').limit(1).execute()
            count = row_count_result.count if hasattr(row_count_result, 'count') else 'N/A'
            if table_name not in [t[0] for t in tables_60]:
                tables_60.append((table_name, count))
                print(f"✅ {table_name:40s} ({count} 件)")
        except:
            pass

print("\n" + "=" * 80)
print(f"合計: {len(tables_60)} 個の60番台テーブル")
print("=" * 80)
```

### services\netsuper-search\requirements.txt

```txt
streamlit==1.39.0
supabase==2.9.1
openai==1.58.1
```

### services\netsuper-search\reverse_engineer_embedding.py

```py
"""
embeddingの生成元を逆算
実際のembeddingと各カラムから生成したembeddingを比較
"""
import os
import json
from supabase import create_client
from dotenv import load_dotenv
from openai import OpenAI
import numpy as np

# .envファイルを読み込む
env_path = os.path.join(os.path.dirname(__file__), '..', '.env')
load_dotenv(env_path)

# Supabase接続
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = ***REDACTED***"SUPABASE_KEY")
OPENAI_API_KEY = ***REDACTED***"OPENAI_API_KEY")

db = create_client(SUPABASE_URL, SUPABASE_KEY)
openai_client = OpenAI(api_key=***REDACTED***

def generate_embedding(text):
    """OpenAI text-embedding-3-smallでembeddingを生成"""
    if not text:
        return None
    response = openai_client.embeddings.create(
        model="text-embedding-3-small",
        input=text,
        dimensions=1536
    )
    return response.data[0].embedding

def cosine_similarity(vec1, vec2):
    """コサイン類似度を計算"""
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

print("=" * 80)
print("embeddingの生成元を逆算")
print("=" * 80)

# サンプルデータを取得
result = db.table('Rawdata_NETSUPER_items').select(
    'product_name, product_name_normalized, general_name, category, manufacturer, embedding'
).limit(3).execute()

for idx, product in enumerate(result.data, 1):
    print(f"\n{'='*80}")
    print(f"商品 {idx}: {product.get('product_name')}")
    print(f"{'='*80}")

    # 実際のembeddingを取得
    actual_embedding_str = product.get('embedding')
    if not actual_embedding_str:
        print("embeddingがありません")
        continue

    actual_embedding = json.loads(actual_embedding_str)

    # 各カラムの組み合わせでembeddingを生成して比較
    test_cases = [
        ("product_name", product.get('product_name')),
        ("product_name_normalized", product.get('product_name_normalized')),
        ("general_name", product.get('general_name')),
        ("category", product.get('category')),
        ("manufacturer", product.get('manufacturer')),
        ("product_name + category", f"{product.get('product_name')} {product.get('category') or ''}".strip()),
        ("product_name + manufacturer", f"{product.get('product_name')} {product.get('manufacturer') or ''}".strip()),
        ("product_name + category + manufacturer",
         f"{product.get('product_name')} {product.get('category') or ''} {product.get('manufacturer') or ''}".strip()),
    ]

    print("\n類似度スコア:")
    print("-" * 80)

    for label, text in test_cases:
        if not text or text.strip() == "":
            continue

        try:
            test_embedding = generate_embedding(text)
            if test_embedding:
                similarity = cosine_similarity(actual_embedding, test_embedding)
                marker = " ★★★" if similarity > 0.99 else ""
                print(f"{label:40s}: {similarity:.6f}{marker}")
        except Exception as e:
            print(f"{label:40s}: エラー - {e}")

print("\n" + "=" * 80)
print("結論")
print("=" * 80)
print("類似度が0.99以上（★★★マーク）のものがembedding生成元です")
```

### shared\__init__.py

```py
# shared package
```

### shared\ai\__init__.py

```py

```

### shared\ai\embeddings\__init__.py

```py

```

### shared\ai\embeddings\embeddings.py

```py
"""
Embedding Client (DEPRECATED - OpenAI text-embedding-3-small を使用してください)
このクラスは後方互換性のために残されていますが、使用は推奨されません。
代わりに LLMClient.generate_embedding() を使用してください。
"""
from typing import List, Optional
from openai import OpenAI
from shared.common.config.settings import settings


class EmbeddingClient:
    """
    OpenAI text-embedding-3-small を使用したEmbedding生成クライアント (1536次元)

    注意: このクラスは非推奨です。LLMClient.generate_embedding() を使用してください。
    """

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = ***REDACTED*** or settings.OPENAI_API_KEY
        if not self.api_key:
            raise ValueError("OpenAI API Key が設定されていません")

        self.client = OpenAI(api_key=***REDACTED***
        self.model_name = "text-embedding-3-small"
        self.dimensions = 1536

    def generate_embedding(self, text: str, task_type: str = "RETRIEVAL_DOCUMENT") -> List[float]:
        """
        Embeddingを生成 (1536次元)

        注意: task_type パラメータは互換性のために残されていますが、使用されません
        """
        if not text or not text.strip():
            raise ValueError("空のテキストはembedding化できません")

        response = self.client.embeddings.create(
            model=self.model_name,
            input=text,
            dimensions=self.dimensions
        )

        return response.data[0].embedding

    def generate_embeddings_batch(self, texts: List[str], task_type: str = "RETRIEVAL_DOCUMENT") -> List[List[float]]:
        """バッチでEmbeddingを生成 (1536次元)"""
        if not texts:
            return []

        embeddings = []
        for text in texts:
            if text and text.strip():
                embedding = self.generate_embedding(text, task_type)
                embeddings.append(embedding)
            else:
                # OpenAI text-embedding-3-smallは1536次元
                embeddings.append([0.0] * 1536)

        return embeddings

    def generate_query_embedding(self, query: str) -> List[float]:
        """クエリ用のEmbeddingを生成 (1536次元)"""
        return self.generate_embedding(query, task_type="RETRIEVAL_QUERY")
```

### shared\ai\llm_client\__init__.py

```py

```

### shared\ai\llm_client\exceptions.py

```py
"""
LLMクライアントのカスタム例外
"""


class MaxTokensExceededError(Exception):
    """max_tokens上限に達して出力が途中で切れた場合のエラー"""

    def __init__(self, message: str, partial_output: str, finish_reason_name: str):
        """
        Args:
            message: エラーメッセージ
            partial_output: 途中で切れた出力テキスト
            finish_reason_name: finish_reasonの名前
        """
        super().__init__(message)
        self.partial_output = partial_output
        self.finish_reason_name = finish_reason_name
```

### shared\ai\llm_client\llm_client.py

```py
"""
LLMクライアント（v3.0: マルチプロバイダ対応）
Gemini / Anthropic / OpenAI を統一インターフェースで利用
"""

import os
import base64
from typing import Dict, List, Any, Optional, Union
from pathlib import Path
import mimetypes

import google.generativeai as genai
from anthropic import Anthropic, RateLimitError
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, RetryError
from loguru import logger

from shared.common.config.model_tiers import AIProvider, get_model_config
from shared.common.config.settings import settings
from .exceptions import MaxTokensExceededError

class LLMClient:
    """統合LLMクライアント"""
    
    def __init__(self):
        """設定からAPIキーを取得し、各プロバイダーを初期化"""

        # Settings経由でAPIキーを取得（環境変数管理の統一）
        self.gemini_api_key = ***REDACTED*** or os.getenv("GOOGLE_API_KEY")  # 後方互換性のためGOOGLE_API_KEYもサポート
        self.anthropic_api_key = ***REDACTED***
        self.openai_api_key = ***REDACTED***

        # Gemini設定 (トップレベル関数のみ使用)
        if self.gemini_api_key:
            genai.configure(api_key=***REDACTED***
        else:
            pass

        # Anthropic設定
        if self.anthropic_api_key:
            self.anthropic_client = Anthropic(api_key=***REDACTED***
        else:
            self.anthropic_client = None

        # OpenAI設定
        if self.openai_api_key:
            self.openai_client = OpenAI(api_key=***REDACTED***
        else:
            self.openai_client = None
    
    def generate_with_images(
        self,
        prompt: str,
        image_data: Union[str, List[str]],
        model: str = "gemini-2.5-flash-lite",
        temperature: float = 0.0,
        max_tokens: int = 8192
    ) -> str:
        """
        画像データを使ってGemini Vision APIを呼び出し

        Args:
            prompt: プロンプト
            image_data: Base64エンコードされた画像データ（単一または複数）
            model: モデル名
            temperature: 温度パラメータ
            max_tokens: 最大トークン数

        Returns:
            生成されたテキスト
        """
        if not self.gemini_api_key:
            raise ValueError("Gemini API key is missing")

        try:
            model_obj = genai.GenerativeModel(model)

            # 画像データをリスト化
            if isinstance(image_data, str):
                image_data_list = [image_data]
            else:
                image_data_list = image_data

            # コンテンツパーツを構築
            content_parts = [prompt]

            # 画像を追加
            for img_base64 in image_data_list:
                # Base64をバイトにデコード
                img_bytes = base64.b64decode(img_base64)

                # Geminiの画像形式に変換
                image_part = {
                    'mime_type': 'image/png',
                    'data': img_bytes
                }
                content_parts.append(image_part)

            # 安全フィルター設定
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]

            # APIを呼び出し
            response = model_obj.generate_content(
                content_parts,
                generation_config=genai.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=temperature
                ),
                safety_settings=safety_settings
            )

            # レスポンスの検証
            if not response.candidates:
                raise ValueError("Gemini returned no candidates")

            candidate = response.candidates[0]

            # finish_reason をチェック
            if candidate.finish_reason != 1:  # 1 = STOP (正常終了)
                finish_reason_name = candidate.finish_reason.name if hasattr(candidate.finish_reason, 'name') else str(candidate.finish_reason)
                error_details = {
                    "finish_reason": candidate.finish_reason,
                    "finish_reason_name": finish_reason_name
                }
                if hasattr(candidate, 'safety_ratings') and candidate.safety_ratings:
                    error_details["safety_ratings"] = [
                        {
                            "category": rating.category.name if hasattr(rating.category, 'name') else str(rating.category),
                            "probability": rating.probability.name if hasattr(rating.probability, 'name') else str(rating.probability)
                        }
                        for rating in candidate.safety_ratings
                    ]
                logger.error(f"Gemini Vision失敗: {error_details}")
                raise ValueError(f"Gemini finish_reason: {finish_reason_name} ({candidate.finish_reason})")

            # テキストを取得
            text_content = candidate.content.parts[0].text if candidate.content.parts else ""
            return text_content

        except Exception as e:
            logger.error(f"Gemini Vision API エラー: {e}")
            raise

    def call_model(
        self,
        tier: str,
        prompt: str,
        file_path: Optional[Path] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        指定されたタスクに最適なモデルを呼び出し

        Args:
            tier: モデル階層("stagea_classification", "stageh_extraction", "ui_response")
            prompt: プロンプト
            file_path: ファイルパス (GeminiのStage A分類用)
            **kwargs: 追加パラメータ

        Returns:
            モデルレスポンス
        """
        config = get_model_config(tier)
        provider = config["provider"]
        # kwargsからmodel_nameが渡されていればそれを優先、なければconfigから取得
        model_name = kwargs.pop('model_name', None) or config["model"]

        # モデル名からプロバイダーを自動判定（明示的なmodel_name指定時）
        if model_name:
            if 'claude' in model_name.lower():
                provider = AIProvider.CLAUDE
            elif 'gemini' in model_name.lower():
                provider = AIProvider.GEMINI
            elif 'gpt' in model_name.lower() or 'text-embedding' in model_name.lower():
                provider = AIProvider.OPENAI

        if provider == AIProvider.GEMINI:
            if not self.gemini_api_key:
                return {"success": False, "error": "Gemini API key is missing", "model": model_name}
            return self._call_gemini(model_name, prompt, file_path, config, **kwargs)

        elif provider == AIProvider.CLAUDE:
            if not self.anthropic_client:
                return {"success": False, "error": "Anthropic API key is missing", "model": model_name}
            try:
                return self._call_claude(model_name, prompt, config, **kwargs)
            except RetryError as e:
                # リトライが全て失敗した場合
                original_error = e.last_attempt.exception()
                return {"success": False, "error": str(original_error), "model": model_name, "provider": "claude"}

        elif provider == AIProvider.OPENAI:
            if not self.openai_client:
                return {"success": False, "error": "OpenAI API key is missing", "model": model_name}
            return self._call_openai(model_name, prompt, config, **kwargs)

        else:
            raise ValueError(f"未対応のプロバイダー: {provider}")

    def _call_gemini(
        self,
        model_name: str,
        prompt: str,
        file_path: Optional[Path],
        config: Dict,
        **kwargs
    ) -> Dict[str, Any]:
        """Gemini API呼び出し（トップレベル関数のみ使用）"""
        uploaded_file = None
        try:
            model = genai.GenerativeModel(model_name)

            content_parts = [prompt]

            if file_path and file_path.exists():
                # MIMEタイプを自動判定
                mime_type, _ = mimetypes.guess_type(str(file_path))
                if not mime_type:
                    mime_type = "application/pdf"  # デフォルト

                # ファイルをアップロード（トップレベル関数のみ使用）
                uploaded_file = genai.upload_file(path=str(file_path), mime_type=mime_type)
                content_parts.append(uploaded_file)

            # 安全フィルター設定（finish_reason: 2 対策）
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]

            # 生成設定
            generation_config = genai.GenerationConfig(
                max_output_tokens=config.get("max_tokens", 65536),
                temperature=config.get("temperature", 0.1)
            )

            # response_format が kwargs に含まれている場合
            response_format = kwargs.get('response_format')
            if response_format in ["json", "json_object"]:
                generation_config.response_mime_type = "application/json"

            response = model.generate_content(
                content_parts,
                generation_config=generation_config,
                safety_settings=safety_settings
            )

            # レスポンスの検証
            if not response.candidates:
                self._cleanup_uploaded_file(uploaded_file)
                return {"success": False, "error": "Gemini returned no candidates", "model": model_name, "provider": "gemini"}

            candidate = response.candidates[0]

            # finish_reason をチェック
            if candidate.finish_reason != 1:  # 1 = STOP (正常終了)
                self._cleanup_uploaded_file(uploaded_file)
                # 詳細なエラー情報を取得
                error_details = {
                    "finish_reason": candidate.finish_reason,
                    "finish_reason_name": candidate.finish_reason.name if hasattr(candidate.finish_reason, 'name') else str(candidate.finish_reason)
                }
                # safety_ratingsがあれば追加
                if hasattr(candidate, 'safety_ratings') and candidate.safety_ratings:
                    error_details["safety_ratings"] = [
                        {
                            "category": rating.category.name if hasattr(rating.category, 'name') else str(rating.category),
                            "probability": rating.probability.name if hasattr(rating.probability, 'name') else str(rating.probability)
                        }
                        for rating in candidate.safety_ratings
                    ]
                logger.error(f"Gemini Vision失敗: {error_details}")
                return {
                    "success": False,
                    "error": f"Gemini finish_reason: {candidate.finish_reason}",
                    "error_details": error_details,
                    "model": model_name,
                    "provider": "gemini"
                }

            # テキストを取得
            text_content = candidate.content.parts[0].text if candidate.content.parts else ""

            # ファイルを削除
            self._cleanup_uploaded_file(uploaded_file)

            return {
                "success": True,
                "content": text_content,
                "model": model_name,
                "provider": "gemini"
            }

        except Exception as e:
            self._cleanup_uploaded_file(uploaded_file)
            return {"success": False, "error": str(e), "model": model_name, "provider": "gemini"}

    def _cleanup_uploaded_file(self, uploaded_file) -> None:
        """
        アップロードされたファイルを削除（トップレベル関数のみ使用）

        Args:
            uploaded_file: アップロードされたファイルオブジェクト
        """
        if not uploaded_file:
            return

        try:
            genai.delete_file(name=uploaded_file.name)
        except Exception:
            # 削除に失敗しても処理は継続
            pass

    @retry(
        retry=retry_if_exception_type(RateLimitError),
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=2, max=60)
    )
    def _call_claude(
        self,
        model_name: str,
        prompt: str,
        config: Dict,
        **kwargs
    ) -> Dict[str, Any]:
        """Anthropic API呼び出し"""
        try:
            # ✅ DEBUG: 送信するプロンプトの先頭部分をログに出力
            from loguru import logger
            logger.debug(f"[Anthropic CALL] Model: {model_name}, Prompt start: {prompt[:300]}...")

            # Anthropicの最大トークン数制限を適用
            max_tokens = config.get("max_tokens", 8192)
            # Anthropic models: max 64000 tokens
            if max_tokens > 64000:
                max_tokens = 64000

            response = self.anthropic_client.messages.create(
                model=model_name,
                max_tokens=max_tokens,
                temperature=config.get("temperature", 0.0),
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )

            # ✅ DEBUG: Anthropic からの生の応答コンテンツ全体をログに出力
            raw_content = response.content[0].text
            logger.debug(f"[Anthropic RAW RESP] Content length: {len(raw_content)} chars")
            # 応答が長すぎる場合があるため、先頭2000文字のみをログに記録
            logger.debug(f"[Anthropic RAW RESP] Content preview: {raw_content[:2000]}")

            return {
                "success": True,
                "content": raw_content,
                "model": model_name,
                "provider": "claude"
            }

        except RateLimitError:
            # RateLimitErrorは再スローしてtenacityにリトライさせる
            raise
        except Exception as e:
            return {"success": False, "error": str(e), "model": model_name, "provider": "claude"}

    def _call_openai(
        self,
        model_name: str,
        prompt: str,
        config: Dict,
        **kwargs
    ) -> Dict[str, Any]:
        """OpenAI API呼び出し"""
        try:
            # ✅ GPT-5.1では max_completion_tokens を使用、旧モデルでは max_tokens（後方互換性）
            max_completion_tokens = config.get("max_completion_tokens")
            max_tokens = config.get("max_tokens", 16384)

            # パラメータを動的に構築
            api_params = {
                "model": model_name,
                "messages": [
                    {"role": "user", "content": prompt}
                ]
            }

            # temperatureはGPT-5.1などの一部モデルでサポートされていないため、configに含まれている場合のみ設定
            if "temperature" in config:
                api_params["temperature"] = config["temperature"]

            # max_completion_tokens が設定されていればそれを使用、なければ max_tokens
            if max_completion_tokens:
                api_params["max_completion_tokens"] = max_completion_tokens
            else:
                api_params["max_tokens"] = max_tokens

            response = self.openai_client.chat.completions.create(**api_params)
            
            return {
                "success": True,
                "content": response.choices[0].message.content,
                "model": model_name,
                "provider": "openai"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e), "model": model_name, "provider": "openai"}

    def generate_embedding(self, text: str) -> List[float]:
        """
        Embedding生成

        Args:
            text: Embeddingを生成するテキスト

        Returns:
            1536次元のembeddingベクトル
        """
        config = get_model_config("embeddings")

        if not self.openai_client:
            raise ConnectionError("OpenAI client not initialized for embedding generation.")

        # text-embedding-3-smallモデルで1536次元を明示的に指定
        response = self.openai_client.embeddings.create(
            model=config["model"],
            input=text,
            dimensions=config.get("dimensions", 1536)  # デフォルト1536次元
        )

        return response.data[0].embedding

    def generate_with_vision(
        self,
        prompt: str,
        image_path: str,
        model: str = "gemini-2.0-flash-exp",
        temperature: float = 0.0,
        max_tokens: int = 65536,
        response_format: Optional[str] = None
    ) -> str:
        """
        画像ファイルを使ってGemini Vision APIを呼び出し

        Args:
            prompt: プロンプト
            image_path: 画像ファイルのパス（PNG, JPEG等）
            model: モデル名
            temperature: 温度パラメータ
            max_tokens: 最大トークン数
            response_format: レスポンスフォーマット（"json", "json_object" など）

        Returns:
            生成されたテキスト

        Raises:
            ValueError: APIキーがない、またはレスポンスが不正な場合
            Exception: その他のエラー
        """
        if not self.gemini_api_key:
            raise ValueError("Gemini API key is missing")

        try:
            model_obj = genai.GenerativeModel(model)

            # ファイルをアップロード
            mime_type, _ = mimetypes.guess_type(image_path)
            if not mime_type:
                mime_type = "image/jpeg"  # デフォルト

            uploaded_file = genai.upload_file(path=image_path, mime_type=mime_type)

            # コンテンツパーツを構築
            content_parts = [prompt, uploaded_file]

            # 安全フィルター設定
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]

            # 生成設定
            generation_config = genai.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=temperature
            )

            # response_format が指定されている場合
            if response_format in ["json", "json_object"]:
                generation_config.response_mime_type = "application/json"

            # APIを呼び出し（タイムアウト5分、1回リトライ）
            max_retries = 1
            last_error = None

            for attempt in range(max_retries + 1):  # 0回目（初回）+ 1回（リトライ）
                try:
                    logger.info(f"[Gemini Vision] API呼び出し試行 {attempt + 1}/{max_retries + 1}")
                    response = model_obj.generate_content(
                        content_parts,
                        generation_config=generation_config,
                        safety_settings=safety_settings,
                        request_options={"timeout": 300}  # 5分タイムアウト
                    )
                    break  # 成功したらループを抜ける
                except Exception as e:
                    last_error = e
                    error_str = str(e).lower()
                    # タイムアウトまたはネットワークエラーの場合
                    if any(keyword in error_str for keyword in ['timeout', 'deadline', 'network', 'connection']):
                        if attempt < max_retries:
                            logger.warning(f"[Gemini Vision] タイムアウト/ネットワークエラー。リトライします（試行 {attempt + 1}/{max_retries + 1}）: {e}")
                            continue
                        else:
                            logger.error(f"[Gemini Vision] タイムアウト/ネットワークエラー。リトライ上限に達しました: {e}")
                            raise
                    else:
                        # タイムアウト以外のエラーは即座に失敗
                        logger.error(f"[Gemini Vision] API エラー（リトライ不可）: {e}")
                        raise
            else:
                # ループが最後まで実行された（全てのリトライが失敗）
                if last_error:
                    raise last_error

            # アップロードファイルを削除
            try:
                genai.delete_file(name=uploaded_file.name)
            except Exception:
                pass

            # レスポンスの検証
            if not response.candidates:
                raise ValueError("Gemini returned no candidates")

            candidate = response.candidates[0]

            # finish_reason をチェック
            finish_reason_name = candidate.finish_reason.name if hasattr(candidate.finish_reason, 'name') else str(candidate.finish_reason)
            logger.info(f"[Gemini Vision] finish_reason: {finish_reason_name} ({candidate.finish_reason})")

            # テキストを取得（finish_reasonに関わらず取得）
            text_content = candidate.content.parts[0].text if candidate.content.parts else ""

            # finish_reason == 3 (MAX_TOKENS): トークン上限に達した場合
            if candidate.finish_reason == 3:
                error_msg = f"MAX_TOKENS上限に達しました。出力が途中で切れています。({len(text_content)}文字)"
                logger.error(f"[Gemini Vision] {error_msg}")
                logger.error(f"[Gemini Vision] 途中で切れた出力（最後の500文字）: {text_content[-500:]}")
                raise MaxTokensExceededError(
                    message=error_msg,
                    partial_output=text_content,
                    finish_reason_name=finish_reason_name
                )

            # finish_reason != 1 (STOP以外のその他のエラー)
            if candidate.finish_reason != 1:  # 1 = STOP (正常終了)
                error_details = {
                    "finish_reason": candidate.finish_reason,
                    "finish_reason_name": finish_reason_name
                }
                if hasattr(candidate, 'safety_ratings') and candidate.safety_ratings:
                    error_details["safety_ratings"] = [
                        {
                            "category": rating.category.name if hasattr(rating.category, 'name') else str(rating.category),
                            "probability": rating.probability.name if hasattr(rating.probability, 'name') else str(rating.probability)
                        }
                        for rating in candidate.safety_ratings
                    ]
                logger.error(f"Gemini Vision失敗: {error_details}")
                raise ValueError(f"Gemini finish_reason: {finish_reason_name} ({candidate.finish_reason})")

            # テキスト長とトークン使用量をログ出力
            logger.info(f"[Gemini Vision] 応答テキスト長: {len(text_content)}文字")
            if hasattr(response, 'usage_metadata'):
                logger.info(f"[Gemini Vision] トークン使用量: {response.usage_metadata}")

            return text_content

        except Exception as e:
            logger.error(f"Gemini Vision API エラー: {e}")
            raise

    def transcribe_image(
        self,
        image_path: Path,
        prompt: str = "この画像内の表組みやリストを、Markdown形式で正確に書き起こしてください。",
        model: str = "gemini-2.5-pro"
    ) -> Dict[str, Any]:
        """
        画像ファイルをGemini Visionで文字起こし

        Args:
            image_path: 画像ファイルのパス（PNG, JPEG等）
            prompt: Geminiに送るプロンプト
            model: 使用するGeminiモデル（デフォルト: gemini-2.5-pro）

        Returns:
            {"success": bool, "content": str, "model": str, "provider": str}
        """
        if not self.gemini_api_key:
            return {"success": False, "error": "Gemini API key is missing", "model": "gemini-2.5-flash"}

        # 指定されたGeminiモデルを使用
        return self._call_gemini(
            model_name=model,
            prompt=prompt,
            file_path=image_path,
            config={
                "max_tokens": 65536,  # Gemini 2.5の最大出力トークン数（65,536）
                "temperature": 0.0
            }
        )
```

### shared\common\__init__.py

```py

```

### shared\common\ai\__init__.py

```py

```

### shared\common\ai\verified_examples.py

```py
"""
手動検証済みデータを取得してAIの分類精度を向上させる

AIが新規商品を分類する際、過去に人間が検証した良い例を
Few-shot learningの参考として提供します。
"""

from typing import List, Dict, Optional
from supabase import Client


class VerifiedExamplesProvider:
    """検証済み商品例を提供するクラス"""

    def __init__(self, db_client: Client):
        """
        Args:
            db_client: Supabaseクライアント
        """
        self.db = db_client

    def get_verified_examples(
        self,
        limit: int = 20,
        general_name: Optional[str] = None,
        small_category: Optional[str] = None,
        diverse: bool = True
    ) -> List[Dict]:
        """
        手動検証済みの商品例を取得

        Args:
            limit: 取得する例の数（デフォルト: 20）
            general_name: 特定の一般名詞に絞る（オプション）
            small_category: 特定の小カテゴリに絞る（オプション）
            diverse: 多様な例を取得するか（True: 異なる分類から均等に取得）

        Returns:
            検証済み商品のリスト
        """
        query = self.db.table('Rawdata_NETSUPER_items').select(
            'product_name, general_name, small_category, keywords'
        ).eq('manually_verified', True)

        # フィルター条件
        if general_name:
            query = query.eq('general_name', general_name)
        if small_category:
            query = query.eq('small_category', small_category)

        # 最新の検証済みデータを優先
        query = query.order('last_verified_at', desc=True)

        # 取得
        result = query.limit(limit * 2 if diverse else limit).execute()

        if not result.data:
            return []

        # 多様性を確保する場合、各分類から均等に取得
        if diverse and not general_name and not small_category:
            return self._diversify_examples(result.data, limit)

        return result.data[:limit]

    def _diversify_examples(self, examples: List[Dict], limit: int) -> List[Dict]:
        """
        異なる分類から均等に例を取得

        Args:
            examples: 検証済み商品のリスト
            limit: 取得する例の数

        Returns:
            多様な分類を含む商品リスト
        """
        # 一般名詞ごとにグループ化
        groups = {}
        for example in examples:
            general_name = example.get('general_name', '未分類')
            if general_name not in groups:
                groups[general_name] = []
            groups[general_name].append(example)

        # 各グループから順番に取得
        diversified = []
        group_lists = list(groups.values())
        index = 0

        while len(diversified) < limit and index < max(len(g) for g in group_lists):
            for group in group_lists:
                if index < len(group) and len(diversified) < limit:
                    diversified.append(group[index])
            index += 1

        return diversified[:limit]

    def format_examples_for_prompt(
        self,
        examples: List[Dict],
        format_type: str = "numbered"
    ) -> str:
        """
        AI プロンプト用に例をフォーマット

        Args:
            examples: 検証済み商品のリスト
            format_type: フォーマットタイプ（"numbered", "json", "markdown"）

        Returns:
            フォーマットされた文字列
        """
        if not examples:
            return "（検証済みデータなし）"

        if format_type == "numbered":
            lines = ["以下は人間が検証した正しい分類例です：\n"]
            for i, ex in enumerate(examples, 1):
                lines.append(
                    f"{i}. 商品名: {ex['product_name']}\n"
                    f"   一般名詞: {ex.get('general_name', '未設定')}\n"
                    f"   小カテゴリ: {ex.get('small_category', '未設定')}\n"
                )
            return "\n".join(lines)

        elif format_type == "json":
            import json
            return json.dumps([{
                "product_name": ex['product_name'],
                "general_name": ex.get('general_name'),
                "small_category": ex.get('small_category')
            } for ex in examples], ensure_ascii=False, indent=2)

        elif format_type == "markdown":
            lines = ["| 商品名 | 一般名詞 | 小カテゴリ |", "| --- | --- | --- |"]
            for ex in examples:
                lines.append(
                    f"| {ex['product_name']} | "
                    f"{ex.get('general_name', '未設定')} | "
                    f"{ex.get('small_category', '未設定')} |"
                )
            return "\n".join(lines)

        return str(examples)

    def get_category_specific_examples(
        self,
        product_name: str,
        limit: int = 10
    ) -> List[Dict]:
        """
        商品名から推測される分類に関連する例を取得

        Args:
            product_name: 新規商品の商品名
            limit: 取得する例の数

        Returns:
            関連する検証済み商品のリスト
        """
        # シンプルなキーワードマッチング
        # 商品名から主要なキーワードを抽出
        keywords = self._extract_keywords(product_name)

        all_examples = []
        for keyword in keywords[:3]:  # 上位3つのキーワードで検索
            result = self.db.table('Rawdata_NETSUPER_items').select(
                'product_name, general_name, small_category'
            ).eq('manually_verified', True).ilike('product_name', f'%{keyword}%').limit(5).execute()

            all_examples.extend(result.data)

        # 重複を除去
        seen = set()
        unique_examples = []
        for ex in all_examples:
            key = (ex['product_name'], ex.get('general_name'))
            if key not in seen:
                seen.add(key)
                unique_examples.append(ex)

        return unique_examples[:limit]

    def _extract_keywords(self, product_name: str) -> List[str]:
        """
        商品名から主要なキーワードを抽出

        Args:
            product_name: 商品名

        Returns:
            キーワードのリスト
        """
        # スペースで分割
        words = product_name.split()

        # 数字や記号を除去して意味のある単語のみ抽出
        keywords = []
        for word in words:
            # 基本的な日本語/英語の単語のみ抽出
            if len(word) >= 2 and not word.isdigit():
                keywords.append(word)

        return keywords


# 使用例
if __name__ == "__main__":
    import os
    from supabase import create_client

    # Supabase接続
    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = ***REDACTED***"SUPABASE_KEY")
    db = create_client(SUPABASE_URL, SUPABASE_KEY)

    # プロバイダー作成
    provider = VerifiedExamplesProvider(db)

    # 検証済み例を取得
    examples = provider.get_verified_examples(limit=10, diverse=True)
    print(f"取得した検証済み例: {len(examples)}件")

    # プロンプト用にフォーマット
    formatted = provider.format_examples_for_prompt(examples, format_type="numbered")
    print("\n" + formatted)

    # 特定商品に関連する例を取得
    related = provider.get_category_specific_examples("明治おいしい牛乳 900ml")
    print(f"\n関連する検証済み例: {len(related)}件")
```

### shared\common\config\__init__.py

```py

```

### shared\common\config\CLASSIFICATION_MAPPING_v2.0.yaml

```yaml
# 分類マッピング設定ファイル v2.0
#
# このファイルはドキュメント分類のためのマッピング定義を含みます

# 現在は使用されていません（将来の拡張用）
mappings: []
```

### shared\common\config\model_tiers.py

```py
"""
AIモデル構成定義（最小限版）
G_unified_pipeline 移行後、実際に使用されている部分のみを保持
"""

from enum import Enum
from typing import Dict, Any


class AIProvider(Enum):
    """AIプロバイダの定義"""
    GEMINI = "gemini"
    CLAUDE = "claude"
    OPENAI = "openai"


class ModelTier:
    """モデル階層の定義（最小限）"""

    # UI回答生成（デフォルト）
    # G_cloud_run で tier="ui_response" として使用
    UI_RESPONSE_GENERATOR = {
        "provider": AIProvider.GEMINI,
        "model": "gemini-2.5-flash",
        "description": "100万トークンコンテキスト、高速で安定した対話応答",
        "temperature": 0.7,
        "max_tokens": 65536,  # Gemini 2.5 Flashの最大出力トークン数
        "cost_per_1k_tokens": 0.0003
    }

    # Embedding生成
    # LLMClient で tier="embeddings" として使用
    EMBEDDING = {
        "provider": AIProvider.OPENAI,
        "model": "text-embedding-3-small",
        "description": "ベクトル検索用Embedding",
        "dimensions": 1536
    }

    @classmethod
    def get_model_for_task(cls, task: str) -> Dict[str, Any]:
        """タスクに応じた最適なモデルを返す"""
        task_mapping = {
            "ui_response": cls.UI_RESPONSE_GENERATOR,
            "embeddings": cls.EMBEDDING
        }
        return task_mapping.get(task, cls.UI_RESPONSE_GENERATOR)


def get_model_config(tier: str) -> Dict[str, Any]:
    """指定されたタスクの最適モデル設定を取得"""
    return ModelTier.get_model_for_task(tier)


# ============================================
# Deep Research 構成フロー定義
# ============================================
class ResearchFlow:
    """Deep Research構成フローの定義"""

    # 各構成フローの定義: {"steps": [model_list], "description": "説明", "cost": コスト, "ability": 能力}
    FLOWS = {
        "flash-x1": {
            "steps": ["gemini-2.5-flash"],
            "description": "Flash×1 (標準・高速)",
            "cost": 0.64,
            "ability": 8.0
        },
        "lite-lite-pro": {
            "steps": ["gemini-2.5-flash-lite", "gemini-2.5-flash-lite", "gemini-2.5-pro"],
            "description": "Lite→Lite→Pro (コスパ最強)",
            "cost": 2.86,
            "ability": 8.8
        },
        "lite-flash-pro": {
            "steps": ["gemini-2.5-flash-lite", "gemini-2.5-flash", "gemini-2.5-pro"],
            "description": "Lite→Flash→Pro (黄金比・標準)",
            "cost": 2.94,
            "ability": 8.95
        },
        "flash-flash-pro": {
            "steps": ["gemini-2.5-flash", "gemini-2.5-flash", "gemini-2.5-pro"],
            "description": "Flash→Flash→Pro (3回ループの王)",
            "cost": 3.02,
            "ability": 9.3
        },
        "lite-flash-flash-pro": {
            "steps": ["gemini-2.5-flash-lite", "gemini-2.5-flash", "gemini-2.5-flash", "gemini-2.5-pro"],
            "description": "Lite→Flash→Flash→Pro (賢い節約術)",
            "cost": 3.1,
            "ability": 9.45
        },
        "flash-flash-flash-pro": {
            "steps": ["gemini-2.5-flash", "gemini-2.5-flash", "gemini-2.5-flash", "gemini-2.5-pro"],
            "description": "Flash→Flash→Flash→Pro (Deep Research推奨)",
            "cost": 3.18,
            "ability": 9.5
        },
        "lite-lite-flash-flash": {
            "steps": ["gemini-2.5-flash-lite", "gemini-2.5-flash-lite", "gemini-2.5-flash", "gemini-2.5-flash"],
            "description": "Lite→Lite→Flash→Flash (粘りの凡人)",
            "cost": 0.48,
            "ability": 7.35
        },
        "flash-flash-flash": {
            "steps": ["gemini-2.5-flash", "gemini-2.5-flash", "gemini-2.5-flash"],
            "description": "Flash→Flash→Flash (ザ・標準)",
            "cost": 0.48,
            "ability": 7.4
        },
        "lite-lite-lite-flash": {
            "steps": ["gemini-2.5-flash-lite", "gemini-2.5-flash-lite", "gemini-2.5-flash-lite", "gemini-2.5-flash"],
            "description": "Lite→Lite→Lite→Flash (人海戦術)",
            "cost": 0.4,
            "ability": 7.5
        },
        "flash-flash-flash-flash": {
            "steps": ["gemini-2.5-flash", "gemini-2.5-flash", "gemini-2.5-flash", "gemini-2.5-flash"],
            "description": "Flash→Flash→Flash→Flash (優等生の限界)",
            "cost": 0.64,
            "ability": 7.85
        },
        "lite-lite-flash-pro": {
            "steps": ["gemini-2.5-flash-lite", "gemini-2.5-flash-lite", "gemini-2.5-flash", "gemini-2.5-pro"],
            "description": "Lite→Lite→Flash→Pro (泥臭い名探偵)",
            "cost": 3.02,
            "ability": 9.28
        },
        "lite-lite-lite-pro": {
            "steps": ["gemini-2.5-flash-lite", "gemini-2.5-flash-lite", "gemini-2.5-flash-lite", "gemini-2.5-pro"],
            "description": "Lite→Lite→Lite→Pro (質より量の極み)",
            "cost": 2.94,
            "ability": 9.15
        },
        "flash-pro-pro": {
            "steps": ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.5-pro"],
            "description": "Flash→Pro→Pro (精鋭部隊)",
            "cost": 5.56,
            "ability": 9.6
        },
        "lite-pro-pro": {
            "steps": ["gemini-2.5-flash-lite", "gemini-2.5-pro", "gemini-2.5-pro"],
            "description": "Lite→Pro→Pro (一点突破・改)",
            "cost": 5.48,
            "ability": 9.55
        },
        "lite-lite-pro-pro": {
            "steps": ["gemini-2.5-flash-lite", "gemini-2.5-flash-lite", "gemini-2.5-pro", "gemini-2.5-pro"],
            "description": "Lite→Lite→Pro→Pro (素材重視のプロ)",
            "cost": 5.56,
            "ability": 9.65
        },
        "lite-flash-pro-pro": {
            "steps": ["gemini-2.5-flash-lite", "gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.5-pro"],
            "description": "Lite→Flash→Pro→Pro (超・黄金比)",
            "cost": 5.64,
            "ability": 9.7
        },
        "flash-flash-pro-pro": {
            "steps": ["gemini-2.5-flash", "gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.5-pro"],
            "description": "Flash→Flash→Pro→Pro (編集長決済)",
            "cost": 5.72,
            "ability": 9.75
        },
        "pro-pro-pro": {
            "steps": ["gemini-2.5-pro", "gemini-2.5-pro", "gemini-2.5-pro"],
            "description": "Pro→Pro→Pro (富豪の鉄板)",
            "cost": 8.1,
            "ability": 9.85
        },
        "lite-pro-pro-pro": {
            "steps": ["gemini-2.5-flash-lite", "gemini-2.5-pro", "gemini-2.5-pro", "gemini-2.5-pro"],
            "description": "Lite→Pro→Pro→Pro (成り上がり)",
            "cost": 8.18,
            "ability": 9.9
        },
        "flash-pro-pro-pro": {
            "steps": ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.5-pro", "gemini-2.5-pro"],
            "description": "Flash→Pro→Pro→Pro (天才集団)",
            "cost": 8.26,
            "ability": 9.95
        },
        "pro-pro-pro-pro": {
            "steps": ["gemini-2.5-pro", "gemini-2.5-pro", "gemini-2.5-pro", "gemini-2.5-pro"],
            "description": "Pro→Pro→Pro→Pro (全知全能)",
            "cost": 10.8,
            "ability": 9.99
        }
    }

    @classmethod
    def get_flow(cls, flow_id: str) -> Dict[str, Any]:
        """指定されたフローIDの構成を取得"""
        return cls.FLOWS.get(flow_id, cls.FLOWS["flash-x1"])

    @classmethod
    def get_all_flows(cls) -> Dict[str, Dict[str, Any]]:
        """全構成フローを取得"""
        return cls.FLOWS
```

### shared\common\config\settings.py

```py
"""
設定管理
環境変数から設定を読み込む
"""
import os
from pathlib import Path
from dotenv import load_dotenv

# プロジェクトルートの .env ファイルを読み込む
# settings.py の位置: shared/common/config/settings.py
# .env の位置: .env (プロジェクトルート)
_settings_file = Path(__file__)
_project_root = _settings_file.parent.parent.parent.parent
_env_file = _project_root / ".env"

if _env_file.exists():
    load_dotenv(_env_file, override=True)
else:
    # フォールバック: カレントディレクトリから探す
    load_dotenv(override=True)


class Settings:
    """アプリケーション設定"""

    # OpenAI API Key
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")

    # Google AI API Key
    GOOGLE_AI_API_KEY: str = os.getenv("GOOGLE_AI_API_KEY", "")

    # Anthropic API Key
    ANTHROPIC_API_KEY: str = os.getenv("ANTHROPIC_API_KEY", "")
    
    # Supabase
    SUPABASE_URL: str = os.getenv("SUPABASE_URL", "")
    SUPABASE_KEY: str = os.getenv("SUPABASE_KEY", "")
    SUPABASE_SERVICE_ROLE_KEY: str = os.getenv("SUPABASE_SERVICE_ROLE_KEY", "")
    
    # Google Drive
    PERSONAL_FOLDER_ID: str = os.getenv("PERSONAL_FOLDER_ID", "")
    FAMILY_FOLDER_ID: str = os.getenv("FAMILY_FOLDER_ID", "")
    WORK_FOLDER_ID: str = os.getenv("WORK_FOLDER_ID", "")

    # Google Drive InBox監視システム用
    INBOX_FOLDER_ID: str = os.getenv("INBOX_FOLDER_ID", "")
    ARCHIVE_FOLDER_ID: str = os.getenv("ARCHIVE_FOLDER_ID", "")
    
    # プロジェクトルート
    PROJECT_ROOT: Path = Path(__file__).parent.parent
    
    # データディレクトリ
    DATA_DIR: Path = PROJECT_ROOT / "data"
    TEMP_DIR: Path = DATA_DIR / "temp"
    SCHEMAS_DIR: Path = PROJECT_ROOT / "config" / "schemas"
    
    def __init__(self):
        """初期化時にディレクトリを作成"""
        self.DATA_DIR.mkdir(exist_ok=True)
        self.TEMP_DIR.mkdir(exist_ok=True)


# シングルトンインスタンス
settings = Settings()
```

### shared\common\config\user_context.yaml

```yaml
# ユーザー前提情報設定ファイル（最新版）
# ご家族の最新プロフィールと所属を反映しました

# 家族構成
family:
  father:
    name: "yoshinori"
    display_name: "宜紀"
    full_name: "大久保 宜紀"
    aliases: ["オオクボ ヨシノリ", "芳紀"]
    birth_date: "1974-09-10"

  mother:
    name: "kayako"
    display_name: "香屋子"
    full_name: "大久保 香屋子"
    aliases: ["オオクボ カヤコ", "カヤコ"]
    birth_date: "1978-04-24"

  children_list:
    - name: "ema"
      display_name: "絵麻"
      full_name: "大久保 絵麻"
      aliases: ["エマ", "えまちゃん"]
    - name: "ikuya"
      display_name: "育哉"
      full_name: "大久保 育哉"
      aliases: ["イクヤ", "いくちゃん"]

# 組織情報（代表的な所属）
organizations:
  schools:
    ema: "東洋英和女学院中学部"
    ikuya: "洗足学園小学校"
  cram_schools:
    ikuya: "早稲田アカデミー 武蔵小杉校"

# 認証・メール設定
auth:
  default_email: "ookubo.y@workspace-o.com"

# ワークスペース定義
workspaces:
  children:
    - person: "ema"
      categories:
        - id: "SCHOOL"
          display_name: "学校関連（東洋英和）"
          labels: ["学校", "東洋英和", "SCHOOL"]
        - id: "MAIL"
          display_name: "メール"
          labels: ["MAIL"]

    - person: "ikuya"
      categories:
        - id: "SCHOOL"
          display_name: "学校関連（洗足）"
          labels: ["学校", "洗足", "SCHOOL"]
        - id: "JUKU"
          display_name: "塾関連（早稲アカ）"
          labels: ["塾", "早稲アカ", "早稲田アカデミー", "JUKU"]
        - id: "MAIL"
          display_name: "メール"
          labels: ["MAIL"]

  family_shared:
    - id: "HOME_LIVING"
      display_name: "家庭・生活関連"
      labels: ["HOME", "家庭"]
    - id: "HOME_COOKING"
      display_name: "料理・レシピ関連"
      labels: ["COOKING", "料理"]

  personal:
    - person: "yoshinori"
      id: "PRIVATE_FOLDER"
      display_name: "個人フォルダ"
      labels: ["PRIVATE"]

  general:
    - id: "BUSINESS_WORK"
      display_name: "仕事全般"
      labels: ["WORK", "仕事"]
    - id: "WORK_MAIL"
      display_name: "仕事メール"
      labels: ["WORK_MAIL"]

# 子供ごとの詳細情報
children:
  - name: "絵麻"
    id: "ema"
    birth_date: "2012-07-14"
    school:
      name: "東洋英和女学院中学部"
    cram_school:
      name: "未設定"

  - name: "育哉"
    id: "ikuya"
    birth_date: "2014-08-24"
    school:
      name: "洗足学園小学校"
    cram_school:
      name: "早稲田アカデミー 武蔵小杉校"

# システム設定
settings:
  search_preferences:
    prioritize_recent: true
    max_results: 5
```

### shared\common\config\yaml_loader.py

```py
"""
YAML Loader
CLASSIFICATION_MAPPING_v2.0.yaml を読み込んで文字列として返す
"""
from pathlib import Path
import yaml


def get_classification_yaml_string() -> str:
    """
    CLASSIFICATION_MAPPING_v2.0.yaml を読み込んで文字列として返す
    
    Returns:
        YAMLファイルの内容（文字列）
    """
    yaml_path = Path(__file__).parent / "CLASSIFICATION_MAPPING_v2.0.yaml"
    
    if not yaml_path.exists():
        raise FileNotFoundError(f"分類マッピングファイルが見つかりません: {yaml_path}")
    
    with open(yaml_path, "r", encoding="utf-8") as f:
        return f.read()


def load_classification_mapping() -> dict:
    """
    CLASSIFICATION_MAPPING_v2.0.yaml を辞書として読み込む

    Returns:
        YAMLファイルの内容（辞書）
    """
    yaml_path = Path(__file__).parent / "CLASSIFICATION_MAPPING_v2.0.yaml"

    if not yaml_path.exists():
        raise FileNotFoundError(f"分類マッピングファイルが見つかりません: {yaml_path}")

    with open(yaml_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def load_user_context() -> dict:
    """
    user_context.yaml を辞書として読み込む

    Returns:
        YAMLファイルの内容（辞書）
        ファイルが存在しない場合は空の辞書を返す
    """
    yaml_path = Path(__file__).parent / "user_context.yaml"

    if not yaml_path.exists():
        # ファイルが存在しない場合は空の辞書を返す（エラーにしない）
        return {"children": [], "settings": {}}

    with open(yaml_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def get_user_context_string() -> str:
    """
    user_context.yaml を読み込んで文字列として返す

    Returns:
        YAMLファイルの内容（文字列）
        ファイルが存在しない場合は空文字列を返す
    """
    yaml_path = Path(__file__).parent / "user_context.yaml"

    if not yaml_path.exists():
        return ""

    with open(yaml_path, "r", encoding="utf-8") as f:
        return f.read()


def get_family_info() -> dict:
    """
    user_context.yaml から家族構成情報を取得

    Returns:
        家族構成情報（辞書）
        例: {'father': {'name': 'yoshinori', 'display_name': '宜紀'}, ...}
    """
    context = load_user_context()
    return context.get("family", {})


def get_organization_info() -> dict:
    """
    user_context.yaml から組織情報（学校名、クラス名など）を取得

    Returns:
        組織情報（辞書）
        例: {'school': {'name': '洗足学園小学校', 'current_class': '2025_5B'}}
    """
    context = load_user_context()
    return context.get("organizations", {})


def get_auth_info() -> dict:
    """
    user_context.yaml から認証情報を取得

    Returns:
        認証情報（辞書）
        例: {'default_email': 'ookubo.y@workspace-o.com'}
    """
    context = load_user_context()
    return context.get("auth", {})
```

### shared\common\connectors\__init__.py

```py

```

### shared\common\connectors\gmail_connector.py

```py
"""
Gmail コネクタ (サービスアカウント + ドメイン全体の委任)

Google Workspaceアカウントで、サービスアカウントを使ってGmail APIにアクセス。
アプリパスワード不要の安全な方法。

設定方法: docs/GMAIL_INTEGRATION_SETUP.md を参照
"""
import os
import base64
from typing import List, Dict, Any, Optional
from google.oauth2 import service_account
from googleapiclient.discovery import build
from loguru import logger
from email.mime.text import MIMEText
from datetime import datetime

# 認証情報ファイルのパス (環境変数から取得、なければローカルのフォールバック)
CREDENTIALS_PATH = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

# ローカル開発用のフォールバックパス
_LOCAL_CREDENTIALS_PATHS = [
    os.path.join(os.path.dirname(__file__), '..', '..', '..', '.local', '_runtime', 'credentials', 'google_credentials.json'),
    os.path.join(os.path.dirname(__file__), '..', '..', '..', '_runtime', 'credentials', 'google_credentials.json'),
]

# 環境変数がない場合、ローカルパスを探す
if not CREDENTIALS_PATH:
    for path in _LOCAL_CREDENTIALS_PATHS:
        abs_path = os.path.abspath(path)
        if os.path.exists(abs_path):
            CREDENTIALS_PATH = abs_path
            break

# Gmail APIのスコープ
SCOPES = [
    'https://www.googleapis.com/auth/gmail.readonly',
    'https://www.googleapis.com/auth/gmail.modify',
]


class GmailConnector:
    """Gmail APIクライアント（サービスアカウント認証）"""

    def __init__(self, user_email: str):
        """
        Gmail APIに接続

        Args:
            user_email: アクセス対象のメールアドレス（例: ookubo.y@workspace-o.com）
        """
        self.user_email = user_email
        self.service = self._authenticate()
        logger.info(f"GmailConnector初期化完了: {user_email}")

    def _authenticate(self):
        """
        サービスアカウント認証 + ドメイン全体の委任

        Returns:
            Gmail APIサービスオブジェクト
        """
        # 優先順位1: 環境変数 GOOGLE_APPLICATION_CREDENTIALS
        if CREDENTIALS_PATH and os.path.exists(CREDENTIALS_PATH):
            try:
                creds = service_account.Credentials.from_service_account_file(
                    CREDENTIALS_PATH,
                    scopes=SCOPES,
                    subject=self.user_email  # ドメイン全体の委任: 対象ユーザーを指定
                )
                logger.info(f"環境変数から認証成功: {CREDENTIALS_PATH}")
                return build('gmail', 'v1', credentials=creds)
            except Exception as e:
                logger.warning(f"環境変数からの認証失敗: {e}")

        # 優先順位2: ADC (Application Default Credentials) - Cloud Run用
        try:
            import google.auth
            creds, project = google.auth.default(scopes=SCOPES)
            # ドメイン全体の委任のため、subjectを指定
            creds_with_subject = creds.with_subject(self.user_email)
            logger.info("ADC (Application Default Credentials) で認証成功")
            return build('gmail', 'v1', credentials=creds_with_subject)
        except Exception as e:
            logger.warning(f"ADC認証失敗: {e}")

        # 優先順位3: Streamlit Secrets (デプロイ環境用)
        try:
            import streamlit as st
            if hasattr(st, 'secrets') and 'gcp_service_account' in st.secrets:
                creds_dict = dict(st.secrets["gcp_service_account"])
                creds = service_account.Credentials.from_service_account_info(
                    creds_dict,
                    scopes=SCOPES,
                    subject=self.user_email
                )
                logger.info("Streamlit Secretsから認証成功")
                return build('gmail', 'v1', credentials=creds)
        except ImportError:
            pass
        except Exception as e:
            logger.warning(f"Streamlit Secretsからの認証失敗: {e}")

        # 全て失敗した場合
        raise FileNotFoundError(
            f"認証情報が見つかりません。以下のいずれかを設定してください:\n"
            f"1. 環境変数 GOOGLE_APPLICATION_CREDENTIALS (現在: {CREDENTIALS_PATH})\n"
            f"2. Cloud Run のサービスアカウントに権限が付与されているか (ADC)\n"
            f"3. Streamlit Secrets の gcp_service_account\n"
            f"\n設定方法: docs/GMAIL_INTEGRATION_SETUP.md を参照"
        )

    def list_messages(
        self,
        query: str = '',
        max_results: int = 10,
        label_ids: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        メール一覧を取得

        Args:
            query: Gmail検索クエリ（例: 'is:unread', 'from:example@gmail.com'）
            max_results: 取得する最大件数
            label_ids: フィルタするラベルID（例: ['INBOX', 'UNREAD']）

        Returns:
            メッセージのリスト（軽量版: IDとスレッドIDのみ）
        """
        try:
            params = {
                'userId': 'me',
                'maxResults': max_results,
            }
            if query:
                params['q'] = query
            if label_ids:
                params['labelIds'] = label_ids

            results = self.service.users().messages().list(**params).execute()
            messages = results.get('messages', [])

            logger.info(f"メール一覧取得: {len(messages)}件")
            return messages

        except Exception as e:
            logger.error(f"メール一覧取得エラー: {e}")
            return []

    def get_message(self, message_id: str, format: str = 'full') -> Optional[Dict[str, Any]]:
        """
        メールの詳細を取得

        Args:
            message_id: メッセージID
            format: 取得形式（'full', 'metadata', 'minimal'）

        Returns:
            メッセージの詳細情報
        """
        try:
            message = self.service.users().messages().get(
                userId='me',
                id=message_id,
                format=format
            ).execute()

            logger.debug(f"メール取得成功: {message_id}")
            return message

        except Exception as e:
            logger.error(f"メール取得エラー ({message_id}): {e}")
            return None

    def get_attachment(self, message_id: str, attachment_id: str) -> Optional[bytes]:
        """
        添付ファイルのデータを取得

        Args:
            message_id: メッセージID
            attachment_id: 添付ファイルのID

        Returns:
            添付ファイルのバイナリデータ
        """
        try:
            attachment = self.service.users().messages().attachments().get(
                userId='me',
                messageId=message_id,
                id=attachment_id
            ).execute()

            # Base64URLデコード
            data = attachment['data']
            file_data = base64.urlsafe_b64decode(data.encode('UTF-8'))

            logger.debug(f"添付ファイル取得成功: {attachment_id}")
            return file_data

        except Exception as e:
            logger.error(f"添付ファイル取得エラー ({attachment_id}): {e}")
            return None

    def parse_message_headers(self, message: Dict[str, Any]) -> Dict[str, str]:
        """
        メールヘッダーをパース

        Args:
            message: get_message()で取得したメッセージ

        Returns:
            ヘッダー情報の辞書（Subject, From, To, Date など）
        """
        headers = {}
        if 'payload' in message and 'headers' in message['payload']:
            for header in message['payload']['headers']:
                name = header['name']
                value = header['value']
                headers[name] = value

        return headers

    def extract_message_parts(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """
        メールの本文と添付ファイルを抽出

        Args:
            message: get_message()で取得したメッセージ

        Returns:
            {
                'text_plain': str,  # テキスト本文
                'text_html': str,   # HTML本文
                'attachments': [    # 添付ファイル情報
                    {
                        'filename': str,
                        'mimeType': str,
                        'attachmentId': str,
                        'size': int
                    },
                    ...
                ]
            }
        """
        result = {
            'text_plain': '',
            'text_html': '',
            'attachments': []
        }

        def parse_parts(parts):
            """再帰的にパートを解析"""
            for part in parts:
                mime_type = part.get('mimeType', '')
                filename = part.get('filename', '')

                # 本文の抽出
                if mime_type == 'text/plain' and not filename:
                    if 'data' in part.get('body', {}):
                        data = part['body']['data']
                        text = base64.urlsafe_b64decode(data.encode('UTF-8')).decode('utf-8', errors='replace')
                        result['text_plain'] += text

                elif mime_type == 'text/html' and not filename:
                    if 'data' in part.get('body', {}):
                        data = part['body']['data']
                        text = base64.urlsafe_b64decode(data.encode('UTF-8')).decode('utf-8', errors='replace')
                        result['text_html'] += text

                # 添付ファイルの抽出
                elif filename or part['body'].get('attachmentId'):
                    # Content-IDヘッダーを取得（インライン画像の場合）
                    headers = {}
                    if 'headers' in part:
                        for header in part['headers']:
                            headers[header['name']] = header['value']

                    attachment_info = {
                        'filename': filename,
                        'mimeType': mime_type,
                        'attachmentId': part['body'].get('attachmentId'),
                        'size': part['body'].get('size', 0),
                        'headers': headers  # Content-IDなどのヘッダー情報
                    }
                    result['attachments'].append(attachment_info)

                # マルチパートの場合は再帰
                if 'parts' in part:
                    parse_parts(part['parts'])

        # メッセージのペイロードを解析
        if 'payload' in message:
            payload = message['payload']
            if 'parts' in payload:
                parse_parts(payload['parts'])
            else:
                # シングルパート（本文のみ）
                parse_parts([payload])

        return result

    def convert_html_with_inline_images(
        self,
        message_id: str,
        html_content: str,
        attachments: List[Dict[str, Any]]
    ) -> str:
        """
        HTMLメール内のCID参照画像をBASE64形式に変換

        Args:
            message_id: メッセージID
            html_content: HTML本文
            attachments: 添付ファイル情報のリスト

        Returns:
            処理済みHTML（CID参照がdata:image形式に置換されたもの）
        """
        import re

        if not html_content or not attachments:
            return html_content

        processed_html = html_content

        # CID参照パターンを検索: src="cid:xxxxx"
        cid_pattern = re.compile(r'src=["\']cid:([^"\']+)["\']', re.IGNORECASE)
        cid_matches = cid_pattern.findall(html_content)

        if not cid_matches:
            return html_content

        logger.info(f"CID参照画像を検出: {len(cid_matches)}件")

        # 各CIDに対して処理
        for cid in cid_matches:
            # Content-IDが一致する添付ファイルを探す
            matching_attachment = None
            for att in attachments:
                # Content-IDは通常 <xxxxx> の形式
                att_headers = att.get('headers', {})
                content_id = att_headers.get('Content-ID', '').strip('<>')

                if content_id == cid:
                    matching_attachment = att
                    break

            if matching_attachment:
                attachment_id = matching_attachment.get('attachmentId')
                mime_type = matching_attachment.get('mimeType', 'image/png')

                if attachment_id:
                    # 添付ファイルデータを取得
                    att_data = self.get_attachment(message_id, attachment_id)

                    if att_data:
                        # BASE64エンコード（既にBASE64の場合もあるので確認）
                        try:
                            # att_dataがbytesの場合
                            if isinstance(att_data, bytes):
                                b64_data = base64.b64encode(att_data).decode('ascii')
                            else:
                                b64_data = att_data

                            # data:image形式に変換
                            data_uri = f"data:{mime_type};base64,{b64_data}"

                            # HTMLを置換
                            processed_html = processed_html.replace(
                                f'src="cid:{cid}"',
                                f'src="{data_uri}"'
                            )
                            processed_html = processed_html.replace(
                                f"src='cid:{cid}'",
                                f"src='{data_uri}'"
                            )

                            logger.info(f"CID画像を埋め込み: cid:{cid} -> {mime_type}")

                        except Exception as e:
                            logger.error(f"画像の埋め込みに失敗: cid:{cid}, {e}")

        return processed_html

    def modify_labels(
        self,
        message_id: str,
        add_labels: Optional[List[str]] = None,
        remove_labels: Optional[List[str]] = None
    ) -> bool:
        """
        メールのラベルを変更（既読マーク、アーカイブなど）

        Args:
            message_id: メッセージID
            add_labels: 追加するラベルID（例: ['STARRED']）
            remove_labels: 削除するラベルID（例: ['UNREAD', 'INBOX']）

        Returns:
            成功したかどうか
        """
        try:
            body = {}
            if add_labels:
                body['addLabelIds'] = add_labels
            if remove_labels:
                body['removeLabelIds'] = remove_labels

            self.service.users().messages().modify(
                userId='me',
                id=message_id,
                body=body
            ).execute()

            logger.info(f"ラベル変更成功: {message_id}")
            return True

        except Exception as e:
            logger.error(f"ラベル変更エラー ({message_id}): {e}")
            return False

    def mark_as_read(self, message_id: str) -> bool:
        """
        メールを既読にする

        Args:
            message_id: メッセージID

        Returns:
            成功したかどうか
        """
        return self.modify_labels(message_id, remove_labels=['UNREAD'])

    def archive_message(self, message_id: str) -> bool:
        """
        メールをアーカイブ（受信トレイから削除）

        Args:
            message_id: メッセージID

        Returns:
            成功したかどうか
        """
        return self.modify_labels(message_id, remove_labels=['INBOX'])

    def list_labels(self) -> List[Dict[str, Any]]:
        """
        利用可能なラベル一覧を取得

        Returns:
            ラベル情報のリスト
        """
        try:
            results = self.service.users().labels().list(userId='me').execute()
            labels = results.get('labels', [])

            logger.info(f"ラベル一覧取得: {len(labels)}件")
            return labels

        except Exception as e:
            logger.error(f"ラベル一覧取得エラー: {e}")
            return []

    def get_label_id_by_name(self, label_name: str) -> Optional[str]:
        """
        ラベル名からラベルIDを取得

        Args:
            label_name: ラベル名（例: 'processed', 'ゴミ箱'）

        Returns:
            ラベルID、見つからない場合はNone
        """
        labels = self.list_labels()
        for label in labels:
            if label.get('name') == label_name:
                return label.get('id')
        logger.warning(f"ラベルが見つかりません: {label_name}")
        return None

    def move_to_trash_label(self, message_id: str) -> bool:
        """
        メールを'processed'ラベルから'ゴミ箱'ラベルに移動

        Args:
            message_id: メッセージID

        Returns:
            成功したかどうか
        """
        try:
            # processedラベルとゴミ箱ラベルのIDを取得
            processed_label_id = self.get_label_id_by_name('processed')
            trash_label_id = self.get_label_id_by_name('ゴミ箱')

            if not processed_label_id or not trash_label_id:
                logger.error("必要なラベルが見つかりません（'processed'または'ゴミ箱'）")
                return False

            # ラベルを変更
            return self.modify_labels(
                message_id,
                add_labels=[trash_label_id],
                remove_labels=[processed_label_id]
            )

        except Exception as e:
            logger.error(f"ゴミ箱ラベルへの移動エラー ({message_id}): {e}")
            return False

    def trash_message(self, message_id: str) -> bool:
        """
        Gmail APIを使ってメールをゴミ箱に移動（実際にGmailのゴミ箱に入る）

        Args:
            message_id: メッセージID

        Returns:
            成功したかどうか
        """
        try:
            self.service.users().messages().trash(
                userId='me',
                id=message_id
            ).execute()
            logger.info(f"メールをゴミ箱に移動しました: {message_id}")
            return True

        except Exception as e:
            logger.error(f"メールのゴミ箱移動エラー ({message_id}): {e}")
            return False
```

### shared\common\connectors\google_drive.py

```py
"""
Google Drive コネクタ (サービスアカウント認証)

設計書: COMPLETE_IMPLEMENTATION_GUIDE_v3.md の 1.4節に基づき、Google Driveと通信する。
"""
import os
from typing import List, Dict, Any, Optional, Union
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload, MediaInMemoryUpload
from io import FileIO, BytesIO
from loguru import logger
from pathlib import Path

# 認証情報ファイルのパス (環境変数から取得、なければローカルのフォールバック)
CREDENTIALS_PATH = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

# ローカル開発用のフォールバックパス
_LOCAL_CREDENTIALS_PATHS = [
    os.path.join(os.path.dirname(__file__), '..', '..', '..', '.local', '_runtime', 'credentials', 'google_credentials.json'),
    os.path.join(os.path.dirname(__file__), '..', '..', '..', '_runtime', 'credentials', 'google_credentials.json'),
]

# 環境変数がない場合、ローカルパスを探す
if not CREDENTIALS_PATH:
    for path in _LOCAL_CREDENTIALS_PATHS:
        abs_path = os.path.abspath(path)
        if os.path.exists(abs_path):
            CREDENTIALS_PATH = abs_path
            break

SCOPES = ['https://www.googleapis.com/auth/drive']

class GoogleDriveConnector:
    """Google Drive APIクライアント"""
    
    def __init__(self):
        self.service = self._authenticate()
        # logger.info("Google Driveコネクタ初期化完了")
    
    def _authenticate(self):
        """サービスアカウント認証（環境変数ファイル -> ADC -> Streamlit Secrets の順で試行）"""
        # 1. 環境変数 (ローカル開発用: JSONファイルパス指定)
        if CREDENTIALS_PATH and os.path.exists(CREDENTIALS_PATH):
            try:
                creds = service_account.Credentials.from_service_account_file(
                    CREDENTIALS_PATH, scopes=SCOPES
                )
                logger.info(f"環境変数から認証成功: {CREDENTIALS_PATH}")
                return build('drive', 'v3', credentials=creds)
            except Exception as e:
                logger.warning(f"環境変数からの認証失敗: {e}")

        # 2. Application Default Credentials (ADC) (★Cloud Run用: これを追加！★)
        try:
            import google.auth
            # Cloud Run等の環境では自動的に認証情報を取得（ファイル不要）
            creds, project = google.auth.default(scopes=SCOPES)
            logger.info("ADC (Application Default Credentials) で認証成功")
            return build('drive', 'v3', credentials=creds)
        except Exception as e:
            logger.warning(f"ADC認証失敗: {e}")

        # 3. Streamlit Secrets (Streamlit Cloud用)
        try:
            import streamlit as st
            if hasattr(st, 'secrets') and 'gcp_service_account' in st.secrets:
                creds_dict = dict(st.secrets["gcp_service_account"])
                creds = service_account.Credentials.from_service_account_info(
                    creds_dict, scopes=SCOPES
                )
                logger.info("Streamlit Secretsから認証成功")
                return build('drive', 'v3', credentials=creds)
        except ImportError:
            pass
        except Exception as e:
            logger.warning(f"Streamlit Secretsからの認証失敗: {e}")

        # 全て失敗した場合
        raise FileNotFoundError(
            f"認証情報が見つかりません。以下のいずれかを設定してください:\n"
            f"1. 環境変数 GOOGLE_APPLICATION_CREDENTIALS (現在: {CREDENTIALS_PATH})\n"
            f"2. Cloud Run のサービスアカウントに権限が付与されているか (ADC)\n"
            f"3. Streamlit Secrets が設定されているか"
        )

    def list_files_in_folder(self, folder_id: str, mime_type_filter: str = None) -> List[Dict[str, Any]]:
        """
        指定されたフォルダ内のファイルを一覧表示
        
        Args:
            folder_id: 親フォルダのID
            mime_type_filter: MIMEタイプによるフィルタリング（オプション）
            
        Returns:
            ファイルメタデータのリスト
        """
        # フォルダを除外するクエリを構築
        query = f"'{folder_id}' in parents and trashed=false"
        
        # デフォルトでフォルダを除外
        if mime_type_filter is None:
            query += " and mimeType != 'application/vnd.google-apps.folder'"
        else:
            query += f" and {mime_type_filter}"
        
        try:
            results = self.service.files().list(
                q=query,
                spaces='drive',
                fields='nextPageToken, files(id, name, mimeType, size)',
                supportsAllDrives=True,  # 共有ドライブ対応
                includeItemsFromAllDrives=True,  # 共有ドライブのアイテムを含む
                corpora='allDrives'  # すべてのドライブから検索
            ).execute()

            return results.get('files', [])
        except Exception as e:
            print(f"Error listing files: {e}")
            return []

    def download_file(self, file_id: str, file_name: str, dest_dir: Union[str, Path]) -> Optional[str]:
        """
        Google Driveからファイルをダウンロードし、一時パスを返す

        Args:
            file_id: ファイルのID
            file_name: ファイル名
            dest_dir: 保存先ディレクトリ

        Returns:
            ダウンロードされたファイルへのローカルパス（失敗時はNone）
        """
        try:
            logger.info(f"ファイルダウンロード開始: {file_name} (ID: {file_id})")

            dest_path = Path(dest_dir) / file_name
            dest_path.parent.mkdir(parents=True, exist_ok=True)

            # DriveのMIMEタイプをチェックし、Google Docs形式の場合はエクスポート
            # ★重要: 共有ドライブや他人所有のファイルにアクセスするためのフラグ
            file_metadata = self.service.files().get(
                fileId=file_id,
                fields='mimeType',
                supportsAllDrives=True
            ).execute()
            mime_type = file_metadata['mimeType']
            logger.info(f"ファイルMIMEタイプ: {mime_type}")

            request = None
            if mime_type == 'application/vnd.google-apps.document':
                # Google Docs -> DOCXとしてエクスポート
                request = self.service.files().export(
                    fileId=file_id,
                    mimeType='application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                    supportsAllDrives=True  # 共有ドライブ対応
                )
                dest_path = dest_path.with_suffix('.docx')
            elif mime_type == 'application/vnd.google-apps.spreadsheet':
                # Google Sheets -> XLSXとしてエクスポート
                request = self.service.files().export(
                    fileId=file_id,
                    mimeType='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                    supportsAllDrives=True  # 共有ドライブ対応
                )
                dest_path = dest_path.with_suffix('.xlsx')
            elif mime_type == 'application/vnd.google-apps.presentation':
                # Google Slides -> PPTXとしてエクスポート
                request = self.service.files().export(
                    fileId=file_id,
                    mimeType='application/vnd.openxmlformats-officedocument.presentationml.presentation',
                    supportsAllDrives=True  # 共有ドライブ対応
                )
                dest_path = dest_path.with_suffix('.pptx')
            else:
                # 通常のファイル (PDF, DOCXなど) はダウンロード
                # ★重要: 共有ドライブや他人所有のファイルにアクセスするためのフラグ
                request = self.service.files().get_media(
                    fileId=file_id,
                    supportsAllDrives=True
                )

            with open(dest_path, 'wb') as fh:
                downloader = MediaIoBaseDownload(fh, request)
                done = False
                while done is False:
                    status, done = downloader.next_chunk()
                    if status:
                        logger.debug(f"ダウンロード進捗: {int(status.progress() * 100)}%")

            logger.info(f"ファイルダウンロード完了: {dest_path} ({dest_path.stat().st_size} bytes)")
            return str(dest_path)

        except Exception as e:
            # 404エラー（ファイル未存在）は想定内の動作なのでINFOレベルで記録
            error_str = str(e)
            is_404 = 'File not found' in error_str or '404' in error_str or 'not found' in error_str.lower()

            if is_404:
                logger.info(f"ファイルが見つかりません（想定内）: {file_name}")
                logger.debug(f"404詳細: {error_str}")
            else:
                # 404以外の実際のエラーはERRORレベルで記録
                logger.error("ファイルダウンロードエラー: " + file_name)
                logger.error(f"エラー内容: {error_str}")
                logger.error(f"エラータイプ: {type(e).__name__}")
                logger.debug("エラー詳細", exc_info=True)

            # エラーを再スローして呼び出し側で処理できるようにする
            raise

    def get_inbox_folder_id(self) -> Optional[str]:
        """
        環境変数からInBoxフォルダIDを取得

        Returns:
            InBoxフォルダID、設定されていない場合はNone
        """
        inbox_folder_id = os.getenv("INBOX_FOLDER_ID")
        if not inbox_folder_id:
            logger.warning("INBOX_FOLDER_ID が環境変数に設定されていません")
        return inbox_folder_id

    def get_archive_folder_id(self) -> Optional[str]:
        """
        環境変数からArchiveフォルダIDを取得

        Returns:
            ArchiveフォルダID、設定されていない場合はNone
        """
        archive_folder_id = os.getenv("ARCHIVE_FOLDER_ID")
        if not archive_folder_id:
            logger.warning("ARCHIVE_FOLDER_ID が環境変数に設定されていません")
        return archive_folder_id

    def list_inbox_files(
        self,
        folder_id: str,
        processed_file_ids: List[str]
    ) -> List[Dict[str, Any]]:
        """
        InBoxフォルダ内の新規ファイルを取得

        Args:
            folder_id: InBoxフォルダのID
            processed_file_ids: 既に処理済みのファイルIDリスト

        Returns:
            未処理のファイルメタデータリスト
        """
        try:
            # フォルダ内の全ファイルを取得（PDFのみ）
            all_files = self.list_files_in_folder(
                folder_id,
                mime_type_filter="mimeType='application/pdf'"
            )

            # 未処理のファイルのみをフィルタリング
            new_files = [
                file for file in all_files
                if file['id'] not in processed_file_ids
            ]

            logger.info(f"InBox内の全ファイル数: {len(all_files)}, 新規ファイル数: {len(new_files)}")
            return new_files

        except Exception as e:
            logger.error(f"InBoxファイルリスト取得エラー: {e}")
            return []

    def move_file(self, file_id: str, new_folder_id: str) -> bool:
        """
        ファイルを別のフォルダに移動

        Args:
            file_id: 移動するファイルのID
            new_folder_id: 移動先フォルダのID

        Returns:
            成功した場合True、失敗した場合False
        """
        try:
            # 現在の親フォルダを取得
            file = self.service.files().get(
                fileId=file_id,
                fields='parents',
                supportsAllDrives=True  # 共有ドライブ対応
            ).execute()

            previous_parents = ",".join(file.get('parents', []))

            # ファイルを新しいフォルダに移動（古い親を削除し、新しい親を追加）
            self.service.files().update(
                fileId=file_id,
                addParents=new_folder_id,
                removeParents=previous_parents,
                fields='id, parents',
                supportsAllDrives=True  # 共有ドライブ対応
            ).execute()

            logger.info(f"ファイル移動成功: {file_id} -> {new_folder_id}")
            return True

        except Exception as e:
            logger.error(f"ファイル移動エラー ({file_id}): {e}")
            return False

    def rename_file(self, file_id: str, new_name: str) -> bool:
        """
        Google Driveのファイル名を変更

        Args:
            file_id: 変更するファイルのID
            new_name: 新しいファイル名（拡張子を含む）

        Returns:
            成功した場合True、失敗した場合False
        """
        try:
            # ファイル名を更新
            self.service.files().update(
                fileId=file_id,
                body={'name': new_name},
                fields='id, name',
                supportsAllDrives=True  # 共有ドライブ対応
            ).execute()

            logger.info(f"ファイル名変更成功: {file_id} -> {new_name}")
            return True

        except Exception as e:
            logger.error(f"ファイル名変更エラー ({file_id}): {e}")
            return False

    def upload_file(
        self,
        file_content: Union[bytes, str],
        file_name: str,
        mime_type: str,
        folder_id: Optional[str] = None,
        max_retries: int = 3
    ) -> Optional[str]:
        """
        ファイルをGoogle Driveにアップロード（共有ドライブ対応、リトライ機能付き）

        Args:
            file_content: ファイルの内容（バイトまたは文字列）
            file_name: ファイル名
            mime_type: MIMEタイプ（例: 'text/html', 'application/pdf'）
            folder_id: 保存先フォルダID（Noneの場合はルート）
            max_retries: 最大リトライ回数（デフォルト: 3）

        Returns:
            アップロードされたファイルのID、失敗時はNone
        """
        import time

        # ファイルメタデータ
        file_metadata = {'name': file_name}
        if folder_id:
            file_metadata['parents'] = [folder_id]

        # 文字列の場合はバイトに変換
        if isinstance(file_content, str):
            file_content = file_content.encode('utf-8')

        # リトライループ
        for attempt in range(max_retries):
            try:
                # メモリ上のデータからアップロード
                media = MediaInMemoryUpload(
                    file_content,
                    mimetype=mime_type,
                    resumable=True
                )

                # 共有ドライブ対応: supportsAllDrives=True を追加
                file = self.service.files().create(
                    body=file_metadata,
                    media_body=media,
                    fields='id, name, webViewLink',
                    supportsAllDrives=True
                ).execute()

                logger.info(f"ファイルアップロード成功: {file_name} (ID: {file['id']})")
                return file['id']

            except Exception as e:
                error_message = str(e)
                is_timeout = 'timeout' in error_message.lower() or 'timed out' in error_message.lower()

                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2  # 2秒、4秒、6秒...
                    logger.warning(
                        f"ファイルアップロードエラー ({file_name}): {error_message} "
                        f"- リトライ {attempt + 1}/{max_retries} ({wait_time}秒待機)"
                    )
                    time.sleep(wait_time)
                else:
                    logger.error(f"ファイルアップロード失敗（最終試行） ({file_name}): {error_message}")
                    return None

    def upload_file_from_path(
        self,
        file_path: Union[str, Path],
        folder_id: Optional[str] = None,
        mime_type: Optional[str] = None
    ) -> Optional[str]:
        """
        ローカルファイルをGoogle Driveにアップロード（共有ドライブ対応）

        Args:
            file_path: ローカルファイルのパス
            folder_id: 保存先フォルダID（Noneの場合はルート）
            mime_type: MIMEタイプ（Noneの場合は自動判定）

        Returns:
            アップロードされたファイルのID、失敗時はNone
        """
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                logger.error(f"ファイルが見つかりません: {file_path}")
                return None

            # ファイルメタデータ
            file_metadata = {'name': file_path.name}
            if folder_id:
                file_metadata['parents'] = [folder_id]

            # MIMEタイプの自動判定
            if mime_type is None:
                import mimetypes
                mime_type, _ = mimetypes.guess_type(str(file_path))
                mime_type = mime_type or 'application/octet-stream'

            # ファイルからアップロード
            media = MediaFileUpload(
                str(file_path),
                mimetype=mime_type,
                resumable=True
            )

            # 共有ドライブ対応: supportsAllDrives=True を追加
            file = self.service.files().create(
                body=file_metadata,
                media_body=media,
                fields='id, name, webViewLink',
                supportsAllDrives=True
            ).execute()

            logger.info(f"ファイルアップロード成功: {file_path.name} (ID: {file['id']})")
            return file['id']

        except Exception as e:
            logger.error(f"ファイルアップロードエラー ({file_path}): {e}")
            return None

    def trash_file(self, file_id: str) -> bool:
        """
        ファイルをゴミ箱に移動（安全な削除）

        Args:
            file_id: ゴミ箱に移動するファイルのID

        Returns:
            成功した場合True、失敗した場合False
        """
        try:
            # trashedフラグをTrueに設定してゴミ箱に移動
            self.service.files().update(
                fileId=file_id,
                body={'trashed': True},
                supportsAllDrives=True
            ).execute()

            logger.info(f"ファイルをゴミ箱に移動しました: {file_id}")
            return True

        except Exception as e:
            logger.error(f"ファイルのゴミ箱移動エラー ({file_id}): {e}")
            return False

    def delete_file_permanently(self, file_id: str) -> bool:
        """
        ファイルを完全に削除（復元不可能）

        Args:
            file_id: 完全に削除するファイルのID

        Returns:
            成功した場合True、失敗した場合False
        """
        from googleapiclient.errors import HttpError

        try:
            # 削除前にファイル情報を取得（デバッグ用）
            try:
                file_info = self.service.files().get(
                    fileId=file_id,
                    fields='id, name, trashed, parents, capabilities/canDelete',
                    supportsAllDrives=True
                ).execute()
                logger.debug(f"削除対象ファイル情報: name={file_info.get('name')}, trashed={file_info.get('trashed')}, canDelete={file_info.get('capabilities', {}).get('canDelete')}")
            except Exception as e:
                logger.warning(f"ファイル情報取得失敗 ({file_id}): {e}")

            # ファイル削除実行
            self.service.files().delete(
                fileId=file_id,
                supportsAllDrives=True
            ).execute()

            logger.info(f"ファイルを完全に削除しました: {file_id}")
            return True

        except HttpError as e:
            if e.resp.status == 404:
                logger.warning(f"ファイルが見つかりません (404): {file_id} - 既に削除されているか、アクセス権がない可能性があります")
            else:
                logger.error(f"ファイルの完全削除エラー ({file_id}): HTTP {e.resp.status} - {e}")
            return False
        except Exception as e:
            logger.error(f"ファイルの完全削除エラー ({file_id}): {e}")
            return False
```

### shared\common\database\__init__.py

```py

```

### shared\common\database\client.py

```py
"""
Database Client
Supabaseデータベースへの接続と操作を管理
"""
import re
from typing import Dict, Any, List, Optional
import asyncio
from supabase import create_client, Client
from shared.common.config.settings import settings


class DatabaseClient:
    """Supabaseデータベースクライアント"""

    def __init__(self, use_service_role: bool = False):
        """Supabaseクライアントの初期化

        Args:
            use_service_role: Trueの場合、Service Role Keyを使用（RLSをバイパス）
        """
        if not settings.SUPABASE_URL:
            raise ValueError("SUPABASE_URL が設定されていません")

        # Service Role Key使用の場合
        if use_service_role:
            if not settings.SUPABASE_SERVICE_ROLE_KEY:
                raise ValueError("SUPABASE_SERVICE_ROLE_KEY が設定されていません")
            api_key = ***REDACTED***
        else:
            if not settings.SUPABASE_KEY:
                raise ValueError("SUPABASE_KEY が設定されていません")
            api_key = ***REDACTED***

        self.client: Client = create_client(
            settings.SUPABASE_URL,
            api_key
        )
    
    def get_document_by_source_id(self, source_id: str) -> Optional[Dict[str, Any]]:
        """
        source_id で文書を検索
        
        Args:
            source_id: Google Drive のファイルID
        
        Returns:
            既存の文書レコード、存在しない場合は None
        """
        try:
            response = self.client.table('Rawdata_FILE_AND_MAIL').select('*').eq('source_id', source_id).execute()
            if response.data:
                return response.data[0]
            return None
        except Exception as e:
            print(f"Error getting document by source_id: {e}")
            return None
    
    async def insert_document(self, table: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        文書をデータベースに挿入

        Args:
            table: テーブル名
            data: 挿入するデータ

        Returns:
            挿入されたレコード
        """
        # embeddingをPostgreSQLのvector型形式に変換
        if 'embedding' in data and data['embedding'] is not None:
            embedding_list = data['embedding']
            if isinstance(embedding_list, list):
                # ベクトル形式の文字列に変換: [0.1,0.2,0.3]
                data = data.copy()  # 元のdataを変更しないようにコピー
                data['embedding'] = '[' + ','.join(str(x) for x in embedding_list) + ']'

        response = self.client.table(table).insert(data).execute()
        return response.data[0] if response.data else {}

    async def upsert_document(
        self,
        table: str,
        data: Dict[str, Any],
        conflict_column: str = 'source_id',
        force_update: bool = False,
        preserve_fields: List[str] = None
    ) -> Dict[str, Any]:
        """
        ドキュメントをupsert（既存レコードがあれば更新、なければ挿入）

        Args:
            table: テーブル名
            data: 挿入・更新するデータ
            conflict_column: 重複判定に使うカラム名（デフォルト: source_id）
            force_update: Trueの場合、全てのフィールドを強制的に更新（再処理時用）
            preserve_fields: force_update=Trueの時でも既存値を保持するフィールドのリスト

        Returns:
            挿入・更新されたレコード
        """
        # embeddingをPostgreSQLのvector型形式に変換
        if 'embedding' in data and data['embedding'] is not None:
            embedding_list = data['embedding']
            if isinstance(embedding_list, list):
                data = data.copy()
                data['embedding'] = '[' + ','.join(str(x) for x in embedding_list) + ']'

        # 既存レコードを取得
        existing = self.client.table(table).select('*').eq(conflict_column, data[conflict_column]).execute()

        if existing.data:
            # 既存レコードがある場合
            existing_record = existing.data[0]
            update_data = {}

            if force_update:
                # 再処理モード：基本的に全て更新するが、preserve_fieldsは既存値を保持
                preserve_fields = preserve_fields or []

                for key, value in data.items():
                    if key in preserve_fields:
                        # 保持対象フィールドは既存値が有効な場合のみ保持
                        if existing_record.get(key) not in [None, '', [], {}]:
                            continue  # 既存値を保持（更新しない）
                    update_data[key] = value
            else:
                # 通常モード：空欄・nullの項目のみ更新
                for key, value in data.items():
                    if existing_record.get(key) in [None, '', [], {}]:
                        update_data[key] = value

            if update_data:
                # 更新するデータがある場合のみUPDATE
                response = self.client.table(table).update(update_data).eq(conflict_column, data[conflict_column]).execute()
                return response.data[0] if response.data else {}
            else:
                # 更新不要の場合は既存レコードを返す
                return existing_record
        else:
            # 既存レコードがない場合：新規挿入
            response = self.client.table(table).insert(data).execute()
            return response.data[0] if response.data else {}

    async def search_documents(
        self,
        query: str,
        embedding: List[float],
        limit: int = 50,
        doc_types: Optional[List[str]] = None,
        date_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        2階層ハイブリッド検索：小チャンク検索 + 大チャンク回答（重複排除＆Rerank対応）

        【検索フロー】
        1. 小チャンク（300文字）でベクトル + 全文検索 → 関連ドキュメントを検出
        2. ドキュメント単位で重複排除（最高スコアのみ） → 重複を削減
        3. 大チャンク（全文）で回答生成 → 最も詳細な情報を使用

        【フィルタの考え方】
        - 階層構造（workspace > doc_type）はフロントエンドで維持
        - データベース検索はdoc_typeのみで絞り込み
        - 理由: workspace内の全doc_typeがON = workspaceがON（冗長なため）

        Args:
            query: 検索クエリ
            embedding: クエリのembeddingベクトル
            limit: 取得する最大件数
            doc_types: ドキュメントタイプフィルタ（配列、複数選択可能）

        Returns:
            検索結果のリスト（小チャンク検索スコア順、回答は大チャンク）
        """
        try:
            # 日付フィルタは all_mentioned_dates 配列で検索するため、
            # filter_year, filter_month の抽出は不要
            # 代わりに、クエリに含まれる日付は全文検索でマッチ

            # DB関数を呼び出し（unified_search_v2: 高度な重み付けスコアリング）
            rpc_params = {
                "query_text": query,
                "query_embedding": embedding,
                "match_threshold": 0.0,
                "match_count": limit,  # 指定された件数を取得
                "vector_weight": 0.7,  # ベクトル検索70% - 意味的類似度を重視
                "fulltext_weight": 0.3,  # キーワード検索30% - 完全一致の補助
                "filter_doc_types": doc_types,  # doc_typeのみで絞り込み
                "filter_chunk_types": None,  # 全chunk_typeを対象（title, summary, display_subject等）
                "filter_workspace": None  # 全workspaceを対象
            }

            print(f"[DEBUG] unified_search_v2 呼び出し: query='{query}', doc_types={doc_types}")
            response = self.client.rpc("unified_search_v2", rpc_params).execute()
            results = response.data if response.data else []

            print(f"[DEBUG] unified_search_v2 結果: {len(results)} 件")

            # 日付フィルタリング
            if date_filter:
                results = self._apply_date_filter(results, date_filter)
                print(f"[DEBUG] 日付フィルタ適用後: {len(results)} 件 (filter={date_filter})")

            # 結果を整形（unified_search_v2: 高度な検索結果）
            final_results = []
            for result in results:
                doc_result = {
                    'id': result.get('document_id'),
                    'file_name': result.get('file_name'),
                    'doc_type': result.get('doc_type'),
                    'workspace': result.get('workspace'),
                    'document_date': result.get('document_date'),
                    'metadata': result.get('metadata'),
                    'summary': result.get('summary'),

                    # 回答用：添付ファイルテキスト（unified_search_v2のattachment_text）
                    'content': result.get('attachment_text'),  # 添付ファイルテキスト
                    'large_chunk_id': result.get('document_id'),  # ドキュメントID

                    # ヒットしたチャンク情報（重み付けされた最適チャンク）
                    'chunk_content': result.get('best_chunk_text'),  # ヒットした最適チャンク
                    'chunk_id': result.get('best_chunk_id'),  # チャンクID
                    'chunk_index': result.get('best_chunk_index'),  # チャンクインデックス
                    'chunk_type': result.get('best_chunk_type'),  # チャンクタイプ（title, summary等）

                    # 検索スコア（unified_search_v2の詳細スコア）
                    'similarity': result.get('combined_score', 0),  # 統合スコア
                    'raw_similarity': result.get('raw_similarity', 0),  # 生の類似度
                    'weighted_similarity': result.get('weighted_similarity', 0),  # 重み付け類似度
                    'fulltext_score': result.get('fulltext_score', 0),  # 全文検索スコア
                    'title_matched': result.get('title_matched', False),  # タイトルマッチフラグ

                    # 後方互換性
                    'chunk_score': result.get('combined_score', 0),
                    'small_chunk_id': result.get('best_chunk_id')
                }

                # ✅ Classroom表示用の追加フィールド（存在する場合のみ追加）
                if 'source_type' in result:
                    doc_result['source_type'] = result.get('source_type')
                if 'source_url' in result:
                    doc_result['source_url'] = result.get('source_url')
                if 'attachment_text' in result:
                    doc_result['attachment_text'] = result.get('attachment_text')
                if 'created_at' in result:
                    doc_result['created_at'] = result.get('created_at')
                if 'display_subject' in result:
                    doc_result['display_subject'] = result.get('display_subject')
                if 'display_sender' in result:
                    doc_result['display_sender'] = result.get('display_sender')
                if 'classroom_sender_email' in result:
                    doc_result['classroom_sender_email'] = result.get('classroom_sender_email')
                if 'display_sent_at' in result:
                    doc_result['display_sent_at'] = result.get('display_sent_at')
                if 'display_post_text' in result:
                    doc_result['display_post_text'] = result.get('display_post_text')
                if 'display_type' in result:
                    doc_result['display_type'] = result.get('display_type')

                final_results.append(doc_result)

            # ✅ 各ドキュメントのヒットチャンク全体を取得
            print(f"[DEBUG] ヒットチャンク詳細を取得中...")
            for doc_result in final_results:
                document_id = doc_result.get('id')
                if not document_id:
                    continue

                try:
                    # このドキュメントの全チャンクを取得（重要度順、上限なし）
                    chunks_response = (
                        self.client.table('10_ix_search_index')
                        .select('chunk_index, chunk_content, chunk_type, chunk_metadata, search_weight')
                        .eq('document_id', document_id)
                        .order('search_weight', desc=True)  # 重要度順
                        .execute()
                    )

                    if chunks_response.data:
                        doc_result['all_chunks'] = chunks_response.data
                        print(f"[DEBUG] ドキュメント {document_id[:8]}: {len(chunks_response.data)}個のチャンク取得")
                    else:
                        doc_result['all_chunks'] = []

                except Exception as e:
                    print(f"[WARNING] チャンク取得エラー (document_id={document_id}): {e}")
                    doc_result['all_chunks'] = []

            print(f"[DEBUG] 最終検索結果: {len(final_results)} 件（unified_search_v2）")
            print(f"[DEBUG] 検索戦略: 重み付けスコアリング + chunk_type優先順位 + タイトルマッチ")

            return final_results

        except Exception as e:
            print(f"2-tier hybrid search error: {e}")
            import traceback
            traceback.print_exc()

            # フォールバック: 従来のベクトル検索（workspaceフィルタなし）
            print("[WARNING] フォールバックモード: match_documents を使用")
            return await self._fallback_vector_search(embedding, limit, None)

    def search_documents_sync(
        self,
        query: str,
        embedding: List[float],
        limit: int = 50,
        doc_types: Optional[List[str]] = None,
        date_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        同期版のsearch_documents（Flaskエンドポイント用）

        非同期版のsearch_documentsをasyncio.run()で実行するラッパー。
        これにより、Flask（同期フレームワーク）から適切に非同期処理を呼び出せる。

        Args:
            query: 検索クエリ
            embedding: クエリのembeddingベクトル
            limit: 取得する最大件数
            doc_types: ドキュメントタイプフィルタ（配列、複数選択可能）
            date_filter: 日付フィルタ ('today', 'this_week', 'this_month', 'recent')

        Returns:
            検索結果のリスト
        """
        try:
            # 既存のイベントループがある場合は取得、なければ新規作成
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # ループが既に動作している場合は新しいループを作成
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    result = loop.run_until_complete(
                        self.search_documents(query, embedding, limit, doc_types, date_filter)
                    )
                    return result
            except RuntimeError:
                # イベントループが存在しない場合
                pass

            # asyncio.run()を使用（推奨される方法）
            return asyncio.run(
                self.search_documents(query, embedding, limit, doc_types, date_filter)
            )
        except Exception as e:
            print(f"Sync wrapper error: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _apply_date_filter(self, results: List[Dict[str, Any]], date_filter: str) -> List[Dict[str, Any]]:
        """
        日付フィルタを適用

        Args:
            results: 検索結果のリスト
            date_filter: フィルタタイプ ('today', 'this_week', 'this_month', 'recent')

        Returns:
            フィルタリングされた結果
        """
        from datetime import datetime, timedelta

        now = datetime.now()
        filtered_results = []

        for result in results:
            document_date_str = result.get('document_date')

            # document_dateが存在しない場合はスキップ
            if not document_date_str:
                # 'recent' の場合は、作成日時 (created_at) でフィルタリング
                if date_filter == 'recent':
                    created_at_str = result.get('created_at')
                    if created_at_str:
                        try:
                            created_at = datetime.fromisoformat(created_at_str.replace('Z', '+00:00'))
                            # 過去30日以内
                            if (now - created_at).days <= 30:
                                filtered_results.append(result)
                        except:
                            pass
                continue

            try:
                # document_dateをパース (YYYY-MM-DD形式を想定)
                document_date = datetime.strptime(document_date_str, '%Y-%m-%d')
            except:
                # パースできない場合はスキップ
                continue

            # フィルタタイプに応じて判定
            if date_filter == 'today':
                if document_date.date() == now.date():
                    filtered_results.append(result)

            elif date_filter == 'this_week':
                # 今週の月曜日を計算
                week_start = now - timedelta(days=now.weekday())
                week_start = week_start.replace(hour=0, minute=0, second=0, microsecond=0)
                if document_date >= week_start:
                    filtered_results.append(result)

            elif date_filter == 'this_month':
                if document_date.year == now.year and document_date.month == now.month:
                    filtered_results.append(result)

            elif date_filter == 'recent':
                # 過去30日以内
                if (now - document_date).days <= 30:
                    filtered_results.append(result)

        return filtered_results

    async def _fallback_vector_search(
        self,
        embedding: List[float],
        limit: int,
        workspace: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        フォールバック用のベクトル検索（エラー時に使用）

        Args:
            embedding: クエリのembeddingベクトル
            limit: 取得する最大件数
            workspace: ワークスペースフィルタ

        Returns:
            検索結果のリスト
        """
        try:
            rpc_params = {
                "query_embedding": embedding,
                "match_threshold": 0.0,
                "match_count": min(limit, 5)
            }
            if workspace:
                rpc_params["filter_workspace"] = workspace

            response = self.client.rpc("match_documents", rpc_params).execute()
            results = response.data if response.data else []

            print(f"[DEBUG] フォールバック検索結果: {len(results)} 件")
            return results

        except Exception as e:
            print(f"Fallback search error: {e}")
            return []

    def _check_date_match(self, doc: Dict[str, Any], target_date: str) -> float:
        """
        ドキュメントのメタデータと本文から target_date を検索
        年を無視して、月・日だけでマッチング（例：12-04）

        Args:
            doc: ドキュメント
            target_date: ターゲット日付（YYYY-MM-DD形式、例：2025-12-04）

        Returns:
            マッチスコア（0.0～0.5）
        """
        # 年を除いた月・日のみを抽出（例：2025-12-04 → 12-04）
        try:
            parts = target_date.split('-')
            month = int(parts[1])
            day = int(parts[2])
        except:
            return 0.0

        # 検索パターン：「12/4」「12月4日」「12-04」など
        date_patterns = [
            f"{month}/{day}",           # 12/4
            f"{month:02d}/{day:02d}",   # 12/04
            f"{month}月{day}日",         # 12月4日
            f"{month:02d}-{day:02d}",   # 12-04
        ]

        # 1. 本文（content, summary, attachment_text）を検索 - 最優先
        text_fields = ['content', 'summary', 'attachment_text']
        for field in text_fields:
            text = doc.get(field, '')
            if text:
                for pattern in date_patterns:
                    if pattern in text:
                        print(f"[DEBUG] 本文で日付マッチ: {doc.get('file_name')} に '{pattern}' が含まれる")
                        return 0.5  # 本文マッチで +0.5 ブースト（最優先）

        # 2. メタデータの weekly_schedule をチェック
        metadata = doc.get('metadata', {})
        weekly_schedule = metadata.get('weekly_schedule', [])
        if isinstance(weekly_schedule, list):
            for day_item in weekly_schedule:
                if isinstance(day_item, dict):
                    date = day_item.get('date', '')
                    if date:
                        try:
                            doc_month = int(date.split('-')[1])
                            doc_day = int(date.split('-')[2])
                            if month == doc_month and day == doc_day:
                                return 0.3  # メタデータマッチで +0.3 ブースト
                        except:
                            pass

        # 3. document_date をチェック
        document_date = doc.get('document_date', '')
        if document_date:
            try:
                doc_month = int(str(document_date).split('-')[1])
                doc_day = int(str(document_date).split('-')[2])
                if month == doc_month and day == doc_day:
                    return 0.2
            except:
                pass

        return 0.0

    def _calculate_keyword_match_score(
        self,
        file_name: str,
        keywords: List[str],
        query: str
    ) -> float:
        """
        file_name とクエリの一致度を計算

        Args:
            file_name: ファイル名
            keywords: 抽出されたキーワードのリスト
            query: 元のクエリ

        Returns:
            一致度スコア（0.0～1.0）
        """
        # クエリから記号を除去して正規化
        normalized_query = query.replace('？', '').replace('?', '').replace('の内容は', '').replace('内容', '').strip()

        # 完全一致（括弧付き単語がそのまま含まれる）
        for kw in keywords:
            if '（' in kw or '(' in kw:
                if kw in file_name:
                    return 1.0  # 「学年通信（29）」が完全一致

        # マッチしたキーワードの数をカウント
        matched_keywords = []
        for kw in keywords:
            if kw in file_name:
                matched_keywords.append(kw)

        if not matched_keywords:
            return 0.0

        # マッチ数に応じてスコアを設定
        match_count = len(matched_keywords)
        total_keywords = len(keywords)

        if match_count == total_keywords:
            # すべてのキーワードがマッチ（ただし完全一致ではない）
            return 0.95
        elif match_count >= 2:
            # 2つ以上マッチ
            return 0.90
        else:
            # 1つだけマッチ
            return 0.85

    def _extract_date(self, query: str) -> Optional[str]:
        """
        クエリから日付を抽出（YYYY-MM-DD形式に正規化）

        Args:
            query: 検索クエリ

        Returns:
            正規化された日付文字列（YYYY-MM-DD）、または None
        """
        from datetime import datetime

        # 現在の年を取得
        current_year = datetime.now().year

        # パターン1: MM/DD形式（例：12/4）
        match = re.search(r'(\d{1,2})/(\d{1,2})', query)
        if match:
            month = int(match.group(1))
            day = int(match.group(2))
            try:
                date_obj = datetime(current_year, month, day)
                return date_obj.strftime('%Y-%m-%d')
            except ValueError:
                pass

        # パターン2: MM月DD日形式（例：12月4日）
        match = re.search(r'(\d{1,2})月(\d{1,2})日', query)
        if match:
            month = int(match.group(1))
            day = int(match.group(2))
            try:
                date_obj = datetime(current_year, month, day)
                return date_obj.strftime('%Y-%m-%d')
            except ValueError:
                pass

        return None

    def _extract_keywords(self, query: str) -> List[str]:
        """
        クエリから重要なキーワードを抽出

        Args:
            query: 検索クエリ

        Returns:
            抽出されたキーワードのリスト
        """
        keywords = []

        # 括弧内の文字を抽出（例：「学年通信（29）」→「29」「学年通信」）
        bracket_matches = re.findall(r'[（(]([^）)]+)[）)]', query)
        keywords.extend(bracket_matches)

        # 括弧を含む単語全体を抽出（例：「学年通信（29）」）
        bracket_words = re.findall(r'[\w一-龠ぁ-んァ-ヶー]+[（(][^）)]+[）)]', query)
        keywords.extend(bracket_words)

        # 助詞を除去してクリーニング
        cleaned_query = query
        particles = ['の', 'は', 'を', 'が', 'に', 'へ', 'と', 'から', 'まで', 'で', '？', '?']
        for particle in particles:
            cleaned_query = cleaned_query.replace(particle, ' ')

        # 名詞的な単語を抽出（漢字・カタカナが2文字以上）
        words = re.findall(r'[一-龠ァ-ヶー]{2,}', cleaned_query)
        keywords.extend(words)

        # 重複削除・空白除去して返す
        keywords = [kw.strip() for kw in keywords if kw.strip()]
        return list(set(keywords))

    def get_documents_for_review(
        self,
        limit: int = 100,
        search_query: Optional[str] = None,
        workspace: Optional[str] = None,
        file_type: Optional[str] = None,
        review_status: Optional[str] = None,
        exclude_workspace: Optional[str] = None,
        doc_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        レビュー対象のドキュメントを取得

        通常モード（search_query=None）: 未レビューのドキュメントのみ取得
        検索モード（search_query指定）: レビュー状態に関係なく全件から検索

        Args:
            limit: 取得する最大件数
            search_query: 検索クエリ（IDまたはファイル名で部分一致）
            workspace: ワークスペースフィルタ（'business', 'personal', またはNone）
            file_type: ファイルタイプフィルタ（'pdf', 'email', またはNone）
            review_status: レビューステータスフィルタ（'reviewed', 'pending', 'all', またはNone）
            exclude_workspace: 除外するワークスペース（'gmail'など）
            doc_type: ドキュメントタイプフィルタ（'DM-mail', 'JOB-mail'など）

        Returns:
            ドキュメントのリスト（更新日時降順）
        """
        try:
            query = self.client.table('Rawdata_FILE_AND_MAIL').select('*')

            # File typeフィルタを適用
            if file_type:
                query = query.eq('file_type', file_type)

            # Workspaceフィルタを適用
            if workspace:
                query = query.eq('workspace', workspace)

            # Workspace除外フィルタを適用
            if exclude_workspace:
                query = query.neq('workspace', exclude_workspace)

            # Doc typeフィルタを適用
            if doc_type:
                query = query.eq('doc_type', doc_type)

            # Review statusフィルタを適用
            if review_status == 'reviewed':
                query = query.eq('review_status', 'reviewed')
            elif review_status == 'pending':
                # review_statusが'pending'またはNULLの場合
                query = query.or_('review_status.eq.pending,review_status.is.null')

            if search_query:
                # 検索モード: レビュー状態に関係なく検索
                # IDでの完全一致検索を試みる
                if len(search_query) == 36 or len(search_query) == 8:  # UUID形式またはID先頭8文字
                    # IDで検索（部分一致）
                    response_id = query.ilike('id', f'{search_query}%').limit(limit).execute()
                    if response_id.data:
                        return response_id.data

                # ファイル名で部分一致検索
                response = (
                    query
                    .ilike('file_name', f'%{search_query}%')
                    .order('updated_at', desc=True)
                    .limit(limit)
                    .execute()
                )
                return response.data if response.data else []
            else:
                # 通常モード: 全ドキュメントを取得（is_reviewed カラムは削除）
                response = (
                    query
                    .order('updated_at', desc=True)
                    .limit(limit)
                    .execute()
                )
                return response.data if response.data else []

        except Exception as e:
            print(f"Error getting documents for review: {e}")
            return []

    def mark_document_reviewed(
        self,
        doc_id: str,
        reviewed_by: Optional[str] = None
    ) -> bool:
        """
        ドキュメントをレビュー済みとしてマークする
        （is_reviewed カラムは削除されたため、review_status で管理）

        Args:
            doc_id: ドキュメントID
            reviewed_by: レビュー担当者のメールアドレス（オプション）

        Returns:
            成功したかどうか
        """
        try:
            from datetime import datetime

            update_data = {
                'review_status': 'reviewed'  # review_status カラムを使用
            }
            if reviewed_by:
                update_data['reviewed_by'] = reviewed_by

            response = (
                self.client.table('Rawdata_FILE_AND_MAIL')
                .update(update_data)
                .eq('id', doc_id)
                .execute()
            )
            return bool(response.data)
        except Exception as e:
            print(f"Error marking document as reviewed: {e}")
            return False

    def mark_document_unreviewed(
        self,
        doc_id: str
    ) -> bool:
        """
        ドキュメントを未レビュー状態に戻す
        （is_reviewed カラムは削除されたため、review_status で管理）

        Args:
            doc_id: ドキュメントID

        Returns:
            成功したかどうか
        """
        try:
            update_data = {
                'review_status': 'pending',  # review_status カラムを使用
                'reviewed_by': None
            }

            response = (
                self.client.table('Rawdata_FILE_AND_MAIL')
                .update(update_data)
                .eq('id', doc_id)
                .execute()
            )
            return bool(response.data)
        except Exception as e:
            print(f"Error marking document as unreviewed: {e}")
            return False

    def get_review_progress(self) -> Dict[str, Any]:
        """
        レビュー進捗状況を取得
        （is_reviewed カラムは削除されたため、review_status で管理）

        Returns:
            進捗情報の辞書
        """
        try:
            # 未レビューの件数（review_status = 'pending' または NULL）
            unreviewed_response = (
                self.client.table('Rawdata_FILE_AND_MAIL')
                .select('*', count='exact')
                .in_('review_status', ['pending', None])
                .execute()
            )
            unreviewed_count = unreviewed_response.count if unreviewed_response else 0

            # レビュー済みの件数（review_status = 'reviewed'）
            reviewed_response = (
                self.client.table('Rawdata_FILE_AND_MAIL')
                .select('*', count='exact')
                .eq('review_status', 'reviewed')
                .execute()
            )
            reviewed_count = reviewed_response.count if reviewed_response else 0

            # 総件数
            total_count = unreviewed_count + reviewed_count

            # 進捗率
            progress_percent = (reviewed_count / total_count * 100) if total_count > 0 else 0

            return {
                'total': total_count,
                'reviewed': reviewed_count,
                'unreviewed': unreviewed_count,
                'progress_percent': round(progress_percent, 2)
            }
        except Exception as e:
            print(f"Error getting review progress: {e}")
            return {
                'total': 0,
                'reviewed': 0,
                'unreviewed': 0,
                'progress_percent': 0
            }

    def get_available_workspaces(self) -> List[str]:
        """
        データベース内の利用可能なワークスペース一覧を取得

        Returns:
            ワークスペース名のリスト（重複なし、ソート済み）
        """
        try:
            # 全ドキュメントからworkspaceを取得
            response = self.client.table('Rawdata_FILE_AND_MAIL').select('workspace').execute()

            workspaces = set()
            for doc in response.data:
                ws = doc.get('workspace')
                if ws:  # NoneやNULLを除外
                    workspaces.add(ws)

            return sorted(list(workspaces))
        except Exception as e:
            print(f"Error getting available workspaces: {e}")
            return []

    def get_available_doc_types(self) -> List[str]:
        """
        データベース内の利用可能なドキュメントタイプ一覧を取得

        Returns:
            ドキュメントタイプ名のリスト（重複なし、ソート済み）
        """
        try:
            # 全ドキュメントからdoc_typeを取得
            response = self.client.table('Rawdata_FILE_AND_MAIL').select('doc_type').execute()

            doc_types = set()
            for doc in response.data:
                dt = doc.get('doc_type')
                if dt:  # NoneやNULLを除外
                    doc_types.add(dt)

            return sorted(list(doc_types))
        except Exception as e:
            print(f"Error getting available doc_types: {e}")
            return []

    def get_workspace_hierarchy(self) -> Dict[str, List[str]]:
        """
        workspace別のdoc_type階層構造を取得

        Returns:
            {workspace: [doc_type1, doc_type2, ...]} の辞書
        """
        try:
            # workspaceとdoc_typeの組み合わせを取得
            response = self.client.table('Rawdata_FILE_AND_MAIL').select('workspace, doc_type').execute()

            hierarchy = {}
            for doc in response.data:
                workspace = doc.get('workspace')
                doc_type = doc.get('doc_type')

                if workspace and doc_type:
                    if workspace not in hierarchy:
                        hierarchy[workspace] = set()
                    hierarchy[workspace].add(doc_type)

            # setをソート済みリストに変換
            result = {ws: sorted(list(types)) for ws, types in hierarchy.items()}

            # workspaceもソート
            result = dict(sorted(result.items()))

            print(f"[DEBUG] workspace階層構造: {len(result)} workspaces")
            return result

        except Exception as e:
            print(f"Error getting workspace hierarchy: {e}")
            return {}

    def get_document_by_id(self, doc_id: str) -> Optional[Dict[str, Any]]:
        """
        IDでドキュメントを取得

        Args:
            doc_id: ドキュメントID

        Returns:
            ドキュメント、存在しない場合は None
        """
        try:
            response = self.client.table('Rawdata_FILE_AND_MAIL').select('*').eq('id', doc_id).execute()
            if response.data:
                return response.data[0]
            return None
        except Exception as e:
            print(f"Error getting document by id: {e}")
            return None

    def update_document_metadata(
        self,
        doc_id: str,
        new_metadata: Dict[str, Any],
        new_doc_type: Optional[str] = None
    ) -> bool:
        """
        ドキュメントのメタデータと文書タイプを更新

        注意: この関数は修正履歴を記録しません。
        修正履歴を記録する場合は record_correction() を使用してください。

        Args:
            doc_id: ドキュメントID
            new_metadata: 新しいメタデータ
            new_doc_type: 新しい文書タイプ（オプション）

        Returns:
            成功したかどうか
        """
        try:
            update_data = {'metadata': new_metadata}
            if new_doc_type:
                update_data['doc_type'] = new_doc_type

            response = (
                self.client.table('Rawdata_FILE_AND_MAIL')
                .update(update_data)
                .eq('id', doc_id)
                .execute()
            )
            return bool(response.data)
        except Exception as e:
            print(f"Error updating document metadata: {e}")
            return False

    def record_correction(
        self,
        doc_id: str,
        new_metadata: Dict[str, Any],
        new_doc_type: Optional[str] = None,
        corrector_email: Optional[str] = None,
        notes: Optional[str] = None
    ) -> bool:
        """
        ドキュメントのメタデータを更新し、修正履歴を記録

        Phase 2: トランザクション管理・ロールバック機能

        Args:
            doc_id: ドキュメントID
            new_metadata: 新しいメタデータ
            new_doc_type: 新しい文書タイプ（オプション）
            corrector_email: 修正者のメールアドレス（オプション）
            notes: 修正に関するメモ（オプション）

        Returns:
            成功したかどうか
        """
        from loguru import logger

        try:
            # Step 1: 現在のドキュメントを取得
            current_doc = self.get_document_by_id(doc_id)
            if not current_doc:
                logger.error(f"[record_correction] Document not found: {doc_id}")
                return False

            old_metadata = current_doc.get('metadata', {})
            old_doc_type = current_doc.get('doc_type')

            logger.info(f"[record_correction] 現在のドキュメント取得成功: doc_id={doc_id}, doc_type={old_doc_type}")

            # Step 2: correction_history に修正履歴を記録
            correction_data = {
                'document_id': doc_id,
                'old_metadata': old_metadata,
                'new_metadata': new_metadata,
                'corrector_email': corrector_email,
                'correction_type': 'manual',
                'notes': notes
            }

            logger.info(f"[record_correction] correction_history へ挿入開始")

            correction_response = (
                self.client.table('99_lg_correction_history')
                .insert(correction_data)
                .execute()
            )

            if not correction_response.data:
                logger.error(f"[record_correction] Failed to insert correction history")
                logger.error(f"[record_correction] Response: {correction_response}")
                return False

            correction_id = correction_response.data[0]['id']
            logger.info(f"[record_correction] ✅ 修正履歴を記録: correction_id={correction_id}")

            # Step 3: documents テーブルを更新
            update_data = {
                'metadata': new_metadata,
                'latest_correction_id': correction_id
            }
            if new_doc_type and new_doc_type != old_doc_type:
                update_data['doc_type'] = new_doc_type
                logger.info(f"[record_correction] doc_type変更: {old_doc_type} → {new_doc_type}")

            # year, month のトップレベルカラムへの同期は削除（metadata 内で管理）

            logger.info(f"[record_correction] Rawdata_FILE_AND_MAIL 更新開始: doc_id={doc_id}")

            document_response = (
                self.client.table('Rawdata_FILE_AND_MAIL')
                .update(update_data)
                .eq('id', doc_id)
                .execute()
            )

            if not document_response.data:
                logger.error(f"[record_correction] Failed to update document")
                logger.error(f"[record_correction] Response: {document_response}")
                return False

            logger.info(f"[record_correction] ✅ ドキュメント更新成功: doc_id={doc_id}")
            return True

        except Exception as e:
            logger.error(f"[record_correction] Error recording correction: {e}", exc_info=True)
            return False

    def rollback_document(self, doc_id: str) -> bool:
        """
        ドキュメントのメタデータを最新の修正前の状態にロールバック

        Phase 2: トランザクション管理・ロールバック機能

        Args:
            doc_id: ドキュメントID

        Returns:
            成功したかどうか
        """
        try:
            # Step 1: 現在のドキュメントを取得
            current_doc = self.get_document_by_id(doc_id)
            if not current_doc:
                print(f"Error: Document not found: {doc_id}")
                return False

            latest_correction_id = current_doc.get('latest_correction_id')
            if not latest_correction_id:
                print(f"Error: No correction history found for document: {doc_id}")
                return False

            # Step 2: 最新の修正履歴を取得
            correction_response = (
                self.client.table('99_lg_correction_history')
                .select('*')
                .eq('id', latest_correction_id)
                .execute()
            )

            if not correction_response.data:
                print(f"Error: Correction history not found: {latest_correction_id}")
                return False

            correction = correction_response.data[0]
            old_metadata = correction['old_metadata']

            # Step 3: documentsテーブルを修正前の状態に戻す
            update_data = {
                'metadata': old_metadata,
                'latest_correction_id': None  # ロールバック後は修正履歴をクリア
            }

            document_response = (
                self.client.table('Rawdata_FILE_AND_MAIL')
                .update(update_data)
                .eq('id', doc_id)
                .execute()
            )

            if not document_response.data:
                print("Error: Failed to rollback document")
                return False

            print(f"✅ ロールバック成功: doc_id={doc_id}, correction_id={latest_correction_id}")
            return True

        except Exception as e:
            print(f"Error rolling back document: {e}")
            import traceback
            traceback.print_exc()
            return False

    def get_correction_history(
        self,
        doc_id: str,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ドキュメントの修正履歴を取得

        Phase 2: トランザクション管理・ロールバック機能

        Args:
            doc_id: ドキュメントID
            limit: 取得する最大件数

        Returns:
            修正履歴のリスト（新しい順）
        """
        try:
            response = (
                self.client.table('99_lg_correction_history')
                .select('*')
                .eq('document_id', doc_id)
                .order('corrected_at', desc=True)
                .limit(limit)
                .execute()
            )
            return response.data if response.data else []
        except Exception as e:
            print(f"Error getting correction history: {e}")
            return []

    def get_processed_file_ids(self) -> List[str]:
        """
        既に処理済みのファイルIDリストを取得

        Returns:
            処理済みファイルのsource_id（Google Drive file ID）のリスト
        """
        try:
            # source_idが存在するすべてのドキュメントを取得
            response = (
                self.client.table('Rawdata_FILE_AND_MAIL')
                .select('source_id')
                .not_.is_('source_id', 'null')
                .execute()
            )

            # source_idのリストを抽出
            if response.data:
                file_ids = [doc['source_id'] for doc in response.data if doc.get('source_id')]
                print(f"Supabaseから {len(file_ids)} 件の処理済みファイルIDを取得しました")
                return file_ids
            return []

        except Exception as e:
            print(f"Error getting processed file IDs: {e}")
            return []

    def check_duplicate_hash(self, content_hash: str) -> bool:
        """
        content_hashが既にデータベースに存在するかチェック

        Args:
            content_hash: SHA256ハッシュ値

        Returns:
            True: 重複あり（既に存在する）
            False: 重複なし（新規）
        """
        try:
            # content_hashが一致するドキュメントを検索
            response = (
                self.client.table('Rawdata_FILE_AND_MAIL')
                .select('id, file_name, content_hash')
                .eq('content_hash', content_hash)
                .limit(1)
                .execute()
            )

            if response.data:
                # 重複が見つかった
                existing_doc = response.data[0]
                print(f"⚠️  重複検知: content_hash={content_hash[:16]}... は既に存在します")
                print(f"   既存ファイル: {existing_doc.get('file_name', 'Unknown')}")
                return True

            # 重複なし
            return False

        except Exception as e:
            print(f"Error checking duplicate hash: {e}")
            # エラー時は安全側に倒して重複なしとして扱う
            return False

    def delete_document(self, doc_id: str) -> bool:
        """
        ドキュメントをデータベースから削除

        Args:
            doc_id: 削除するドキュメントのID

        Returns:
            True: 削除成功
            False: 削除失敗
        """
        try:
            # ドキュメントを削除（ON DELETE CASCADEにより関連データも削除される）
            response = (
                self.client.table('Rawdata_FILE_AND_MAIL')
                .delete()
                .eq('id', doc_id)
                .execute()
            )

            if response.data:
                print(f"✅ ドキュメントを削除しました: {doc_id}")
                return True
            else:
                print(f"⚠️  ドキュメントが見つかりませんでした: {doc_id}")
                return False

        except Exception as e:
            print(f"❌ ドキュメント削除エラー: {e}")
            return False
```

### shared\common\processing\__init__.py

```py
"""
ドキュメント処理モジュール

チャンク処理など、ドキュメントの前処理機能を提供
"""

from .chunk_processor import ChunkProcessor

__all__ = ["ChunkProcessor"]
```

### shared\common\processing\chunk_processor.py

```py
"""
チャンク処理モジュール

ドキュメントを小チャンクに分割し、embeddingを生成してデータベースに保存する
"""

from typing import List, Dict, Optional
import asyncio
from loguru import logger

from shared.common.utils.chunking import TextChunker
from shared.ai.llm_client.llm_client import LLMClient
from shared.common.database.client import DatabaseClient


class ChunkProcessor:
    """
    ドキュメントをチャンク分割してembedding生成するクラス
    """

    def __init__(
        self,
        chunk_size: int = 300,
        overlap: int = 50,
        max_concurrent: int = 3
    ):
        """
        Args:
            chunk_size: 1チャンクの文字数（デフォルト300文字）
            overlap: チャンク間のオーバーラップ文字数（デフォルト50文字）
            max_concurrent: 同時実行可能なembedding生成数（レート制限対策）
        """
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.llm_client = LLMClient()
        self.db = DatabaseClient()
        self.max_concurrent = max_concurrent

    async def process_document(
        self,
        document_id: str,
        full_text: str,
        force_reprocess: bool = False
    ) -> Dict[str, any]:
        """
        ドキュメントをチャンク分割してembedding生成

        1. テキストを小チャンクに分割
        2. 各チャンクのembeddingを生成
        3. データベースに保存

        Args:
            document_id: ドキュメントID（UUID）
            full_text: ドキュメント全文
            force_reprocess: Trueの場合、既存のチャンクを削除して再処理

        Returns:
            {
                "success": True/False,
                "document_id": "...",
                "chunks_created": 10,
                "chunks_failed": 0,
                "error": "..." (失敗時のみ)
            }
        """
        try:
            logger.info(f"[ChunkProcessor] Processing document: {document_id}")

            # 既存のチャンクを確認
            if not force_reprocess:
                existing_chunks = self.db.client.table('10_ix_search_index').select('id').eq('document_id', document_id).execute()
                if existing_chunks.data:
                    logger.info(f"[ChunkProcessor] Document {document_id} already has {len(existing_chunks.data)} chunks. Skipping.")
                    return {
                        "success": True,
                        "document_id": document_id,
                        "chunks_created": 0,
                        "chunks_failed": 0,
                        "message": "Already processed"
                    }

            # 再処理の場合は既存チャンクを削除
            if force_reprocess:
                logger.info(f"[ChunkProcessor] Deleting existing chunks for document {document_id}")
                self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()

            # テキストを小チャンクに分割
            logger.info(f"[ChunkProcessor] Splitting text into chunks (size={self.chunk_size}, overlap={self.overlap})")
            chunker = TextChunker(chunk_size=self.chunk_size, chunk_overlap=self.overlap)
            chunks = chunker.split_text(full_text)

            if not chunks:
                logger.warning(f"[ChunkProcessor] No chunks created for document {document_id}")
                return {
                    "success": False,
                    "document_id": document_id,
                    "chunks_created": 0,
                    "chunks_failed": 0,
                    "error": "No chunks created (empty document?)"
                }

            logger.info(f"[ChunkProcessor] Created {len(chunks)} chunks")

            # 各チャンクのembeddingを生成（並列実行）
            chunks_with_embeddings = await self._generate_embeddings_for_chunks(chunks)

            # データベースに保存
            chunks_created = 0
            chunks_failed = 0

            for chunk_data in chunks_with_embeddings:
                if chunk_data.get("embedding"):
                    try:
                        chunk_text = chunk_data.get("chunk_text", chunk_data.get("content", ""))
                        self.db.client.table('10_ix_search_index').insert({
                            "document_id": document_id,
                            "chunk_index": chunk_data["chunk_index"],
                            "chunk_content": chunk_text,
                            "chunk_size": len(chunk_text),
                            "chunk_type": "content_small",
                            "search_weight": 1.0,
                            "embedding": chunk_data["embedding"]
                        }).execute()
                        chunks_created += 1
                    except Exception as e:
                        logger.error(f"[ChunkProcessor] Failed to save chunk {chunk_data['chunk_index']}: {e}")
                        chunks_failed += 1
                else:
                    logger.warning(f"[ChunkProcessor] Chunk {chunk_data['chunk_index']} has no embedding, skipping")
                    chunks_failed += 1

            logger.info(f"[ChunkProcessor] Document {document_id} processed: {chunks_created} chunks created, {chunks_failed} failed")

            return {
                "success": True,
                "document_id": document_id,
                "chunks_created": chunks_created,
                "chunks_failed": chunks_failed
            }

        except Exception as e:
            logger.error(f"[ChunkProcessor] Error processing document {document_id}: {e}")
            return {
                "success": False,
                "document_id": document_id,
                "chunks_created": 0,
                "chunks_failed": 0,
                "error": str(e)
            }

    async def _generate_embeddings_for_chunks(
        self,
        chunks: List[Dict[str, any]]
    ) -> List[Dict[str, any]]:
        """
        チャンクリストに対してembeddingを生成（並列処理）

        Args:
            chunks: チャンクリスト

        Returns:
            embeddingが追加されたチャンクリスト
        """
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def generate_single_embedding(chunk: Dict[str, any]) -> Dict[str, any]:
            async with semaphore:
                try:
                    # チャンクのテキストを取得（既存の形式に対応）
                    chunk_text = chunk.get("chunk_text", chunk.get("content", ""))
                    if not chunk_text:
                        raise ValueError("Chunk has no text content")

                    # LLMClient.generate_embeddingは同期関数なのでasyncio.to_threadで実行
                    embedding = await asyncio.to_thread(
                        self.llm_client.generate_embedding,
                        chunk_text
                    )
                    chunk["embedding"] = embedding
                    logger.debug(f"[ChunkProcessor] Generated embedding for chunk {chunk['chunk_index']}")
                except Exception as e:
                    logger.error(f"[ChunkProcessor] Failed to generate embedding for chunk {chunk['chunk_index']}: {e}")
                    chunk["embedding"] = None

                return chunk

        # 並列でembedding生成
        tasks = [generate_single_embedding(chunk) for chunk in chunks]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # 例外が発生したタスクをフィルタリング
        chunks_with_embeddings = [
            result for result in results
            if not isinstance(result, Exception) or logger.error(f"[ChunkProcessor] Exception during embedding generation: {result}")
        ]

        return chunks_with_embeddings

    def delete_document_chunks(self, document_id: str) -> bool:
        """
        指定されたドキュメントの全チャンクを削除

        Args:
            document_id: ドキュメントID

        Returns:
            成功/失敗
        """
        try:
            self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
            logger.info(f"[ChunkProcessor] Deleted all chunks for document {document_id}")
            return True
        except Exception as e:
            logger.error(f"[ChunkProcessor] Failed to delete chunks for document {document_id}: {e}")
            return False

    def get_document_chunks(self, document_id: str) -> List[Dict[str, any]]:
        """
        指定されたドキュメントの全チャンクを取得

        Args:
            document_id: ドキュメントID

        Returns:
            チャンクリスト
        """
        try:
            result = self.db.client.table('10_ix_search_index').select('*').eq('document_id', document_id).order('chunk_index').execute()
            return result.data if result.data else []
        except Exception as e:
            logger.error(f"[ChunkProcessor] Failed to get chunks for document {document_id}: {e}")
            return []
```

### shared\common\processing\metadata_chunker.py

```py
"""
メタデータ別ベクトル化戦略

タイトル、サマリー、日付などのメタデータを別々にチャンク化し、
検索時に重み付けを行うことで精度を向上させます。

旧方式: タイトルと本文を混ぜて小チャンク化 → タイトル情報が希釈
新方式: メタデータ種別ごとに独立したチャンクを生成 → 検索精度向上
"""
import re
from typing import List, Dict, Any, Optional
from loguru import logger


class MetadataChunker:
    """
    メタデータを種類別にチャンク化するクラス

    チャンク種別と重み付け:
    - title: タイトル専用チャンク（重み2.0）- 最高優先度
    - persons: 担当者・関係者チャンク（重み1.8）- 高優先度 ★新規
    - organizations: 組織名チャンク（重み1.7）- 高優先度 ★新規
    - summary: サマリー専用チャンク（重み1.5）
    - date: 日付専用チャンク（重み1.3）
    - tags: タグ専用チャンク（重み1.2）
    - people: AI抽出人物チャンク（重み1.2）★新規
    - content_small: 本文小チャンク（重み1.0）- 150文字
    - content_large: 本文大チャンク（重み1.0）- 全文
    - synthetic: 合成チャンク（重み1.0）- 構造化データ
    """

    # チャンク長の上限（300文字 - Stage Hガイドラインに準拠）
    MAX_CHUNK_LENGTH = 300

    # チャンク種別と検索重み
    CHUNK_WEIGHTS = {
        'title': 2.0,                    # タイトルマッチは最優先
        'display_subject': 2.0,          # 表示件名（最重要）
        'table': 2.0,                    # 構造化表（最重要）★新規追加
        'doc_type': 1.8,                 # 授業名・ドキュメント種別（重要）
        'display_post_text': 1.8,        # 表示投稿本文（重要）
        'persons': 1.8,                  # 担当者・関係者（重要）
        'text_block': 1.8,               # テキストブロック（重要）★新規追加
        'schedule': 1.7,                 # 週間予定（重要）★新規追加
        'organizations': 1.7,            # 組織名（重要）
        'summary': 1.5,                  # サマリーは高優先
        'display_type': 1.5,             # 表示種別（お知らせ/課題/資料）
        'task': 1.5,                     # タスク（重要）
        'display_sender': 1.3,           # 表示送信者名
        'display_sent_at': 1.3,          # 表示送信日時
        'date': 1.3,                     # 日付検索
        'calendar_event': 1.3,           # カレンダーイベント
        'tags': 1.2,                     # タグ検索
        'people': 1.2,                   # AI抽出人物（やや重要）
        'other': 1.0,                    # その他のテキスト（標準）★新規追加
        'classroom_sender_email': 1.0,   # Classroom送信者メール
        'content_small': 1.0,            # 本文検索（標準）
        'content_large': 1.0,            # 回答生成用
        'synthetic': 1.0,                # 構造化データ
    }

    def __init__(self):
        """初期化"""
        self.chunk_counter = 0

    def create_metadata_chunks(
        self,
        document_data: Dict[str, Any],
        existing_content_chunks: Optional[List[Dict[str, Any]]] = None
    ) -> List[Dict[str, Any]]:
        """
        ドキュメントデータからメタデータチャンクを生成

        Args:
            document_data: ドキュメント情報
                - file_name: ファイル名（タイトル）
                - summary: AI生成サマリー
                - document_date: 文書日付
                - tags: タグリスト
                - full_text: 全文（オプション）
            existing_content_chunks: 既存の本文チャンク（オプション）
                指定された場合、本文チャンク生成をスキップ

        Returns:
            メタデータチャンクのリスト
            [{
                'chunk_type': 'title',
                'chunk_text': '...',
                'search_weight': 2.0,
                'chunk_index': 0
            }, ...]
        """
        chunks = []
        self.chunk_counter = 0

        # 1. タイトルチャンク（最高優先度）
        title = document_data.get('file_name') or document_data.get('title')
        if title:
            # 拡張子を除去してクリーンなタイトルを作成
            clean_title = self._clean_title(title)
            if clean_title:
                chunks.extend(self._create_chunk(
                    chunk_type='title',
                    text=clean_title,
                    metadata={'original_filename': title}
                ))
                logger.debug(f"[MetadataChunker] タイトルチャンク作成: {clean_title[:50]}...")

        # 2. サマリーチャンク
        summary = document_data.get('summary')
        if summary and len(summary.strip()) > 10:
            chunks.extend(self._create_chunk(
                chunk_type='summary',
                text=summary.strip()
            ))
            logger.debug(f"[MetadataChunker] サマリーチャンク作成: {len(summary)}文字")

        # 3. 日付チャンク
        date_text = self._format_date_chunk(document_data)
        if date_text:
            chunks.extend(self._create_chunk(
                chunk_type='date',
                text=date_text
            ))
            logger.debug(f"[MetadataChunker] 日付チャンク作成: {date_text}")

        # 4. タグチャンク
        tags = document_data.get('tags', [])
        if tags:
            tag_text = self._format_tags_chunk(tags)
            if tag_text:
                chunks.extend(self._create_chunk(
                    chunk_type='tags',
                    text=tag_text
                ))
                logger.debug(f"[MetadataChunker] タグチャンク作成: {len(tags)}個のタグ")

        # 5. doc_type（授業名・ドキュメント種別）チャンク（高優先度）
        doc_type = document_data.get('doc_type')
        if doc_type and doc_type.strip():
            chunks.extend(self._create_chunk(
                chunk_type='doc_type',
                text=f"授業名: {doc_type.strip()}"
            ))
            logger.debug(f"[MetadataChunker] doc_typeチャンク作成: {doc_type}")

        # 6. 表示用専用チャンク（Google Classroom等の投稿情報）
        display_subject = document_data.get('display_subject')
        if display_subject and display_subject.strip():
            chunks.extend(self._create_chunk(
                chunk_type='display_subject',
                text=f"件名: {display_subject.strip()}"
            ))
            logger.debug(f"[MetadataChunker] 表示件名チャンク作成: {len(display_subject)}文字")

        display_post_text = document_data.get('display_post_text')
        if display_post_text and display_post_text.strip():
            chunks.extend(self._create_chunk(
                chunk_type='display_post_text',
                text=f"投稿本文: {display_post_text.strip()}"
            ))
            logger.debug(f"[MetadataChunker] 表示投稿本文チャンク作成: {len(display_post_text)}文字")

        display_type = document_data.get('display_type')
        if display_type:
            chunks.extend(self._create_chunk(
                chunk_type='display_type',
                text=f"種別: {display_type}"
            ))
            logger.debug(f"[MetadataChunker] 表示種別チャンク作成: {display_type}")

        display_sender = document_data.get('display_sender')
        if display_sender:
            chunks.extend(self._create_chunk(
                chunk_type='display_sender',
                text=f"送信者: {display_sender}"
            ))
            logger.debug(f"[MetadataChunker] 表示送信者チャンク作成: {display_sender}")

        display_sent_at = document_data.get('display_sent_at')
        if display_sent_at:
            chunks.extend(self._create_chunk(
                chunk_type='display_sent_at',
                text=f"送信日時: {display_sent_at}"
            ))
            logger.debug(f"[MetadataChunker] 表示送信日時チャンク作成: {display_sent_at}")

        classroom_sender_email = document_data.get('classroom_sender_email')
        if classroom_sender_email:
            chunks.extend(self._create_chunk(
                chunk_type='classroom_sender_email',
                text=f"送信者メール: {classroom_sender_email}"
            ))
            logger.debug(f"[MetadataChunker] Classroom送信者メールチャンク作成")

        # 7. persons（担当者・関係者）チャンク（高重要）
        persons = document_data.get('persons', [])
        if persons:
            persons_text = self._format_persons_chunk(persons)
            if persons_text:
                chunks.extend(self._create_chunk(
                    chunk_type='persons',
                    text=persons_text
                ))
                logger.debug(f"[MetadataChunker] personsチャンク作成: {len(persons)}名")

        # 8. organizations（組織名）チャンク（高重要）
        organizations = document_data.get('organizations', [])
        if organizations:
            orgs_text = self._format_organizations_chunk(organizations)
            if orgs_text:
                chunks.extend(self._create_chunk(
                    chunk_type='organizations',
                    text=orgs_text
                ))
                logger.debug(f"[MetadataChunker] organizationsチャンク作成: {len(organizations)}組織")

        # 9. people（AI抽出人物）チャンク
        people = document_data.get('people', [])
        if people:
            people_text = self._format_people_chunk(people)
            if people_text:
                chunks.extend(self._create_chunk(
                    chunk_type='people',
                    text=people_text
                ))
                logger.debug(f"[MetadataChunker] peopleチャンク作成: {len(people)}名")

        # 10. text_blocks（テキストブロック）チャンク
        text_blocks = document_data.get('text_blocks', [])
        if text_blocks:
            for i, block in enumerate(text_blocks):
                block_text = self._format_text_block_chunk(block, i)
                if block_text:
                    chunks.extend(self._create_chunk(
                        chunk_type=f'text_block_{i}',
                        text=block_text,
                        metadata={
                            'original_structure': block,
                            'structure_type': 'text_block'
                        }
                    ))
                    logger.debug(f"[MetadataChunker] text_blockチャンク作成: block_{i}")

        # 11. structured_tables（構造化表）チャンク
        structured_tables = document_data.get('structured_tables', [])
        if structured_tables:
            for i, table in enumerate(structured_tables):
                table_text = self._format_table_chunk(table, i)
                if table_text:
                    chunks.extend(self._create_chunk(
                        chunk_type=f'table_{i}',
                        text=table_text,
                        metadata={
                            'original_structure': table,
                            'structure_type': 'table'
                        }
                    ))
                    logger.debug(f"[MetadataChunker] tableチャンク作成: table_{i}")

        # 12. weekly_schedule（週間予定）チャンク
        weekly_schedule = document_data.get('weekly_schedule', [])
        if weekly_schedule:
            for schedule in weekly_schedule:
                schedule_text = self._format_schedule_chunk(schedule)
                if schedule_text:
                    date_str = schedule.get('date', f'schedule_{len(chunks)}')
                    chunks.extend(self._create_chunk(
                        chunk_type=f'schedule_{date_str}',
                        text=schedule_text,
                        metadata={
                            'original_structure': schedule,
                            'structure_type': 'schedule'
                        }
                    ))
                    logger.debug(f"[MetadataChunker] scheduleチャンク作成: {date_str}")

        # 13. other_text（その他のテキスト）チャンク
        other_text = document_data.get('other_text', [])
        if other_text:
            for i, other_item in enumerate(other_text):
                if isinstance(other_item, dict):
                    item_type = other_item.get('type', 'misc')
                    content = other_item.get('content', '')
                    if content and content.strip():
                        chunks.extend(self._create_chunk(
                            chunk_type=f'other_{item_type}_{i}',
                            text=content.strip(),
                            metadata={
                                'original_structure': other_item,
                                'structure_type': 'other_text',
                                'other_type': item_type
                            }
                        ))
                        logger.debug(f"[MetadataChunker] other_textチャンク作成: {item_type}")

        # 14. calendar_events（カレンダーイベント）チャンク
        calendar_events = document_data.get('calendar_events', [])
        if calendar_events:
            for i, event in enumerate(calendar_events):
                if isinstance(event, dict):
                    event_date = event.get('event_date', '')
                    event_time = event.get('event_time', '')
                    event_name = event.get('event_name', '')
                    location = event.get('location', '')
                    description = event.get('description', '')
                    participants = event.get('participants', [])

                    # イベントの説明文を生成
                    event_text_parts = []
                    if event_date:
                        event_text_parts.append(f"日付: {event_date}")
                    if event_time:
                        event_text_parts.append(f"時刻: {event_time}")
                    if event_name:
                        event_text_parts.append(f"イベント名: {event_name}")
                    if location:
                        event_text_parts.append(f"場所: {location}")
                    if description:
                        event_text_parts.append(f"詳細: {description}")
                    if participants:
                        event_text_parts.append(f"参加者: {', '.join(participants)}")

                    event_text = '\n'.join(event_text_parts)

                    if event_text.strip():
                        chunks.extend(self._create_chunk(
                            chunk_type=f'calendar_event_{i}',
                            text=event_text.strip(),
                            metadata={
                                'original_structure': event,
                                'structure_type': 'calendar_event',
                                'event_date': event_date,
                                'event_name': event_name
                            }
                        ))
                        logger.debug(f"[MetadataChunker] calendar_eventチャンク作成: {event_name}")

        # 15. tasks（タスク）チャンク
        tasks = document_data.get('tasks', [])
        if tasks:
            for i, task in enumerate(tasks):
                if isinstance(task, dict):
                    task_name = task.get('task_name', '')
                    deadline = task.get('deadline', '')
                    priority = task.get('priority', '')
                    category = task.get('category', '')
                    description = task.get('description', '')
                    checklist = task.get('checklist', [])
                    assignee = task.get('assignee', '')

                    # タスクの説明文を生成
                    task_text_parts = []
                    if task_name:
                        task_text_parts.append(f"タスク名: {task_name}")
                    if deadline:
                        task_text_parts.append(f"期限: {deadline}")
                    if priority:
                        task_text_parts.append(f"優先度: {priority}")
                    if category:
                        task_text_parts.append(f"カテゴリ: {category}")
                    if description:
                        task_text_parts.append(f"詳細: {description}")
                    if checklist:
                        task_text_parts.append(f"チェックリスト:\n" + '\n'.join([f"  - {item}" for item in checklist]))
                    if assignee:
                        task_text_parts.append(f"担当者: {assignee}")

                    task_text = '\n'.join(task_text_parts)

                    if task_text.strip():
                        chunks.extend(self._create_chunk(
                            chunk_type=f'task_{i}',
                            text=task_text.strip(),
                            metadata={
                                'original_structure': task,
                                'structure_type': 'task',
                                'task_name': task_name,
                                'deadline': deadline,
                                'priority': priority
                            }
                        ))
                        logger.debug(f"[MetadataChunker] taskチャンク作成: {task_name}")

        logger.info(f"[MetadataChunker] メタデータチャンク生成完了: {len(chunks)}個")
        return chunks

    def _split_text_by_sentences(self, text: str) -> List[str]:
        """
        長すぎるテキストを文の境界で分割（300文字制限）

        全体との関係を保つため、分割は文の境界で行い、
        メタデータに元の構造情報を記録する。

        Args:
            text: 分割するテキスト

        Returns:
            分割されたテキストのリスト（300文字以下の場合は1要素のリスト）
        """
        if len(text) <= self.MAX_CHUNK_LENGTH:
            return [text]

        # 文の境界で分割（。！？改行）
        sentences = re.split(r'([。！？\n])', text)

        parts = []
        current = ""

        i = 0
        while i < len(sentences):
            sentence = sentences[i]
            delimiter = sentences[i + 1] if i + 1 < len(sentences) else ""

            # 次の文を追加しても制限内に収まるか確認
            if len(current) + len(sentence) + len(delimiter) <= self.MAX_CHUNK_LENGTH:
                current += sentence + delimiter
                i += 2 if delimiter else 1
            else:
                # 現在のバッファを保存
                if current:
                    parts.append(current.strip())
                    current = ""

                # 単一の文が長すぎる場合は強制分割
                if len(sentence + delimiter) > self.MAX_CHUNK_LENGTH:
                    # 文を強制的に分割
                    for j in range(0, len(sentence), self.MAX_CHUNK_LENGTH):
                        chunk_part = sentence[j:j + self.MAX_CHUNK_LENGTH]
                        parts.append(chunk_part.strip())
                    if delimiter:
                        parts[-1] += delimiter
                    i += 2 if delimiter else 1
                else:
                    current = sentence + delimiter
                    i += 2 if delimiter else 1

        # 残りを追加
        if current.strip():
            parts.append(current.strip())

        return parts if parts else [text]

    def _create_chunk(
        self,
        chunk_type: str,
        text: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        チャンクオブジェクトを作成（長すぎる場合は自動分割）

        300文字を超える場合、文の境界で自動分割し、
        全体との関係を保つためメタデータに分割情報を記録。

        Args:
            chunk_type: チャンク種別
            text: チャンクテキスト
            metadata: 追加メタデータ

        Returns:
            チャンクオブジェクトのリスト（通常は1要素、分割時は複数要素）
        """
        # テキストを分割（300文字以下なら1要素のリスト）
        text_parts = self._split_text_by_sentences(text)

        if len(text_parts) > 1:
            logger.warning(
                f"[MetadataChunker] チャンク長超過により分割: {chunk_type} "
                f"({len(text)}文字 → {len(text_parts)}個に分割)"
            )

        # プレフィックスベースの重み付け（text_block_0 → text_block, table_1 → table）
        weight = self.CHUNK_WEIGHTS.get(chunk_type, None)
        if weight is None:
            # プレフィックスを抽出して検索
            for prefix in ['text_block', 'table', 'schedule', 'calendar_event', 'task', 'other']:
                if chunk_type.startswith(prefix):
                    weight = self.CHUNK_WEIGHTS.get(prefix, 1.0)
                    break
            if weight is None:
                weight = 1.0

        chunks = []
        for i, part in enumerate(text_parts):
            # 分割された場合はサフィックスを付与（全体との関係を明示）
            if len(text_parts) > 1:
                type_with_suffix = f"{chunk_type}_part{i}"
            else:
                type_with_suffix = chunk_type

            chunk = {
                'chunk_type': type_with_suffix,
                'chunk_text': part,
                'chunk_size': len(part),
                'search_weight': weight,
                'chunk_index': self.chunk_counter
            }

            if metadata:
                chunk['metadata'] = metadata.copy()
                # 分割情報を追加（全体との関係を保持）
                if len(text_parts) > 1:
                    chunk['metadata']['split_info'] = {
                        'original_chunk_type': chunk_type,
                        'original_length': len(text),
                        'part_index': i,
                        'total_parts': len(text_parts)
                    }

            chunks.append(chunk)
            self.chunk_counter += 1

        return chunks

    def _clean_title(self, title: str) -> str:
        """
        ファイル名からクリーンなタイトルを抽出

        - 拡張子を除去
        - 日付プレフィックスを整形
        - アンダースコアをスペースに変換
        """
        # 拡張子を除去
        clean = re.sub(r'\.(pdf|docx?|xlsx?|pptx?|txt|html?)$', '', title, flags=re.IGNORECASE)

        # アンダースコアをスペースに変換（ただしスネークケースを考慮）
        # 例: "2024_12_01_会議資料" → "2024年12月01日 会議資料"

        # 日付パターンを検出して整形
        date_pattern = r'^(\d{4})[_\-]?(\d{2})[_\-]?(\d{2})[_\s]*'
        match = re.match(date_pattern, clean)
        if match:
            year, month, day = match.groups()
            date_str = f"{year}年{month}月{day}日"
            rest = clean[match.end():].strip('_- ')
            clean = f"{date_str} {rest}" if rest else date_str

        # 残りのアンダースコアをスペースに
        clean = clean.replace('_', ' ')

        return clean.strip()

    def _format_date_chunk(self, document_data: Dict[str, Any]) -> Optional[str]:
        """
        日付関連情報をチャンク用テキストに整形

        document_date, event_dates, relevant_dateを統合
        """
        parts = []

        # メイン文書日付
        doc_date = document_data.get('document_date')
        if doc_date:
            parts.append(f"文書日付: {doc_date}")

        # イベント日付（複数可）
        event_dates = document_data.get('event_dates', [])
        if event_dates:
            if isinstance(event_dates, list):
                for event in event_dates:
                    if isinstance(event, dict):
                        label = event.get('label', 'イベント')
                        date = event.get('date', '')
                        if date:
                            parts.append(f"{label}: {date}")
                    elif isinstance(event, str):
                        parts.append(f"イベント日: {event}")

        # 関連日付
        relevant_date = document_data.get('relevant_date')
        if relevant_date and relevant_date != doc_date:
            parts.append(f"関連日付: {relevant_date}")

        return '\n'.join(parts) if parts else None

    def _format_tags_chunk(self, tags: List[str]) -> Optional[str]:
        """
        タグリストをチャンク用テキストに整形
        """
        if not tags:
            return None

        # 重複除去と整形
        unique_tags = list(dict.fromkeys(tags))  # 順序を保持して重複除去

        # タグをカンマ区切りとハッシュタグ形式の両方で表現
        tag_text = ', '.join(unique_tags)
        hashtags = ' '.join([f"#{tag}" for tag in unique_tags])

        return f"タグ: {tag_text}\n{hashtags}"

    def _format_persons_chunk(self, persons: List[str]) -> Optional[str]:
        """
        担当者・関係者リストをチャンク用テキストに整形

        配列には複数の表記（漢字、ひらがな、アルファベット）が含まれる想定
        例: ['山田太郎', 'やまだたろう', 'Yamada Taro', '山田']
        """
        if not persons:
            return None

        # 重複除去
        unique_persons = list(dict.fromkeys(persons))

        # スペース区切りで結合（ベクトル化時に全表記が考慮される）
        persons_text = ' '.join(unique_persons)

        return f"担当者: {persons_text}"

    def _format_organizations_chunk(self, organizations: List[str]) -> Optional[str]:
        """
        組織名リストをチャンク用テキストに整形

        配列には複数の表記（正式名称、略称、ひらがな、英語）が含まれる想定
        例: ['東京大学', 'とうきょうだいがく', 'Tokyo University', '東大', 'UTokyo']
        """
        if not organizations:
            return None

        # 重複除去
        unique_orgs = list(dict.fromkeys(organizations))

        # スペース区切りで結合
        orgs_text = ' '.join(unique_orgs)

        return f"組織: {orgs_text}"

    def _format_people_chunk(self, people: List[str]) -> Optional[str]:
        """
        AI抽出人物リストをチャンク用テキストに整形

        Args:
            people: AIが文書から抽出した人物名のリスト
        """
        if not people:
            return None

        # 重複除去
        unique_people = list(dict.fromkeys(people))

        # カンマ区切りとスペース区切りの両方で表現
        people_text = ', '.join(unique_people)
        people_spaced = ' '.join(unique_people)

        return f"関係者: {people_text}\n{people_spaced}"

    def _format_text_block_chunk(self, block: Dict[str, Any], index: int) -> Optional[str]:
        """
        テキストブロックをチャンク用テキストに展開

        Args:
            block: テキストブロック {'title': str, 'content': str}
            index: ブロックのインデックス

        Returns:
            展開されたテキスト
        """
        if not block or not isinstance(block, dict):
            return None

        title = block.get('title', '')
        content = block.get('content', '')

        if not content:
            return None

        # タイトルと本文を結合
        parts = []
        if title:
            parts.append(f"【{title}】")
        parts.append(content)

        return '\n'.join(parts)

    def _format_table_chunk(self, table: Dict[str, Any], index: int) -> Optional[str]:
        """
        構造化表をテキストに展開

        Args:
            table: {
                'table_title': str,
                'table_type': str,
                'headers': List[str],
                'rows': List[Dict[str, str]]
            }
            index: テーブルのインデックス

        Returns:
            テーブル全体をテキスト化したもの
        """
        if not table or not isinstance(table, dict):
            return None

        table_title = table.get('table_title', f'表{index + 1}')
        headers = table.get('headers', [])
        rows = table.get('rows', [])

        if not rows:
            return None

        # テーブルをテキスト化
        lines = []
        lines.append(f"【{table_title}】")
        lines.append("")

        # 各行をテキスト化
        for row_idx, row in enumerate(rows, 1):
            if isinstance(row, dict):
                # 辞書形式の行
                row_parts = []
                for header in headers:
                    value = row.get(header, '')
                    # valueがリストの場合は、改行区切りで結合
                    if isinstance(value, list):
                        value = '\n'.join(str(v) for v in value)
                    elif value is not None:
                        value = str(value)
                    else:
                        value = ''

                    if value:
                        row_parts.append(f"{header}: {value}")
                if row_parts:
                    lines.append(f"{row_idx}. " + ", ".join(row_parts))
            elif isinstance(row, list):
                # リスト形式の行（ヘッダーと組み合わせる）
                row_parts = []
                for i, value in enumerate(row):
                    # valueがリストの場合は、改行区切りで結合
                    if isinstance(value, list):
                        value_str = '\n'.join(str(v) for v in value)
                    else:
                        value_str = str(value) if value is not None else ''

                    if i < len(headers):
                        header = headers[i]
                        if isinstance(header, list):
                            header = ', '.join(str(h) for h in header)
                        row_parts.append(f"{header}: {value_str}")
                    else:
                        row_parts.append(value_str)
                if row_parts:
                    lines.append(f"{row_idx}. " + ", ".join(row_parts))

        return '\n'.join(lines) if len(lines) > 2 else None

    def _format_schedule_chunk(self, schedule: Dict[str, Any]) -> Optional[str]:
        """
        週間予定をテキストに展開

        Args:
            schedule: {
                'date': str,
                'day_of_week': str,
                'events': List[str],
                'class_schedules': List[Dict],
                'note': str
            }

        Returns:
            予定全体をテキスト化したもの
        """
        if not schedule or not isinstance(schedule, dict):
            return None

        date = schedule.get('date', '')
        day_of_week = schedule.get('day_of_week', '')
        events = schedule.get('events', [])
        class_schedules = schedule.get('class_schedules', [])
        note = schedule.get('note', '')

        if not date:
            return None

        lines = []

        # ヘッダー
        if day_of_week:
            lines.append(f"{date}（{day_of_week}）")
        else:
            lines.append(date)
        lines.append("")

        # クラス別時間割
        if class_schedules:
            for class_schedule in class_schedules:
                class_name = class_schedule.get('class', '')
                periods = class_schedule.get('periods', [])

                if class_name:
                    lines.append(f"【{class_name}】")

                for period in periods:
                    period_num = period.get('period', '')
                    subject = period.get('subject', '')
                    if period_num and subject:
                        lines.append(f"{period_num}時間目: {subject}")

                lines.append("")

        # イベント
        if events:
            lines.append("【行事】")
            for event in events:
                lines.append(f"- {event}")
            lines.append("")

        # 連絡事項
        if note:
            lines.append("【連絡事項】")
            lines.append(note)

        return '\n'.join(lines).strip() if lines else None
```

### shared\common\processors\__init__.py

```py

```

### shared\common\processors\office.py

```py
#【実行場所】: ターミナルまたはVS Code
#【対象ファイル】: 新規作成
#【ファイルパス】: core/processors/office.py
#【実行方法】: 以下のコードをファイルにコピー＆ペーストして保存してください。

"""
Office プロセッサ (テキスト抽出)

設計書: COMPLETE_IMPLEMENTATION_GUIDE_v3.md の 1.4節に基づき、Officeファイルからテキストを抽出する。
対応形式: DOCX, XLSX, PPTX
"""
from typing import Dict, Any
from pathlib import Path
from io import BytesIO
from docx import Document
from openpyxl import load_workbook
from pptx import Presentation
from loguru import logger

class OfficeProcessor:
    """DOCX, XLSX, PPTXファイルからテキストを抽出するプロセッサ"""
    
    def __init__(self):
        # logger.info("Officeプロセッサ初期化完了")
        pass

    def extract_from_docx(self, file_path: str) -> Dict[str, Any]:
        """DOCXファイルから全文を抽出する（Windows WinError 32対策済み）"""
        file_path = Path(file_path)
        full_text = []

        try:
            # BytesIOを使用してファイルロックを回避
            with open(file_path, 'rb') as f:
                file_data = BytesIO(f.read())

            document = Document(file_data)
            for paragraph in document.paragraphs:
                full_text.append(paragraph.text)

            # テーブルのテキストも追加
            for table in document.tables:
                for row in table.rows:
                    row_text = []
                    for cell in row.cells:
                        row_text.append(cell.text)
                    full_text.append(" | ".join(row_text))

            content = "\n".join(full_text)

            return {
                "content": content,
                "metadata": {
                    "file_type": "docx",
                    "text_extraction_model": "python-docx",
                    "vision_model": None
                },
                "success": True
            }

        except Exception as e:
            # logger.error(f"DOCXテキスト抽出エラー ({file_path}): {e}")
            return {"content": "", "metadata": {"file_type": "docx", "error": str(e)}, "success": False, "error_message": str(e)}

    def extract_from_xlsx(self, file_path: str) -> Dict[str, Any]:
        """XLSXファイルから全シートのテキストを抽出する（Windows WinError 32対策済み）"""
        file_path = Path(file_path)
        full_text = []

        try:
            # BytesIOを使用してファイルロックを回避
            with open(file_path, 'rb') as f:
                file_data = BytesIO(f.read())

            workbook = load_workbook(file_data, data_only=True)
            for sheet_name in workbook.sheetnames:
                sheet = workbook[sheet_name]
                full_text.append(f"\n---SHEET: {sheet_name}---\n")

                for row in sheet.iter_rows():
                    row_data = []
                    for cell in row:
                        value = str(cell.value) if cell.value is not None else ""
                        row_data.append(value)
                    full_text.append(" | ".join(row_data))

            # ワークブックを明示的に閉じる
            workbook.close()

            content = "\n".join(full_text)

            return {
                "content": content,
                "metadata": {
                    "file_type": "xlsx",
                    "text_extraction_model": "openpyxl",
                    "vision_model": None
                },
                "success": True
            }

        except Exception as e:
            # logger.error(f"XLSXテキスト抽出エラー ({file_path}): {e}")
            return {"content": "", "metadata": {"file_type": "xlsx", "error": str(e)}, "success": False, "error_message": str(e)}

    def extract_from_pptx(self, file_path: str) -> Dict[str, Any]:
        """PPTXファイルから全スライドのテキストを抽出する（Windows WinError 32対策済み）"""
        file_path = Path(file_path)
        full_text = []

        try:
            # BytesIOを使用してファイルロックを回避
            with open(file_path, 'rb') as f:
                file_data = BytesIO(f.read())

            presentation = Presentation(file_data)
            for i, slide in enumerate(presentation.slides):
                full_text.append(f"\n---SLIDE: {i+1}---")
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        full_text.append(shape.text)

            content = "\n".join(full_text)

            return {
                "content": content,
                "metadata": {
                    "file_type": "pptx",
                    "text_extraction_model": "python-pptx",
                    "vision_model": None
                },
                "success": True
            }

        except Exception as e:
            # logger.error(f"PPTXテキスト抽出エラー ({file_path}): {e}")
            return {"content": "", "metadata": {"file_type": "pptx", "error": str(e)}, "success": False, "error_message": str(e)}

    def extract_text(self, file_path: str) -> Dict[str, Any]:
        """ファイルタイプに応じてテキストを抽出する（ディスパッチメソッド）"""
        file_path = Path(file_path)
        extension = file_path.suffix.lower()

        if extension == '.docx':
            return self.extract_from_docx(file_path)
        elif extension == '.xlsx':
            return self.extract_from_xlsx(file_path)
        elif extension == '.pptx':
            return self.extract_from_pptx(file_path)
        else:
            return {
                "content": "",
                "metadata": {"file_type": extension, "error": "Unsupported file type"},
                "success": False,
                "error_message": f"Unsupported file type: {extension}"
            }
```

### shared\common\processors\pdf.py

```py
"""
PDF プロセッサ (総力戦アーキテクチャ: pdfplumber + Table + Vision)

設計書: COMPLETE_IMPLEMENTATION_GUIDE_v3.md の 1.4節に基づき、PDFファイルからテキストを抽出する。
v3.0: 「テキスト解析（pdfplumber）」で基礎を固め、「表構造解析」で論理を通し、
      最後に「Vision」で視覚情報を補完する総力戦アーキテクチャ。
"""
from typing import Dict, Any, List, Optional
from pathlib import Path
import os
import tempfile
import hashlib
from loguru import logger

import pdfplumber

# pdf2image は poppler が必要（macOS: brew install poppler）
try:
    from pdf2image import convert_from_path
    PDF2IMAGE_AVAILABLE = True
except ImportError:
    PDF2IMAGE_AVAILABLE = False
    logger.warning("pdf2image が利用できません。Gemini Vision補完機能は無効化されます。")


class PDFProcessor:
    """PDFファイルからテキストを抽出するプロセッサ（総力戦方式）"""

    def __init__(self, llm_client=None):
        """
        Args:
            llm_client: LLMClient インスタンス（Vision補完用）
        """
        self.llm_client = llm_client
        logger.info("PDFプロセッサ初期化完了 (総力戦アーキテクチャ)")

    def extract_text(self, file_path: str, progress_callback=None) -> Dict[str, Any]:
        """
        PDFファイルからテキストを抽出（pdfplumber + OCR分離方式）

        処理フロー:
        Layer 1: pdfplumber でテキスト + 表を抽出（正確）
        Layer 2: 画像があるページで OCR 実行（下読み・文字拾い徹底）
        Layer 3: 2つを別々に返す（統合しない）

        Args:
            file_path: PDFファイルのローカルパス

        Returns:
            抽出結果 (pdfplumber_text, ocr_text, metadata, success)
        """
        file_path = Path(file_path)

        if not file_path.exists():
            logger.error(f"ファイルが見つかりません: {file_path}")
            return {
                "content": "",
                "metadata": {"error": "File not found"},
                "success": False,
                "error_message": "File not found"
            }

        if file_path.suffix.lower() not in ['.pdf']:
            logger.warning(f"PDFファイルではありません: {file_path}")
            return {
                "content": "",
                "metadata": {"error": "Not a PDF file"},
                "success": False,
                "error_message": "Not a PDF file"
            }

        try:
            # ============================================
            # Layer 1: pdfplumber でテキスト + 表を抽出
            # ============================================
            pdfplumber_result = self._extract_with_pdfplumber(file_path)

            if not pdfplumber_result["success"]:
                logger.warning(f"pdfplumber完全失敗: {file_path.name} → E-4で全面補完を実行します")
                # pdfplumberが失敗してもE-4（Gemini Vision）で全面補完する
                # 空のpage_textsを用意してE-4に進む
                import fitz  # PyMuPDF
                pdf_doc = fitz.open(file_path)
                page_count = len(pdf_doc)
                pdf_doc.close()

                page_texts = [""] * page_count  # 全ページ空テキスト
                page_images = [True] * page_count  # 全ページ画像あり扱い
                metadata = {"total_pages": page_count, "total_tables": 0, "pdfplumber_failed": True}
            else:
                page_texts = pdfplumber_result["page_texts"]
                page_images = pdfplumber_result["page_images"]
                metadata = pdfplumber_result["metadata"]

            total_tables = metadata.get('total_tables', 0)
            logger.info(f"pdfplumber抽出完了（E-1~E-3）: {len(page_texts)} ページ, {total_tables} 表")

            # ============================================
            # Layer 2: Vision戦略の判定（E-4の準備）
            # ============================================
            vision_target_pages = self._detect_vision_target_pages(page_texts, page_images)

            logger.info(f"Vision差分検出対象: {len(vision_target_pages)}/{len(page_texts)} ページ")

            # ============================================
            # E-4: Gemini Vision 差分検出（対象ページのみ）
            # ============================================
            if progress_callback:
                progress_callback("E4")
            vision_corrections = {}

            if vision_target_pages and self.llm_client and PDF2IMAGE_AVAILABLE:
                logger.info(f"[E-4] Gemini Vision差分検出開始: {len(vision_target_pages)} ページ")
                vision_corrections = self._extract_with_gemini_vision(
                    file_path,
                    vision_target_pages,
                    page_texts
                )
            elif vision_target_pages and not PDF2IMAGE_AVAILABLE:
                logger.warning("[E-4] pdf2image が利用できないため、Vision差分検出をスキップします")
            elif vision_target_pages and not self.llm_client:
                logger.warning("[E-4] LLMClient が未指定のため、Vision差分検出をスキップします")

            # ============================================
            # E-5: VisionのOCR結果を適用
            # ============================================
            if progress_callback:
                progress_callback("E5")

            # E-3の統合Markdown文字数（適用前）
            e3_total_chars = sum(len(text) for text in page_texts)
            logger.info(f"[E-5] Vision OCR結果適用開始")
            logger.info(f"  ├─ E-3統合Markdown: {e3_total_chars}文字")

            # E-4の完全OCR結果を補完
            # - E-3がゼロの場合：E-4をそのまま使用（全体補完）
            # - E-3が非空の場合：E-3にE-4を追加（部分補完）
            vision_total = 0
            for i in vision_corrections:
                e4_markdown = vision_corrections[i].strip()
                if e4_markdown:
                    e3_chars = len(page_texts[i])
                    e4_chars = len(e4_markdown)

                    if e3_chars == 0:
                        # 全体補完：E-4をそのまま使用
                        page_texts[i] = e4_markdown
                        logger.info(f"  ├─ ページ{i+1}: E-4を使用 (全体補完: {e4_chars}文字)")
                    else:
                        # 部分補完：E-3にE-4を追加
                        page_texts[i] = page_texts[i] + f"\n\n---\n\n## Vision OCR 補完情報\n\n{e4_markdown}"
                        final_chars = len(page_texts[i])
                        logger.info(f"  ├─ ページ{i+1}: E-3にE-4を追加 (部分補完: E-3 {e3_chars}文字 + E-4 {e4_chars}文字 = {final_chars}文字)")

                    vision_total += e4_chars

            # E-5適用後の文字数
            e5_total_chars = sum(len(text) for text in page_texts)

            logger.info(f"[E-5] Vision OCR結果適用完了")
            logger.info(f"  ├─ Vision OCR適用ページ数: {len(vision_corrections)}ページ")
            logger.info(f"  └─ E-5最終Markdown: {e5_total_chars}文字")

            # ============================================
            # 最終統合: 全ページをMarkdown文書に統合
            # ============================================
            complete_parts = []
            for i, text in enumerate(page_texts):
                page_num = i + 1
                complete_parts.append(f"\n\n---\n\n# Page {page_num}\n\n")
                complete_parts.append(text)

            complete_text = "".join(complete_parts)

            # Stage E 完了ログ
            logger.info(f"[Stage E完了] PDF抽出結果:")
            logger.info(f"  ├─ E-1~E-3: {e3_total_chars}文字")
            logger.info(f"  ├─ E-4: Vision差分検出 {len(vision_corrections)}ページ")
            logger.info(f"  ├─ E-5: Vision修正適用 {vision_total}文字")
            logger.info(f"  └─ 最終Markdown文書: {len(complete_text)}文字")

            # メタデータ更新
            metadata['vision_corrected'] = len(vision_corrections) > 0
            metadata['vision_pages'] = len(vision_corrections)
            metadata['pdfplumber_model'] = 'pdfplumber'
            metadata['vision_model'] = 'gemini-2.5-flash' if vision_corrections else None
            metadata['e3_chars'] = e3_total_chars
            metadata['vision_correction_chars'] = vision_total
            metadata['e5_chars'] = e5_total_chars
            metadata['total_chars'] = len(complete_text)

            return {
                "content": complete_text,
                "metadata": metadata,
                "success": True
            }

        except Exception as e:
            logger.error(f"PDFテキスト抽出エラー ({file_path}): {e}")
            import traceback
            traceback.print_exc()
            return {
                "content": "",
                "metadata": {"error": str(e)},
                "success": False,
                "error_message": str(e)
            }

    def _extract_with_pdfplumber(self, file_path: Path) -> Dict[str, Any]:
        """
        pdfplumber でテキスト + 表を抽出

        Returns:
            {
                "success": bool,
                "page_texts": List[str],
                "page_tables": List[List[str]],  # ページごとのMarkdown表リスト
                "metadata": dict
            }
        """
        try:
            with pdfplumber.open(file_path) as pdf:
                num_pages = len(pdf.pages)
                page_texts = []
                page_images = []  # 各ページの画像有無
                all_stats = []  # 全ページの統計情報

                for i, page in enumerate(pdf.pages):
                    page_num = i + 1

                    # 画像検出
                    images = page.images
                    page_images.append(len(images) > 0)  # 画像があればTrue

                    # E-1からE-3: 位置情報を使った統合Markdown生成
                    unified_markdown, stats = self._merge_with_position(page, page_num)
                    page_texts.append(unified_markdown)
                    all_stats.append(stats)

                # 統計情報をメタデータに追加
                total_tables = sum(s['num_tables'] for s in all_stats)
                total_unified_chars = sum(s['unified_chars'] for s in all_stats)

                metadata = {
                    'num_pages': num_pages,
                    'extractor': 'pdfplumber',
                    'total_tables': total_tables,
                    'total_unified_chars': total_unified_chars,
                    'per_page_stats': all_stats
                }

                # 全ページが空テキストの場合
                if not any(page_texts):
                    logger.warning(f"pdfplumber: テキストを抽出できませんでした ({file_path})")
                    return {
                        "success": False,
                        "page_texts": [],
                        "page_images": [],
                        "metadata": metadata,
                        "error_message": "No text extracted"
                    }

                return {
                    "success": True,
                    "page_texts": page_texts,
                    "page_images": page_images,
                    "metadata": metadata
                }

        except Exception as e:
            logger.error(f"pdfplumber抽出エラー: {e}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "page_texts": [],
                "page_images": [],
                "metadata": {"error": str(e)},
                "error_message": str(e)
            }

    def _table_to_markdown(self, table: List[List]) -> str:
        """
        pdfplumberの表データをMarkdown形式に変換

        Args:
            table: pdfplumberのextract_tables()が返す2次元リスト

        Returns:
            Markdown形式の表文字列
        """
        if not table:
            return ""

        # None を空文字列に変換
        cleaned_table = []
        for row in table:
            cleaned_row = [str(cell).strip() if cell is not None else "" for cell in row]
            cleaned_table.append(cleaned_row)

        if not cleaned_table:
            return ""

        # Markdownテーブル生成
        md_lines = []

        # ヘッダー行
        header = cleaned_table[0]
        md_lines.append("| " + " | ".join(header) + " |")

        # セパレーター
        md_lines.append("| " + " | ".join(["---"] * len(header)) + " |")

        # データ行
        for row in cleaned_table[1:]:
            # ヘッダーと列数を合わせる
            if len(row) < len(header):
                row.extend([""] * (len(header) - len(row)))
            elif len(row) > len(header):
                row = row[:len(header)]

            md_lines.append("| " + " | ".join(row) + " |")

        return "\n".join(md_lines)

    def _convert_to_markdown(self, text: str) -> str:
        """
        プレーンテキストをMarkdown形式に変換

        Args:
            text: プレーンテキスト

        Returns:
            Markdown形式のテキスト
        """
        if not text or not text.strip():
            return ""

        lines = text.split('\n')
        markdown_lines = []

        for line in lines:
            stripped = line.strip()

            # 空行はそのまま
            if not stripped:
                markdown_lines.append("")
                continue

            # 箇条書き（・で始まる行）を Markdown の - に変換
            if stripped.startswith('・'):
                markdown_line = "- " + stripped[1:].strip()
                markdown_lines.append(markdown_line)

            # 短い行（20文字以下）で全て大文字または数字のみ → 見出しとして扱う
            elif len(stripped) <= 20 and (stripped.isupper() or stripped.isdigit()):
                markdown_lines.append(f"## {stripped}")

            # その他はそのまま
            else:
                markdown_lines.append(stripped)

        return "\n".join(markdown_lines)

    def _is_inside_bbox(self, word: Dict, bbox: tuple) -> bool:
        """
        単語が表の範囲（bbox）内にあるか判定

        Args:
            word: pdfplumberのextract_words()が返す単語辞書 {'x0', 'x1', 'top', 'bottom', 'text'}
            bbox: 表の範囲 (x0, top, x1, bottom)

        Returns:
            True if 単語が表の範囲内にある
        """
        wx0, wx1 = word['x0'], word['x1']
        wtop, wbottom = word['top'], word['bottom']
        bx0, btop, bx1, bbottom = bbox

        # 単語の中心点が表の範囲内にあるか判定
        word_center_x = (wx0 + wx1) / 2
        word_center_y = (wtop + wbottom) / 2

        return (bx0 <= word_center_x <= bx1) and (btop <= word_center_y <= bbottom)

    def _merge_with_position(self, page, page_num: int) -> tuple:
        """
        位置情報を使ってテキストと表を正しい順序で統合（重複削除）

        E-1: テキスト抽出 (Markdown形式)
        E-2: 表抽出 (Markdown形式)
        E-3: 位置情報で統合 + 重複削除

        Args:
            page: pdfplumberのPageオブジェクト
            page_num: ページ番号（ログ用）

        Returns:
            (unified_markdown: str, stats: dict) 統合されたMarkdownと統計情報
        """
        # E-1: テキスト抽出
        words = page.extract_words()
        raw_text_chars = sum(len(w['text']) for w in words)
        logger.info(f"[E-1] ページ{page_num} テキスト抽出:")
        logger.info(f"  ├─ 生テキスト: {raw_text_chars}文字 ({len(words)}単語)")

        # E-2: 表抽出
        tables = page.find_tables()
        logger.info(f"[E-2] ページ{page_num} 表抽出:")
        logger.info(f"  ├─ 表の数: {len(tables)}個")

        # 表のMarkdown変換
        table_elements = []
        table_markdown_total = 0
        for table in tables:
            table_data = table.extract()
            table_md = self._table_to_markdown(table_data)
            if table_md:
                # table.bbox = (x0, top, x1, bottom)
                table_elements.append({
                    'type': 'table',
                    'y': table.bbox[1],  # top (上端) でソート
                    'content': table_md,
                    'bbox': table.bbox
                })
                table_markdown_total += len(table_md)

        logger.info(f"  └─ Markdown変換後: {table_markdown_total}文字")

        # E-3: 統合（重複削除 + 順序保持）
        logger.info(f"[E-3] ページ{page_num} 統合:")

        # 表の範囲内にある単語を除外（重複削除）
        words_outside_tables = []
        table_bboxes = [te['bbox'] for te in table_elements]

        for word in words:
            is_inside = False
            for bbox in table_bboxes:
                if self._is_inside_bbox(word, bbox):
                    is_inside = True
                    break
            if not is_inside:
                words_outside_tables.append(word)

        # テキスト要素を行ごとにグループ化（y座標が近い単語をまとめる）
        text_lines = self._group_words_into_lines(words_outside_tables)
        text_elements = []
        for line in text_lines:
            text_elements.append({
                'type': 'text',
                'y': line['top'],  # top (上端) でソート
                'content': line['text']
            })

        # テキストと表を統合
        all_elements = text_elements + table_elements

        # y座標でソート（上から下: top が小さい → 大きい）
        all_elements.sort(key=lambda e: e['y'])

        # Markdown統合
        markdown_parts = []
        for elem in all_elements:
            if elem['type'] == 'text':
                markdown_text = self._convert_to_markdown(elem['content'])
                if markdown_text.strip():
                    markdown_parts.append(markdown_text)
            else:  # table
                markdown_parts.append(elem['content'])

        unified_markdown = "\n\n".join(markdown_parts)

        # 統計情報
        removed_text_chars = raw_text_chars - sum(len(e['content']) for e in text_elements if e['type'] == 'text')

        logger.info(f"  ├─ 統合前（テキスト + 表）: {raw_text_chars + table_markdown_total}文字")
        logger.info(f"  ├─ 重複除去（表内テキスト）: -{removed_text_chars}文字")
        logger.info(f"  └─ 統合後: {len(unified_markdown)}文字")

        stats = {
            'raw_text_chars': raw_text_chars,
            'table_markdown_chars': table_markdown_total,
            'removed_chars': removed_text_chars,
            'unified_chars': len(unified_markdown),
            'num_tables': len(tables)
        }

        return unified_markdown, stats

    def _group_words_into_lines(self, words: List[Dict]) -> List[Dict]:
        """
        単語をy座標でグループ化して行にまとめる

        Args:
            words: pdfplumberのextract_words()が返す単語リスト

        Returns:
            行のリスト [{'top', 'bottom', 'text'}, ...]
        """
        if not words:
            return []

        # y座標でソート（上から下: top が小さい → 大きい）
        sorted_words = sorted(words, key=lambda w: (w['top'], w['x0']))

        lines = []
        current_line = None
        y_tolerance = 3  # y座標の許容範囲（ピクセル）

        for word in sorted_words:
            if current_line is None:
                # 新しい行を開始
                current_line = {
                    'top': word['top'],
                    'bottom': word['bottom'],
                    'words': [word]
                }
            elif abs(word['top'] - current_line['top']) <= y_tolerance:
                # 同じ行に追加
                current_line['words'].append(word)
                current_line['bottom'] = max(current_line['bottom'], word['bottom'])
            else:
                # 現在の行を確定して新しい行を開始
                current_line['text'] = " ".join(w['text'] for w in current_line['words'])
                lines.append(current_line)
                current_line = {
                    'top': word['top'],
                    'bottom': word['bottom'],
                    'words': [word]
                }

        # 最後の行を追加
        if current_line:
            current_line['text'] = " ".join(w['text'] for w in current_line['words'])
            lines.append(current_line)

        return lines

    def _detect_vision_target_pages(
        self,
        page_texts: List[str],
        page_images: List[bool]
    ) -> List[int]:
        """
        Vision差分検出が必要なページを検出

        判定基準:
        画像が埋め込まれている可能性があるPDFは全てVision対象
        一番大事な情報が画像化されている場合があるため、
        文字数に関わらず画像があれば処理する

        Args:
            page_texts: 各ページのテキストリスト（E-3統合後）
            page_images: 各ページの画像有無リスト（True=画像あり）

        Returns:
            Vision差分検出対象のページ番号リスト（0-indexed）
        """
        target_pages = []

        for i, has_image in enumerate(page_images):
            # 画像が埋め込まれているページは全てVision処理
            if has_image:
                target_pages.append(i)
                logger.debug(f"ページ {i+1}: 画像検出 → Vision差分検出")

        return target_pages

    def _extract_with_gemini_vision(
        self,
        pdf_path: Path,
        page_numbers: List[int],
        page_texts: List[str]
    ) -> Dict[int, str]:
        """
        指定されたページをGemini Visionで解析（E-4: 差分検出型）

        Args:
            pdf_path: PDFファイルパス
            page_numbers: 解析対象のページ番号リスト（0-indexed）
            page_texts: 各ページの E-3統合Markdown テキスト

        Returns:
            {ページ番号: Vision修正情報} の辞書
        """
        vision_results = {}

        # PDFを画像に変換
        try:
            logger.info(f"PDF→画像変換開始: {pdf_path}")
            images = convert_from_path(
                pdf_path,
                dpi=200,  # 表組み認識に十分な解像度
                fmt='png'
            )

            # 該当ページのみ処理
            for page_num in page_numbers:
                if page_num >= len(images):
                    logger.warning(f"ページ {page_num} は範囲外です")
                    continue

                image = images[page_num]

                # 一時ファイルに保存
                with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp_file:
                    tmp_path = Path(tmp_file.name)
                    image.save(tmp_path, 'PNG')

                try:
                    # このページの E-3統合Markdown を取得
                    e3_markdown = page_texts[page_num] if page_num < len(page_texts) else ""
                    e3_chars = len(e3_markdown)

                    # E-4: 常に完全OCRモード（画像から完全なMarkdownを生成）
                    prompt = """この画像を詳細に見て、全ての文字と構造を正確にMarkdown形式で再現してください。

【あなたの役割】
画像から全ての文字を漏らさず拾い、**文書の構造も正確に再現**してください。

【文字拾いの徹底指示】
- **小さな文字**: 注釈、脚注、コピーライト表記なども全て拾う
- **ロゴ化された文字**: 画像として埋め込まれたタイトル、会社名、ブランド名なども全て読み取る
- **装飾された文字**: 太字、斜体、色付きなど、装飾に関わらず全て拾う
- **背景に埋もれた文字**: 薄い色、透かし文字なども可能な限り読み取る
- **手書き文字**: 判読可能な範囲で全て拾う
- **表構造**: 表がある場合は、セルの位置関係を正確に再現
- **文字色・装飾**: 重要な情報（赤字、太字など）は注釈として記載

【出力形式（重要）】
画像内の全てのテキストを **Markdown形式** で出力してください：

1. **通常のテキスト**: そのまま記述
2. **見出し**: `##` を使用
3. **箇条書き**: `・` または `- ` を使用（画像に応じて）
4. **表**: Markdown table形式で出力
   ```
   | ヘッダー1 | ヘッダー2 | ヘッダー3 |
   | --- | --- | --- |
   | セル1 | セル2 | セル3 |
   ```
5. **段落**: 空行で区切る

**重要**:
- 表がある場合は必ずMarkdown table形式で出力してください
- セルの結合は空白セルで表現してください
- 画像を見たまま、上から下、左から右の順に正確に再現してください
- 1文字も見逃さないでください"""

                    # Gemini Vision で解析（E-4）
                    logger.info(f"[E-4] Vision差分検出: ページ {page_num + 1} (E-3: {e3_chars}文字)")
                    result = self.llm_client.transcribe_image(
                        image_path=tmp_path,
                        model="gemini-2.5-flash",
                        prompt=prompt
                    )

                    if result.get("success"):
                        content = result.get("content", "")
                        vision_chars = len(content)
                        vision_results[page_num] = content
                        logger.info(f"[E-4] Vision差分検出成功: ページ {page_num + 1}")
                        logger.info(f"  ├─ E-3統合Markdown: {e3_chars}文字")
                        logger.info(f"  └─ Vision差分情報: {vision_chars}文字")
                    else:
                        logger.warning(f"[E-4] Vision差分検出失敗: ページ {page_num + 1}, エラー: {result.get('error')}")

                finally:
                    # 一時ファイル削除
                    if tmp_path.exists():
                        os.unlink(tmp_path)

            return vision_results

        except Exception as e:
            logger.error(f"[E-4] Gemini Vision差分検出エラー: {e}")
            import traceback
            traceback.print_exc()
            return {}


def calculate_content_hash(pdf_path: str) -> str:
    """
    PDFファイルの内容全体からSHA256ハッシュを計算する

    Args:
        pdf_path: PDFファイルのローカルパス

    Returns:
        SHA256ハッシュ値（16進数文字列）

    Raises:
        FileNotFoundError: ファイルが存在しない場合
        IOError: ファイル読み込みエラー
    """
    file_path = Path(pdf_path)

    if not file_path.exists():
        raise FileNotFoundError(f"ファイルが見つかりません: {pdf_path}")

    # SHA256ハッシュオブジェクトを作成
    sha256_hash = hashlib.sha256()

    # ファイルをバイナリモードで読み込み、チャンクごとにハッシュを更新
    # 大きなファイルでもメモリ効率的に処理
    with open(file_path, 'rb') as f:
        # 64KBずつ読み込む
        for byte_block in iter(lambda: f.read(65536), b""):
            sha256_hash.update(byte_block)

    # 16進数文字列として返す
    return sha256_hash.hexdigest()
```

### shared\common\utils\__init__.py

```py
"""
Core Utilities
"""
```

### shared\common\utils\chunking.py

```py
"""
テキストチャンク分割ユーティリティ

長いテキストを適切なサイズのチャンクに分割し、検索精度を向上させます。
"""
from typing import List, Dict, Any
import re
from loguru import logger


class TextChunker:
    """
    テキストをチャンクに分割するクラス

    設計方針:
    - 意味のある単位（段落、セクション）を優先的に保持
    - 各チャンクは500-1000文字程度に収める
    - オーバーラップを設けることで文脈の連続性を確保
    """

    def __init__(
        self,
        chunk_size: int = 800,
        chunk_overlap: int = 100,
        min_chunk_size: int = 100
    ):
        """
        Args:
            chunk_size: 目標チャンクサイズ（文字数）
            chunk_overlap: チャンク間のオーバーラップ（文字数）
            min_chunk_size: 最小チャンクサイズ（これより小さいチャンクは前のチャンクにマージ）
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size

    def split_text(self, text: str, metadata: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """
        テキストをチャンクに分割

        Args:
            text: 分割対象のテキスト
            metadata: チャンクに付与する追加メタデータ

        Returns:
            チャンクのリスト、各チャンクは以下の形式:
            {
                "chunk_index": int,
                "chunk_text": str,
                "chunk_size": int,
                "page_numbers": List[int] (オプション),
                "section_title": str (オプション)
            }
        """
        if not text or not text.strip():
            logger.warning("空のテキストが渡されました")
            return []

        # ページ情報を抽出（--- Page N --- の形式）
        page_info = self._extract_page_info(text)

        # ステップ1: セクション分割（大きな区切りを優先）
        sections = self._split_by_sections(text)

        # ステップ2: 各セクションをチャンクサイズに収まるように分割
        chunks = []
        chunk_index = 0

        for section in sections:
            section_chunks = self._split_section(section, chunk_index, page_info)
            chunks.extend(section_chunks)
            chunk_index += len(section_chunks)

        logger.info(f"テキスト分割完了: {len(chunks)} チャンク（総文字数: {len(text)}）")

        return chunks

    def _extract_page_info(self, text: str) -> Dict[int, int]:
        """
        テキストからページ情報を抽出

        Returns:
            {文字位置: ページ番号} の辞書
        """
        page_markers = re.finditer(r'---\s*Page\s+(\d+)\s*---', text)
        page_info = {}

        for match in page_markers:
            page_num = int(match.group(1))
            position = match.start()
            page_info[position] = page_num

        return page_info

    def _get_page_number(self, position: int, page_info: Dict[int, int]) -> int:
        """
        特定の文字位置が属するページ番号を取得

        Args:
            position: テキスト内の文字位置
            page_info: _extract_page_info() の出力

        Returns:
            ページ番号（不明な場合は1）
        """
        if not page_info:
            return 1

        # positionより前にある最新のページマーカーを探す
        valid_pages = [(pos, page) for pos, page in page_info.items() if pos <= position]

        if valid_pages:
            # 最も近いページマーカーのページ番号を返す
            return max(valid_pages, key=lambda x: x[0])[1]

        return 1  # デフォルトはページ1

    def _split_by_sections(self, text: str) -> List[str]:
        """
        テキストをセクションに分割（大きな区切りを優先）

        優先順位:
        1. ページ区切り（--- Page N ---）
        2. 大見出し（### で始まる行）
        3. 段落（空行で区切られた塊）

        Returns:
            セクションのリスト
        """
        # まず、ページ単位で分割
        page_sections = re.split(r'(?=---\s*Page\s+\d+\s*---)', text)

        all_sections = []

        for page_section in page_sections:
            if not page_section.strip():
                continue

            # 各ページ内で、見出しまたは段落単位に分割
            # 改行2つ以上を区切りとする
            sub_sections = re.split(r'\n\s*\n', page_section)

            for sub_section in sub_sections:
                if sub_section.strip():
                    all_sections.append(sub_section.strip())

        return all_sections

    def _split_section(
        self,
        section: str,
        start_index: int,
        page_info: Dict[int, int]
    ) -> List[Dict[str, Any]]:
        """
        1つのセクションをチャンクサイズに収まるように分割

        Args:
            section: 分割対象のセクション
            start_index: このセクションの開始チャンク番号
            page_info: ページ情報

        Returns:
            チャンクのリスト
        """
        chunks = []

        # セクション全体がチャンクサイズ以下の場合はそのまま1チャンクにする
        if len(section) <= self.chunk_size:
            chunks.append({
                "chunk_index": start_index,
                "chunk_text": section,
                "chunk_size": len(section)
            })
            return chunks

        # セクションを文単位で分割
        sentences = self._split_sentences(section)

        current_chunk = []
        current_size = 0
        chunk_index = start_index

        for sentence in sentences:
            sentence_size = len(sentence)

            # 現在のチャンクに追加すると chunk_size を超える場合
            if current_size + sentence_size > self.chunk_size and current_chunk:
                # 現在のチャンクを確定
                chunk_text = "".join(current_chunk)
                chunks.append({
                    "chunk_index": chunk_index,
                    "chunk_text": chunk_text,
                    "chunk_size": len(chunk_text)
                })

                # 次のチャンクを開始（オーバーラップを考慮）
                chunk_index += 1

                # オーバーラップ分のテキストを次のチャンクに引き継ぐ
                overlap_text = chunk_text[-self.chunk_overlap:] if len(chunk_text) > self.chunk_overlap else chunk_text
                current_chunk = [overlap_text, sentence]
                current_size = len(overlap_text) + sentence_size
            else:
                # 現在のチャンクに追加
                current_chunk.append(sentence)
                current_size += sentence_size

        # 最後のチャンクを追加
        if current_chunk:
            chunk_text = "".join(current_chunk)

            # 最小サイズチェック: 小さすぎる場合は前のチャンクにマージ
            if len(chunk_text) < self.min_chunk_size and chunks:
                # 前のチャンクに追加
                chunks[-1]["chunk_text"] += "\n" + chunk_text
                chunks[-1]["chunk_size"] = len(chunks[-1]["chunk_text"])
            else:
                chunks.append({
                    "chunk_index": chunk_index,
                    "chunk_text": chunk_text,
                    "chunk_size": len(chunk_text)
                })

        return chunks

    def _split_sentences(self, text: str) -> List[str]:
        """
        テキストを文単位で分割

        日本語の句読点（。、！、？）および英語のピリオドで分割

        Args:
            text: 分割対象のテキスト

        Returns:
            文のリスト
        """
        # 日本語の句点・疑問符・感嘆符、英語のピリオド、改行で分割
        # ただし、分割記号自体は保持する
        pattern = r'([。！？\.?!]\s*|\n+)'

        parts = re.split(pattern, text)

        sentences = []
        current = ""

        for part in parts:
            if not part:
                continue

            current += part

            # 句読点または改行を含む場合、文として確定
            if re.match(pattern, part):
                sentences.append(current)
                current = ""

        # 残りがあれば追加
        if current.strip():
            sentences.append(current)

        return sentences


class ParentChildChunker:
    """
    Parent-Child Indexing用のチャンク分割クラス

    設計方針:
    - 親チャンク（1000-2000文字）: 回答用の十分なコンテキスト
    - 子チャンク（200-400文字）: 検索用の細かい粒度
    - 検索は子チャンクで実行し、ヒットしたら親チャンクを返す
    """

    def __init__(
        self,
        parent_chunk_size: int = 1500,
        parent_chunk_overlap: int = 200,
        child_chunk_size: int = 300,
        child_chunk_overlap: int = 50
    ):
        """
        Args:
            parent_chunk_size: 親チャンクの目標サイズ（1000-2000文字推奨）
            parent_chunk_overlap: 親チャンク間のオーバーラップ
            child_chunk_size: 子チャンクの目標サイズ（200-400文字推奨）
            child_chunk_overlap: 子チャンク間のオーバーラップ
        """
        self.parent_chunker = TextChunker(
            chunk_size=parent_chunk_size,
            chunk_overlap=parent_chunk_overlap
        )
        self.child_chunk_size = child_chunk_size
        self.child_chunk_overlap = child_chunk_overlap

    def split_text(self, text: str) -> Dict[str, List[Dict[str, Any]]]:
        """
        テキストを親子チャンクに分割

        Args:
            text: 分割対象のテキスト

        Returns:
            {
                "parent_chunks": List[Dict],  # 親チャンクのリスト
                "child_chunks": List[Dict]    # 子チャンクのリスト
            }
        """
        # ステップ1: 親チャンクを作成（1000-2000文字）
        parent_chunks = self.parent_chunker.split_text(text)

        # ステップ2: 各親チャンクを子チャンクに分割（200-400文字）
        all_child_chunks = []
        child_global_index = 0

        for parent_idx, parent_chunk in enumerate(parent_chunks):
            parent_text = parent_chunk["chunk_text"]

            # 親チャンクに識別用のメタデータを追加
            parent_chunk["is_parent"] = True
            parent_chunk["chunk_level"] = "parent"
            parent_chunk["parent_local_index"] = parent_idx

            # 親チャンクのテキストを子チャンクに分割
            child_chunks = self._split_into_children(
                parent_text=parent_text,
                parent_index=parent_idx,
                start_child_index=child_global_index
            )

            all_child_chunks.extend(child_chunks)
            child_global_index += len(child_chunks)

        logger.info(
            f"Parent-Child分割完了: "
            f"{len(parent_chunks)}親チャンク、{len(all_child_chunks)}子チャンク"
        )

        return {
            "parent_chunks": parent_chunks,
            "child_chunks": all_child_chunks
        }

    def _split_into_children(
        self,
        parent_text: str,
        parent_index: int,
        start_child_index: int
    ) -> List[Dict[str, Any]]:
        """
        親チャンクを子チャンクに分割

        Args:
            parent_text: 親チャンクのテキスト
            parent_index: 親チャンクのインデックス
            start_child_index: 子チャンクの開始インデックス

        Returns:
            子チャンクのリスト
        """
        # 子チャンク用のTextChunkerを作成
        child_chunker = TextChunker(
            chunk_size=self.child_chunk_size,
            chunk_overlap=self.child_chunk_overlap,
            min_chunk_size=100
        )

        # 子チャンクに分割
        child_chunks = child_chunker.split_text(parent_text)

        # 各子チャンクに親情報を追加
        for i, child_chunk in enumerate(child_chunks):
            child_chunk["chunk_index"] = start_child_index + i
            child_chunk["is_parent"] = False
            child_chunk["chunk_level"] = "child"
            child_chunk["parent_local_index"] = parent_index
            child_chunk["child_local_index"] = i

        return child_chunks
```

### shared\common\utils\context_extractor.py

```py
"""
Context Extractor
クエリから関連するユーザーコンテキストを抽出する
"""
from typing import Dict, List, Any, Optional
import re


class ContextExtractor:
    """
    クエリからユーザーコンテキスト（user_context.yaml）の
    関連情報のみを抽出するクラス
    """

    def __init__(self, user_context: Dict[str, Any]):
        """
        Args:
            user_context: user_context.yamlから読み込んだ辞書
        """
        self.user_context = user_context
        self.children = user_context.get('children', [])

    def extract_relevant_context(
        self,
        query: str,
        include_schedules: bool = False
    ) -> Dict[str, Any]:
        """
        クエリから関連するコンテキストを抽出

        Args:
            query: ユーザーの質問
            include_schedules: スケジュール情報を含めるか（回答生成時はTrue、検索時はFalse）

        Returns:
            抽出されたコンテキスト（子供の情報など）
        """
        if not self.children:
            return {"children": [], "relevance": "none"}

        # クエリから子供の名前を検出
        matched_children = self._find_children_by_name(query)

        # 名前が見つからない場合、学年やクラスから検出
        if not matched_children:
            matched_children = self._find_children_by_grade_or_class(query)

        # それでも見つからない場合
        if not matched_children:
            # クエリが学校や塾に関する一般的な質問の場合、全員を含める（後方互換性）
            if self._is_general_school_query(query):
                matched_children = self.children
                relevance = "general"
            else:
                # 関連性が低い場合は空
                return {"children": [], "relevance": "low"}
        else:
            relevance = "high"

        # 抽出されたコンテキストを構築
        extracted_children = []
        for child in matched_children:
            child_info = {
                "name": child.get("name"),
                "grade": child.get("grade"),
                "birth_date": child.get("birth_date"),
            }

            # 学校情報（基本）
            school = child.get("school", {})
            if school:
                child_info["school"] = {
                    "name": school.get("name"),
                    "class": school.get("class")
                }
                # スケジュールを含める場合のみ追加
                if include_schedules and school.get("schedule"):
                    child_info["school"]["schedule"] = school.get("schedule")

            # 塾情報（基本）
            cram_school = child.get("cram_school", {})
            if cram_school:
                child_info["cram_school"] = {
                    "name": cram_school.get("name")
                }
                # スケジュールを含める場合のみ追加
                if include_schedules and cram_school.get("schedule"):
                    child_info["cram_school"]["schedule"] = cram_school.get("schedule")

            extracted_children.append(child_info)

        return {
            "children": extracted_children,
            "relevance": relevance
        }

    def _find_children_by_name(self, query: str) -> List[Dict[str, Any]]:
        """
        クエリから子供の名前を検出して該当する子供を返す
        別名（aliases）もチェックする

        Args:
            query: ユーザーの質問

        Returns:
            該当する子供のリスト
        """
        matched = []
        for child in self.children:
            # 正式名でチェック
            name = child.get("name", "")
            if name and name in query:
                matched.append(child)
                continue

            # 別名（aliases）でもチェック
            aliases = child.get("aliases", [])
            if aliases:
                for alias in aliases:
                    if alias and alias in query:
                        matched.append(child)
                        break  # 一度マッチしたら次の子供へ

        return matched

    def _find_children_by_grade_or_class(self, query: str) -> List[Dict[str, Any]]:
        """
        クエリから学年やクラスを検出して該当する子供を返す

        Args:
            query: ユーザーの質問

        Returns:
            該当する子供のリスト
        """
        matched = []

        # 学年の検出（例: 「5年」「3年生」）
        grade_match = re.search(r'(\d+)年', query)
        if grade_match:
            grade = int(grade_match.group(1))
            for child in self.children:
                if child.get("grade") == grade:
                    matched.append(child)
            if matched:
                return matched

        # クラスの検出（例: 「1組」「2組」）
        class_match = re.search(r'(\d+)組', query)
        if class_match:
            class_num = class_match.group(1) + "組"
            for child in self.children:
                school = child.get("school", {})
                class_name = school.get("class", "")
                if class_num in class_name:
                    matched.append(child)

        return matched

    def _is_general_school_query(self, query: str) -> bool:
        """
        学校や塾に関する一般的な質問かどうかを判定

        Args:
            query: ユーザーの質問

        Returns:
            一般的な質問ならTrue
        """
        # 一般的な学校関連キーワード
        school_keywords = [
            "学校", "授業", "時間割", "科目", "宿題",
            "塾", "クラス", "先生", "教室", "学年通信"
        ]

        # 時間に関するキーワード（「今日の」「明日の」など）
        time_keywords = ["今日", "明日", "今週", "来週", "月曜", "火曜", "水曜", "木曜", "金曜"]

        # いずれかのキーワードが含まれているか
        for keyword in school_keywords + time_keywords:
            if keyword in query:
                return True

        return False

    def build_search_context_string(self, extracted_context: Dict[str, Any]) -> str:
        """
        検索用の軽量なコンテキスト文字列を構築（名前、学年、クラスのみ）

        Args:
            extracted_context: extract_relevant_contextで抽出されたコンテキスト

        Returns:
            検索用のコンテキスト文字列
        """
        if not extracted_context.get("children"):
            return ""

        parts = []
        for child in extracted_context["children"]:
            name = child.get("name", "")
            grade = child.get("grade", "")
            school = child.get("school", {})
            class_name = school.get("class", "") if school else ""

            if name:
                child_info = f"{name}"
                if grade:
                    child_info += f"（{grade}年生）"
                if class_name:
                    child_info += f" {class_name}"
                parts.append(child_info)

        if parts:
            return "関連: " + ", ".join(parts)
        return ""

    def build_answer_context_string(self, extracted_context: Dict[str, Any]) -> str:
        """
        回答生成用の詳細なコンテキスト文字列を構築（スケジュール含む）

        Args:
            extracted_context: extract_relevant_contextで抽出されたコンテキスト

        Returns:
            回答生成用のコンテキスト文字列
        """
        if not extracted_context.get("children"):
            return ""

        lines = ["【ユーザーの前提情報】"]

        for child in extracted_context["children"]:
            name = child.get("name", "不明")
            birth_date = child.get("birth_date", "不明")
            grade = child.get("grade", "不明")

            lines.append(f"\n■ {name} さん（{grade}年生、生年月日: {birth_date}）")

            # 学校情報
            school = child.get("school", {})
            if school:
                school_name = school.get("name", "不明")
                class_name = school.get("class", "不明")
                lines.append(f"  - 学校: {school_name} {class_name}")

                # 学校の時間割
                schedule = school.get("schedule", {})
                if schedule:
                    lines.append("  - 学校の授業:")
                    for day, periods in schedule.items():
                        subjects = [p.get("subject", "") for p in periods if isinstance(p, dict)]
                        if subjects:
                            lines.append(f"    {day}: {', '.join(subjects)}")

            # 塾情報
            cram_school = child.get("cram_school", {})
            if cram_school:
                cram_name = cram_school.get("name", "不明")
                lines.append(f"  - 塾: {cram_name}")

                # 塾のスケジュール
                schedule = cram_school.get("schedule", {})
                if schedule:
                    lines.append("  - 塾の授業:")
                    for day, sessions in schedule.items():
                        if sessions:
                            session_info = []
                            for s in sessions:
                                if isinstance(s, dict):
                                    time = s.get("time", "")
                                    subject = s.get("subject", "")
                                    session_info.append(f"{subject}({time})")
                            if session_info:
                                lines.append(f"    {day}: {', '.join(session_info)}")

        return "\n".join(lines)
```

### shared\common\utils\date_extractor.py

```py
"""
日付抽出ユーティリティ

本文からすべての日付パターンを抽出し、正規化します。
日付は検索において最重要項目であるため、漏れなく抽出することが重要です。
"""
import re
from typing import List, Set
from datetime import datetime, timedelta
from loguru import logger


class DateExtractor:
    """日付抽出クラス - 本文からあらゆる日付パターンを抽出"""

    def __init__(self):
        # 日付パターン（優先度順）
        self.patterns = [
            # YYYY-MM-DD, YYYY/MM/DD
            (r'\b(\d{4})[-/年](\d{1,2})[-/月](\d{1,2})日?\b', 'ymd'),

            # MM月DD日、M月D日
            (r'\b(\d{1,2})月(\d{1,2})日\b', 'md'),

            # MM/DD, M/D
            (r'\b(\d{1,2})/(\d{1,2})\b', 'md_slash'),

            # YYYY年MM月、YYYY年M月
            (r'\b(\d{4})年(\d{1,2})月\b', 'ym'),

            # MM月、M月
            (r'\b(\d{1,2})月\b', 'm'),

            # 相対表現（後で処理）
            (r'(明日|明後日|明々後日|今日|昨日|一昨日)', 'relative'),
            (r'(来週|再来週|先週|今週)', 'relative_week'),
            (r'(来月|再来月|先月|今月)', 'relative_month'),
        ]

    def extract_all_dates(
        self,
        text: str,
        reference_date: str = None
    ) -> List[str]:
        """
        本文からすべての日付を抽出

        Args:
            text: 抽出対象のテキスト
            reference_date: 基準日（YYYY-MM-DD形式、相対日付の計算に使用）

        Returns:
            日付のリスト（YYYY-MM-DD形式、重複なし、ソート済み）
        """
        if not text:
            return []

        dates: Set[str] = set()

        # 基準日を設定（指定がない場合は今日）
        if reference_date:
            try:
                base_date = datetime.strptime(reference_date, '%Y-%m-%d')
            except:
                base_date = datetime.now()
        else:
            base_date = datetime.now()

        current_year = base_date.year
        current_month = base_date.month

        # 各パターンで抽出
        for pattern, date_type in self.patterns:
            matches = re.finditer(pattern, text)

            for match in matches:
                try:
                    if date_type == 'ymd':
                        # YYYY-MM-DD
                        year = int(match.group(1))
                        month = int(match.group(2))
                        day = int(match.group(3))
                        date_str = f"{year:04d}-{month:02d}-{day:02d}"
                        dates.add(date_str)

                    elif date_type == 'md':
                        # MM月DD日 → 今年として処理
                        month = int(match.group(1))
                        day = int(match.group(2))

                        # 月日が今日より前なら来年の可能性を考慮
                        date_str = f"{current_year:04d}-{month:02d}-{day:02d}"
                        try:
                            parsed = datetime.strptime(date_str, '%Y-%m-%d')
                            dates.add(date_str)

                            # 過去の日付の場合、来年も候補に
                            if parsed < base_date - timedelta(days=30):
                                next_year_date = f"{current_year + 1:04d}-{month:02d}-{day:02d}"
                                dates.add(next_year_date)
                        except:
                            pass

                    elif date_type == 'md_slash':
                        # MM/DD → 今年として処理
                        month = int(match.group(1))
                        day = int(match.group(2))

                        # MM/DDは月/日の可能性が高い（日本語文脈）
                        if month <= 12 and day <= 31:
                            date_str = f"{current_year:04d}-{month:02d}-{day:02d}"
                            try:
                                parsed = datetime.strptime(date_str, '%Y-%m-%d')
                                dates.add(date_str)

                                # 過去の日付の場合、来年も候補に
                                if parsed < base_date - timedelta(days=30):
                                    next_year_date = f"{current_year + 1:04d}-{month:02d}-{day:02d}"
                                    dates.add(next_year_date)
                            except:
                                pass

                    elif date_type == 'ym':
                        # YYYY年MM月 → その月の1日
                        year = int(match.group(1))
                        month = int(match.group(2))
                        date_str = f"{year:04d}-{month:02d}-01"
                        dates.add(date_str)

                    elif date_type == 'm':
                        # MM月 → 今年のMM月1日
                        month = int(match.group(1))
                        date_str = f"{current_year:04d}-{month:02d}-01"
                        dates.add(date_str)

                    elif date_type == 'relative':
                        # 相対日付
                        relative_text = match.group(1)
                        relative_dates = self._parse_relative_date(relative_text, base_date)
                        dates.update(relative_dates)

                    elif date_type == 'relative_week':
                        # 週単位の相対日付
                        relative_text = match.group(1)
                        relative_dates = self._parse_relative_week(relative_text, base_date)
                        dates.update(relative_dates)

                    elif date_type == 'relative_month':
                        # 月単位の相対日付
                        relative_text = match.group(1)
                        relative_dates = self._parse_relative_month(relative_text, base_date)
                        dates.update(relative_dates)

                except Exception as e:
                    logger.debug(f"日付パースエラー: {match.group(0)} - {e}")
                    continue

        # リストに変換してソート
        date_list = sorted(list(dates))

        logger.info(f"[日付抽出] {len(date_list)}件の日付を抽出: {date_list[:5]}...")

        return date_list

    def _parse_relative_date(self, text: str, base_date: datetime) -> List[str]:
        """相対日付表現を絶対日付に変換"""
        dates = []

        mapping = {
            '今日': 0,
            '明日': 1,
            '明後日': 2,
            '明々後日': 3,
            '昨日': -1,
            '一昨日': -2,
        }

        if text in mapping:
            delta_days = mapping[text]
            target_date = base_date + timedelta(days=delta_days)
            dates.append(target_date.strftime('%Y-%m-%d'))

        return dates

    def _parse_relative_week(self, text: str, base_date: datetime) -> List[str]:
        """週単位の相対日付を絶対日付に変換"""
        dates = []

        mapping = {
            '今週': 0,
            '来週': 7,
            '再来週': 14,
            '先週': -7,
        }

        if text in mapping:
            delta_days = mapping[text]
            # その週の月曜日を基準
            target_date = base_date + timedelta(days=delta_days)
            # 月曜日〜日曜日を追加
            weekday = target_date.weekday()
            monday = target_date - timedelta(days=weekday)

            for i in range(7):
                day = monday + timedelta(days=i)
                dates.append(day.strftime('%Y-%m-%d'))

        return dates

    def _parse_relative_month(self, text: str, base_date: datetime) -> List[str]:
        """月単位の相対日付を絶対日付に変換"""
        dates = []

        mapping = {
            '今月': 0,
            '来月': 1,
            '再来月': 2,
            '先月': -1,
        }

        if text in mapping:
            delta_months = mapping[text]
            year = base_date.year
            month = base_date.month + delta_months

            # 年をまたぐ場合の調整
            while month > 12:
                month -= 12
                year += 1
            while month < 1:
                month += 12
                year -= 1

            # その月の1日を追加
            dates.append(f"{year:04d}-{month:02d}-01")

        return dates
```

### shared\common\utils\html_screenshot.py

```py
"""
HTML to Screenshot Utility

PlaywrightでHTMLをスクリーンショット画像に変換（Async版）
"""
import base64
import re
import io
from pathlib import Path
from typing import Optional, Union
from playwright.async_api import async_playwright
from PIL import Image
from loguru import logger


class HTMLScreenshotGenerator:
    """HTMLをスクリーンショット画像に変換するクラス（Async対応）"""

    def __init__(self, viewport_width: int = 1200, viewport_height: int = 800):
        """
        初期化

        Args:
            viewport_width: ビューポート幅
            viewport_height: ビューポート高さ
        """
        self.viewport_width = viewport_width
        self.viewport_height = viewport_height

    async def html_to_screenshot(
        self,
        html_content: str,
        output_path: Optional[Union[str, Path]] = None,
        full_page: bool = True
    ) -> bytes:
        """
        HTMLコンテンツをスクリーンショット画像に変換

        Args:
            html_content: HTML文字列
            output_path: 保存先パス（Noneの場合は保存しない）
            full_page: フルページスクリーンショット（Trueの場合）

        Returns:
            スクリーンショット画像のバイナリデータ（PNG形式）
        """
        try:
            async with async_playwright() as p:
                # ブラウザを起動（ヘッドレスモード）
                browser = await p.chromium.launch(headless=True)

                # ページを作成
                page = await browser.new_page(
                    viewport={'width': self.viewport_width, 'height': self.viewport_height}
                )

                # ⚠️ 巨大画像対策：Base64埋め込み画像を実際にリサイズ
                html_content = self._resize_embedded_images(html_content)

                # HTMLコンテンツを設定
                await page.set_content(html_content, wait_until='networkidle')

                # スクリーンショット撮影
                screenshot_bytes = await page.screenshot(
                    full_page=full_page,
                    type='png'
                )

                # ファイルに保存（オプション）
                if output_path:
                    output_path = Path(output_path)
                    output_path.parent.mkdir(parents=True, exist_ok=True)
                    output_path.write_bytes(screenshot_bytes)
                    logger.info(f"スクリーンショット保存: {output_path}")

                await browser.close()
                return screenshot_bytes

        except Exception as e:
            logger.error(f"スクリーンショット生成エラー: {e}")
            raise

    def _resize_embedded_images(self, html_content: str, max_height: int = 800) -> str:
        """
        HTMLに埋め込まれたBase64画像を実際にリサイズ

        Args:
            html_content: HTML文字列
            max_height: 画像の最大高さ（px）

        Returns:
            リサイズ後の画像を含むHTML
        """
        def resize_base64_image(match):
            """Base64画像をリサイズして置き換え"""
            try:
                # data:image/png;base64,xxxxx の形式
                full_data_uri = match.group(0)
                mime_type = match.group(1)  # image/png など
                base64_data = match.group(2)

                # Base64デコード
                img_bytes = base64.b64decode(base64_data)
                img = Image.open(io.BytesIO(img_bytes))

                # リサイズが必要かチェック
                if img.height > max_height:
                    # アスペクト比を保ってリサイズ
                    ratio = max_height / img.height
                    new_width = int(img.width * ratio)
                    img_resized = img.resize((new_width, max_height), Image.Resampling.LANCZOS)

                    # Base64に再エンコード
                    buffer = io.BytesIO()
                    img_format = img.format or 'PNG'
                    img_resized.save(buffer, format=img_format)
                    resized_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')

                    logger.debug(f"画像リサイズ: {img.width}x{img.height} → {new_width}x{max_height}")

                    return f'data:{mime_type};base64,{resized_base64}'
                else:
                    # リサイズ不要
                    return full_data_uri

            except Exception as e:
                logger.warning(f"Base64画像リサイズ失敗: {e}")
                return match.group(0)  # 元のまま返す

        # data:image/xxx;base64,xxxxx パターンを検索してリサイズ
        pattern = r'data:(image/[^;]+);base64,([A-Za-z0-9+/=]+)'
        resized_html = re.sub(pattern, resize_base64_image, html_content)

        return resized_html
```

### shared\common\utils\hypothetical_questions.py

```py
"""
Hypothetical Questions（仮想質問生成）

文書保存時に「ユーザーが聞きそうな質問」を事前生成し、検索精度を向上させます。
"""
from typing import List, Dict, Any, Optional
from loguru import logger
import json


class HypotheticalQuestionGenerator:
    """
    LLMを使って文書チャンクから仮想質問を生成

    落とし穴対策:
    - AIが「嘘の質問」を作らないように強い制約を設定
    - 文書内に書かれている事実のみに基づいて質問を生成
    - 生成された質問は検索用としてのみ使用（回答ソースにしない）
    """

    def __init__(self, llm_client):
        """
        Args:
            llm_client: LLMClient インスタンス
        """
        self.llm_client = llm_client

    def generate_questions(
        self,
        chunk_text: str,
        num_questions: int = 3,
        document_metadata: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        チャンクテキストから仮想質問を生成

        Args:
            chunk_text: チャンクのテキスト
            num_questions: 生成する質問数（3-5推奨）
            document_metadata: 文書のメタデータ（コンテキスト用）

        Returns:
            質問のリスト:
            [
                {
                    "question_text": str
                },
                ...
            ]
        """
        if not chunk_text or len(chunk_text.strip()) < 50:
            logger.warning("チャンクが短すぎます。質問生成をスキップ。")
            return []

        prompt = self._build_question_generation_prompt(
            chunk_text=chunk_text,
            num_questions=num_questions,
            document_metadata=document_metadata
        )

        try:
            # LLMで質問を生成
            response = self.llm_client.call_model(
                tier="extraction",  # 軽量なモデルで十分
                prompt=prompt
            )

            if not response.get("success"):
                logger.error(f"[HypotheticalQ] LLM呼び出し失敗: {response.get('error')}")
                return []

            content = response.get("content", "")

            # JSONを抽出
            questions = self._extract_questions_from_response(content)

            if questions:
                logger.info(f"[HypotheticalQ] 質問生成成功: {len(questions)}件")
                return questions
            else:
                logger.warning("[HypotheticalQ] 質問抽出失敗")
                return []

        except Exception as e:
            logger.error(f"[HypotheticalQ] エラー: {e}", exc_info=True)
            return []

    def _build_question_generation_prompt(
        self,
        chunk_text: str,
        num_questions: int,
        document_metadata: Optional[Dict[str, Any]]
    ) -> str:
        """
        質問生成用のプロンプトを構築

        Args:
            chunk_text: チャンクのテキスト
            num_questions: 生成する質問数
            document_metadata: 文書のメタデータ

        Returns:
            LLMに送信するプロンプト
        """
        metadata_context = ""
        if document_metadata:
            doc_type = document_metadata.get("doc_type", "不明")
            file_name = document_metadata.get("file_name", "不明")
            metadata_context = f"\n文書タイプ: {doc_type}\nファイル名: {file_name}\n"

        prompt = f"""あなたは質問生成の専門家です。以下の文書の一部を読んで、ユーザーが聞きそうな質問を生成してください。

# 文書の一部
{metadata_context}
---
{chunk_text}
---

# タスク
上記の文書に対して、ユーザーが聞きそうな質問を{num_questions}個生成してください。

# ★★★ 重要な制約（落とし穴対策）★★★

1. **文書内に明確に書かれている事実のみに基づいて質問を作成すること**
   - 文書に書かれていない情報について質問を作らないでください
   - 推測や想像で質問を作らないでください
   - 「〇〇について書いてありますか？」のような質問で、〇〇が実際に書かれていない場合は作成しないでください

2. **具体的で検索しやすい質問を作成すること**
   - 曖昧な質問ではなく、具体的なキーワードを含む質問を作成
   - 例: 「予定は？」❌ → 「12月4日の予定は？」✅

3. **自然な日本語で質問を作成すること**
   - ユーザーが実際に入力しそうな自然な表現を使う
   - 例: 「2024年12月4日に予定されているイベントの詳細情報」❌
   - 例: 「12月4日の予定を教えて」✅

4. **質問の多様性を確保すること**
   - 同じような質問を繰り返さない
   - 異なる観点から質問を作成（日付、人名、場所、内容など）

# 出力形式

以下のJSON形式で出力してください（JSON以外の説明は不要）:

```json
[
  {{
    "question_text": "質問1のテキスト"
  }},
  {{
    "question_text": "質問2のテキスト"
  }},
  {{
    "question_text": "質問3のテキスト"
  }}
]
```

# 例

**文書の一部**:
「2024年12月4日（水）14:00-16:00 社内MTG 議題:Q4振り返り 参加者:営業部全員 場所:会議室A」

**良い質問の例（✅）**:
1. 「12月4日の社内MTGの議題は？」
2. 「Q4振り返りのMTGはいつ？」
3. 「営業部のMTGの場所は？」

**悪い質問の例（❌）**:
1. 「Q4の売上目標は？」← 文書に書かれていない❌
2. 「MTGの結果は？」← まだ開催されていない（文書は予定表）❌
3. 「会議室Aの収容人数は？」← 文書のテーマから逸れている❌

それでは、上記の文書に対する質問を{num_questions}個生成してください:
"""

        return prompt

    def _extract_questions_from_response(self, content: str) -> List[Dict[str, Any]]:
        """
        LLMのレスポンスから質問を抽出

        Args:
            content: LLMのレスポンス

        Returns:
            質問のリスト
        """
        import json_repair

        try:
            # マークダウンコードブロックを除去
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                parts = content.split("```")
                if len(parts) >= 3:
                    content = parts[1]

            # JSON部分のみを抽出
            start_idx = content.find('[')
            end_idx = content.rfind(']')

            if start_idx == -1 or end_idx == -1:
                logger.warning("[HypotheticalQ] JSON配列が見つかりません")
                return []

            json_str = content[start_idx:end_idx+1]

            # json_repair を使用して構文エラーを自動修復
            questions = json_repair.loads(json_str)

            if not isinstance(questions, list):
                logger.warning("[HypotheticalQ] JSONが配列ではありません")
                return []

            # バリデーションとクリーニング
            validated_questions = []
            for q in questions:
                if not isinstance(q, dict):
                    continue

                question_text = q.get("question_text", "").strip()

                # 質問テキストが空でないか、または短すぎないか
                if not question_text or len(question_text) < 5:
                    continue

                validated_questions.append({
                    "question_text": question_text
                })

            return validated_questions

        except Exception as e:
            logger.error(f"[HypotheticalQ] JSON抽出エラー: {e}")
            return []
```

### shared\common\utils\metadata_extractor.py

```py
"""
メタデータ抽出ユーティリティ

Stage Hで抽出されたメタデータから、フィルタリング用の構造化データを抽出します。
"""
from typing import Dict, Any, Optional, List
from datetime import datetime
import re
from loguru import logger


class MetadataExtractor:
    """メタデータから構造化データを抽出"""

    @staticmethod
    def extract_filtering_metadata(
        metadata: Dict[str, Any],
        document_date: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        メタデータからフィルタリング用の構造化データを抽出

        Args:
            metadata: Stage Hで抽出されたメタデータ
            document_date: 文書の日付（YYYY-MM-DD形式）

        Returns:
            フィルタリング用のメタデータ辞書:
            {
                "year": int,
                "month": int,
                "amount": float,
                "event_dates": List[str],
                "grade_level": str,
                "school_name": str
            }
        """
        result = {
            "year": None,
            "month": None,
            "amount": None,
            "event_dates": None,
            "grade_level": None,
            "school_name": None
        }

        # 年・月の抽出（document_dateから優先）
        if document_date:
            result["year"], result["month"] = MetadataExtractor._parse_date(document_date)
        elif metadata.get("basic_info", {}).get("issue_date"):
            # basic_info.issue_date から抽出
            result["year"], result["month"] = MetadataExtractor._parse_date(metadata["basic_info"]["issue_date"])

        # 金額の抽出（請求書、契約書など）
        if "amount" in metadata:
            result["amount"] = MetadataExtractor._parse_amount(metadata["amount"])
        elif metadata.get("basic_info", {}).get("amount"):
            result["amount"] = MetadataExtractor._parse_amount(metadata["basic_info"]["amount"])

        # イベント日付の抽出（学校行事など）
        event_dates = MetadataExtractor._extract_event_dates(metadata)
        if event_dates:
            result["event_dates"] = event_dates

        # 学年の抽出（学校関連文書）
        if metadata.get("basic_info", {}).get("grade"):
            result["grade_level"] = metadata["basic_info"]["grade"]

        # 学校名の抽出（学校関連文書）
        if metadata.get("basic_info", {}).get("school_name"):
            result["school_name"] = metadata["basic_info"]["school_name"]

        logger.debug(f"抽出されたフィルタリングメタデータ: {result}")

        return result

    @staticmethod
    def _parse_date(date_str: str) -> tuple[Optional[int], Optional[int]]:
        """
        日付文字列から年・月を抽出

        Args:
            date_str: 日付文字列（YYYY-MM-DD形式）

        Returns:
            (年, 月) のタプル
        """
        if not date_str:
            return None, None

        try:
            # YYYY-MM-DD 形式をパース
            if isinstance(date_str, str) and '-' in date_str:
                parts = date_str.split('-')
                if len(parts) >= 2:
                    year = int(parts[0])
                    month = int(parts[1])
                    return year, month
        except Exception as e:
            logger.warning(f"日付パースエラー: {date_str}, {e}")

        return None, None

    @staticmethod
    def _parse_amount(amount_value: Any) -> Optional[float]:
        """
        金額を数値に変換

        Args:
            amount_value: 金額（文字列または数値）

        Returns:
            金額（float）
        """
        if amount_value is None:
            return None

        try:
            # 既に数値の場合
            if isinstance(amount_value, (int, float)):
                return float(amount_value)

            # 文字列の場合、カンマや通貨記号を除去
            if isinstance(amount_value, str):
                # 数字とピリオド、マイナス記号のみを抽出
                cleaned = re.sub(r'[^0-9.\-]', '', amount_value)
                if cleaned:
                    return float(cleaned)

        except Exception as e:
            logger.warning(f"金額パースエラー: {amount_value}, {e}")

        return None

    @staticmethod
    def _extract_event_dates(metadata: Dict[str, Any]) -> Optional[List[str]]:
        """
        メタデータからイベント日付を抽出

        Args:
            metadata: メタデータ辞書

        Returns:
            イベント日付のリスト（YYYY-MM-DD形式）
        """
        event_dates = []

        # weekly_schedule から日付を抽出
        weekly_schedule = metadata.get("weekly_schedule", [])
        if isinstance(weekly_schedule, list):
            for day_item in weekly_schedule:
                if isinstance(day_item, dict) and "date" in day_item:
                    date_str = day_item["date"]
                    # 日付の正規化（YYYY-MM-DD形式に統一）
                    normalized_date = MetadataExtractor._normalize_date(date_str)
                    if normalized_date:
                        event_dates.append(normalized_date)

        # monthly_schedule_blocks から日付を抽出
        monthly_schedule = metadata.get("monthly_schedule_blocks", [])
        if isinstance(monthly_schedule, list):
            for item in monthly_schedule:
                if isinstance(item, dict) and "date" in item:
                    date_str = item["date"]
                    normalized_date = MetadataExtractor._normalize_date(date_str)
                    if normalized_date:
                        event_dates.append(normalized_date)

        # 重複を除去してソート
        if event_dates:
            event_dates = sorted(list(set(event_dates)))
            return event_dates

        return None

    @staticmethod
    def _normalize_date(date_str: str) -> Optional[str]:
        """
        日付を YYYY-MM-DD 形式に正規化

        Args:
            date_str: 日付文字列（様々な形式）

        Returns:
            正規化された日付（YYYY-MM-DD）
        """
        if not date_str or not isinstance(date_str, str):
            return None

        try:
            # 既に YYYY-MM-DD 形式の場合
            if re.match(r'^\d{4}-\d{2}-\d{2}$', date_str):
                return date_str

            # MM-DD 形式の場合、現在の年を補完
            if re.match(r'^\d{2}-\d{2}$', date_str):
                current_year = datetime.now().year
                return f"{current_year}-{date_str}"

            # その他の形式はパースを試みる
            # 例: "2024/12/04" → "2024-12-04"
            date_obj = datetime.strptime(date_str, "%Y/%m/%d")
            return date_obj.strftime("%Y-%m-%d")

        except Exception:
            pass

        return None
```

### shared\common\utils\query_expansion.py

```py
"""
クエリ拡張（Query Expansion）ユーティリティ

ユーザーの検索クエリをLLMで拡張し、同義語や関連語を含めることで
検索精度を向上させます。
"""
import re
from typing import Dict, Any, Optional
from loguru import logger
from shared.ai.llm_client.llm_client import LLMClient


class QueryExpander:
    """
    検索クエリを拡張するクラス

    目的：
    - ユーザーが「予定」と検索したときに、「行事」「イベント」「スケジュール」も検索対象に含める
    - ベクトル検索と全文検索の両方で効果を発揮
    """

    def __init__(self, llm_client: Optional[LLMClient] = None):
        """
        Args:
            llm_client: LLMクライアント（指定しない場合は新規作成）
        """
        self.llm_client = llm_client or LLMClient()

    def expand_query(self, query: str, max_keywords: int = 10) -> Dict[str, Any]:
        """
        検索クエリを拡張

        Args:
            query: 元のクエリ
            max_keywords: 拡張キーワードの最大数

        Returns:
            {
                "original_query": str,      # 元のクエリ
                "expanded_query": str,      # 拡張されたクエリ（スペース区切り）
                "keywords": List[str],      # 抽出されたキーワードリスト
                "expansion_applied": bool   # 拡張が適用されたかどうか
            }
        """
        if not query or len(query.strip()) < 2:
            logger.warning("[クエリ拡張] クエリが空または短すぎます")
            return {
                "original_query": query,
                "expanded_query": query,
                "keywords": [],
                "expansion_applied": False
            }

        # 拡張が必要かどうかを判定（短いクエリやキーワード検索には効果的）
        if not self._should_expand(query):
            logger.info(f"[クエリ拡張] スキップ: '{query}'")
            return {
                "original_query": query,
                "expanded_query": query,
                "keywords": [query],
                "expansion_applied": False
            }

        try:
            # LLMでクエリを拡張
            prompt = self._build_expansion_prompt(query, max_keywords)
            response = self.llm_client.call_model(
                tier="utility",  # 軽量なタスク
                prompt=prompt,
                model_name="gemini-2.5-flash"  # 高速モデル
            )

            if not response.get('success'):
                logger.warning(f"[クエリ拡張] LLM呼び出し失敗: {response.get('error')}")
                return self._fallback_expansion(query)

            expanded_text = response.get('content', '').strip()

            if not expanded_text:
                logger.warning("[クエリ拡張] 空の応答")
                return self._fallback_expansion(query)

            # キーワードを抽出（スペース区切り）
            keywords = [kw.strip() for kw in expanded_text.split() if kw.strip()]

            # 重複削除
            keywords = list(dict.fromkeys(keywords))

            # 最大数に制限
            if len(keywords) > max_keywords:
                keywords = keywords[:max_keywords]

            expanded_query = " ".join(keywords)

            logger.info(f"[クエリ拡張] 成功: '{query}' → '{expanded_query}'")

            return {
                "original_query": query,
                "expanded_query": expanded_query,
                "keywords": keywords,
                "expansion_applied": True
            }

        except Exception as e:
            logger.error(f"[クエリ拡張] エラー: {e}", exc_info=True)
            return self._fallback_expansion(query)

    def _should_expand(self, query: str) -> bool:
        """
        クエリ拡張が必要かどうか判定

        Args:
            query: 検索クエリ

        Returns:
            True: 拡張すべき
            False: 拡張不要
        """
        # 長すぎるクエリは拡張しない（すでに十分詳細）
        if len(query) > 100:
            return False

        # 固有名詞のみのクエリは拡張しない（例: 「学年通信（29）」）
        # カッコ付き固有名詞を含む場合は拡張しない
        if '（' in query or '(' in query:
            return False

        # 日付のみのクエリは拡張しない（例: 「12月4日」）
        date_pattern = r'^\d{1,2}月\d{1,2}日$|^\d{1,2}/\d{1,2}$'
        if re.match(date_pattern, query):
            return False

        # その他のケースは拡張する
        return True

    def _build_expansion_prompt(self, query: str, max_keywords: int) -> str:
        """
        クエリ拡張用のプロンプトを構築

        Args:
            query: 元のクエリ
            max_keywords: 最大キーワード数

        Returns:
            プロンプト文字列
        """
        prompt = f"""あなたは検索エンジンの専門家です。ユーザーの検索クエリを、検索システムが拾いやすいように関連語を含めて拡張してください。

【重要なルール】
1. 元のクエリの意図を保ちながら、同義語・関連語を追加する
2. 回答は拡張したキーワードのみをスペース区切りで返す（説明文は不要）
3. 最大{max_keywords}個のキーワードに収める
4. 日本語の検索なので、日本語でよく使われる表現を優先する

【拡張例】
入力: 12月の予定
出力: 12月 予定 行事 イベント スケジュール カレンダー 実施計画 日程

入力: 委員会の議事録
出力: 委員会 議事録 会議 決定事項 議題 協議 打ち合わせ

入力: 時間割
出力: 時間割 授業 科目 スケジュール クラス 予定表 カリキュラム

【ユーザー入力】
{query}

【拡張キーワード（スペース区切り）】
"""
        return prompt

    def _fallback_expansion(self, query: str) -> Dict[str, Any]:
        """
        LLMが使えない場合のフォールバック（ルールベース拡張）

        Args:
            query: 元のクエリ

        Returns:
            拡張結果
        """
        # ルールベースの簡易拡張
        expansion_rules = {
            "予定": ["予定", "スケジュール", "行事", "イベント", "日程", "カレンダー"],
            "議事録": ["議事録", "会議", "決定事項", "議題", "協議"],
            "時間割": ["時間割", "授業", "科目", "スケジュール", "クラス"],
            "連絡": ["連絡", "お知らせ", "通知", "案内"],
        }

        keywords = [query]  # 元のクエリを最初に含める

        # クエリに含まれるキーワードに対応する拡張語を追加
        for key, related_words in expansion_rules.items():
            if key in query:
                keywords.extend([w for w in related_words if w not in keywords])

        expanded_query = " ".join(keywords)

        logger.info(f"[クエリ拡張] フォールバック適用: '{query}' → '{expanded_query}'")

        return {
            "original_query": query,
            "expanded_query": expanded_query,
            "keywords": keywords,
            "expansion_applied": True
        }
```

### shared\common\utils\synthetic_chunks.py

```py
"""
合成チャンク生成ユーティリティ

構造化されたメタデータ（JSON）から検索専用の合成チャンクを生成します。
これにより、本文には詳しく書かれていないが重要な情報（スケジュール、議題等）を
検索結果の上位に押し上げることができます。
"""
from typing import Dict, Any, List, Optional
from loguru import logger


def create_schedule_synthetic_chunk(metadata: Dict[str, Any], file_name: str = "") -> Optional[str]:
    """
    monthly_schedule_blocks からスケジュール専用の合成チャンクを生成

    Args:
        metadata: ドキュメントのメタデータ（JSON）
        file_name: ファイル名（コンテキスト情報として追加）

    Returns:
        検索専用の合成テキスト、スケジュールがない場合は None
    """
    if not metadata:
        return None

    schedule_blocks = metadata.get('monthly_schedule_blocks', [])
    if not schedule_blocks:
        # フォールバック: weekly_schedule も確認
        weekly_schedule = metadata.get('weekly_schedule', [])
        if weekly_schedule:
            return create_weekly_schedule_synthetic_chunk(metadata, file_name)
        return None

    # スケジュール専用チャンクの構築
    lines = []
    lines.append(f"【{file_name} - 月間行事予定・スケジュール】\n")

    for block in schedule_blocks:
        date = block.get('date', '')
        event = block.get('event', '')
        notes = block.get('notes', '')

        if not event:
            continue

        # 検索に引っかかりやすい形式に整形
        # 日付と曜日情報を含める
        line_parts = []

        if date:
            # 日付をフォーマット（例: 2025-12-19 → 12月19日）
            try:
                if '-' in date:
                    parts = date.split('-')
                    if len(parts) == 3:
                        month = int(parts[1])
                        day = int(parts[2])
                        line_parts.append(f"{month}月{day}日")
                    else:
                        line_parts.append(date)
                else:
                    line_parts.append(date)
            except:
                line_parts.append(date)

        # イベント名
        line_parts.append(event)

        # メモ・詳細情報
        if notes:
            line_parts.append(f"({notes})")

        line = " ".join(line_parts)
        lines.append(f"- {line}")

    if len(lines) <= 1:  # ヘッダーのみの場合はスケジュールなし
        return None

    # 検索用キーワードを追加（検索精度向上のため）
    lines.append("\n【キーワード】")
    lines.append("行事 予定 スケジュール イベント 月間予定 日程 カレンダー")

    synthetic_text = "\n".join(lines)
    logger.debug(f"[合成チャンク] monthly_schedule_blocks から {len(schedule_blocks)}件のスケジュールを抽出")

    return synthetic_text


def create_weekly_schedule_synthetic_chunk(metadata: Dict[str, Any], file_name: str = "") -> Optional[str]:
    """
    weekly_schedule から週間スケジュール専用の合成チャンクを生成

    Args:
        metadata: ドキュメントのメタデータ（JSON）
        file_name: ファイル名（コンテキスト情報として追加）

    Returns:
        検索専用の合成テキスト、スケジュールがない場合は None
    """
    if not metadata:
        return None

    weekly_schedule = metadata.get('weekly_schedule', [])
    if not weekly_schedule:
        return None

    # 週間スケジュール専用チャンクの構築
    lines = []
    lines.append(f"【{file_name} - 週間スケジュール】\n")

    for day_item in weekly_schedule:
        date = day_item.get('date', '')
        day = day_item.get('day', '')
        day_of_week = day_item.get('day_of_week', '')
        events = day_item.get('events', [])
        note = day_item.get('note', '')
        class_schedules = day_item.get('class_schedules', [])

        # 日付ヘッダー
        header_parts = []
        if date:
            try:
                if '-' in date:
                    parts = date.split('-')
                    if len(parts) == 3:
                        month = int(parts[1])
                        day_num = int(parts[2])
                        header_parts.append(f"{month}月{day_num}日")
                    else:
                        header_parts.append(date)
                else:
                    header_parts.append(date)
            except:
                header_parts.append(date)

        if day:
            header_parts.append(f"{day}曜日")
        elif day_of_week:
            header_parts.append(day_of_week)

        if header_parts:
            lines.append(f"\n■ {' '.join(header_parts)}")

        # イベント
        if events:
            for event in events:
                if event:
                    lines.append(f"  - {event}")

        # クラススケジュール
        if class_schedules:
            for class_schedule in class_schedules:
                class_name = class_schedule.get('class', '')
                subjects = class_schedule.get('subjects', [])
                periods = class_schedule.get('periods', [])

                if class_name:
                    lines.append(f"  【{class_name}】")

                if subjects:
                    lines.append(f"    科目: {', '.join(subjects)}")

                if periods:
                    for period in periods:
                        subject = period.get('subject', '')
                        time = period.get('time', '')
                        if subject:
                            period_line = f"    - {subject}"
                            if time:
                                period_line += f" ({time})"
                            lines.append(period_line)

        # ノート
        if note:
            lines.append(f"  ※ {note}")

    if len(lines) <= 1:  # ヘッダーのみの場合はスケジュールなし
        return None

    # 検索用キーワードを追加
    lines.append("\n【キーワード】")
    lines.append("週間予定 スケジュール 時間割 授業 クラス 科目 行事")

    synthetic_text = "\n".join(lines)
    logger.debug(f"[合成チャンク] weekly_schedule から {len(weekly_schedule)}日分のスケジュールを抽出")

    return synthetic_text


def create_agenda_synthetic_chunk(metadata: Dict[str, Any], file_name: str = "") -> Optional[str]:
    """
    議事録の議題情報から合成チャンクを生成

    Args:
        metadata: ドキュメントのメタデータ（JSON）
        file_name: ファイル名

    Returns:
        検索専用の合成テキスト、議題がない場合は None
    """
    if not metadata:
        return None

    # tables フィールド内の meeting_minutes を探す
    tables = metadata.get('tables', [])
    agenda_groups = None

    for table in tables:
        if table.get('table_type') == 'meeting_minutes':
            agenda_groups = table.get('agenda_groups', [])
            break

    if not agenda_groups:
        return None

    # 議題専用チャンクの構築
    lines = []
    lines.append(f"【{file_name} - 会議議事録・決定事項】\n")

    for group in agenda_groups:
        topic = group.get('topic', '')
        items = group.get('items', [])

        if topic:
            lines.append(f"\n■ {topic}")

        for item in items:
            decision = item.get('decision', '')
            assignee = item.get('assignee', '')
            deadline = item.get('deadline', '')

            if not decision:
                continue

            item_parts = [f"  - {decision}"]

            if assignee:
                item_parts.append(f"（担当: {assignee}）")
            if deadline:
                item_parts.append(f"（期限: {deadline}）")

            lines.append(" ".join(item_parts))

    if len(lines) <= 1:
        return None

    # 検索用キーワードを追加
    lines.append("\n【キーワード】")
    lines.append("議事録 会議 決定事項 議題 担当 期限 アクション")

    synthetic_text = "\n".join(lines)
    logger.debug(f"[合成チャンク] 議事録から {len(agenda_groups)}件の議題グループを抽出")

    return synthetic_text


def create_table_synthetic_chunk(metadata: Dict[str, Any], file_name: str = "") -> Optional[str]:
    """
    表データから合成チャンクを生成（汎用）

    Args:
        metadata: ドキュメントのメタデータ（JSON）
        file_name: ファイル名

    Returns:
        検索専用の合成テキスト、表がない場合は None
    """
    if not metadata:
        return None

    tables = metadata.get('tables', [])
    if not tables:
        return None

    lines = []
    lines.append(f"【{file_name} - 表データ】\n")

    for idx, table in enumerate(tables, 1):
        table_type = table.get('table_type', 'table')
        headers = table.get('headers', [])
        rows = table.get('rows', [])

        lines.append(f"\n■ 表{idx} ({table_type})")

        # ヘッダー
        if isinstance(headers, list) and headers:
            lines.append(f"  項目: {', '.join(str(h) for h in headers)}")
        elif isinstance(headers, dict):
            classes = headers.get('classes', [])
            if classes:
                lines.append(f"  クラス: {', '.join(str(c) for c in classes)}")

        # 行データ
        if rows:
            lines.append(f"  データ行数: {len(rows)}件")

            # 最初の数行をサンプルとして追加（検索用）
            for row in rows[:3]:  # 最大3行
                if isinstance(row, dict):
                    if 'cells' in row:
                        cells = row['cells']
                        cell_values = []
                        for cell in cells:
                            if isinstance(cell, dict):
                                cell_values.append(str(cell.get('value', '')))
                            else:
                                cell_values.append(str(cell))
                        if cell_values:
                            lines.append(f"    {' | '.join(cell_values)}")
                    else:
                        values = [str(v) for v in row.values()]
                        if values:
                            lines.append(f"    {' | '.join(values)}")

    if len(lines) <= 1:
        return None

    # 検索用キーワードを追加
    lines.append("\n【キーワード】")
    lines.append("表 テーブル データ 一覧 リスト")

    synthetic_text = "\n".join(lines)
    logger.debug(f"[合成チャンク] {len(tables)}件の表データを抽出")

    return synthetic_text


def create_all_synthetic_chunks(metadata: Dict[str, Any], file_name: str = "") -> List[Dict[str, str]]:
    """
    メタデータから全ての合成チャンクを生成

    Args:
        metadata: ドキュメントのメタデータ（JSON）
        file_name: ファイル名

    Returns:
        合成チャンクのリスト、各要素は {"type": str, "content": str} の形式
    """
    synthetic_chunks = []

    # 1. スケジュール専用チャンク
    schedule_chunk = create_schedule_synthetic_chunk(metadata, file_name)
    if schedule_chunk:
        synthetic_chunks.append({
            "type": "monthly_schedule",
            "content": schedule_chunk
        })
        logger.info(f"[合成チャンク] 月間スケジュールチャンク生成: {len(schedule_chunk)}文字")

    # 2. 週間スケジュールチャンク（monthly_schedule_blocksがない場合）
    if not schedule_chunk:
        weekly_chunk = create_weekly_schedule_synthetic_chunk(metadata, file_name)
        if weekly_chunk:
            synthetic_chunks.append({
                "type": "weekly_schedule",
                "content": weekly_chunk
            })
            logger.info(f"[合成チャンク] 週間スケジュールチャンク生成: {len(weekly_chunk)}文字")

    # 3. 議事録専用チャンク
    agenda_chunk = create_agenda_synthetic_chunk(metadata, file_name)
    if agenda_chunk:
        synthetic_chunks.append({
            "type": "meeting_agenda",
            "content": agenda_chunk
        })
        logger.info(f"[合成チャンク] 議事録チャンク生成: {len(agenda_chunk)}文字")

    # 4. 汎用表データチャンク（他の専用チャンクがない場合のフォールバック）
    if not synthetic_chunks:
        table_chunk = create_table_synthetic_chunk(metadata, file_name)
        if table_chunk:
            synthetic_chunks.append({
                "type": "table_data",
                "content": table_chunk
            })
            logger.info(f"[合成チャンク] 表データチャンク生成: {len(table_chunk)}文字")

    return synthetic_chunks
```

### shared\kakeibo\__init__.py

```py
"""
家計簿自動化システム (K_kakeibo)

ScanSnap → Google Drive → Gemini OCR → Supabase の自動化処理
"""

__version__ = "1.0.0"
```

### shared\kakeibo\config.py

```py
"""
設定ファイル
環境変数から各種APIキー・設定を読み込み
"""

import os
from pathlib import Path
from dotenv import load_dotenv

# .envファイル読み込み（ローカル環境用）
load_dotenv()

# Streamlit Cloud環境の場合はSecretsから読み込む
try:
    import streamlit as st
    # st.secretsファイルが存在するかチェック
    try:
        if "KAKEIBO_INBOX_EASY_FOLDER_ID" in st.secrets:
            os.environ["KAKEIBO_INBOX_EASY_FOLDER_ID"] = st.secrets["KAKEIBO_INBOX_EASY_FOLDER_ID"]
        if "KAKEIBO_INBOX_HARD_FOLDER_ID" in st.secrets:
            os.environ["KAKEIBO_INBOX_HARD_FOLDER_ID"] = st.secrets["KAKEIBO_INBOX_HARD_FOLDER_ID"]
        # Supabase設定もStreamlit Secretsから読み込む
        if "SUPABASE_URL" in st.secrets:
            os.environ["SUPABASE_URL"] = st.secrets["SUPABASE_URL"]
        if "SUPABASE_SERVICE_ROLE_KEY" in st.secrets:
            os.environ["SUPABASE_SERVICE_ROLE_KEY"] = st.secrets["SUPABASE_SERVICE_ROLE_KEY"]
        if "SUPABASE_KEY" in st.secrets:
            os.environ["SUPABASE_KEY"] = st.secrets["SUPABASE_KEY"]
    except FileNotFoundError:
        # secrets.tomlファイルが存在しない（ローカル環境など）
        pass
except ImportError:
    # streamlitがない環境（CLIスクリプト実行時など）ではスキップ
    pass

# ========================================
# Google Drive 設定
# ========================================
GOOGLE_DRIVE_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS", "service_account.json")

# 2つのInboxフォルダ（難易度別）
INBOX_EASY_FOLDER_ID = os.getenv("KAKEIBO_INBOX_EASY_FOLDER_ID")  # 00_Inbox_Easy (読みやすいレシート)
INBOX_HARD_FOLDER_ID = os.getenv("KAKEIBO_INBOX_HARD_FOLDER_ID")  # 00_Inbox_Hard (読みづらいレシート)

ARCHIVE_FOLDER_ID = os.getenv("KAKEIBO_ARCHIVE_FOLDER_ID")  # 99_Archive のフォルダID
ERROR_FOLDER_ID = os.getenv("KAKEIBO_ERROR_FOLDER_ID")  # errors フォルダID

# ========================================
# Gemini API 設定
# ========================================
GEMINI_API_KEY = ***REDACTED***"GOOGLE_AI_API_KEY")  # 既存の.envに合わせる

# 2つのモデルを使い分け
GEMINI_MODEL_EASY = "gemini-2.5-flash-lite"  # 読みやすいレシート用（低コスト・高速）
GEMINI_MODEL_HARD = "gemini-2.5-flash"       # 読みづらいレシート用（高精度）

GEMINI_TEMPERATURE = 0.1  # 低めに設定（精度重視）

# フォルダとモデルのマッピング
FOLDER_MODEL_MAP = {
    "INBOX_EASY": {
        "folder_id": INBOX_EASY_FOLDER_ID,
        "model": GEMINI_MODEL_EASY,
        "description": "読みやすいレシート（きれい、シンプル）"
    },
    "INBOX_HARD": {
        "folder_id": INBOX_HARD_FOLDER_ID,
        "model": GEMINI_MODEL_HARD,
        "description": "読みづらいレシート（かすれ、複雑、手書き）"
    }
}

# ========================================
# Supabase 設定
# ========================================
SUPABASE_URL = os.getenv("SUPABASE_URL")
# service_role キー推奨（anonキーでも動作するが権限制限あり）
SUPABASE_KEY = ***REDACTED***"SUPABASE_SERVICE_ROLE_KEY") or os.getenv("SUPABASE_KEY")

# ========================================
# 処理設定
# ========================================
TEMP_DIR = Path("K_kakeibo/temp")  # 一時ダウンロードフォルダ
TEMP_DIR.mkdir(parents=True, exist_ok=True)

MAX_RETRY = 3  # Gemini API リトライ回数
POLL_INTERVAL = 300  # 監視間隔（秒） = 5分

# ========================================
# Gemini プロンプトテンプレート
# ========================================
GEMINI_PROMPT = """
このレシート画像からすべてのテキスト情報を抽出し、JSON形式で返してください。

【最重要原則】
このタスクの目的は「目の代わり」として機能することです。
人間がレシートを見て読み取れるすべての情報を、一字一句漏らさず抽出してください。

【抽出ルール】
1. **すべての文字・数字・記号を抽出**
   - 店名、住所、電話番号、営業時間
   - すべての商品名（略語もそのまま）
   - すべての価格、数量、金額
   - 小計、消費税、合計
   - レジ番号、取引番号、店舗コード
   - レシート番号、会員番号
   - 日付、時刻
   - バーコード番号
   - その他、印字されているすべてのテキスト

2. **レイアウト・位置関係を保持**
   - 上から順に読み取る
   - 左右の配置（左寄せ、右寄せ）も記録
   - 行の区切りを保持

3. **数値は必ずレシート記載のまま抽出**
   - 推測・計算は一切しない
   - 記載されている通りの数値を抽出
   - 8%対象額、8%税額、10%対象額、10%税額が記載されていれば必ず抽出

4. **税率の扱い**
   - レシートに税率の記載（※、★、8%、10%などのマーク）があればそのまま抽出
   - 記載がない場合は「税率記載なし」と明記

【出力JSON形式】
{
  "raw_text": "レシート全体のテキスト（改行・スペースを含む）",
  "shop_info": {
    "name": "店舗名（レシート記載のまま）",
    "address": "住所（記載されていれば）",
    "phone": "電話番号（記載されていれば）",
    "store_code": "店舗コード（記載されていれば）"
  },
  "transaction_info": {
    "date": "YYYY-MM-DD",
    "time": "HH:MM:SS（記載されていれば）",
    "register_number": "レジ番号（記載されていれば）",
    "receipt_number": "レシート番号（記載されていれば）",
    "transaction_number": "取引番号（記載されていれば）"
  },
  "items": [
    {
      "line_text": "この行のテキストそのまま",
      "product_name": "商品名（レシート記載のまま）",
      "quantity": 1,
      "unit_price": 100,
      "amount": 100,
      "tax_mark": "※または★またはなし",
      "tax_rate": "8または10またはnull"
    }
  ],
  "amounts": {
    "subtotal": "小計（記載されていれば）",
    "tax_8_base": "8%対象額（税抜）",
    "tax_8_amount": "8%消費税額",
    "tax_10_base": "10%対象額（税抜）",
    "tax_10_amount": "10%消費税額",
    "total_tax": "消費税合計",
    "total": "合計（支払額）",
    "received": "お預かり（記載されていれば）",
    "change": "お釣り（記載されていれば）"
  },
  "payment": {
    "method": "現金/カード/電子マネー等",
    "card_info": "カード情報（記載されていれば）"
  },
  "other_info": {
    "barcode": "バーコード番号（記載されていれば）",
    "points": "ポイント情報（記載されていれば）",
    "campaign": "キャンペーン情報（記載されていれば）",
    "notes": "その他の情報"
  }
}

【重要な注意】
- 「記載されていない」項目は null を設定してください
- 推測や計算は一切行わないでください
- レシート画像が不鮮明で読み取れない部分があれば、その旨を notes に記載してください
- 複数のレシートが写っている場合は error: "multiple_receipts" を返してください
- レシートではない画像の場合は error: "not_a_receipt" を返してください

エラーの場合:
{
  "error": "multiple_receipts" または "not_a_receipt" または "unreadable",
  "message": "エラーの詳細"
}
"""

# ========================================
# バリデーション
# ========================================
def validate_config():
    """必須設定のチェック"""
    required_vars = {
        "KAKEIBO_INBOX_EASY_FOLDER_ID": INBOX_EASY_FOLDER_ID,
        "KAKEIBO_INBOX_HARD_FOLDER_ID": INBOX_HARD_FOLDER_ID,
        "GOOGLE_AI_API_KEY": GEMINI_API_KEY,
        "SUPABASE_URL": SUPABASE_URL,
        "SUPABASE_KEY": SUPABASE_KEY,
    }

    missing = [k for k, v in required_vars.items() if not v]

    if missing:
        raise ValueError(
            f"Missing required environment variables: {', '.join(missing)}\n"
            "Please set them in .env file."
        )

if __name__ == "__main__":
    validate_config()
    print("Configuration is valid!")
```

### shared\kakeibo\kakeibo_db_handler.py

```py
"""
家計簿データのDB保存ハンドラー

Stage Hで処理された家計簿データを2層構造のDBに保存する
- Rawdata_RECEIPT_shops（親）
- Rawdata_RECEIPT_items（子）← OCR生データ + 標準化データ統合
- 99_lg_image_proc_log（処理ログ）
"""

from typing import Dict, List
from loguru import logger
from datetime import datetime

from shared.common.database.client import DatabaseClient


class KakeiboDBHandler:
    """家計簿データのDB保存専用ハンドラー"""

    def __init__(self, db_client: DatabaseClient):
        """
        Args:
            db_client: データベースクライアント
        """
        self.db = db_client

    def save_receipt(
        self,
        stage_h_output: Dict,
        file_name: str,
        drive_file_id: str,
        model_name: str,
        source_folder: str
    ) -> Dict:
        """
        レシートデータを3層構造でDBに保存

        Args:
            stage_h_output: Stage Hの出力
            file_name: ファイル名
            drive_file_id: Google DriveのファイルID
            model_name: 使用したモデル名
            source_folder: ソースフォルダ

        Returns:
            Dict: {"receipt_id": "...", "transaction_ids": [...], ...}
        """
        try:
            # 1. レシート（親）を登録
            receipt_id = self._insert_receipt(
                stage_h_output["receipt"],
                file_name,
                drive_file_id,
                model_name,
                source_folder
            )

            # 2. トランザクション（OCRデータ + 標準化データを統合して登録）
            transaction_ids = []

            for line_num, item_data in enumerate(stage_h_output["items"], start=1):
                item = item_data["raw_item"]
                normalized = item_data["normalized"]

                # displayed_amount は normalized から取得（レシート記載の金額）
                displayed_amount = normalized.get("displayed_amount") or item.get("amount") or 0

                # 値引き行でない場合は標準化データも含める
                is_discount = item.get("line_type") == "DISCOUNT"

                # 商品名を取得（nullや空文字列の場合は代替値を使用）
                product_name = normalized.get("product_name") or item.get("product_name") or item.get("line_text") or "不明"
                if not product_name or not product_name.strip():
                    product_name = "不明"

                transaction_id = self._insert_transaction(
                    receipt_id=receipt_id,
                    line_number=line_num,
                    line_type=item.get("line_type", "ITEM"),
                    ocr_raw_text=item.get("line_text", ""),
                    product_name=product_name,
                    quantity=normalized.get("quantity", item.get("quantity", 1)),
                    unit_price=item.get("unit_price"),
                    displayed_amount=displayed_amount,
                    discount_text=item.get("discount_text"),
                    base_price=normalized.get("base_price"),
                    tax_amount=normalized.get("tax_amount"),
                    tax_included_amount=normalized.get("tax_included_amount"),
                    tax_display_type=normalized.get("tax_display_type"),
                    tax_rate=normalized.get("tax_rate"),
                    # 標準化データ（値引き行以外）
                    normalized=normalized if not is_discount else None,
                    situation_id=stage_h_output["receipt"]["situation_id"] if not is_discount else None
                )
                transaction_ids.append(transaction_id)

            # 3. 処理ログを記録
            log_id = self._log_processing_success(
                file_name=file_name,
                drive_file_id=drive_file_id,
                receipt_id=receipt_id,
                transaction_ids=transaction_ids,
                model_name=model_name
            )

            logger.info(f"Saved receipt {receipt_id} with {len(transaction_ids)} items")

            return {
                "success": True,
                "receipt_id": receipt_id,
                "transaction_ids": transaction_ids,
                "log_id": log_id
            }

        except Exception as e:
            logger.error(f"Failed to save receipt: {e}")
            # エラーログを記録（エラー時は失敗しても続行）
            try:
                self._log_processing_error(
                    file_name=file_name,
                    drive_file_id=drive_file_id,
                    error_info={"error": str(e)},
                    model_name=model_name
                )
            except Exception as log_error:
                logger.warning(f"Failed to log error (ignoring): {log_error}")
            raise

    def _insert_receipt(
        self,
        receipt_data: Dict,
        file_name: str,
        drive_file_id: str,
        model_name: str,
        source_folder: str
    ) -> str:
        """レシート情報をDBに登録（親テーブル）"""
        trans_date = datetime.strptime(receipt_data["date"], "%Y-%m-%d").date()

        data = {
            "transaction_date": receipt_data["date"],
            "shop_name": receipt_data["name"],
            "total_amount_check": receipt_data.get("total") or 0,
            "subtotal_amount": receipt_data.get("subtotal") or 0,
            "tax_8_amount": receipt_data.get("tax_8_amount"),  # 8%消費税額
            "tax_10_amount": receipt_data.get("tax_10_amount"),  # 10%消費税額
            "tax_8_subtotal": receipt_data.get("tax_8_subtotal"),  # 8%対象額（税抜）
            "tax_10_subtotal": receipt_data.get("tax_10_subtotal"),  # 10%対象額（税抜）
            "image_path": f"99_Archive/{trans_date.strftime('%Y-%m')}/{file_name}",
            "drive_file_id": drive_file_id,
            "source_folder": source_folder,
            "ocr_model": model_name,
            "workspace": "household",
            "is_verified": False
        }

        result = self.db.client.table("Rawdata_RECEIPT_shops").insert(data).execute()
        return result.data[0]["id"]

    def _insert_transaction(
        self,
        receipt_id: str,
        line_number: int,
        line_type: str,
        ocr_raw_text: str,
        product_name: str,
        quantity: int,
        unit_price: int,
        displayed_amount: int,
        discount_text: str = None,
        base_price: int = None,
        tax_amount: int = None,
        tax_included_amount: int = None,
        tax_display_type: str = None,
        tax_rate: int = None,
        normalized: Dict = None,
        situation_id: str = None
    ) -> str:
        """トランザクション情報 + 標準化データをDBに登録（統合テーブル）"""
        data = {
            "receipt_id": receipt_id,
            "line_number": line_number,
            "line_type": line_type,
            "ocr_raw_text": ocr_raw_text,
            "product_name": product_name,
            "item_name": product_name,  # product_nameと同じ値を設定
            "quantity": quantity,
            "unit_price": unit_price,
            "displayed_amount": displayed_amount,  # レシート記載の表示金額
            "discount_text": discount_text,
            "base_price": base_price,  # 本体価（税抜）
            "tax_amount": tax_amount,  # 税額
            "tax_included_amount": tax_included_amount,  # 税込価
            "tax_display_type": tax_display_type,  # 外税or内税
            "tax_rate": tax_rate  # 税率（8 or 10）
        }

        # 標準化データがあれば追加
        if normalized:
            data.update({
                "official_name": normalized["product_name"],
                "category_id": normalized.get("category_id"),
                "situation_id": situation_id,
                "std_unit_price": base_price,  # 税抜本体価格
                "std_amount": tax_included_amount,  # 税込額
                "needs_review": normalized.get("tax_rate_source") != "master"
            })

        result = self.db.client.table("Rawdata_RECEIPT_items").insert(data).execute()
        return result.data[0]["id"]

    def _log_processing_success(
        self,
        file_name: str,
        drive_file_id: str,
        receipt_id: str,
        transaction_ids: List[str],
        model_name: str = None
    ) -> str:
        """処理成功をログに記録"""
        log_data = {
            "file_name": file_name,
            "drive_file_id": drive_file_id,
            "receipt_id": receipt_id,
            "status": "success",
            "transaction_ids": transaction_ids
        }
        if model_name:
            log_data["ocr_model"] = model_name

        # file_name をユニークキーとして upsert
        result = self.db.client.table("99_lg_image_proc_log").upsert(
            log_data,
            on_conflict="file_name"
        ).execute()
        return result.data[0]["id"]

    def _log_processing_error(
        self,
        file_name: str,
        drive_file_id: str,
        error_info: Dict,
        model_name: str = None,
        receipt_id: str = None
    ):
        """処理エラーをログに記録"""
        log_data = {
            "file_name": file_name,
            "drive_file_id": drive_file_id,
            "status": "failed",
            "error_message": error_info.get("error")
        }
        if model_name:
            log_data["ocr_model"] = model_name
        if receipt_id:
            log_data["receipt_id"] = receipt_id

        # file_name をユニークキーとして upsert
        self.db.client.table("99_lg_image_proc_log").upsert(
            log_data,
            on_conflict="file_name"
        ).execute()
```

### shared\kakeibo\review_ui.py

```py
"""
家計簿レビューUI (Streamlit)

レシート単位でのレビュー：
- レシート画像プレビュー
- 商品一覧（表形式）
- 合計金額
- レシート単位での承認・編集
"""

import streamlit as st
import pandas as pd
from supabase import create_client
from datetime import datetime
from google.oauth2 import service_account
from googleapiclient.discovery import build
from PIL import Image
import io

# 設定
try:
    # Streamlit Cloud環境
    from shared.kakeibo.config import SUPABASE_URL, SUPABASE_KEY, GOOGLE_DRIVE_CREDENTIALS
except ImportError:
    # ローカル環境
    from config import SUPABASE_URL, SUPABASE_KEY, GOOGLE_DRIVE_CREDENTIALS

# Supabase接続
db = create_client(SUPABASE_URL, SUPABASE_KEY)

# Google Drive接続
@st.cache_resource
def get_drive_service():
    """Google Drive APIサービスを取得"""
    import json
    from pathlib import Path

    # Streamlit Cloudの場合はSecretsから、ローカルの場合はファイルから
    if "gcp_service_account" in st.secrets:
        # Streamlit CloudのSecrets
        credentials = service_account.Credentials.from_service_account_info(
            st.secrets["gcp_service_account"],
            scopes=["https://www.googleapis.com/auth/drive.readonly"]
        )
    else:
        # ローカル環境
        cred_path = Path(GOOGLE_DRIVE_CREDENTIALS)
        if not cred_path.exists():
            st.error(f"サービスアカウントファイルが見つかりません: {GOOGLE_DRIVE_CREDENTIALS}")
            st.stop()
        credentials = service_account.Credentials.from_service_account_file(
            GOOGLE_DRIVE_CREDENTIALS,
            scopes=["https://www.googleapis.com/auth/drive.readonly"]
        )
    return build("drive", "v3", credentials=credentials)


def get_receipt_image(drive_file_id: str):
    """Google Driveからレシート画像を取得"""
    try:
        service = get_drive_service()
        request = service.files().get_media(
            fileId=drive_file_id,
            supportsAllDrives=True
        )

        file_bytes = io.BytesIO()
        from googleapiclient.http import MediaIoBaseDownload
        downloader = MediaIoBaseDownload(file_bytes, request)

        done = False
        while not done:
            status, done = downloader.next_chunk()

        file_bytes.seek(0)
        return Image.open(file_bytes)
    except Exception as e:
        st.error(f"画像取得エラー: {e}")
        return None


def main():
    st.set_page_config(page_title="家計簿レビュー", layout="wide")
    st.title("📊 家計簿レビューシステム")

    # メインタブ
    tab1, tab2 = st.tabs(["📄 レシートレビュー", "🏷️ 商品分類管理"])

    with tab1:
        show_receipt_review_tab()

    with tab2:
        show_product_classification_tab()


def show_receipt_review_tab():
    """レシートレビュータブ"""
    # サイドバー：自動取り込み情報
    st.sidebar.header("📥 レシート取り込み")

    with st.sidebar.expander("ℹ️ 自動取り込みについて"):
        st.markdown("""
        **レシートは自動的に取り込まれます**

        - Google Drive の Inbox フォルダに画像を配置すると、1時間ごとに自動処理されます
        - 処理成功 → Archive フォルダに移動
        - 処理失敗 → Error フォルダに移動

        手動で取り込む場合は、GitHubの Actions タブから実行できます。
        """)

        st.divider()

        if st.button("📊 データ件数を確認"):
            try:
                receipts = db.table("Rawdata_RECEIPT_shops").select("*", count="exact").execute()
                transactions = db.table("Rawdata_RECEIPT_items").select("*", count="exact").execute()
                st.success(f"レシート: {receipts.count}件、商品: {transactions.count}件")
            except Exception as e:
                st.error(f"エラー: {e}")

    st.sidebar.divider()

    # サイドバー：レシート一覧
    st.sidebar.header("レシート一覧")

    # 処理ログ取得（レシート単位）
    try:
        logs = db.table("99_lg_image_proc_log") \
            .select("*") \
            .order("processed_at", desc=True) \
            .limit(100) \
            .execute()
    except Exception as e:
        st.error(f"処理ログの取得エラー: {str(e)}")
        st.info("エラー詳細を確認してください")
        import traceback
        st.code(traceback.format_exc())
        return

    if not logs.data:
        st.info("処理済みレシートがありません")
        return

    # フィルター
    status_filter = st.sidebar.selectbox(
        "ステータス",
        ["すべて", "未確認", "確認済み", "エラー"]
    )

    # レシート選択
    receipt_options = []
    for log in logs.data:
        status_icon = {
            "success": "✅",
            "failed": "❌"
        }.get(log["status"], "⚠️")

        label = f"{status_icon} {log['file_name']} ({log['processed_at'][:10]})"
        receipt_options.append((label, log))

    if not receipt_options:
        st.info("表示するレシートがありません")
        return

    selected_label = st.sidebar.radio(
        "レシートを選択",
        [opt[0] for opt in receipt_options],
        key="receipt_selector"
    )

    # 選択されたレシートを取得
    selected_log = next(opt[1] for opt in receipt_options if opt[0] == selected_label)

    # メイン画面：レシート詳細
    show_receipt_detail(selected_log)


def determine_expense_category(db, product_category: str, person: str, purpose: str):
    """
    2次分類（費目）を決定

    優先順位:
    1. 名目 + 人物 + 1次分類の完全一致（priority=80）
    2. 名目 + 1次分類（priority=90）
    3. 名目 + 人物（priority=90）
    4. 名目のみ（priority=100）
    5. 人物 + 1次分類（priority=50）
    6. 1次分類のみ（priority=30）

    Returns:
        str: 2次分類（費目）名、またはNone
    """
    try:
        # 1次分類IDを取得
        product_category_id = None
        if product_category:
            result = db.table("MASTER_Categories_product").select("id") \
                .eq("name", product_category) \
                .limit(1) \
                .execute()
            if result.data:
                product_category_id = result.data[0]["id"]

        # 名目IDを取得
        purpose_id = None
        if purpose:
            result = db.table("MASTER_Categories_purpose").select("id") \
                .eq("name", purpose) \
                .limit(1) \
                .execute()
            if result.data:
                purpose_id = result.data[0]["id"]

        # ルールを検索（優先度の高い順）
        # SQLでNULL比較を正しく処理
        query = db.table("MASTER_Rules_expense_mapping") \
            .select("expense_category_id, MASTER_Categories_expense(name)") \
            .order("priority", desc=True) \
            .limit(1)

        # 条件を動的に構築
        conditions = []

        # 完全一致を優先
        if purpose_id and person and product_category_id:
            query = query.eq("purpose_id", purpose_id) \
                        .eq("person", person) \
                        .eq("product_category_id", product_category_id)
        elif purpose_id and product_category_id:
            query = query.eq("purpose_id", purpose_id) \
                        .eq("product_category_id", product_category_id) \
                        .is_("person", "null")
        elif purpose_id and person:
            query = query.eq("purpose_id", purpose_id) \
                        .eq("person", person) \
                        .is_("product_category_id", "null")
        elif purpose_id:
            query = query.eq("purpose_id", purpose_id) \
                        .is_("person", "null") \
                        .is_("product_category_id", "null")
        elif person and product_category_id:
            query = query.is_("purpose_id", "null") \
                        .eq("person", person) \
                        .eq("product_category_id", product_category_id)
        elif product_category_id:
            query = query.is_("purpose_id", "null") \
                        .is_("person", "null") \
                        .eq("product_category_id", product_category_id)
        else:
            return None

        result = query.execute()

        if result.data:
            # JOINした結果から費目名を取得
            expense_category_data = result.data[0].get("MASTER_Categories_expense")
            if expense_category_data:
                return expense_category_data.get("name")

        return None

    except Exception as e:
        st.warning(f"2次分類決定エラー: {e}")
        return None


def auto_classify_transaction(db, shop_name: str, product_name: str, official_name: str = "", general_name: str = ""):
    """
    辞書テーブルを参照して、分類・人物・名目を自動判定

    優先順位:
    1. 店舗名 + 商品名の完全一致
    2. 店舗名のみ（店舗全体のデフォルト）
    3. 商品名のみ
    4. official_nameのみ
    5. general_nameのみ

    Returns:
        dict: {"category": str, "person": str, "purpose": str} または None
    """
    try:
        # 1. 店舗名 + 商品名の完全一致
        if shop_name and product_name:
            result = db.table("MASTER_Rules_transaction_dict").select("*") \
                .eq("shop_name", shop_name) \
                .eq("product_name", product_name) \
                .order("priority") \
                .limit(1) \
                .execute()
            if result.data:
                match = result.data[0]
                return {
                    "category": match.get("category"),
                    "person": match.get("person"),
                    "purpose": match.get("purpose")
                }

        # 2. 店舗名のみ（店舗全体のデフォルト）
        if shop_name:
            result = db.table("MASTER_Rules_transaction_dict").select("*") \
                .eq("shop_name", shop_name) \
                .eq("rule_type", "shop_only") \
                .order("priority") \
                .limit(1) \
                .execute()
            if result.data:
                match = result.data[0]
                return {
                    "category": match.get("category"),
                    "person": match.get("person"),
                    "purpose": match.get("purpose")
                }

        # 3. 商品名のみ
        if product_name:
            result = db.table("MASTER_Rules_transaction_dict").select("*") \
                .eq("product_name", product_name) \
                .is_("shop_name", "null") \
                .order("priority") \
                .limit(1) \
                .execute()
            if result.data:
                match = result.data[0]
                return {
                    "category": match.get("category"),
                    "person": match.get("person"),
                    "purpose": match.get("purpose")
                }

        # 4. official_nameのみ
        if official_name:
            result = db.table("MASTER_Rules_transaction_dict").select("*") \
                .eq("official_name", official_name) \
                .order("priority") \
                .limit(1) \
                .execute()
            if result.data:
                match = result.data[0]
                return {
                    "category": match.get("category"),
                    "person": match.get("person"),
                    "purpose": match.get("purpose")
                }

        # 5. general_nameのみ
        if general_name:
            result = db.table("MASTER_Rules_transaction_dict").select("*") \
                .eq("general_name", general_name) \
                .order("priority") \
                .limit(1) \
                .execute()
            if result.data:
                match = result.data[0]
                return {
                    "category": match.get("category"),
                    "person": match.get("person"),
                    "purpose": match.get("purpose")
                }

        # マッチなし
        return None

    except Exception as e:
        st.warning(f"自動判定エラー: {e}")
        return None


def save_to_dictionary(db, shop_name: str, product_name: str, official_name: str, general_name: str,
                      category: str, person: str, purpose: str):
    """
    辞書テーブルに保存（または更新）

    商品名をキーとして、分類・人物・名目を保存
    既存レコードがあれば使用回数をインクリメント
    """
    try:
        # 既存レコードを検索（shop_name + product_nameの組み合わせ）
        existing = db.table("MASTER_Rules_transaction_dict").select("*") \
            .eq("shop_name", shop_name) \
            .eq("product_name", product_name) \
            .execute()

        if existing.data:
            # 既存レコードを更新（使用回数をインクリメント）
            record = existing.data[0]
            db.table("MASTER_Rules_transaction_dict").update({
                "category": category,
                "person": person,
                "purpose": purpose,
                "official_name": official_name,
                "general_name": general_name,
                "usage_count": record.get("usage_count", 0) + 1,
                "updated_at": "NOW()"
            }).eq("id", record["id"]).execute()
        else:
            # 新規レコードを作成
            # ルールタイプを判定
            if shop_name and product_name:
                rule_type = "shop_product"
                priority = 10
            elif shop_name:
                rule_type = "shop_only"
                priority = 20
            elif official_name:
                rule_type = "official"
                priority = 30
            elif general_name:
                rule_type = "general"
                priority = 40
            else:
                rule_type = "product"
                priority = 50

            db.table("MASTER_Rules_transaction_dict").insert({
                "shop_name": shop_name,
                "product_name": product_name,
                "official_name": official_name,
                "general_name": general_name,
                "category": category,
                "person": person,
                "purpose": purpose,
                "rule_type": rule_type,
                "priority": priority,
                "usage_count": 1
            }).execute()

    except Exception as e:
        st.warning(f"辞書保存エラー: {e}")


def show_receipt_detail(log: dict):
    """レシート詳細表示"""

    st.header(f"📄 {log['file_name']}")

    # 2カラムレイアウト
    col_left, col_right = st.columns([1, 1])

    with col_left:
        st.subheader("レシート画像")

        if log.get("drive_file_id"):
            with st.spinner("画像を読み込み中..."):
                image = get_receipt_image(log["drive_file_id"])
                if image:
                    st.image(image, use_column_width=True)
                else:
                    st.warning("画像を取得できませんでした")
        else:
            st.info("画像IDがありません")

        # レシート情報
        st.subheader("処理情報")
        info_data = {
            "ファイル名": log["file_name"],
            "処理日時": log["processed_at"],
            "ステータス": log["status"],
            "OCRモデル": log.get("ocr_model", "不明"),
            "エラー": log.get("error_message", "なし")
        }

        for key, value in info_data.items():
            st.text(f"{key}: {value}")

    with col_right:
        st.subheader("取引明細")

        if log["status"] == "success" and log.get("receipt_id"):
            try:
                # レシート情報を取得
                receipt_result = db.table("Rawdata_RECEIPT_shops") \
                    .select("*") \
                    .eq("id", log["receipt_id"]) \
                    .execute()

                if not receipt_result.data:
                    st.warning("レシート情報が見つかりません")
                    return

                receipt = receipt_result.data[0]
            except Exception as e:
                st.error(f"データベースエラーが発生しました")
                st.exception(e)
                st.write("**エラー詳細:**")
                st.write(f"- エラー型: {type(e).__name__}")
                st.write(f"- エラーメッセージ: {str(e)}")

                # より詳細な情報を取得
                if hasattr(e, 'message'):
                    st.write(f"- APIメッセージ: {e.message}")
                if hasattr(e, 'details'):
                    st.write(f"- 詳細: {e.details}")
                if hasattr(e, 'hint'):
                    st.write(f"- ヒント: {e.hint}")
                if hasattr(e, 'code'):
                    st.write(f"- エラーコード: {e.code}")

                st.info("**解決方法:**\n"
                       "1. Streamlit Secretsに `SUPABASE_SERVICE_ROLE_KEY` が設定されているか確認してください\n"
                       "2. Supabaseのダッシュボードで、テーブル `Rawdata_RECEIPT_shops` のRLSポリシーを確認してください\n"
                       "3. `receipt_id` が正しく設定されているか確認してください")
                return

            # 税表示タイプを判定（レシートレベル）
            # すべてのレシートには小計がある（前提）
            # 合計が省略されている場合は、小計と同じ値とする
            subtotal = receipt.get('subtotal_amount')
            total = receipt.get('total_amount_check')

            # 合計が省略されている場合、小計と同じとする
            if total is None and subtotal is not None:
                total = subtotal
                st.info(f"合計が省略されているため、小計と同じ値（¥{subtotal:,}）を使用")

            # 念のため：小計がない場合のフォールバック
            if subtotal is None and total is not None:
                subtotal = total

            # 判定：小計 < 合計 → 外税、小計 = 合計 → 内税
            if subtotal and total:
                if subtotal < total:
                    tax_display_type = "外税"
                else:
                    tax_display_type = "内税"
            else:
                tax_display_type = "不明"

            # トランザクションを取得（JOINは使わず2段階クエリ）
            # 注意: 小計・合計行を除外するため、line_type = 'ITEM' のみ取得
            try:
                transactions = db.table("Rawdata_RECEIPT_items") \
                    .select("*") \
                    .eq("receipt_id", log["receipt_id"]) \
                    .or_("line_type.eq.ITEM,line_type.is.null") \
                    .order("line_number") \
                    .execute()

                # Note: standardized data is now stored directly in Rawdata_RECEIPT_items
                # No need to fetch from separate table

            except Exception as e:
                st.error(f"トランザクション取得エラー: {str(e)}")
                import traceback
                st.code(traceback.format_exc())
                return

            if transactions.data:
                # 🔍 デバッグ：最初のトランザクションのデータ構造を確認
                if len(transactions.data) > 0:
                    first_t = transactions.data[0]
                    with st.expander("🔍 デバッグ情報（最初の商品）"):
                        # データベースキー情報
                        import os
                        service_role_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
                        anon_key = os.getenv("SUPABASE_KEY")
                        st.write("**🔑 データベースキー情報**")
                        if service_role_key:
                            st.write(f"SERVICE_ROLE_KEY設定: ✅ あり (...{service_role_key[-4:]})")
                        else:
                            st.write("SERVICE_ROLE_KEY設定: ❌ なし")
                        if anon_key:
                            st.write(f"ANON_KEY設定: ✅ あり (...{anon_key[-4:]})")
                        else:
                            st.write("ANON_KEY設定: ❌ なし")
                        st.write(f"使用中のキー末尾: ...{SUPABASE_KEY[-4:]}")
                        st.write("---")

                        # 生のトランザクションデータを表示
                        st.write("**📦 生のトランザクションデータ（全キー）:**")
                        st.json(first_t)
                        st.write("---")

                        # データ構造情報（standardized data is now in the same record）
                        st.write(f"**商品名**: {first_t.get('product_name')}")
                        st.write(f"**std_unit_price**: {first_t.get('std_unit_price')}")
                        st.write(f"**tax_amount**: {first_t.get('tax_amount')}")
                        st.write(f"**std_amount**: {first_t.get('std_amount')}")

                # DataFrameに変換（7要素構造 + ID情報）
                df_data = []
                for t in transactions.data:
                    # Note: standardized data is now directly in t (no separate table)

                    # 7要素データを取得
                    quantity = t.get("quantity") or 1
                    std_unit_price = t.get('std_unit_price')  # 本体単価（1個あたりの税抜価格）
                    tax_amount = t.get('tax_amount')  # 税額
                    tax_included_amount = t.get('std_amount')  # 税込価

                    # 本体価を計算（税込価 - 税額）
                    # 注意: 本体価は数量分の合計なので、数量を掛けない
                    base_price_total = None
                    if tax_included_amount is not None and tax_amount is not None:
                        base_price_total = tax_included_amount - tax_amount

                    # 表示額を取得
                    # 1. transactionsテーブルのdisplayed_amountを優先（レシート記載値）
                    # 2. なければ計算で求める（後方互換性）
                    displayed_amount = t.get("displayed_amount")
                    if displayed_amount is None:
                        if tax_display_type == "内税":
                            displayed_amount = tax_included_amount
                        elif tax_display_type == "外税":
                            displayed_amount = base_price_total

                    # 税込単価を計算（税込価 ÷ 数量）
                    tax_included_unit_price = None
                    if tax_included_amount and quantity:
                        tax_included_unit_price = tax_included_amount // quantity

                    # 分類の階層表示（内部的には大中小の3階層、表示は最下層のみ）
                    major = t.get("major_category") or ""
                    middle = t.get("middle_category") or ""
                    minor = t.get("minor_category") or ""

                    # 表示用の分類（最下層のみ、なければ順に上位を表示）
                    category_display = minor or middle or major or ""

                    # 人物と名目を取得
                    person_value = t.get("person") or "家族"  # デフォルト: 家族
                    purpose_value = t.get("purpose") or "日常"  # デフォルト: 日常

                    # 2次分類（費目）を自動判定
                    expense_category = determine_expense_category(
                        db=db,
                        product_category=category_display,
                        person=person_value,
                        purpose=purpose_value
                    ) or ""

                    df_data.append({
                        "_transaction_id": t["id"],  # 更新用（非表示）
                        "_std_id": t.get("id"),  # 更新用（非表示、now same as transaction_id）
                        "_major_category": major,  # 内部保持（非表示）
                        "_middle_category": middle,  # 内部保持（非表示）
                        "_minor_category": minor,  # 内部保持（非表示）
                        "商品名": t["product_name"],
                        "数量": quantity,
                        "表示額": displayed_amount if displayed_amount is not None else 0,
                        "外or内": tax_display_type,
                        "税率": t.get('tax_rate', 10),
                        "本体価": base_price_total if base_price_total is not None else 0,
                        "税額": tax_amount if tax_amount is not None else 0,
                        "税込価": tax_included_amount if tax_included_amount is not None else 0,
                        "単価": tax_included_unit_price if tax_included_unit_price is not None else 0,
                        "正式名": t.get("official_name") or "",
                        "物品名": t.get("item_name") or "",
                        "分類": category_display,
                        "人物": person_value,
                        "名目": purpose_value,
                        "費目": expense_category,  # 2次分類（自動判定）
                        "要確認": "⚠️" if t.get("needs_review") else ""
                    })

                df = pd.DataFrame(df_data)

                # 人物と名目の選択肢を取得
                person_options = ["家族", "パパ", "ママ", "絵麻", "育哉"]

                # 名目の選択肢（DBから取得）
                try:
                    purposes_result = db.table("MASTER_Categories_purpose").select("name").order("display_order").execute()
                    purpose_options = [p["name"] for p in purposes_result.data] if purposes_result.data else ["日常"]
                except:
                    # テーブルがまだ存在しない場合のフォールバック
                    existing_purposes = set()
                    for t in transactions.data:
                        purpose = t.get("purpose")
                        if purpose:
                            existing_purposes.add(purpose)
                    purpose_options = sorted(list(existing_purposes)) if existing_purposes else []
                    if "日常" not in purpose_options:
                        purpose_options.insert(0, "日常")

                # 費目の選択肢（DBから取得）
                try:
                    expense_cats_result = db.table("MASTER_Categories_expense").select("name").order("display_order").execute()
                    expense_category_options = [c["name"] for c in expense_cats_result.data] if expense_cats_result.data else []
                except:
                    expense_category_options = []

                # AI自動判定ボタン
                st.divider()
                col_ai1, col_ai2 = st.columns([3, 1])
                with col_ai1:
                    st.info("🤖 AI自動判定: 辞書テーブルを参照して、店舗名と商品名から分類・人物・名目を自動で設定します")
                with col_ai2:
                    if st.button("🤖 AI自動判定", type="secondary", key="ai_auto_classify"):
                        # 店舗名を取得
                        shop_name = receipt.get("shop_name", "")

                        # 各商品に対してAI自動判定を実行
                        auto_classified_count = 0
                        for idx in df.index:
                            product_name = df.loc[idx, "商品名"]
                            official_name = df.loc[idx, "正式名"] or ""
                            general_name = ""  # 現時点では未実装

                            # AI自動判定
                            result = auto_classify_transaction(
                                db=db,
                                shop_name=shop_name,
                                product_name=product_name,
                                official_name=official_name,
                                general_name=general_name
                            )

                            if result:
                                # 判定結果をdfに反映
                                if result.get("category"):
                                    df.loc[idx, "分類"] = result["category"]
                                if result.get("person"):
                                    df.loc[idx, "人物"] = result["person"]
                                if result.get("purpose"):
                                    df.loc[idx, "名目"] = result["purpose"]

                                # 2次分類（費目）を再判定
                                expense_cat = determine_expense_category(
                                    db=db,
                                    product_category=df.loc[idx, "分類"],
                                    person=df.loc[idx, "人物"],
                                    purpose=df.loc[idx, "名目"]
                                )
                                if expense_cat:
                                    df.loc[idx, "費目"] = expense_cat

                                auto_classified_count += 1

                        if auto_classified_count > 0:
                            st.success(f"✅ {auto_classified_count}件の商品を自動判定しました。下の表で確認して、必要に応じて修正してください。")
                            st.rerun()
                        else:
                            st.warning("辞書に該当するデータがありませんでした。手動で設定後、「辞書に保存」をチェックしてデータを更新してください。")

                st.divider()

                # 全行一括編集機能
                with st.expander("🔧 全行一括編集", expanded=False):
                    st.info("分類、人物、名目を全行に一括で適用できます。適用後、下の表で確認してから「データを更新」ボタンを押してください。")

                    col1, col2, col3 = st.columns(3)

                    with col1:
                        bulk_category = st.text_input("分類（全行）", key="bulk_category", placeholder="例: 根菜")
                        if st.button("✅ 分類を全行に適用", key="apply_bulk_category"):
                            if bulk_category:
                                for idx in df.index:
                                    df.loc[idx, "分類"] = bulk_category
                                st.success(f"分類を「{bulk_category}」に変更しました（表を確認後、下の「データを更新」ボタンを押してください）")

                    with col2:
                        bulk_person = st.selectbox("人物（全行）", options=person_options, index=0, key="bulk_person")
                        if st.button("✅ 人物を全行に適用", key="apply_bulk_person"):
                            for idx in df.index:
                                df.loc[idx, "人物"] = bulk_person
                            st.success(f"人物を「{bulk_person}」に変更しました（表を確認後、下の「データを更新」ボタンを押してください）")

                    with col3:
                        bulk_purpose_index = 0 if "日常" in purpose_options else 0
                        bulk_purpose = st.selectbox("名目（全行）", options=purpose_options if purpose_options else ["日常"], index=bulk_purpose_index, key="bulk_purpose")
                        if st.button("✅ 名目を全行に適用", key="apply_bulk_purpose"):
                            for idx in df.index:
                                df.loc[idx, "名目"] = bulk_purpose
                            st.success(f"名目を「{bulk_purpose}」に変更しました（表を確認後、下の「データを更新」ボタンを押してください）")

                # 編集可能なデータエディタ
                edited_df = st.data_editor(
                    df,
                    hide_index=True,
                    height=400,
                    column_config={
                        "_transaction_id": None,  # 非表示
                        "_std_id": None,  # 非表示
                        "_major_category": None,  # 非表示
                        "_middle_category": None,  # 非表示
                        "_minor_category": None,  # 非表示
                        "商品名": st.column_config.TextColumn("商品名", disabled=True),
                        "数量": st.column_config.NumberColumn("数量", min_value=1, step=1),
                        "表示額": st.column_config.NumberColumn("表示額", format="¥%d"),
                        "外or内": st.column_config.TextColumn("外or内", disabled=True),
                        "税率": st.column_config.NumberColumn("税率", format="%d%%", disabled=True),
                        "本体価": st.column_config.NumberColumn("本体価", format="¥%d"),
                        "税額": st.column_config.NumberColumn("税額", format="¥%d"),
                        "税込価": st.column_config.NumberColumn("税込価", format="¥%d"),
                        "単価": st.column_config.NumberColumn("単価", format="¥%d", disabled=True),
                        "分類": st.column_config.TextColumn("分類", width="medium"),
                        "人物": st.column_config.SelectboxColumn("人物", options=person_options, default="家族"),
                        "名目": st.column_config.SelectboxColumn("名目", options=purpose_options, default="日常") if purpose_options else st.column_config.TextColumn("名目"),
                        "費目": st.column_config.SelectboxColumn("費目", options=expense_category_options) if expense_category_options else st.column_config.TextColumn("費目", help="2次分類（自動判定）"),
                    },
                    use_container_width=True
                )

                # 更新ボタン
                col_update1, col_update2 = st.columns([3, 1])
                with col_update1:
                    save_to_dict = st.checkbox("辞書に保存（次回から自動判定に活用）", value=True, key="save_to_dict_check")
                with col_update2:
                    update_button = st.button("💾 データを更新", type="primary")

                if update_button:
                    # 変更されたデータをDBに保存
                    updated_count = 0
                    for idx, row in edited_df.iterrows():
                        std_id = row["_std_id"]
                        transaction_id = row["_transaction_id"]
                        if std_id:
                            # 本体単価を逆算（本体価 ÷ 数量）
                            quantity = row["数量"]
                            base_price = row["本体価"]
                            std_unit_price = base_price // quantity if quantity > 0 else 0

                            # 分類の処理: 現時点では入力された分類を minor_category として保存
                            # 将来的には階層構造のマスターテーブルから major/middle を自動判定
                            category_value = row["分類"]

                            # Rawdata_RECEIPT_itemsを更新
                            try:
                                db.table("Rawdata_RECEIPT_items").update({
                                    "std_unit_price": std_unit_price,
                                    "tax_amount": row["税額"],
                                    "std_amount": row["税込価"],
                                    "minor_category": category_value,  # 分類を更新
                                    "person": row["人物"],  # 人物を更新
                                    "purpose": row["名目"],  # 名目を更新
                                }).eq("id", transaction_id).execute()
                                updated_count += 1

                                # 辞書に保存（オプション）
                                if save_to_dict and category_value and row["人物"] and row["名目"]:
                                    # トランザクションデータから商品情報を取得
                                    product_name = row["商品名"]
                                    official_name = row["正式名"] or ""
                                    general_name = ""  # 現時点では未実装

                                    # 店舗名を取得
                                    shop_name = receipt.get("shop_name", "")

                                    # 辞書に保存
                                    save_to_dictionary(
                                        db=db,
                                        shop_name=shop_name,
                                        product_name=product_name,
                                        official_name=official_name,
                                        general_name=general_name,
                                        category=category_value,
                                        person=row["人物"],
                                        purpose=row["名目"]
                                    )

                            except Exception as e:
                                st.error(f"更新エラー ({row['商品名']}): {e}")

                    if updated_count > 0:
                        msg = f"✅ {updated_count}件のデータを更新しました"
                        if save_to_dict:
                            msg += "（辞書に保存しました）"
                        st.success(msg)
                        st.rerun()  # ページをリロードしてレシート情報サマリーも更新

                # 合計金額・税額サマリー
                total = sum(
                    t.get("std_amount", 0) or 0
                    for t in transactions.data
                )
                # 税率別の集計
                # 税額合計（割引行を含む全トランザクションから計算）
                total_tax_8 = sum(
                    t.get("tax_amount", 0) or 0
                    for t in transactions.data
                    if t.get("tax_rate") == 8
                )
                total_tax_10 = sum(
                    t.get("tax_amount", 0) or 0
                    for t in transactions.data
                    if t.get("tax_rate") == 10
                )
                # 税込合計（8%、10%それぞれ）
                total_amount_8 = sum(
                    t.get("std_amount", 0) or 0
                    for t in transactions.data
                    if t.get("tax_rate") == 8
                )
                total_amount_10 = sum(
                    t.get("std_amount", 0) or 0
                    for t in transactions.data
                    if t.get("tax_rate") == 10
                )

                # 税額サマリー取得（レシート記載値との比較）
                # Rawdata_RECEIPT_shopsテーブルから税率別の小計・税額を取得
                try:
                    # レシート記載の税額と小計を取得
                    receipt_tax_8 = receipt.get('tax_8_amount')
                    receipt_tax_10 = receipt.get('tax_10_amount')
                    receipt_8_subtotal = receipt.get('tax_8_subtotal')
                    receipt_10_subtotal = receipt.get('tax_10_subtotal')

                    # 簡易的なsummaryオブジェクトを作成
                    if receipt_tax_10 is not None or receipt_tax_8 is not None:
                        tax_summary = type('obj', (object,), {
                            'data': [{
                                'tax_10_subtotal': receipt_10_subtotal,
                                'tax_10_amount': receipt_tax_10,
                                'tax_8_subtotal': receipt_8_subtotal,
                                'tax_8_amount': receipt_tax_8,
                                'calculated_matches_actual': True  # 仮の値
                            }]
                        })()
                    else:
                        tax_summary = None
                except Exception as e:
                    # エラーの場合はスキップ
                    tax_summary = None

                # ========================================
                # レシート情報サマリー（詳細版）
                # ========================================

                # 計算値を集計（税抜・税込両方）
                calc_subtotal_excluding_tax = sum(  # 税抜合計（外税用）
                    (t.get("std_unit_price", 0) or 0) * (t.get("quantity") or 1)
                    for t in transactions.data
                )
                calc_total = sum(  # 税込合計
                    t.get("std_amount", 0) or 0
                    for t in transactions.data
                )

                # 税率別の対象額（8%, 10%）
                # 10%対象額（税込）- 内税用
                calc_10_amount_including_tax = sum(
                    t.get("std_amount", 0) or 0
                    for t in transactions.data
                    if t.get("tax_rate") == 10
                )
                # 10%対象額（税抜）- 外税用
                calc_10_amount_excluding_tax = sum(
                    (t.get("std_unit_price", 0) or 0) * (t.get("quantity") or 1)
                    for t in transactions.data
                    if t.get("tax_rate") == 10
                )
                # 8%対象額（税込）- 内税用
                calc_8_amount_including_tax = sum(
                    t.get("std_amount", 0) or 0
                    for t in transactions.data
                    if t.get("tax_rate") == 8
                )
                # 8%対象額（税抜）- 外税用
                calc_8_amount_excluding_tax = sum(
                    (t.get("std_unit_price", 0) or 0) * (t.get("quantity") or 1)
                    for t in transactions.data
                    if t.get("tax_rate") == 8
                )

                # レシート記載値
                receipt_subtotal = receipt.get("subtotal_amount")
                receipt_total = receipt.get("total_amount_check")

                # 整合性チェック
                if tax_summary and tax_summary.data:
                    summary = tax_summary.data[0]
                    match_icon = "✅" if summary.get("calculated_matches_actual") else "⚠️"
                    st.subheader(f"📊 レシート情報サマリー（{tax_display_type}） {match_icon}")
                else:
                    st.subheader(f"📊 レシート情報サマリー（{tax_display_type}）")
                    summary = None

                # テーブルデータを作成（内税・外税で項目名と計算方法を変える）
                table_data = {
                    "項目": [],
                    "レシート記載": [],
                    "計算値（差分）": []
                }

                # 1. 小計
                if tax_display_type == "内税":
                    # 内税の場合：小計 = 税込合計
                    table_data["項目"].append("小計（税込）")
                    table_data["レシート記載"].append(f"¥{receipt_subtotal:,}" if receipt_subtotal is not None else "—")

                    subtotal_diff = calc_total - receipt_subtotal if receipt_subtotal else 0
                    if subtotal_diff != 0:
                        table_data["計算値（差分）"].append(f"¥{calc_total:,}|{subtotal_diff:+,}円")
                    else:
                        table_data["計算値（差分）"].append(f"¥{calc_total:,}|")
                else:
                    # 外税の場合：小計 = 税抜合計
                    table_data["項目"].append("小計（税抜）")
                    table_data["レシート記載"].append(f"¥{receipt_subtotal:,}" if receipt_subtotal is not None else "—")

                    subtotal_diff = calc_subtotal_excluding_tax - receipt_subtotal if receipt_subtotal else 0
                    if subtotal_diff != 0:
                        table_data["計算値（差分）"].append(f"¥{calc_subtotal_excluding_tax:,}|{subtotal_diff:+,}円")
                    else:
                        table_data["計算値（差分）"].append(f"¥{calc_subtotal_excluding_tax:,}|")

                # 2. 税率別の対象額と税額
                if summary:
                    if tax_display_type == "内税":
                        # 内税10%対象額（税込）
                        table_data["項目"].append("内税10%対象額（税込）")
                        tax_10_subtotal = summary.get('tax_10_subtotal')
                        tax_10_amount = summary.get('tax_10_amount')
                        # レシート記載 = 対象額（税抜） + 税額 = 税込
                        if tax_10_subtotal is not None and tax_10_amount is not None:
                            receipt_10_including = tax_10_subtotal + tax_10_amount
                            table_data["レシート記載"].append(f"¥{receipt_10_including:,}")
                        else:
                            table_data["レシート記載"].append("—")
                            receipt_10_including = None

                        # 計算値 = 10%対象商品の税込価合計
                        amount_diff = calc_10_amount_including_tax - receipt_10_including if receipt_10_including else 0
                        if amount_diff != 0:
                            table_data["計算値（差分）"].append(f"¥{calc_10_amount_including_tax:,}|{amount_diff:+,}円")
                        else:
                            table_data["計算値（差分）"].append(f"¥{calc_10_amount_including_tax:,}|")

                        # 内税10%税額
                        table_data["項目"].append("内税10%税額")
                        table_data["レシート記載"].append(
                            f"¥{tax_10_amount:,}" if tax_10_amount is not None else "—"
                        )
                        # 計算値は total_tax_10 を使う（実際に計算した税額）
                        tax_10_diff = total_tax_10 - tax_10_amount if tax_10_amount else 0
                        if tax_10_diff != 0:
                            table_data["計算値（差分）"].append(f"¥{total_tax_10:,}|{tax_10_diff:+d}円")
                        else:
                            table_data["計算値（差分）"].append(f"¥{total_tax_10:,}|")

                        # 内税8%対象額（税込）
                        if calc_8_amount_including_tax > 0:
                            table_data["項目"].append("内税8%対象額（税込）")
                            tax_8_subtotal = summary.get('tax_8_subtotal')
                            tax_8_amount = summary.get('tax_8_amount')
                            if tax_8_subtotal is not None and tax_8_amount is not None:
                                receipt_8_including = tax_8_subtotal + tax_8_amount
                                table_data["レシート記載"].append(f"¥{receipt_8_including:,}")
                            else:
                                table_data["レシート記載"].append("—")
                                receipt_8_including = None

                            amount_diff = calc_8_amount_including_tax - receipt_8_including if receipt_8_including else 0
                            if amount_diff != 0:
                                table_data["計算値（差分）"].append(f"¥{calc_8_amount_including_tax:,}|{amount_diff:+,}円")
                            else:
                                table_data["計算値（差分）"].append(f"¥{calc_8_amount_including_tax:,}|")

                            # 内税8%税額
                            table_data["項目"].append("内税8%税額")
                            table_data["レシート記載"].append(
                                f"¥{tax_8_amount:,}" if tax_8_amount is not None else "—"
                            )
                            # 計算値は total_tax_8 を使う（実際に計算した税額）
                            tax_8_diff = total_tax_8 - tax_8_amount if tax_8_amount else 0
                            if tax_8_diff != 0:
                                table_data["計算値（差分）"].append(f"¥{total_tax_8:,}|{tax_8_diff:+d}円")
                            else:
                                table_data["計算値（差分）"].append(f"¥{total_tax_8:,}|")

                    else:  # 外税
                        # 外税10%対象額（税抜）
                        table_data["項目"].append("外税10%対象額（税抜）")
                        tax_10_subtotal = summary.get('tax_10_subtotal')
                        table_data["レシート記載"].append(
                            f"¥{tax_10_subtotal:,}" if tax_10_subtotal is not None else "—"
                        )

                        # 計算値 = 10%対象商品の税抜価合計
                        amount_diff = calc_10_amount_excluding_tax - tax_10_subtotal if tax_10_subtotal else 0
                        if amount_diff != 0:
                            table_data["計算値（差分）"].append(f"¥{calc_10_amount_excluding_tax:,}|{amount_diff:+,}円")
                        else:
                            table_data["計算値（差分）"].append(f"¥{calc_10_amount_excluding_tax:,}|")

                        # 外税10%税額
                        table_data["項目"].append("外税10%税額")
                        tax_10_amount = summary.get('tax_10_amount')
                        table_data["レシート記載"].append(
                            f"¥{tax_10_amount:,}" if tax_10_amount is not None else "—"
                        )
                        # 計算値は total_tax_10 を使う（実際に計算した税額）
                        tax_10_diff = total_tax_10 - tax_10_amount if tax_10_amount else 0
                        if tax_10_diff != 0:
                            table_data["計算値（差分）"].append(f"¥{total_tax_10:,}|{tax_10_diff:+d}円")
                        else:
                            table_data["計算値（差分）"].append(f"¥{total_tax_10:,}|")

                        # 外税8%対象額（税抜）
                        if calc_8_amount_excluding_tax > 0:
                            table_data["項目"].append("外税8%対象額（税抜）")
                            tax_8_subtotal = summary.get('tax_8_subtotal')
                            table_data["レシート記載"].append(
                                f"¥{tax_8_subtotal:,}" if tax_8_subtotal is not None else "—"
                            )

                            amount_diff = calc_8_amount_excluding_tax - tax_8_subtotal if tax_8_subtotal else 0
                            if amount_diff != 0:
                                table_data["計算値（差分）"].append(f"¥{calc_8_amount_excluding_tax:,}|{amount_diff:+,}円")
                            else:
                                table_data["計算値（差分）"].append(f"¥{calc_8_amount_excluding_tax:,}|")

                            # 外税8%税額
                            table_data["項目"].append("外税8%税額")
                            tax_8_amount = summary.get('tax_8_amount')
                            table_data["レシート記載"].append(
                                f"¥{tax_8_amount:,}" if tax_8_amount is not None else "—"
                            )
                            # 計算値は total_tax_8 を使う（実際に計算した税額）
                            tax_8_diff = total_tax_8 - tax_8_amount if tax_8_amount else 0
                            if tax_8_diff != 0:
                                table_data["計算値（差分）"].append(f"¥{total_tax_8:,}|{tax_8_diff:+d}円")
                            else:
                                table_data["計算値（差分）"].append(f"¥{total_tax_8:,}|")
                else:
                    # tax_summaryがない場合
                    if tax_display_type == "内税":
                        table_data["項目"].append("内税10%対象額（税込）")
                        table_data["レシート記載"].append("—")
                        table_data["計算値（差分）"].append(f"¥{calc_10_amount_including_tax:,}|")

                        table_data["項目"].append("内税10%税額")
                        table_data["レシート記載"].append("—")
                        table_data["計算値（差分）"].append(f"¥{total_tax_10:,}|")
                    else:
                        table_data["項目"].append("外税10%対象額（税抜）")
                        table_data["レシート記載"].append("—")
                        table_data["計算値（差分）"].append(f"¥{calc_10_amount_excluding_tax:,}|")

                        table_data["項目"].append("外税10%税額")
                        table_data["レシート記載"].append("—")
                        table_data["計算値（差分）"].append(f"¥{total_tax_10:,}|")

                # 3. 税込合計
                table_data["項目"].append("税込合計")
                table_data["レシート記載"].append(f"¥{receipt_total:,}" if receipt_total is not None else "—")

                total_diff = calc_total - receipt_total if receipt_total else 0
                if total_diff != 0:
                    table_data["計算値（差分）"].append(f"¥{calc_total:,}|{total_diff:+,}円")
                else:
                    table_data["計算値（差分）"].append(f"¥{calc_total:,}|")

                # HTMLテーブルを作成（差分を赤字で表示）
                html_table = '<table style="width:100%; border-collapse: collapse;">'
                html_table += '<tr style="background-color: #f0f0f0;"><th style="padding: 8px; text-align: left; border: 1px solid #ddd;">項目</th><th style="padding: 8px; text-align: left; border: 1px solid #ddd;">レシート記載</th><th style="padding: 8px; text-align: left; border: 1px solid #ddd;">計算値（差分）</th></tr>'

                for i in range(len(table_data["項目"])):
                    item = table_data["項目"][i]
                    receipt_text = table_data["レシート記載"][i]
                    calc_val = table_data["計算値（差分）"][i]

                    # "|" で分割して、差分部分を赤字にする
                    if "|" in calc_val:
                        parts = calc_val.split("|")
                        if parts[1]:  # 差分がある場合
                            calc_display = f'{parts[0]} <span style="color: red;">({parts[1]})</span>'
                        else:  # 差分がない場合
                            calc_display = parts[0]
                    else:
                        calc_display = calc_val

                    html_table += f'<tr><td style="padding: 8px; border: 1px solid #ddd;">{item}</td><td style="padding: 8px; border: 1px solid #ddd;">{receipt_text}</td><td style="padding: 8px; border: 1px solid #ddd;">{calc_display}</td></tr>'

                html_table += '</table>'
                st.markdown(html_table, unsafe_allow_html=True)

                # CSVダウンロードボタンを追加
                summary_df = pd.DataFrame(table_data)
                csv_data = summary_df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="📥 CSV形式でダウンロード",
                    data=csv_data,
                    file_name=f"receipt_summary_{receipt['shop_name']}_{receipt['transaction_date']}.csv",
                    mime="text/csv",
                    key="download_receipt_summary"
                )

                # 店名・日付（レシートから取得）
                st.text(f"店名: {receipt['shop_name']}")
                st.text(f"取引日: {receipt['transaction_date']}")
                st.text(f"レシート合計: ¥{receipt.get('total_amount_check', 0):,}")

                # 確認状態（レシート単位）
                if receipt["is_verified"]:
                    st.success("✅ このレシートは確認済みです")
                else:
                    st.warning(f"⏸️ このレシートは未確認です")

                # アクションボタン
                st.divider()

                col1, col2, col3 = st.columns(3)

                with col1:
                    if st.button("✅ 全て承認", key="approve_all"):
                        # レシート単位で承認
                        db.table("Rawdata_RECEIPT_shops") \
                            .update({"is_verified": True}) \
                            .eq("id", log["receipt_id"]) \
                            .execute()
                        st.success("承認しました")
                        st.rerun()

                with col2:
                    if st.button("📝 個別編集", key="edit_mode"):
                        st.session_state.edit_mode = True
                        st.rerun()

                with col3:
                    if st.button("🗑️ 全て削除", key="delete_all"):
                        # レシートを削除（CASCADE で子・孫も削除される）
                        db.table("Rawdata_RECEIPT_shops") \
                            .delete() \
                            .eq("id", log["receipt_id"]) \
                            .execute()
                        st.warning("削除しました")
                        st.rerun()

                # 個別編集モード
                if st.session_state.get("edit_mode"):
                    st.divider()
                    st.subheader("個別編集")

                    for idx, t in enumerate(transactions.data):
                        # Note: standardized data is now directly in t
                        amount = t.get('std_amount', 0) or 0
                        with st.expander(f"{t['product_name']} (¥{amount:,})"):
                            col_a, col_b, col_c = st.columns(3)

                            with col_a:
                                new_product = st.text_input(
                                    "商品名",
                                    value=t["product_name"],
                                    key=f"prod_{idx}"
                                )

                                new_amount = st.number_input(
                                    "金額",
                                    value=amount,
                                    key=f"amt_{idx}"
                                )

                                new_tax_included = st.number_input(
                                    "内税額",
                                    value=t.get("tax_amount", 0) or 0,
                                    key=f"tax_{idx}"
                                )

                                new_official_name = st.text_input(
                                    "正式名",
                                    value=t.get("official_name") or "",
                                    key=f"official_{idx}"
                                )

                                new_item_name = st.text_input(
                                    "物品名",
                                    value=t.get("item_name") or "",
                                    key=f"item_{idx}"
                                )

                            with col_b:
                                # 分類（最下層のみ表示）
                                current_category = t.get("minor_category") or t.get("middle_category") or t.get("major_category") or ""
                                new_category = st.text_input(
                                    "分類",
                                    value=current_category,
                                    key=f"category_{idx}",
                                    placeholder="例: 根菜"
                                )

                            with col_c:
                                # 人物（プルダウン）
                                current_person = t.get("person") or "家族"
                                person_index = person_options.index(current_person) if current_person in person_options else 0
                                new_person = st.selectbox(
                                    "人物",
                                    options=person_options,
                                    index=person_index,
                                    key=f"person_{idx}"
                                )

                                # 名目（プルダウン）
                                current_purpose = t.get("purpose") or "日常"
                                if current_purpose not in purpose_options:
                                    purpose_options.append(current_purpose)
                                purpose_index = purpose_options.index(current_purpose) if current_purpose in purpose_options else 0
                                new_purpose = st.selectbox(
                                    "名目",
                                    options=purpose_options,
                                    index=purpose_index,
                                    key=f"purpose_{idx}"
                                )

                            if st.button("💾 更新", key=f"update_{idx}"):
                                # Rawdata_RECEIPT_items を更新（全データを同じテーブルに保存）
                                db.table("Rawdata_RECEIPT_items").update({
                                    "product_name": new_product,
                                    "item_name": new_item_name,
                                    "std_amount": new_amount,
                                    "tax_amount": new_tax_included,
                                    "official_name": new_official_name,
                                    "minor_category": new_category,  # 分類を更新
                                    "person": new_person,
                                    "purpose": new_purpose
                                }).eq("id", t["id"]).execute()

                                # レシート全体を確認済みにマーク
                                db.table("Rawdata_RECEIPT_shops").update({
                                    "is_verified": True
                                }).eq("id", log["receipt_id"]).execute()

                                st.success("更新しました")
                                st.rerun()

                    if st.button("編集モード終了", key="exit_edit"):
                        st.session_state.edit_mode = False
                        st.rerun()

            else:
                st.warning("トランザクションデータが見つかりません")

        elif log["status"] == "failed":
            st.error(f"❌ 処理エラー: {log.get('error_message', '不明')}")
            st.info("errorsフォルダを確認してください")

        else:
            st.info("トランザクションデータがありません")


def show_product_classification_tab():
    """商品分類管理タブ"""
    st.header("🏷️ 商品分類管理")

    # サブタブ
    subtab1, subtab2, subtab3, subtab4, subtab5 = st.tabs([
        "📥 日次承認インボックス",
        "🔍 承認済み商品の検索・編集",
        "✅ クラスタ承認",
        "🌳 カテゴリ管理",
        "⚙️ ルール管理"
    ])

    with subtab1:
        show_daily_inbox()

    with subtab2:
        show_approved_products_search()

    with subtab3:
        show_bulk_clustering()

    with subtab4:
        show_category_tree()

    with subtab5:
        show_rule_management()


def show_daily_inbox():
    """日次承認インボックス（信号機UI）"""
    st.subheader("📥 日次承認インボックス")
    st.info("新規商品の分類結果を確認・承認します")

    # 初期化: セッション状態でデータを保持してリロードを最小化
    if 'pending_products_data' not in st.session_state or st.session_state.get('refresh_pending_products', False):
        try:
            # データ取得（初回または明示的なリフレッシュ時のみ）
            all_pending = db.table('Rawdata_NETSUPER_items').select(
                'id, product_name, product_name_normalized, general_name, category_id, classification_confidence, organization'
            ).eq('needs_approval', True).execute()

            st.session_state['pending_products_data'] = all_pending.data
            st.session_state['refresh_pending_products'] = False
        except Exception as e:
            st.error(f"データ取得エラー: {e}")
            return

    # セッション状態からデータを取得
    all_pending_data = st.session_state.get('pending_products_data', [])

    # 承認待ち商品を信頼度別に分類
    try:
        # Pythonで信頼度別に分類（NULL対応）
        high_data = []
        medium_data = []
        low_data = []

        for product in all_pending_data:
            confidence = product.get('classification_confidence')
            if confidence is not None and confidence >= 0.9:
                high_data.append(product)
            elif confidence is not None and confidence >= 0.7:
                medium_data.append(product)
            else:
                # confidence < 0.7 または NULL
                low_data.append(product)

        # データを返す形式に合わせる
        class Response:
            def __init__(self, data):
                self.data = data

        high = Response(high_data)
        medium = Response(medium_data)
        low = Response(low_data)

        # タブ表示
        tab_high, tab_medium, tab_low = st.tabs([
            f"🟢 高信頼度 ({len(high.data)}件)",
            f"🟡 中信頼度 ({len(medium.data)}件)",
            f"🔴 要確認 ({len(low.data)}件)"
        ])

        with tab_high:
            render_product_approval_table(high.data, "高信頼度", "🟢")

        with tab_medium:
            render_product_approval_table(medium.data, "中信頼度", "🟡")

        with tab_low:
            render_product_approval_table(low.data, "要確認", "🔴")

    except Exception as e:
        st.error(f"データ取得エラー: {e}")


def render_product_approval_table(products, title, icon):
    """商品承認テーブル表示"""
    if not products:
        st.info(f"{title}: 該当なし")
        return

    st.markdown(f"### {icon} {title} ({len(products)}件)")

    df = pd.DataFrame([{
        "id": p["id"],  # 内部IDは非表示だが承認処理で使用
        "承認": False,
        "product_name": p.get("product_name", ""),
        "product_name_normalized": p.get("product_name_normalized", ""),
        "general_name": p.get("general_name", "未設定"),
        "店舗": p.get("organization", ""),
        "信頼度": f"{p.get('classification_confidence', 0):.1%}" if p.get('classification_confidence') else "—"
    } for p in products])

    edited_df = st.data_editor(
        df,
        column_config={
            "承認": st.column_config.CheckboxColumn("承認", default=False, width="small"),
            "product_name": st.column_config.TextColumn("product_name", width="large", disabled=False),
            "product_name_normalized": st.column_config.TextColumn("product_name_normalized", width="large", disabled=False),
            "general_name": st.column_config.TextColumn("general_name", width="medium", disabled=False),
            "店舗": st.column_config.TextColumn("店舗", width="medium", disabled=True),  # 店舗は編集不可
            "信頼度": st.column_config.TextColumn("信頼度", width="small", disabled=True)  # 信頼度は編集不可
        },
        column_order=["承認", "product_name", "product_name_normalized", "general_name", "店舗", "信頼度"],
        hide_index=True,
        use_container_width=True,
        key=f"table_{title}"
    )

    # ボタンを横に並べる
    col1, col2 = st.columns(2)

    with col1:
        if st.button(f"✏️ 修正を反映", key=f"btn_save_{title}"):
            checked_rows = edited_df[edited_df["承認"] == True]
            if len(checked_rows) > 0:
                for _, row in checked_rows.iterrows():
                    # 修正内容のみ保存（承認はしない）
                    db.table('Rawdata_NETSUPER_items').update({
                        "product_name": row['product_name'],
                        "product_name_normalized": row['product_name_normalized'],
                        "general_name": row['general_name']
                    }).eq('id', row['id']).execute()
                st.success(f"{len(checked_rows)}件の修正を反映しました（未承認のまま）")
                # データをリフレッシュ
                st.session_state['refresh_pending_products'] = True
                st.rerun()
            else:
                st.warning("反映する項目を選択してください")

    with col2:
        if st.button(f"✅ 修正して承認", key=f"btn_approve_{title}"):
            checked_rows = edited_df[edited_df["承認"] == True]
            if len(checked_rows) > 0:
                for _, row in checked_rows.iterrows():
                    # 修正内容も保存して承認
                    db.table('Rawdata_NETSUPER_items').update({
                        "product_name": row['product_name'],
                        "product_name_normalized": row['product_name_normalized'],
                        "general_name": row['general_name'],
                        "needs_approval": False
                    }).eq('id', row['id']).execute()
                st.success(f"{len(checked_rows)}件を修正して承認しました")
                # データをリフレッシュ
                st.session_state['refresh_pending_products'] = True
                st.rerun()
            else:
                st.warning("承認する項目を選択してください")


def show_bulk_clustering():
    """一括クラスタリング承認"""
    st.subheader("✅ クラスタ一括承認")
    st.info("Geminiが自動生成したクラスタを確認・承認します")

    # 初期化: セッション状態でデータを保持してリロードを最小化
    if 'clustering_data' not in st.session_state or st.session_state.get('refresh_clustering', False):
        try:
            # データ取得（初回または明示的なリフレッシュ時のみ）
            clusters = db.table('99_tmp_gemini_clustering').select(
                '*'
            ).eq('approval_status', 'pending').execute()

            st.session_state['clustering_data'] = clusters.data
            st.session_state['refresh_clustering'] = False
        except Exception as e:
            st.error(f"データ取得エラー: {e}")
            return

    # セッション状態からデータを取得
    clusters_data = st.session_state.get('clustering_data', [])

    if not clusters_data:
        st.success("承認待ちのクラスタはありません")
        return

    try:
        # カテゴリマスタを取得
        categories = db.table('60_ms_categories').select('id, name').execute()
        category_map = {cat["name"]: cat["id"] for cat in categories.data}

        st.markdown(f"### 全{len(clusters_data)}クラスタ")

        df = pd.DataFrame([{
            "id": c["id"],
            "承認": False,
            "一般名詞": c["general_name"],
            "カテゴリ": c.get("category_name", "食材"),
            "商品数": len(c["product_ids"]),
            "信頼度": f"{c['confidence_avg']:.1%}",
            "商品例": ", ".join(c["product_names"][:3]) + "..."
        } for c in clusters_data])

        edited_df = st.data_editor(
            df,
            column_config={
                "id": st.column_config.TextColumn("ID", disabled=True, width="small"),
                "承認": st.column_config.CheckboxColumn("承認", default=False),
                "一般名詞": st.column_config.TextColumn("一般名詞", width="medium"),
                "カテゴリ": st.column_config.SelectboxColumn(
                    "カテゴリ",
                    options=list(category_map.keys()),
                    width="medium"
                ),
                "商品数": st.column_config.NumberColumn("商品数", format="%d"),
                "信頼度": st.column_config.TextColumn("信頼度", width="small"),
                "商品例": st.column_config.TextColumn("商品例（先頭3件）", width="large")
            },
            hide_index=True,
            use_container_width=True
        )

        if st.button("選択を一括承認", type="primary"):
            approved_rows = edited_df[edited_df["承認"] == True]

            if len(approved_rows) == 0:
                st.warning("承認する項目を選択してください")
            else:
                # 最初の行のカテゴリIDを取得
                category_name = approved_rows.iloc[0]["カテゴリ"]
                category_id = category_map[category_name]

                for _, row in approved_rows.iterrows():
                    # クラスタ情報を取得
                    cluster = next(c for c in clusters_data if c["id"] == row["id"])
                    general_name = cluster["general_name"]
                    product_ids = cluster["product_ids"]
                    product_names = cluster["product_names"]
                    confidence = cluster["confidence_avg"]

                    # Tier 1: 各商品名 → general_name のマッピング
                    for product_name in set(product_names):
                        db.table('MASTER_Product_generalize').upsert({
                            "raw_keyword": product_name,
                            "general_name": general_name,
                            "confidence_score": confidence,
                            "source": "gemini_batch"
                        }, on_conflict="raw_keyword,general_name").execute()

                    # Tier 2: general_name + context → category_id
                    db.table('MASTER_Product_classify').upsert({
                        "general_name": general_name,
                        "source_type": "online_shop",
                        "workspace": "shopping",
                        "doc_type": "online shop",
                        "organization": None,
                        "category_id": category_id,
                        "approval_status": "approved",
                        "confidence_score": confidence
                    }, on_conflict="general_name,source_type,workspace,doc_type,organization").execute()

                    # Rawdata_NETSUPER_itemsを更新
                    for product_id in product_ids:
                        db.table('Rawdata_NETSUPER_items').update({
                            "general_name": general_name,
                            "category_id": category_id,
                            "needs_approval": False,
                            "classification_confidence": confidence
                        }).eq('id', product_id).execute()

                    # クラスタのステータスを更新
                    db.table('99_tmp_gemini_clustering').update({
                        "approval_status": "approved"
                    }).eq('id', row["id"]).execute()

                st.success(f"{len(approved_rows)}件のクラスタを承認しました")
                # データをリフレッシュ
                st.session_state['refresh_clustering'] = True
                st.session_state['refresh_pending_products'] = True  # 商品データも更新
                st.rerun()

    except Exception as e:
        st.error(f"エラー: {e}")


def show_category_tree():
    """カテゴリツリー編集"""
    st.subheader("🌳 カテゴリ管理")
    st.info("1次分類（商品カテゴリー）、2次分類（費目）、名目を管理します")

    # 3つのサブタブ
    cat_tab1, cat_tab2, cat_tab3 = st.tabs([
        "📦 1次分類（商品カテゴリー）",
        "💰 2次分類（費目）",
        "🎯 名目"
    ])

    with cat_tab1:
        show_product_category_management()

    with cat_tab2:
        show_expense_category_management()

    with cat_tab3:
        show_purpose_management()


def show_product_category_management():
    """1次分類（商品カテゴリー）管理"""
    st.markdown("### 📦 1次分類（商品カテゴリー）")
    st.info("商品の物理的カテゴリー（文房具、ゲームソフト、食材など）")

    try:
        # カテゴリ取得
        categories = db.table('MASTER_Categories_product').select('*').order('name').execute()

        if not categories.data:
            st.warning("カテゴリがありません。新規追加してください。")
        else:
            # ツリー構築
            def build_tree(parent_id=None, level=0):
                items = []
                for cat in categories.data:
                    if cat.get("parent_id") == parent_id:
                        items.append({
                            "id": cat["id"],
                            "name": cat["name"],
                            "level": level,
                            "description": cat.get("description", ""),
                            "parent_id": parent_id
                        })
                        items.extend(build_tree(cat["id"], level + 1))
                return items

            tree = build_tree()

            # ツリー表示
            st.markdown("#### 現在のカテゴリツリー")

            for item in tree:
                indent = "　" * item["level"] * 2
                icon = "📁" if item["level"] == 0 else "📄"

                col1, col2 = st.columns([4, 1])
                with col1:
                    desc_text = f" ({item['description']})" if item['description'] else ""
                    st.markdown(f"{indent}{icon} {item['name']}{desc_text}")
                with col2:
                    if st.button("🗑️", key=f"del_prod_{item['id']}", help="削除"):
                        db.table('MASTER_Categories_product').delete().eq('id', item['id']).execute()
                        st.success("削除しました")
                        st.rerun()

        st.divider()

        # 新規追加フォーム
        st.markdown("#### 新規カテゴリ追加")

        col1, col2, col3 = st.columns(3)

        with col1:
            new_name = st.text_input("カテゴリ名", key="new_prod_cat_name", placeholder="例: 野菜")

        with col2:
            parent_options = {"（親なし）": None}
            if categories.data:
                parent_options.update({cat["name"]: cat["id"] for cat in categories.data})
            selected_parent = st.selectbox("親カテゴリ", options=list(parent_options.keys()), key="new_prod_cat_parent")

        with col3:
            new_desc = st.text_input("説明（任意）", key="new_prod_cat_desc", placeholder="例: 生鮮野菜")

        if st.button("追加", type="primary", key="add_prod_cat"):
            if new_name:
                parent_id = parent_options[selected_parent]
                db.table('MASTER_Categories_product').insert({
                    "name": new_name,
                    "parent_id": parent_id,
                    "description": new_desc if new_desc else None
                }).execute()
                st.success(f"カテゴリ「{new_name}」を追加しました")
                st.rerun()
            else:
                st.warning("カテゴリ名を入力してください")

    except Exception as e:
        st.error(f"エラー: {e}")
        import traceback
        st.code(traceback.format_exc())


def show_expense_category_management():
    """2次分類（費目）管理"""
    st.markdown("### 💰 2次分類（費目）")
    st.info("家計簿の費目（食費、教育費、娯楽費など）")

    try:
        # 費目取得
        expense_cats = db.table('MASTER_Categories_expense').select('*').order('display_order').execute()

        if expense_cats.data:
            st.markdown("#### 現在の費目一覧")

            # テーブル表示
            df_data = []
            for cat in expense_cats.data:
                df_data.append({
                    "id": cat["id"],
                    "名前": cat["name"],
                    "説明": cat.get("description", ""),
                    "表示順": cat.get("display_order", 100)
                })

            df = pd.DataFrame(df_data)

            # データエディタで表示・編集
            edited_df = st.data_editor(
                df,
                hide_index=True,
                column_config={
                    "id": None,  # 非表示
                    "名前": st.column_config.TextColumn("名前", width="medium"),
                    "説明": st.column_config.TextColumn("説明", width="large"),
                    "表示順": st.column_config.NumberColumn("表示順", width="small")
                },
                num_rows="dynamic",  # 行の追加・削除を許可
                use_container_width=True,
                key="expense_cat_editor"
            )

            # 更新ボタン
            if st.button("💾 変更を保存", key="save_expense_cats"):
                for idx, row in edited_df.iterrows():
                    cat_id = row.get("id")
                    if cat_id:
                        # 既存データを更新
                        db.table('MASTER_Categories_expense').update({
                            "name": row["名前"],
                            "description": row["説明"],
                            "display_order": int(row["表示順"])
                        }).eq("id", cat_id).execute()
                st.success("変更を保存しました")
                st.rerun()

        st.divider()

        # 新規追加フォーム
        st.markdown("#### 新規費目追加")

        col1, col2, col3 = st.columns(3)

        with col1:
            new_name = st.text_input("費目名", key="new_exp_cat_name", placeholder="例: 娯楽費")

        with col2:
            new_desc = st.text_input("説明（任意）", key="new_exp_cat_desc", placeholder="例: ゲーム、趣味など")

        with col3:
            new_order = st.number_input("表示順", min_value=1, value=100, key="new_exp_cat_order")

        if st.button("追加", type="primary", key="add_exp_cat"):
            if new_name:
                db.table('MASTER_Categories_expense').insert({
                    "name": new_name,
                    "description": new_desc if new_desc else None,
                    "display_order": new_order
                }).execute()
                st.success(f"費目「{new_name}」を追加しました")
                st.rerun()
            else:
                st.warning("費目名を入力してください")

    except Exception as e:
        st.error(f"エラー: {e}")
        import traceback
        st.code(traceback.format_exc())


def show_purpose_management():
    """名目管理"""
    st.markdown("### 🎯 名目")
    st.info("状況に応じて拡張可能な名目（日常、旅行、学校行事など）")

    try:
        # 名目取得
        purposes = db.table('MASTER_Categories_purpose').select('*').order('display_order').execute()

        if purposes.data:
            st.markdown("#### 現在の名目一覧")

            # テーブル表示
            df_data = []
            for purpose in purposes.data:
                df_data.append({
                    "id": purpose["id"],
                    "名前": purpose["name"],
                    "説明": purpose.get("description", ""),
                    "表示順": purpose.get("display_order", 100)
                })

            df = pd.DataFrame(df_data)

            # データエディタで表示・編集
            edited_df = st.data_editor(
                df,
                hide_index=True,
                column_config={
                    "id": None,  # 非表示
                    "名前": st.column_config.TextColumn("名前", width="medium"),
                    "説明": st.column_config.TextColumn("説明", width="large"),
                    "表示順": st.column_config.NumberColumn("表示順", width="small")
                },
                num_rows="dynamic",  # 行の追加・削除を許可
                use_container_width=True,
                key="purpose_editor"
            )

            # 更新ボタン
            if st.button("💾 変更を保存", key="save_purposes"):
                for idx, row in edited_df.iterrows():
                    purpose_id = row.get("id")
                    if purpose_id:
                        # 既存データを更新
                        db.table('MASTER_Categories_purpose').update({
                            "name": row["名前"],
                            "description": row["説明"],
                            "display_order": int(row["表示順"])
                        }).eq("id", purpose_id).execute()
                st.success("変更を保存しました")
                st.rerun()

        st.divider()

        # 新規追加フォーム
        st.markdown("#### 新規名目追加")

        col1, col2, col3 = st.columns(3)

        with col1:
            new_name = st.text_input("名目名", key="new_purpose_name", placeholder="例: 習い事")

        with col2:
            new_desc = st.text_input("説明（任意）", key="new_purpose_desc", placeholder="例: 習い事・塾など")

        with col3:
            new_order = st.number_input("表示順", min_value=1, value=100, key="new_purpose_order")

        if st.button("追加", type="primary", key="add_purpose"):
            if new_name:
                db.table('MASTER_Categories_purpose').insert({
                    "name": new_name,
                    "description": new_desc if new_desc else None,
                    "display_order": new_order
                }).execute()
                st.success(f"名目「{new_name}」を追加しました")
                st.rerun()
            else:
                st.warning("名目名を入力してください")

    except Exception as e:
        st.error(f"エラー: {e}")
        import traceback
        st.code(traceback.format_exc())


def show_rule_management():
    """2次分類決定ルール管理"""
    st.subheader("⚙️ 2次分類決定ルール")
    st.info("名目、人物、1次分類から2次分類（費目）を決定するルールを管理します")

    try:
        # ルール一覧を取得（ビューを使用）
        rules = db.table("v_expense_category_rules").select("*").execute()

        if rules.data:
            st.markdown("### 現在のルール一覧")
            st.caption("優先度が高い順に表示（100=最優先）")

            # テーブル表示
            df_data = []
            for rule in rules.data:
                df_data.append({
                    "id": rule["id"],
                    "名目": rule.get("purpose") or "（任意）",
                    "人物": rule.get("person") or "（任意）",
                    "1次分類": rule.get("product_category") or "（任意）",
                    "→ 費目": rule["expense_category"],
                    "優先度": rule["priority"],
                    "作成者": rule.get("created_by") or "手動"
                })

            df = pd.DataFrame(df_data)

            # データエディタで表示
            st.dataframe(
                df,
                hide_index=True,
                column_config={
                    "id": None,  # 非表示
                    "名目": st.column_config.TextColumn("名目", width="small"),
                    "人物": st.column_config.TextColumn("人物", width="small"),
                    "1次分類": st.column_config.TextColumn("1次分類", width="medium"),
                    "→ 費目": st.column_config.TextColumn("→ 費目", width="medium"),
                    "優先度": st.column_config.NumberColumn("優先度", width="small"),
                    "作成者": st.column_config.TextColumn("作成者", width="small")
                },
                use_container_width=True
            )

            # 削除機能
            st.markdown("#### ルールの削除")
            rule_to_delete = st.selectbox(
                "削除するルールを選択",
                options=[f"{r['名目']} + {r['人物']} + {r['1次分類']} → {r['→ 費目']}" for r in df_data],
                key="rule_to_delete"
            )

            if st.button("🗑️ 選択したルールを削除", key="delete_rule"):
                # 選択されたルールのIDを取得
                selected_idx = [f"{r['名目']} + {r['人物']} + {r['1次分類']} → {r['→ 費目']}" for r in df_data].index(rule_to_delete)
                rule_id = df_data[selected_idx]["id"]

                db.table("MASTER_Rules_expense_mapping").delete().eq("id", rule_id).execute()
                st.success("ルールを削除しました")
                st.rerun()

        st.divider()

        # 新規ルール追加フォーム
        st.markdown("### 新規ルール追加")

        # 選択肢を取得
        purposes = db.table("MASTER_Categories_purpose").select("id, name").order("display_order").execute()
        purpose_options = {"（任意）": None}
        if purposes.data:
            purpose_options.update({p["name"]: p["id"] for p in purposes.data})

        product_cats = db.table("MASTER_Categories_product").select("id, name").order("name").execute()
        product_cat_options = {"（任意）": None}
        if product_cats.data:
            product_cat_options.update({c["name"]: c["id"] for c in product_cats.data})

        expense_cats = db.table("MASTER_Categories_expense").select("id, name").order("display_order").execute()
        expense_cat_options = {}
        if expense_cats.data:
            expense_cat_options.update({c["name"]: c["id"] for c in expense_cats.data})

        person_options_list = ["（任意）", "家族", "パパ", "ママ", "絵麻", "育哉"]

        # フォーム
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            selected_purpose = st.selectbox("名目", options=list(purpose_options.keys()), key="new_rule_purpose")

        with col2:
            selected_person = st.selectbox("人物", options=person_options_list, key="new_rule_person")

        with col3:
            selected_product_cat = st.selectbox("1次分類", options=list(product_cat_options.keys()), key="new_rule_product_cat")

        with col4:
            selected_expense_cat = st.selectbox("→ 費目（必須）", options=list(expense_cat_options.keys()), key="new_rule_expense_cat")

        # 優先度を自動計算
        priority = 50  # デフォルト
        if selected_purpose != "（任意）" and selected_person != "（任意）" and selected_product_cat != "（任意）":
            priority = 80  # 全て指定
        elif selected_purpose != "（任意）" and (selected_person != "（任意）" or selected_product_cat != "（任意）"):
            priority = 90  # 名目 + (人物 or 1次分類)
        elif selected_purpose != "（任意）":
            priority = 100  # 名目のみ
        elif selected_person != "（任意）" and selected_product_cat != "（任意）":
            priority = 50  # 人物 + 1次分類
        elif selected_product_cat != "（任意）":
            priority = 30  # 1次分類のみ

        st.info(f"優先度: {priority} （自動計算）")

        if st.button("➕ ルールを追加", type="primary", key="add_rule"):
            if not selected_expense_cat or selected_expense_cat not in expense_cat_options:
                st.warning("費目を選択してください")
            else:
                # ルールを挿入
                purpose_id = purpose_options[selected_purpose]
                person_value = None if selected_person == "（任意）" else selected_person
                product_cat_id = product_cat_options[selected_product_cat]
                expense_cat_id = expense_cat_options[selected_expense_cat]

                db.table("MASTER_Rules_expense_mapping").insert({
                    "purpose_id": purpose_id,
                    "person": person_value,
                    "product_category_id": product_cat_id,
                    "expense_category_id": expense_cat_id,
                    "priority": priority,
                    "created_by": "manual"
                }).execute()

                st.success(f"ルールを追加しました: {selected_purpose} + {selected_person} + {selected_product_cat} → {selected_expense_cat}")
                st.rerun()

    except Exception as e:
        st.error(f"エラー: {e}")
        import traceback
        st.code(traceback.format_exc())


def show_approved_products_search():
    """承認済み商品の検索・編集"""
    st.subheader("🔍 承認済み商品の検索・編集")
    st.info("承認済み商品を検索して修正できます")

    try:
        # 検索フィルター
        st.markdown("### 検索条件")

        col1, col2, col3 = st.columns(3)

        with col1:
            # 店舗フィルター
            stores_result = db.table('Rawdata_NETSUPER_items').select('organization').execute()
            unique_stores = sorted(list(set([p.get('organization', '') for p in stores_result.data if p.get('organization')])))
            selected_store = st.selectbox("店舗", options=["全て"] + unique_stores)

        with col2:
            # 商品名検索
            search_text = st.text_input("商品名（部分一致）")

        with col3:
            # カテゴリフィルター
            categories_result = db.table('60_ms_categories').select('id, name').execute()
            category_options = {"全て": None}
            category_options.update({cat["name"]: cat["id"] for cat in categories_result.data})
            selected_category = st.selectbox("カテゴリ", options=list(category_options.keys()))

        # 検索ボタン
        if st.button("🔍 検索", type="primary"):
            # クエリ構築
            query = db.table('Rawdata_NETSUPER_items').select(
                'id, product_name, product_name_normalized, general_name, category_id, organization, classification_confidence'
            ).eq('needs_approval', False)  # 承認済みのみ

            # 店舗フィルター
            if selected_store != "全て":
                query = query.eq('organization', selected_store)

            # カテゴリフィルター
            if selected_category != "全て":
                query = query.eq('category_id', category_options[selected_category])

            # 商品名検索（部分一致）
            if search_text:
                query = query.ilike('product_name', f'%{search_text}%')

            # 実行
            results = query.limit(100).execute()

            if not results.data:
                st.warning("該当する商品が見つかりませんでした")
                return

            st.success(f"{len(results.data)}件見つかりました（最大100件表示）")

            # 結果表示・編集
            st.markdown("### 検索結果")

            df = pd.DataFrame([{
                "id": p["id"],
                "選択": False,
                "product_name": p.get("product_name", ""),
                "product_name_normalized": p.get("product_name_normalized", ""),
                "general_name": p.get("general_name", ""),
                "店舗": p.get("organization", ""),
                "信頼度": f"{p.get('classification_confidence', 0):.1%}" if p.get('classification_confidence') else "—"
            } for p in results.data])

            edited_df = st.data_editor(
                df,
                column_config={
                    "選択": st.column_config.CheckboxColumn("選択", default=False, width="small"),
                    "product_name": st.column_config.TextColumn("product_name", width="large", disabled=False),
                    "product_name_normalized": st.column_config.TextColumn("product_name_normalized", width="large", disabled=False),
                    "general_name": st.column_config.TextColumn("general_name", width="medium", disabled=False),
                    "店舗": st.column_config.TextColumn("店舗", width="medium", disabled=True),
                    "信頼度": st.column_config.TextColumn("信頼度", width="small", disabled=True)
                },
                column_order=["選択", "product_name", "product_name_normalized", "general_name", "店舗", "信頼度"],
                hide_index=True,
                use_container_width=True,
                key="approved_products_table"
            )

            # 修正保存ボタン
            if st.button("💾 選択した商品の修正を保存"):
                checked_rows = edited_df[edited_df["選択"] == True]
                if len(checked_rows) > 0:
                    for _, row in checked_rows.iterrows():
                        db.table('Rawdata_NETSUPER_items').update({
                            "product_name": row['product_name'],
                            "product_name_normalized": row['product_name_normalized'],
                            "general_name": row['general_name']
                        }).eq('id', row['id']).execute()
                    st.success(f"{len(checked_rows)}件の修正を保存しました")
                    st.rerun()
                else:
                    st.warning("保存する商品を選択してください")

    except Exception as e:
        st.error(f"エラー: {e}")


if __name__ == "__main__":
    main()
```

### shared\kakeibo\schema.sql

```sql
-- ========================================
-- 家計簿システム データベーススキーマ
-- ========================================

-- 必要な拡張機能を有効化
CREATE EXTENSION IF NOT EXISTS "btree_gist";  -- UUID の GIST インデックス用
CREATE EXTENSION IF NOT EXISTS "pg_trgm";     -- テキスト検索用

-- 1. シチュエーション（文脈）マスタ
CREATE TABLE IF NOT EXISTS money_situations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name TEXT UNIQUE NOT NULL,  -- '日常', '家族旅行', '出張', '教育'
    description TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- 2. イベント期間定義
CREATE TABLE IF NOT EXISTS money_events (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name TEXT NOT NULL,  -- '沖縄旅行 2024夏'
    start_date DATE NOT NULL,
    end_date DATE NOT NULL,
    situation_id UUID REFERENCES money_situations(id),
    description TEXT,
    created_at TIMESTAMP DEFAULT NOW(),

    -- 期間の重複チェック制約（同一シチュエーション内）
    CONSTRAINT no_overlapping_events EXCLUDE USING gist (
        daterange(start_date, end_date, '[]') WITH &&,
        situation_id WITH =
    )
);

-- 3. カテゴリ（費目）マスタ
CREATE TABLE IF NOT EXISTS money_categories (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name TEXT UNIQUE NOT NULL,  -- '食費', '交通費', 'カード引落'
    is_expense BOOLEAN DEFAULT TRUE,  -- FALSE = 移動（集計対象外）
    parent_id UUID REFERENCES money_categories(id),  -- 階層構造用
    created_at TIMESTAMP DEFAULT NOW()
);

-- 4. 商品辞書（正規化ルール）
CREATE TABLE IF NOT EXISTS money_product_dictionary (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    raw_keyword TEXT NOT NULL,  -- レシート上の表記（例: 'ｷﾞｭｳﾆｭｳ'）
    official_name TEXT NOT NULL,  -- 正式名称（例: '牛乳'）
    category_id UUID REFERENCES money_categories(id),
    tax_rate INTEGER DEFAULT 10,  -- 8 or 10 (%)
    notes TEXT,
    created_at TIMESTAMP DEFAULT NOW(),

    -- 複合ユニーク制約
    UNIQUE(raw_keyword, official_name)
);

-- インデックス（部分一致検索用）
CREATE INDEX IF NOT EXISTS idx_product_raw_keyword ON money_product_dictionary USING gin(raw_keyword gin_trgm_ops);

-- 5. エイリアステーブル（表記ゆれ吸収）
CREATE TABLE IF NOT EXISTS money_aliases (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    input_word TEXT UNIQUE NOT NULL,  -- 間違った表記
    correct_word TEXT NOT NULL,  -- 正しい表記
    created_at TIMESTAMP DEFAULT NOW()
);

-- 6. トランザクション（明細）
CREATE TABLE IF NOT EXISTS money_transactions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    -- 基本情報
    transaction_date DATE NOT NULL,
    shop_name TEXT NOT NULL,

    -- 商品情報
    product_name TEXT NOT NULL,
    quantity INTEGER DEFAULT 1,
    unit_price INTEGER NOT NULL,  -- 単価（税込）
    total_amount INTEGER NOT NULL,  -- 合計（税込）

    -- 分類
    category_id UUID REFERENCES money_categories(id),
    situation_id UUID REFERENCES money_situations(id),

    -- メタデータ
    image_path TEXT,  -- Google Drive上のパス
    drive_file_id TEXT,
    notes TEXT,

    -- OCR処理情報
    ocr_model TEXT,  -- 使用したGeminiモデル（gemini-2.5-flash / gemini-2.5-flash-lite）
    source_folder TEXT,  -- ソースフォルダ（INBOX_EASY / INBOX_HARD）

    -- 処理情報
    is_verified BOOLEAN DEFAULT FALSE,  -- 手動確認済みフラグ
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- インデックス
CREATE INDEX IF NOT EXISTS idx_transactions_date ON money_transactions(transaction_date);
CREATE INDEX IF NOT EXISTS idx_transactions_shop ON money_transactions USING gin(shop_name gin_trgm_ops);
CREATE INDEX IF NOT EXISTS idx_transactions_situation ON money_transactions(situation_id);

-- 7. 画像処理ログ（重複防止）
CREATE TABLE IF NOT EXISTS money_image_processing_log (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    file_name TEXT UNIQUE NOT NULL,  -- '20241027_001.jpg'
    drive_file_id TEXT,

    status TEXT NOT NULL,  -- 'success', 'failed', 'manual_review', 'duplicate_receipt'
    error_message TEXT,

    transaction_ids UUID[],  -- 生成された明細のID配列

    ocr_model TEXT,  -- 使用したGeminiモデル名

    processed_at TIMESTAMP DEFAULT NOW(),
    retry_count INTEGER DEFAULT 0
);

CREATE INDEX IF NOT EXISTS idx_processing_log_status ON money_image_processing_log(status);

-- ========================================
-- 初期マスタデータ投入
-- ========================================

-- シチュエーション
INSERT INTO money_situations (name, description) VALUES
    ('日常', '通常の生活費'),
    ('家族旅行', '家族での旅行・レジャー'),
    ('出張', '仕事関連の出張'),
    ('教育', '子どもの教育関連費用')
ON CONFLICT (name) DO NOTHING;

-- カテゴリ
INSERT INTO money_categories (name, is_expense) VALUES
    ('食費', TRUE),
    ('日用品', TRUE),
    ('交通費', TRUE),
    ('医療費', TRUE),
    ('娯楽費', TRUE),
    ('カード引落', FALSE),  -- 集計対象外
    ('移動', FALSE)  -- 口座間移動など
ON CONFLICT (name) DO NOTHING;

-- ========================================
-- ビュー（集計用）
-- ========================================

-- 日次集計ビュー
CREATE OR REPLACE VIEW v_daily_summary AS
SELECT
    transaction_date,
    s.name AS situation,
    c.name AS category,
    COUNT(*) AS item_count,
    SUM(total_amount) AS total
FROM money_transactions t
LEFT JOIN money_situations s ON t.situation_id = s.id
LEFT JOIN money_categories c ON t.category_id = c.id
WHERE c.is_expense = TRUE  -- 集計対象のみ
GROUP BY transaction_date, s.name, c.name
ORDER BY transaction_date DESC;

-- 月次集計ビュー
CREATE OR REPLACE VIEW v_monthly_summary AS
SELECT
    DATE_TRUNC('month', transaction_date) AS month,
    s.name AS situation,
    c.name AS category,
    COUNT(*) AS item_count,
    SUM(total_amount) AS total
FROM money_transactions t
LEFT JOIN money_situations s ON t.situation_id = s.id
LEFT JOIN money_categories c ON t.category_id = c.id
WHERE c.is_expense = TRUE
GROUP BY month, s.name, c.name
ORDER BY month DESC;

-- ========================================
-- RLS (Row Level Security) 設定
-- ========================================
-- 必要に応じて有効化

-- ALTER TABLE money_transactions ENABLE ROW LEVEL SECURITY;
-- CREATE POLICY "Allow authenticated users to view transactions"
--     ON money_transactions FOR SELECT
--     TO authenticated
--     USING (true);
```

### shared\kakeibo\transaction_processor.py

```py
"""
トランザクション処理モジュール

- Gemini OCRの結果を正規化
- エイリアス変換・辞書マッチング
- シチュエーション設定（デフォルト: 日常）
- Supabaseへの登録
"""

import re
from datetime import datetime, date
from typing import Dict, List, Optional
from supabase import create_client, Client
from loguru import logger

from .config import SUPABASE_URL, SUPABASE_KEY
from shared.ai.llm_client.llm_client import LLMClient


class TransactionProcessor:
    """トランザクション処理クラス"""

    def __init__(self):
        self.db: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        self.llm_client = LLMClient()  # AI一般名詞抽出用

        # マスタデータをキャッシュ
        self.aliases = self._load_aliases()
        self.product_dict = self._load_product_dictionary()
        self.product_generalize = self._load_product_generalize()
        self.situations = self._load_situations()
        self.categories = self._load_categories()

    def process(
        self,
        ocr_result: Dict,
        file_name: str,
        drive_file_id: str,
        model_name: str = "gemini-2.5-flash",
        source_folder: str = "INBOX"
    ) -> Dict:
        """
        OCR結果を処理してDBに登録（3層構造対応）

        Args:
            ocr_result: Gemini OCRの結果
            file_name: ファイル名
            drive_file_id: Google DriveのファイルID
            model_name: 使用したGeminiモデル名
            source_folder: ソースフォルダ（INBOX_EASY/INBOX_HARD）

        Returns:
            Dict: 処理結果
                - success: {"receipt_id": "...", "transaction_ids": [...]}
                - error: {"error": "...", "message": "..."}
        """
        try:
            # エラーチェック
            if "error" in ocr_result:
                self._log_processing_error(file_name, drive_file_id, ocr_result, model_name, None)
                return ocr_result

            # トランザクション日付
            trans_date = datetime.strptime(ocr_result["transaction_date"], "%Y-%m-%d").date()

            # 1. レシート情報を登録（親テーブル）
            receipt_id = self._insert_receipt(
                ocr_result=ocr_result,
                file_name=file_name,
                drive_file_id=drive_file_id,
                model_name=model_name,
                source_folder=source_folder
            )

            # 2. イベント期間判定 → シチュエーション確定
            situation_id = self._determine_situation(trans_date)

            # 3. 各商品を正規化（税率決定のみ、税額はまだ計算しない）
            # 注意: 小計・合計行は税額按分の対象外なので、ITEM行のみ処理
            normalized_items = []
            for item in ocr_result["items"]:
                # SUBTOTAL/TOTAL行はスキップ（税額按分の対象外）
                line_type = item.get("line_type", "ITEM")
                if line_type in ["SUBTOTAL", "TOTAL"]:
                    # 小計・合計行はそのまま保持（税額按分しない）
                    # 商品名を取得（nullや空文字列の場合は代替値を使用）
                    subtotal_name = item.get("product_name") or item.get("line_text") or item.get("ocr_raw_text") or "小計"
                    if not subtotal_name or not subtotal_name.strip():
                        subtotal_name = "小計" if line_type == "SUBTOTAL" else "合計"

                    normalized_items.append({
                        "raw_item": item,
                        "normalized": {
                            "product_name": subtotal_name,
                            "general_name": None,  # 小計/合計行は一般名詞なし
                            "category_id": None,
                            "tax_rate": item.get("tax_rate", 10),
                            "tax_rate_fixed": False,
                            "tax_amount": item.get("tax_amount", 0)  # レシート記載の税額をそのまま使用
                        }
                    })
                    continue

                normalized = self._normalize_item(item, ocr_result["shop_name"], ocr_result.get("tax_summary"))
                normalized_items.append({
                    "raw_item": item,
                    "normalized": normalized
                })

            # 4. 税額を按分計算
            items_with_tax = self._calculate_and_distribute_tax(
                normalized_items,
                ocr_result.get("tax_summary")
            )

            # 5. 各明細を登録（OCRデータ + 標準化データを統合）
            transaction_ids = []

            for line_num, item_data in enumerate(items_with_tax, start=1):
                item = item_data["raw_item"]
                normalized = item_data["normalized"]

                # Rawdata_RECEIPT_items: OCRデータ + 標準化データを同時に保存
                trans_id = self._insert_transaction(
                    receipt_id=receipt_id,
                    line_number=line_num,
                    ocr_raw_text=item.get("ocr_raw", item["product_name"]),  # OCR原文
                    ocr_confidence=item.get("confidence", None),
                    product_name=normalized["product_name"],
                    item_name=None,  # 将来的にOCRから取得
                    unit_price=item.get("unit_price"),
                    quantity=item.get("quantity", 1),
                    marks_text=item.get("tax_mark"),  # 税率マーク
                    discount_text=item.get("discount_text"),  # 割引情報
                    # 標準化データも同時に保存
                    normalized=normalized,
                    situation_id=situation_id,
                    total_amount=item["total_amount"],
                    tax_amount=normalized["tax_amount"],
                    needs_review=item_data.get("needs_review", False)
                )
                transaction_ids.append(trans_id)

            # 6. 処理ログ記録（receipt_idとocr_resultも保存）
            processing_log_id = self._log_processing_success(
                file_name=file_name,
                drive_file_id=drive_file_id,
                receipt_id=receipt_id,
                transaction_ids=transaction_ids,
                ocr_result=ocr_result,
                model_name=model_name
            )

            # 7. 税額サマリー保存
            if "tax_summary" in ocr_result:
                self._save_tax_summary(
                    receipt_id=receipt_id,
                    processing_log_id=processing_log_id,
                    tax_summary=ocr_result["tax_summary"],
                    items_with_tax=items_with_tax
                )

            logger.info(f"Processed receipt {receipt_id} with {len(transaction_ids)} transactions from {file_name} using {model_name}")

            return {
                "success": True,
                "receipt_id": receipt_id,
                "transaction_ids": transaction_ids
            }

        except Exception as e:
            logger.error(f"Failed to process {file_name}: {e}")
            self._log_processing_error(file_name, drive_file_id, {"error": str(e)}, model_name, None)
            return {
                "error": "processing_failed",
                "message": str(e)
            }

    def _normalize_item(self, item: Dict, shop_name: str, tax_summary: Dict = None) -> Dict:
        """
        商品情報を正規化（税率のみ決定、税額は後で按分計算）

        Priority:
        1. レシート全体の税率情報（最優先）
        2. tax_markから税率を判定
        3. エイリアス変換
        4. 商品辞書マッチング（税率も取得）
        5. そのまま使用（Geminiの推測税率を使用）

        Args:
            item: {"product_name": "...", "tax_rate": 10, "tax_mark": "※", ...}
            shop_name: 店舗名
            tax_summary: レシート全体の税額情報（税率判定に使用）

        Returns:
            Dict: {"product_name": "正規化後", "category_id": "...", "tax_rate": 10, "tax_rate_fixed": True/False}
        """
        # 商品名を取得（nullや空文字列の場合は代替値を使用）
        product_name = item.get("product_name") or item.get("line_text") or item.get("ocr_raw_text") or "不明"
        # 空文字列の場合は「不明」に
        if not product_name or not product_name.strip():
            product_name = "不明"

        gemini_tax_rate = item.get("tax_rate", 10)  # Geminiの推測税率（デフォルト10%）

        # レシート全体の税率情報を確認（最優先）
        receipt_level_tax_rate = None
        if tax_summary:
            tax_8_amount = tax_summary.get("tax_8_amount") or 0
            tax_10_amount = tax_summary.get("tax_10_amount") or 0

            # 8%のみの場合
            if tax_8_amount > 0 and tax_10_amount == 0:
                receipt_level_tax_rate = 8
                logger.debug(f"Receipt has only 8% tax, setting all items to 8%")
            # 10%のみの場合
            elif tax_10_amount > 0 and tax_8_amount == 0:
                receipt_level_tax_rate = 10
                logger.debug(f"Receipt has only 10% tax, setting all items to 10%")
            # 混在の場合は個別判定に進む

        # レシート全体の税率が判定できた場合はそれを使用（最優先）
        if receipt_level_tax_rate is not None:
            return {
                "product_name": product_name,
                "category_id": None,
                "tax_rate": receipt_level_tax_rate,
                "tax_rate_fixed": True  # レシート全体の税率は確定
            }

        # 1. tax_markから税率を判定
        tax_mark = item.get("tax_mark")
        tax_rate_from_mark = None

        # 商品名から税率パターンを検出（「外8」「内8」などのレシート記載パターン）
        product_name_lower = product_name.lower()
        if "外8" in product_name or "内8" in product_name or "外 8" in product_name or "内 8" in product_name:
            tax_rate_from_mark = 8
            # 商品名から税率パターンを削除
            product_name = product_name.replace("外8", "").replace("内8", "").replace("外 8", "").replace("内 8", "").strip()
            # 空文字列になった場合は元の商品名を保持（削除しない）
            if not product_name:
                product_name = item["product_name"]  # 元の商品名に戻す
        elif "外10" in product_name or "内10" in product_name or "外 10" in product_name or "内 10" in product_name:
            tax_rate_from_mark = 10
            # 商品名から税率パターンを削除
            product_name = product_name.replace("外10", "").replace("内10", "").replace("外 10", "").replace("内 10", "").strip()
            # 空文字列になった場合は元の商品名を保持（削除しない）
            if not product_name:
                product_name = item["product_name"]  # 元の商品名に戻す

        # tax_markフィールドからも判定
        if tax_mark:
            # 8%マークの判定（複数パターン対応）
            if (
                tax_mark in ["*", "※", "◆"] or  # よくある軽減税率マーク
                "8%" in str(tax_mark) or
                "8" in str(tax_mark) or
                "(軽)" in str(tax_mark) or
                "外8" in str(tax_mark) or  # 外税8%のパターン
                "内8" in str(tax_mark)  # 内税8%のパターン
            ):
                tax_rate_from_mark = 8
            # 10%マークの判定
            elif (
                tax_mark in ["★", "☆"] or  # よくある標準税率マーク
                "10%" in str(tax_mark) or
                "10" in str(tax_mark) or
                "外10" in str(tax_mark) or  # 外税10%のパターン
                "内10" in str(tax_mark)  # 内税10%のパターン
            ):
                tax_rate_from_mark = 10

        # 2. エイリアス変換
        product_name = self.aliases.get(product_name.lower(), product_name)

        # 3. 商品辞書マッチング
        for entry in self.product_dict:
            if entry["raw_keyword"].lower() in product_name.lower():
                # 辞書に登録されている税率を優先（確定）
                # ただし、tax_markがあればそちらを最優先
                # 一般名詞を取得
                general_name = self._get_general_name(entry["official_name"])

                return {
                    "product_name": entry["official_name"],
                    "general_name": general_name,
                    "category_id": entry["category_id"],
                    "tax_rate": tax_rate_from_mark if tax_rate_from_mark else entry.get("tax_rate", 10),
                    "tax_rate_fixed": True  # 辞書由来の税率は確定
                }

        # 4. マッチしなければtax_markまたはGeminiの推測を使用（暫定）
        # 一般名詞を取得（正規化後の商品名から）
        general_name = self._get_general_name(product_name)

        return {
            "product_name": product_name,
            "general_name": general_name,
            "category_id": None,
            "tax_rate": tax_rate_from_mark if tax_rate_from_mark else gemini_tax_rate,
            "tax_rate_fixed": bool(tax_rate_from_mark)  # tax_markがあれば確定
        }

    def _determine_situation(self, trans_date: date) -> str:
        """
        取引日からシチュエーションを判定

        Args:
            trans_date: 取引日

        Returns:
            str: situation_id（常に「日常」を返す。必要に応じて手動でpurposeカラムを設定）
        """
        # デフォルトは「日常」を返す
        default_situation = next(
            (s for s in self.situations if s["name"] == "日常"),
            self.situations[0] if self.situations else None
        )

        if default_situation:
            return default_situation["id"]
        else:
            logger.warning("「日常」シチュエーションが見つかりません。最初のシチュエーションを使用します。")
            return self.situations[0]["id"] if self.situations else None

    def _insert_receipt(self, ocr_result: Dict, file_name: str, drive_file_id: str, model_name: str, source_folder: str) -> str:
        """レシート情報をDBに登録（親テーブル）"""
        trans_date = datetime.strptime(ocr_result["transaction_date"], "%Y-%m-%d").date()

        # レシートの合計金額を計算（複数のフィールドから取得を試みる）
        total_amount = 0
        for item in ocr_result.get("items", []):
            # total_amount, amount, displayed_amount のいずれかを使用
            item_amount = item.get("total_amount") or item.get("amount") or item.get("displayed_amount") or 0
            total_amount += item_amount

        # 税額サマリーから税抜小計を計算
        tax_summary = ocr_result.get("tax_summary", {})
        subtotal_amount = None
        if tax_summary:
            # 8%税抜 + 10%税抜
            subtotal_8 = tax_summary.get("tax_8_subtotal", 0)
            subtotal_10 = tax_summary.get("tax_10_subtotal", 0)
            if subtotal_8 or subtotal_10:
                subtotal_amount = subtotal_8 + subtotal_10

        receipt_data = {
            "transaction_date": ocr_result["transaction_date"],
            "shop_name": ocr_result["shop_name"],
            "total_amount_check": ocr_result.get("total") or total_amount or 0,  # nullを許容しない
            "subtotal_amount": subtotal_amount,
            "image_path": f"99_Archive/{trans_date.strftime('%Y-%m')}/{file_name}",
            "drive_file_id": drive_file_id,
            "source_folder": source_folder,
            "ocr_model": model_name,
            "workspace": "household",
            "is_verified": False
        }

        result = self.db.table("Rawdata_RECEIPT_shops").insert(receipt_data).execute()
        return result.data[0]["id"]

    def _insert_transaction(self, receipt_id: str, line_number: int, ocr_raw_text: str,
                           ocr_confidence: float, product_name: str, item_name: str,
                           unit_price: int, quantity: int, marks_text: str = None,
                           discount_text: str = None, normalized: Dict = None,
                           situation_id: str = None, total_amount: int = None,
                           tax_amount: int = None, needs_review: bool = False) -> str:
        """トランザクション（明細行）+ 標準化データをDBに登録（統合テーブル）"""
        trans_data = {
            "receipt_id": receipt_id,
            "line_number": line_number,
            "line_type": "ITEM",  # 将来的にOCRで判定
            "ocr_raw_text": ocr_raw_text,
            "ocr_confidence": ocr_confidence,
            "product_name": product_name,
            "item_name": item_name,
            "unit_price": unit_price,
            "quantity": quantity,
            "marks_text": marks_text,  # 税率マーク（※、★、8%、10%など）
            "discount_text": discount_text  # 割引情報
        }

        # 標準化データがあれば追加
        if normalized:
            # 7要素構造のデータを取得
            base_price = normalized.get("base_price")  # 本体価（税抜）

            # 本体単価を計算（本体価 ÷ 数量）
            std_unit_price = None
            if base_price is not None and quantity and quantity > 0:
                std_unit_price = base_price // quantity  # 整数除算

            trans_data.update({
                "official_name": normalized.get("official_name"),
                "general_name": normalized.get("general_name"),  # 一般名詞
                "category_id": normalized.get("category_id"),
                "situation_id": situation_id,
                "tax_rate": normalized["tax_rate"],
                "std_amount": total_amount,  # 税込価
                "std_unit_price": std_unit_price,  # 本体単価
                "tax_amount": tax_amount,
                "needs_review": needs_review
            })

        result = self.db.table("Rawdata_RECEIPT_items").insert(trans_data).execute()
        return result.data[0]["id"]

    def _log_processing_success(self, file_name: str, drive_file_id: str, receipt_id: str, transaction_ids: List[str], ocr_result: Dict = None, model_name: str = None) -> str:
        """処理成功をログに記録"""
        log_data = {
            "file_name": file_name,
            "drive_file_id": drive_file_id,
            "receipt_id": receipt_id,
            "status": "success",
            "transaction_ids": transaction_ids
        }
        if model_name:
            log_data["ocr_model"] = model_name
        if ocr_result:
            log_data["ocr_result"] = ocr_result

        result = self.db.table("99_lg_image_proc_log").insert(log_data).execute()
        return result.data[0]["id"]

    def _log_processing_error(self, file_name: str, drive_file_id: str, error_info: Dict, model_name: str = None, receipt_id: str = None):
        """処理エラーをログに記録"""
        log_data = {
            "file_name": file_name,
            "drive_file_id": drive_file_id,
            "status": "failed",
            "error_message": error_info.get("message", error_info.get("error"))
        }
        if model_name:
            log_data["ocr_model"] = model_name
        if receipt_id:
            log_data["receipt_id"] = receipt_id

        self.db.table("99_lg_image_proc_log").insert(log_data).execute()

    def _calculate_and_distribute_tax(self, normalized_items: List[Dict], tax_summary: Dict) -> List[Dict]:
        """
        税額を按分計算

        重要な前提：
        - 税込合計額（支払い金額）が絶対に正しい
        - レシート記載の8%/10%対象税込額から税額を逆算
        - 各商品に税額を按分（端数は最初の商品に加算）

        Args:
            normalized_items: 正規化済み商品リスト
            tax_summary: レシート記載の税額サマリー

        Returns:
            List[Dict]: 税額が計算された商品リスト
        """
        # 商品を8%と10%にグループ化
        # 注意: SUBTOTAL/TOTAL行は税額按分の対象外
        items_8 = []
        items_10 = []

        for item_data in normalized_items:
            # SUBTOTAL/TOTAL行はスキップ
            line_type = item_data["raw_item"].get("line_type", "ITEM")
            if line_type in ["SUBTOTAL", "TOTAL"]:
                continue

            if item_data["normalized"]["tax_rate"] == 8:
                items_8.append(item_data)
            else:
                items_10.append(item_data)

        # 各グループの税込合計を計算
        total_8 = sum(item["raw_item"]["total_amount"] for item in items_8)
        total_10 = sum(item["raw_item"]["total_amount"] for item in items_10)

        needs_review = False

        # tax_summaryがある場合は整合性チェック
        if tax_summary:
            receipt_total_8 = tax_summary.get("tax_8_subtotal", 0) + tax_summary.get("tax_8_amount", 0)
            receipt_total_10 = tax_summary.get("tax_10_subtotal", 0) + tax_summary.get("tax_10_amount", 0)

            # 整合性チェック（±数円程度の誤差は許容）
            if abs(total_8 - receipt_total_8) > 5:
                logger.warning(f"8% total mismatch: calculated={total_8}, receipt={receipt_total_8}")
                needs_review = True

            if abs(total_10 - receipt_total_10) > 5:
                logger.warning(f"10% total mismatch: calculated={total_10}, receipt={receipt_total_10}")
                needs_review = True

            # レシート記載値を使用
            if receipt_total_8 > 0:
                total_8 = receipt_total_8
            if receipt_total_10 > 0:
                total_10 = receipt_total_10

        # 税額を計算（グループ全体で）
        # 8%税額 = 8%対象税込額 × (8/108)
        # 10%税額 = 10%対象税込額 × (10/110)
        tax_8_total = round(total_8 * 8 / 108) if total_8 > 0 else 0
        tax_10_total = round(total_10 * 10 / 110) if total_10 > 0 else 0

        # 各商品に税額を按分
        self._distribute_tax_to_items(items_8, tax_8_total)
        self._distribute_tax_to_items(items_10, tax_10_total)

        # needs_reviewフラグを設定
        result = normalized_items
        if needs_review:
            for item in result:
                item["needs_review"] = True

        return result

    def _distribute_tax_to_items(self, items: List[Dict], total_tax: int):
        """
        税額を各商品に按分（端数は最初の商品に加算）

        例：5円の商品×2、税額1円の場合
        - 商品1: 1円
        - 商品2: 0円

        Args:
            items: 商品リスト
            total_tax: 配分する税額合計
        """
        if not items or total_tax == 0:
            for item in items:
                item["normalized"]["tax_amount"] = 0
            return

        # 各商品の税込額の比率で按分
        total_amount = sum(item["raw_item"]["total_amount"] for item in items)

        distributed_tax = []
        for item in items:
            item_amount = item["raw_item"]["total_amount"]
            # 比率で按分（切り捨て）
            tax = int(total_tax * item_amount / total_amount)
            distributed_tax.append(tax)

        # 端数を計算
        remainder = total_tax - sum(distributed_tax)

        # 端数を最初の商品に加算
        if remainder != 0 and len(distributed_tax) > 0:
            distributed_tax[0] += remainder

        # 各商品に税額を設定
        for i, item in enumerate(items):
            item["normalized"]["tax_amount"] = distributed_tax[i]

        logger.debug(f"Distributed {total_tax}円 tax: {distributed_tax} (remainder={remainder})")

    def _save_tax_summary(self, receipt_id: str, processing_log_id: str, tax_summary: Dict, items_with_tax: List[Dict]):
        """
        税額サマリーを保存

        Args:
            receipt_id: レシートID
            processing_log_id: 処理ログID
            tax_summary: レシート記載の税額サマリー
            items_with_tax: 税額計算済み商品リスト
        """
        # 計算した税額を集計
        calculated_tax_8 = sum(
            item["normalized"]["tax_amount"]
            for item in items_with_tax
            if item["normalized"]["tax_rate"] == 8
        )
        calculated_tax_10 = sum(
            item["normalized"]["tax_amount"]
            for item in items_with_tax
            if item["normalized"]["tax_rate"] == 10
        )

        # レシート記載の税額
        actual_tax_8 = tax_summary.get("tax_8_amount", 0)
        actual_tax_10 = tax_summary.get("tax_10_amount", 0)

        # 差分計算（理論的には±1円以内になるはず）
        tax_8_diff = calculated_tax_8 - actual_tax_8 if actual_tax_8 > 0 else 0
        tax_10_diff = calculated_tax_10 - actual_tax_10 if actual_tax_10 > 0 else 0

        # 整合性フラグ（±1円以内なら一致とみなす）
        matches = abs(tax_8_diff) <= 1 and abs(tax_10_diff) <= 1

        # サマリー保存
        summary_data = {
            "receipt_id": receipt_id,
            "tax_8_subtotal": tax_summary.get("tax_8_subtotal"),
            "tax_8_amount": actual_tax_8,
            "tax_10_subtotal": tax_summary.get("tax_10_subtotal"),
            "tax_10_amount": actual_tax_10,
            "total_amount": tax_summary.get("total_amount"),
            "calculated_tax_8_amount": calculated_tax_8,
            "calculated_tax_10_amount": calculated_tax_10,
            "calculated_matches_actual": matches,
            "tax_8_diff": tax_8_diff,
            "tax_10_diff": tax_10_diff
        }

        self.db.table("60_ag_receipt_summary").insert(summary_data).execute()

        if matches:
            logger.info(f"Tax calculation successful: 8%={calculated_tax_8}円, 10%={calculated_tax_10}円")
        else:
            logger.warning(f"Tax diff: 8%={tax_8_diff}円, 10%={tax_10_diff}円")

    # ========================================
    # マスタデータ読み込み
    # ========================================
    def _load_aliases(self) -> Dict[str, str]:
        """エイリアステーブルを読み込み"""
        result = self.db.table("MASTER_Rules_transaction_dict").select("*").execute()
        # product_name → official_name のマッピング
        aliases = {}
        for row in result.data:
            if row.get("product_name") and row.get("official_name"):
                aliases[row["product_name"].lower()] = row["official_name"]
        return aliases

    def _load_product_dictionary(self) -> List[Dict]:
        """商品辞書を読み込み"""
        result = self.db.table("MASTER_Product_classify").select("*").execute()
        return result.data

    def _load_situations(self) -> List[Dict]:
        """シチュエーション一覧を読み込み（名目）"""
        result = self.db.table("MASTER_Categories_purpose").select("*").execute()
        return result.data

    def _load_categories(self) -> List[Dict]:
        """カテゴリ一覧を読み込み（商品カテゴリ）"""
        result = self.db.table("MASTER_Categories_product").select("*").execute()
        return result.data

    def _load_product_generalize(self) -> Dict[str, str]:
        """商品名→一般名詞のマッピングを読み込み"""
        result = self.db.table("MASTER_Product_generalize").select("*").execute()
        # raw_keyword → general_name のマッピング
        generalize_map = {}
        for row in result.data:
            if row.get("raw_keyword") and row.get("general_name"):
                generalize_map[row["raw_keyword"].lower()] = row["general_name"]
        return generalize_map

    def _get_general_name(self, product_name: str) -> Optional[str]:
        """
        商品名から一般名詞を取得

        「買い物を依頼したら、その名前で買ってこれるレベルの名称」を抽出

        ロジック:
        1. MASTER_Product_generalizeでブランド名変換（完全一致・部分一致）
        2. 見つからない場合はGemini 2.5 FlashでAI抽出
        3. AI失敗時は正規表現でメーカー名・容量・産地を除去（フォールバック）

        例:
        - 「明治おいしい牛乳 1000ml」→「牛乳」（ブランド名変換）
        - 「日本ハム 豚ひき肉 200g」→「豚ひき肉」（正規表現除去）
        - 「国産ほうれん草 1束」→「ほうれん草」（正規表現除去）

        Args:
            product_name: 商品名

        Returns:
            str: 一般名詞（例: 「豚ひき肉」「ほうれん草」「ベーコン」）
        """
        if not product_name:
            return None

        # Step 1: MASTER_Product_generalizeで完全一致チェック（ブランド名変換）
        general_name = self.product_generalize.get(product_name.lower())
        if general_name:
            return general_name

        # Step 2: 部分一致でブランド名変換
        for keyword, gen_name in self.product_generalize.items():
            if keyword in product_name.lower():
                return gen_name

        # Step 2.5: Gemini 2.5 FlashでAI抽出
        try:
            ai_result = self._extract_general_name_with_ai(product_name)
            if ai_result:
                logger.debug(f"AI抽出成功: {product_name} → {ai_result}")
                return ai_result
        except Exception as e:
            logger.warning(f"AI抽出失敗（正規表現フォールバックへ）: {e}")

        # Step 3: AI失敗時のフォールバック - 正規表現で自動抽出
        import re
        cleaned = product_name

        # 先頭の特殊な括弧・記号を除去
        cleaned = re.sub(r'^\([^\)]+\)\s*', '', cleaned)  # (10本パック)ヤクルト → ヤクルト
        cleaned = re.sub(r'^\[[^\]]+\]\s*', '', cleaned)  # [A]チーズ → チーズ

        # メーカー名を除去
        manufacturer_patterns = [
            r'明治\s*', r'森永\s*', r'雪印\s*', r'メグミルク\s*', r'雪印メグミルク\s*',
            r'日本ハム\s*', r'伊藤ハム\s*', r'プリマハム\s*', r'丸大食品\s*',
            r'カゴメ\s*', r'キューピー\s*', r'味の素\s*', r'キッコーマン\s*',
            r'ヤマサ\s*', r'ミツカン\s*', r'ハウス食品\s*', r'S&B\s*', r'エスビー\s*',
            r'日清\s*', r'東洋水産\s*', r'サッポロ\s*', r'サントリー\s*',
            r'キリン\s*', r'アサヒ\s*', r'コカ・コーラ\s*', r'ペプシ\s*',
            r'ダノン\s*', r'チチヤス\s*', r'オハヨー\s*', r'六甲バター\s*',
            r'QBB\s*', r'Q・B・B\s*', r'クラフト\s*',
            r'西友オリジナル\s*', r'みなさまのお墨付き\s*',
            r'セブンプレミアム\s*', r'トップバリュ\s*',
        ]
        for pattern in manufacturer_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)

        # 産地・品質表示を除去
        # 注意: 「低脂肪」「減塩」などは商品名の一部なので除去しない
        # （例: 「低脂肪牛乳」「減塩醤油」は商品の種類として認識される）
        origin_patterns = [
            r'国産\s*', r'北海道産\s*', r'九州産\s*',
            r'有機\s*', r'オーガニック\s*', r'無添加\s*', r'食塩無添加\s*',
        ]
        for pattern in origin_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)

        # 容量・数量を除去
        volume_patterns = [
            r'\d+(?:\.\d+)?(?:ml|ML|ミリリットル)',
            r'\d+(?:\.\d+)?(?:l|L|リットル)',
            r'\d+(?:\.\d+)?(?:g|グラム)',
            r'\d+(?:\.\d+)?(?:kg|キロ)',
            r'\d+枚切り?',  # 6枚切り、6枚切（パン用）
            r'[0-9０-９]+(?:個|本|束|枚|切れ?|缶|袋|パック|玉|入)',  # 全角数字にも対応
            r'[（(][0-9０-９]+(?:個|本|束|枚|切れ?|缶|袋|パック|玉|入)[)）]',
            r'×[0-9０-９]+', r'[0-9０-９]+入り?', r'ケース',
        ]
        for pattern in volume_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)

        # 特殊な記号・余分な空白を除去
        cleaned = re.sub(r'[【】\[\]『』「」（）()]', '', cleaned)
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = cleaned.strip()

        # 空になった場合は元の商品名を返す
        if not cleaned:
            return product_name

        return cleaned

    def _get_general_name_and_keywords(self, product_name: str) -> Optional[Dict]:
        """
        商品名から一般名詞とキーワードを取得

        ロジック:
        1. MASTER_Product_generalizeでブランド名変換（完全一致・部分一致）
        2. 見つからない場合はGemini 2.5 FlashでAI抽出（general_name + keywords）
        3. AI失敗時は正規表現でgeneral_nameのみ生成（フォールバック）

        Args:
            product_name: 商品名

        Returns:
            dict: {"general_name": str, "keywords": list} (失敗時はNone)
                  例: {"general_name": "食パン", "keywords": ["食パン", "パスコ", "超熟", "6枚切り"]}
        """
        if not product_name:
            return None

        # Step 1: MASTER_Product_generalizeで完全一致チェック（ブランド名変換）
        general_name = self.product_generalize.get(product_name.lower())
        if general_name:
            # ブランド名変換の場合、キーワードは商品名を分割して生成
            # 簡易的に空白と記号で分割
            keywords = [general_name] + [
                word.strip() for word in re.split(r'[\s　]+', product_name)
                if word.strip() and word.strip() != general_name
            ]
            return {"general_name": general_name, "keywords": keywords}

        # Step 2: 正規化後の完全一致（空白のゆれ・量のバリエーション許容）
        def normalize_product_name(name):
            """空白除去 + 量情報除去"""
            # 空白を全て除去
            normalized = re.sub(r'[\s　]+', '', name.lower())
            # 量情報を除去（100g, 500ml, 6個入り など）
            normalized = re.sub(r'\d+(?:g|ml|l|個|枚|本|袋|パック|切|入り|ケース)(?:入り)?', '', normalized)
            # ×6 のようなパターンも除去
            normalized = re.sub(r'[×x]\d+', '', normalized)
            # () や【】内も除去
            normalized = re.sub(r'[\(（\[【].*?[\)）\]】]', '', normalized)
            return normalized.strip()

        normalized_product = normalize_product_name(product_name)

        for keyword, gen_name in self.product_generalize.items():
            normalized_keyword = normalize_product_name(keyword)
            if normalized_product == normalized_keyword:
                keywords = [gen_name] + [
                    word.strip() for word in re.split(r'[\s　]+', product_name)
                    if word.strip() and word.strip() != gen_name
                ]
                return {"general_name": gen_name, "keywords": keywords}

        # Step 2.5: Gemini 2.5 FlashでAI抽出
        try:
            ai_result = self._extract_general_name_with_ai(product_name)
            if ai_result:
                logger.debug(f"AI抽出成功: {product_name} → {ai_result}")
                return ai_result
        except Exception as e:
            logger.warning(f"AI抽出失敗（正規表現フォールバックへ）: {e}")

        # Step 3: AI失敗時のフォールバック - 正規表現でgeneral_nameのみ生成
        # _get_general_name()の正規表現ロジックを再利用
        general_name_only = self._get_general_name(product_name)
        if general_name_only:
            # キーワードは商品名を分割して生成
            keywords = [general_name_only] + [
                word.strip() for word in re.split(r'[\s　]+', product_name)
                if word.strip() and word.strip() != general_name_only
            ]
            return {"general_name": general_name_only, "keywords": keywords}

        return None

    def _extract_general_name_with_ai(self, product_name: str) -> Optional[Dict]:
        """
        Gemini 2.5 Flashで商品名から一般名詞とキーワードを抽出

        Args:
            product_name: 商品名

        Returns:
            dict: {"general_name": str, "keywords": list} (失敗時はNone)
                  例: {"general_name": "食パン", "keywords": ["食パン", "パスコ", "超熟", "6枚切り"]}
        """
        prompt = f"""あなたは商品名から一般名詞とキーワードを抽出する専門家です。

商品名: {product_name}

以下の2つを抽出してください:
1. general_name（コア概念）: この商品のカテゴリーを表す最も重要な単語
2. keywords（キーワード配列）: 商品名に含まれる全ての意味のある単語（general_nameを含む）

ルール:
- general_nameは商品のコア概念（例: 食パン、ボトルコーヒー、牛乳、豚ひき肉）
- keywordsは検索に使える全ての単語を個別に抽出
- メーカー名、ブランド名、容量、数量も全てkeywordsに含める
- キーワードは辞書に登録できる単位で分割

例:
商品名: 「パスコ 超熟 6枚切り」
general_name: 食パン
keywords: ["食パン", "パスコ", "超熟", "6枚切り"]

商品名: 「ジョージア 無糖 1000ml」
general_name: ボトルコーヒー
keywords: ["ボトルコーヒー", "ジョージア", "無糖", "1000ml"]

商品名: 「明治おいしい牛乳 1000ml」
general_name: 牛乳
keywords: ["牛乳", "明治", "おいしい牛乳", "1000ml"]

商品名: 「日本ハム 豚ひき肉 200g」
general_name: 豚ひき肉
keywords: ["豚ひき肉", "日本ハム", "200g"]

以下のJSON形式で出力してください:
{{"general_name": "...", "keywords": ["...", "..."]}}"""

        try:
            response = self.llm_client.call_model(
                tier="stageh_extraction",
                prompt=prompt,
                model_name="gemini-2.5-flash",
                max_output_tokens=8192
            )

            if response.get("success"):
                content = response.get("content", "").strip()

                # JSONパース
                import json
                try:
                    # マークダウンのコードブロックを除去（```json ... ```）
                    if content.startswith("```"):
                        content = content.split("```")[1]
                        if content.startswith("json"):
                            content = content[4:]
                        content = content.strip()

                    result = json.loads(content)

                    # 必須フィールドの検証
                    if "general_name" in result and "keywords" in result:
                        # keywordsが配列であることを確認
                        if isinstance(result["keywords"], list):
                            return result
                        else:
                            logger.warning(f"keywords is not a list: {result}")
                    else:
                        logger.warning(f"Missing required fields in AI response: {result}")

                except json.JSONDecodeError as e:
                    logger.error(f"JSON parse error: {e}, content: {content}")
                    return None

            return None

        except Exception as e:
            logger.error(f"Gemini API呼び出しエラー: {e}")
            return None
```

### shared\kakeibo\verify_7_elements.py

```py
"""
7要素データ構造の検証スクリプト

DB に保存された商品データが7要素構造になっているか確認
"""
from shared.common.database.client import DatabaseClient
from loguru import logger
import json


def verify_7_elements():
    """7要素データ構造を検証"""
    db = DatabaseClient(use_service_role=True)

    # レシート一覧を取得
    receipts = db.client.table("Rawdata_RECEIPT_shops").select("*").limit(3).execute()

    for receipt in receipts.data:
        logger.info(f"\n{'='*80}")
        logger.info(f"📄 レシート: {receipt['shop_name']} ({receipt['transaction_date']})")
        logger.info(f"   receipt_id: {receipt['id']}")
        logger.info(f"   合計: {receipt['total_amount_check']}円")
        logger.info(f"   小計: {receipt['subtotal_amount']}円")

        # 外税 or 内税判定
        if receipt['subtotal_amount'] and receipt['total_amount_check']:
            if receipt['subtotal_amount'] < receipt['total_amount_check']:
                tax_type = "外税"
            else:
                tax_type = "内税"
        else:
            tax_type = "不明"
        logger.info(f"   税表示タイプ: {tax_type}")

        # トランザクションを取得
        transactions = db.client.table("Rawdata_RECEIPT_items").select("*").eq("receipt_id", receipt['id']).execute()

        logger.info(f"\n   商品一覧:")
        for trans in transactions.data:
            # 正規化データは同じレコードに含まれる
            logger.info(f"\n   商品: {trans['product_name']}")
            logger.info(f"     1. 数量:      {trans['quantity']}")
            logger.info(f"     2. 表示額:    {trans.get('unit_price', 'N/A')}円 (※unit_priceから推測)")
            logger.info(f"     3. 外or内:    (レシートレベルで{tax_type})")
            logger.info(f"     4. 税率:      {trans.get('tax_rate', 'N/A')}%")
            logger.info(f"     5. 本体価:    {trans.get('std_unit_price', 'N/A')}円")
            logger.info(f"     6. 税額:      {trans.get('tax_amount', 'N/A')}円")
            logger.info(f"     7. 税込価:    {trans.get('std_amount', 'N/A')}円")

            # 計算検証
            if trans.get('std_unit_price') is not None and trans.get('std_amount') is not None and trans.get('tax_amount') is not None:
                if tax_type == "外税":
                    expected_total = trans['std_unit_price'] + trans['tax_amount']
                    if expected_total == trans['std_amount']:
                        logger.success(f"     ✅ 計算正しい: {trans['std_unit_price']} + {trans['tax_amount']} = {trans['std_amount']}")
                    else:
                        logger.warning(f"     ⚠️  計算ずれ: {trans['std_unit_price']} + {trans['tax_amount']} ≠ {trans['std_amount']}")
                else:
                    expected_base = trans['std_amount'] - trans['tax_amount']
                    if expected_base == trans['std_unit_price']:
                        logger.success(f"     ✅ 計算正しい: {trans['std_amount']} - {trans['tax_amount']} = {trans['std_unit_price']}")
                    else:
                        logger.warning(f"     ⚠️  計算ずれ: {trans['std_amount']} - {trans['tax_amount']} ≠ {trans['std_unit_price']}")


if __name__ == "__main__":
    verify_7_elements()
```

### shared\kakeibo\verify_foodium_receipt.py

```py
"""Verify the foodium (外税) receipt's 7-element structure"""
from shared.common.database.client import DatabaseClient
from loguru import logger


def verify_foodium():
    db = DatabaseClient(use_service_role=True)

    # Get the foodium receipt (ordered by created_at desc, should be second latest)
    receipts = db.client.table("Rawdata_RECEIPT_shops").select("*").eq("shop_name", "foodium武蔵小杉").order("created_at", desc=True).limit(1).execute()

    if not receipts.data:
        logger.error("No foodium receipt found!")
        return

    receipt = receipts.data[0]
    logger.info(f"\n{'='*80}")
    logger.info(f"📄 最新foodiumレシート: {receipt['shop_name']} ({receipt['transaction_date']})")
    logger.info(f"   receipt_id: {receipt['id']}")
    logger.info(f"   合計: {receipt['total_amount_check']}円")
    logger.info(f"   小計: {receipt['subtotal_amount']}円")

    # Tax type detection
    if receipt['subtotal_amount'] and receipt['total_amount_check']:
        if receipt['subtotal_amount'] < receipt['total_amount_check']:
            tax_type = "外税"
        else:
            tax_type = "内税"
    else:
        tax_type = "不明"
    logger.info(f"   税表示タイプ: {tax_type}")

    # Get transactions
    transactions = db.client.table("Rawdata_RECEIPT_items").select("*").eq("receipt_id", receipt['id']).execute()
    logger.info(f"\n   商品数: {len(transactions.data)}件")

    for trans in transactions.data:  # Show all items
        # 正規化データは同じレコードに含まれる
        logger.info(f"\n   商品: {trans['product_name']}")
        logger.info(f"     1. 数量:      {trans['quantity']}")
        logger.info(f"     2. 表示額:    {trans.get('unit_price', 'N/A')}円")
        logger.info(f"     3. 外or内:    {tax_type}")
        logger.info(f"     4. 税率:      {trans.get('tax_rate', 'N/A')}%")
        logger.info(f"     5. 本体価:    {trans.get('std_unit_price', 'N/A')}円")
        logger.info(f"     6. 税額:      {trans.get('tax_amount', 'N/A')}円")
        logger.info(f"     7. 税込価:    {trans.get('std_amount', 'N/A')}円")

        # Verify calculation
        if trans.get('std_unit_price') is not None and trans.get('std_amount') is not None and trans.get('tax_amount') is not None:
            if tax_type == "外税":
                expected = trans['std_unit_price'] + trans['tax_amount']
                if expected == trans['std_amount']:
                    logger.success(f"     ✅ 計算正しい: {trans['std_unit_price']} + {trans['tax_amount']} = {trans['std_amount']}")
                else:
                    logger.warning(f"     ⚠️  計算ずれ: {trans['std_unit_price']} + {trans['tax_amount']} = {expected} ≠ {trans['std_amount']}")
            else:
                expected = trans['std_amount'] - trans['tax_amount']
                if expected == trans['std_unit_price']:
                    logger.success(f"     ✅ 計算正しい: {trans['std_amount']} - {trans['tax_amount']} = {trans['std_unit_price']}")
                else:
                    logger.warning(f"     ⚠️  計算ずれ: {trans['std_amount']} - {trans['tax_amount']} = {expected} ≠ {trans['std_unit_price']}")


if __name__ == "__main__":
    verify_foodium()
```

### shared\kakeibo\verify_latest_receipt.py

```py
"""Verify the latest receipt's 7-element structure"""
from shared.common.database.client import DatabaseClient
from loguru import logger


def verify_latest():
    db = DatabaseClient(use_service_role=True)

    # Get the most recent receipt (by created_at)
    receipts = db.client.table("Rawdata_RECEIPT_shops").select("*").order("created_at", desc=True).limit(1).execute()

    if not receipts.data:
        logger.error("No receipts found!")
        return

    receipt = receipts.data[0]
    logger.info(f"\n{'='*80}")
    logger.info(f"📄 最新レシート: {receipt['shop_name']} ({receipt['transaction_date']})")
    logger.info(f"   receipt_id: {receipt['id']}")
    logger.info(f"   合計: {receipt['total_amount_check']}円")
    logger.info(f"   小計: {receipt['subtotal_amount']}円")

    # Tax type detection
    if receipt['subtotal_amount'] and receipt['total_amount_check']:
        if receipt['subtotal_amount'] < receipt['total_amount_check']:
            tax_type = "外税"
        else:
            tax_type = "内税"
    else:
        tax_type = "不明"
    logger.info(f"   税表示タイプ: {tax_type}")

    # Get transactions
    transactions = db.client.table("Rawdata_RECEIPT_items").select("*").eq("receipt_id", receipt['id']).execute()
    logger.info(f"\n   商品数: {len(transactions.data)}件")

    for trans in transactions.data[:3]:  # Show first 3 items
        # 正規化データは同じレコードに含まれる
        logger.info(f"\n   商品: {trans['product_name']}")
        logger.info(f"     1. 数量:      {trans['quantity']}")
        logger.info(f"     2. 表示額:    {trans.get('unit_price', 'N/A')}円")
        logger.info(f"     3. 外or内:    {tax_type}")
        logger.info(f"     4. 税率:      {trans.get('tax_rate', 'N/A')}%")
        logger.info(f"     5. 本体価:    {trans.get('std_unit_price', 'N/A')}円")
        logger.info(f"     6. 税額:      {trans.get('tax_amount', 'N/A')}円")
        logger.info(f"     7. 税込価:    {trans.get('std_amount', 'N/A')}円")

        # Verify calculation
        if trans.get('std_unit_price') is not None and trans.get('std_amount') is not None and trans.get('tax_amount') is not None:
            if tax_type == "外税":
                expected = trans['std_unit_price'] + trans['tax_amount']
                if expected == trans['std_amount']:
                    logger.success(f"     ✅ 計算正しい: {trans['std_unit_price']} + {trans['tax_amount']} = {trans['std_amount']}")
                else:
                    logger.warning(f"     ⚠️  計算ずれ: {trans['std_unit_price']} + {trans['tax_amount']} = {expected} ≠ {trans['std_amount']}")
            else:
                expected = trans['std_amount'] - trans['tax_amount']
                if expected == trans['std_unit_price']:
                    logger.success(f"     ✅ 計算正しい: {trans['std_amount']} - {trans['tax_amount']} = {trans['std_unit_price']}")
                else:
                    logger.warning(f"     ⚠️  計算ずれ: {trans['std_amount']} - {trans['tax_amount']} = {expected} ≠ {trans['std_unit_price']}")


if __name__ == "__main__":
    verify_latest()
```

### shared\pipeline\__init__.py

```py
"""
G_unified_pipeline: 統合ドキュメント処理パイプライン

Stage E-K を統合した、堅牢かつ高精度なドキュメント処理フロー

使用方法:
    from shared.pipeline import UnifiedDocumentPipeline

    pipeline = UnifiedDocumentPipeline()
    result = await pipeline.process_document(
        file_path=Path("document.pdf"),
        file_name="document.pdf",
        doc_type="invoice",
        workspace="personal",
        mime_type="application/pdf",
        source_id="drive_file_id"
    )
"""

from .pipeline import UnifiedDocumentPipeline

__all__ = ['UnifiedDocumentPipeline']
__version__ = '1.0.0'
```

### shared\pipeline\config\models.yaml

```yaml
# ============================================
# AIモデル定義
# ============================================
# 各ステージで使用するAIモデルを定義
# キー: default, flyer, classroom など (doc_type や条件に対応)

models:
  # Stage E: Preprocessing (前処理・テキスト抽出)
  stage_e:
    default: "gemini-2.5-flash"
    gmail: "gemini-2.5-flash-lite"  # Gmail用（軽量）

  # Stage F: Visual Analysis (視覚解析)
  # flyer: 商品写真の視覚理解が重要なため Pro を使用
  # classroom: 日常的なお知らせの正確なOCRが目的のため Flash で十分
  stage_f:
    default: "gemini-2.5-flash"
    flyer: "gemini-2.5-pro"
    classroom: "gemini-2.5-flash"
    flash_lite: "gemini-2.5-flash-lite"  # 家計簿用（最軽量）
    gmail: "gemini-2.5-flash-lite"  # Gmail用（軽量）

  # Stage H: Structuring (構造化)
  stage_h:
    default: "gemini-2.5-flash"
    flyer: "gemini-2.5-flash"
    classroom: "gemini-2.5-flash"
    flash_lite: "gemini-2.5-flash-lite"  # 家計簿用（Pythonロジック中心）
    gmail: "gemini-2.5-flash-lite"  # Gmail用（軽量）

  # Stage I: Synthesis (統合・要約)
  stage_i:
    default: "gemini-2.5-flash"
    flyer: "gemini-2.5-flash"
    classroom: "gemini-2.5-flash"
    gmail: "gemini-2.5-flash-lite"  # Gmail用（軽量）

  # Stage K: Embedding (ベクトル化)
  stage_k:
    default: "text-embedding-3-small"
    flyer: "text-embedding-3-small"
    classroom: "text-embedding-3-small"

# ============================================
# ハイブリッドOCR設定 (Surya + PaddleOCR + Gemini)
# ============================================
# Stage F で使用するハイブリッドOCRの有効/無効を workspace ベースで制御
#
# ハイブリッドOCRの特徴:
# - Surya: レイアウト解析（210領域検出）
# - PaddleOCR: 日本語テキスト認識（1514文字抽出）
# - Gemini Vision: 統合・補完（最終2418文字）
# - コスト: Gemini のみと同じ（PaddleOCR/Surya は無料）
# - 処理時間: 約10分（Gemini のみの場合は1分）
#
# 推奨設定:
# - 複雑なレイアウト・表が多いドキュメント: true
# - シンプルなレイアウト・速度重視: false
#
hybrid_ocr:
  # デフォルト: 有効（Hybrid OCR）
  # 適用workspace: ikuya_classroom, ema_classroom, waseda_academy
  default: true

  # === Workspace 別設定 ===

  # 家計簿: 有効
  # 理由: レシートの表構造・小さい文字の高精度抽出が必要
  kakeibo: true

# ============================================
# モデルのコスト情報（参考）
# ============================================
# gemini-2.5-flash: 高速、安価
# gemini-2.5-flash: 構造化に最適
# text-embedding-3-small: 1536次元ベクトル
```

### shared\pipeline\config\pipeline_routing.yaml

```yaml
# ============================================
# パイプラインルーティング設定
# ============================================
# このファイルを編集することで、workspace と doc_type に基づいて
# 使用するスキーマ（プロンプトセット）を設定できます
#
# 優先順位:
# 1. workspace (最優先)
# 2. doc_type
# 3. default (フォールバック)

routing:
  # ============================================
  # workspace ベースのルート（優先順位1）
  # ============================================
  by_workspace:
    # Gmail ワークスペース
    gmail:
      description: "Gmailワークスペース（DM-MAIL, JOB-MAILなど）"
      schema: "default"
      stages:
        stage_e:
          prompt_key: "default"
          model_key: "gmail"
        stage_f:
          prompt_key: "default"
          model_key: "gmail"
        stage_h:
          prompt_key: "default"
          model_key: "gmail"
        stage_i:
          prompt_key: "default"
          model_key: "gmail"

    # Classroom ワークスペース（育哉）
    ikuya_classroom:
      description: "育哉のClassroomワークスペース"
      schema: "classroom"
      stages:
        stage_f:
          prompt_key: "classroom"
          model_key: "classroom"
        stage_h:
          prompt_key: "classroom"
          model_key: "classroom"
        stage_i:
          prompt_key: "classroom"
          model_key: "classroom"

    # Classroom ワークスペース（絵麻）
    ema_classroom:
      description: "絵麻のClassroomワークスペース"
      schema: "classroom"
      stages:
        stage_f:
          prompt_key: "classroom"
          model_key: "classroom"
        stage_h:
          prompt_key: "classroom"
          model_key: "classroom"
        stage_i:
          prompt_key: "classroom"
          model_key: "classroom"

    # 早稲田アカデミー
    waseda_academy:
      description: "早稲田アカデミーのドキュメント"
      schema: "classroom"
      stages:
        stage_f:
          prompt_key: "classroom"
          model_key: "classroom"
        stage_h:
          prompt_key: "classroom"
          model_key: "classroom"
        stage_i:
          prompt_key: "classroom"
          model_key: "classroom"

  # ============================================
  # doc_type ベースのルート（優先順位2）
  # ============================================
  by_doc_type:
    # 実店舗チラシ
    physical_shop:
      description: "実店舗のチラシ（スーパー、ドラッグストアなど）"
      schema: "flyer"
      stages:
        stage_f:
          prompt_key: "flyer"
          model_key: "flyer"
        stage_h:
          prompt_key: "flyer"
          model_key: "flyer"
        stage_i:
          prompt_key: "flyer"
          model_key: "flyer"

    # Classroom ドキュメント（workspace で指定されない場合）
    classroom_document:
      description: "Google Classroomドキュメント"
      schema: "classroom"
      stages:
        stage_f:
          prompt_key: "classroom"
          model_key: "classroom"
        stage_h:
          prompt_key: "classroom"
          model_key: "classroom"
        stage_i:
          prompt_key: "classroom"
          model_key: "classroom"

    # 家計簿レシート
    kakeibo:
      description: "家計簿レシート（全テキスト抽出 → 構造化 → 税額按分）"
      schema: "kakeibo"
      stages:
        stage_f:
          prompt_key: "kakeibo"
          model_key: "flash_lite"
        stage_h:
          prompt_key: "kakeibo"
          model_key: "flash_lite"
          custom_handler: "kakeibo"  # stage_h_kakeibo.py を使用
        stage_i:
          skip: true  # 家計簿は不要

    # デフォルト（優先順位3）
    default:
      description: "デフォルト処理（汎用ドキュメント）"
      schema: "default"
      stages:
        stage_f:
          prompt_key: "default"
          model_key: "default"
        stage_h:
          prompt_key: "default"
          model_key: "default"
        stage_i:
          prompt_key: "default"
          model_key: "default"

# ============================================
# 使用例
# ============================================
#
# 【例1】workspace='ikuya_classroom', doc_type='other'
# → by_workspace.ikuya_classroom が選択される（Classroom スキーマ）
#
# 【例2】workspace='inbox', doc_type='physical_shop'
# → by_doc_type.physical_shop が選択される（Flyer スキーマ）
#
# 【例3】workspace='inbox', doc_type='other'
# → by_doc_type.default が選択される（Default スキーマ）
#
# ============================================
# 設定の追加方法
# ============================================
#
# 新しいワークスペースを追加する場合:
# 1. by_workspace セクションに新しいエントリを追加
# 2. schema を指定（classroom, flyer, default など）
# 3. 各 stage の prompt_key と model_key を設定
#
# 新しい doc_type を追加する場合:
# 1. by_doc_type セクションに新しいエントリを追加
# 2. schema を指定
# 3. 各 stage の prompt_key と model_key を設定
```

### shared\pipeline\config\prompts.yaml

```yaml
# ============================================
# 統一パイプライン プロンプト設定
# ============================================
# 全ステージのプロンプトを一元管理
#
# 使用方法:
#   config_loader.get_prompt('stage_f/classroom')
#   -> prompts.stage_f.classroom の内容を返す

prompts:
  stage_f:
    classroom: |
      # Stage F: Visual Analysis - Classroom Documents
      
      あなたはGoogle Classroom課題ドキュメントの視覚解析を専門とするAIアシスタントです。
      
      ## タスク
      
      Stage E で抽出したテキストを基準として、画像を詳細に見て、**完璧な3つの情報**を作成してください。
      
      ## 出力する情報
      
      1. **全テキスト（完璧版）**
         - **Stage E のテキストを基準として、画像と照合して以下を行う:**
           - **検証**: Stage E のテキストが正しいか確認
           - **修正**: 間違いがあれば正しく修正
           - **補完**: 欠けている部分を補完（画像化されたタイトル、ロゴ、装飾文字など）
         - **結果として、全ての文字を1文字も漏らさない完璧なテキストを作成**
         - 課題タイトル、本文、期限、添付ファイル情報、UI要素、**表の全データ**、全て含む
         - **重要**: Stage E に表が含まれている場合、その表の全データを `full_text` に含めてください
         - レイアウトと位置関係を保持
         - 表、リスト、見出しなどの構造を維持
      
      2. **レイアウト情報**
         - セクション区切り
         - **表構造（行・列）**: Stage E に表が含まれている場合、全ての表を `layout_info.tables` に格納してください
         - リスト項目
         - 見出しレベル

      3. **視覚的要素**
         - 強調されているテキスト（太字、色付きなど）
         - 提出期限の表示
         - 重要マーク・アイコン
         - 添付ファイルのリスト

      4. **構造化データの抽出**
         - 視覚的に表として表示されていなくても、文章中に含まれる構造化データを識別して表形式に変換してください
         - 例: 課題の配点情報、提出要件、スケジュール情報など
         - 価格情報、数量情報、日時情報、メタデータなど、構造化可能な情報は全て表形式に変換してください
         - データソース情報（例: 投稿文の文字数、OCRテキストの文字数など）も表形式で記録してください
      
      ## 出力形式
      
      JSON形式で以下の構造で出力してください：
      
      ```json
      {{
        "full_text": "完璧な全テキスト（Stage E を検証・修正・補完した結果）",
        "layout_info": {{
          "sections": [
            {{
              "type": "heading/paragraph/table/list/deadline",
              "content": "セクションの内容",
              "level": 1
            }}
          ],
          "tables": [
            {{
              "table_title": "表のタイトル（任意）",
              "table_type": "visual_table/structured_data/metadata/requirements など",
              "rows": [["ヘッダー1", "ヘッダー2"], ["データ1", "データ2"]],
              "caption": "表の説明"
            }}
          ]
        }},
        "visual_elements": {{
          "emphasized_text": ["強調されているテキスト"],
          "deadline_info": "提出期限（YYYY-MM-DD HH:MM形式）",
          "attachments": ["ファイル名1", "ファイル名2"],
          "notes": ["注釈・コメント"]
        }}
      }}
      ```
      
      ## 注意事項
      
      - 手書き文字も可能な限り読み取る
      - 日付は必ず YYYY-MM-DD 形式に統一
      - 不明な情報は null とする
      
      **重要:**
      - **Stage E のテキストを基準として、画像を詳細に見て検証・修正・補完してください**
      - **間違いがあれば正しく修正し、欠けている部分があれば全て補完してください**
      - **文字が省略されたり、間引かれたりすることは絶対に許されません**
      - **UIボタン、メニュー、装飾文字、全て含めてください**
      - レイアウト構造を正確に把握してください
      - **構造化可能なデータは全て `layout_info.tables` に表形式で含めてください**
      - 表には `table_type` フィールドを追加して、表の種類を明示してください（visual_table, structured_data, metadata, requirements など）
      
      ## Stage E テキストの活用方法
      
      **Stage E のテキストが提供されている場合:**
      1. **基準として使用**: Stage E のテキストはほぼ正確なので、これを基準とする
      2. **画像と照合**: 画像を見て、Stage E のテキストが正しいか検証する
      3. **間違いを修正**: もし間違いがあれば、画像を見て正しく修正する
      4. **表の検証と修正**:
         - Stage E に表が含まれている場合、画像を見て表の内容が正しいか検証する
         - 表のセルの値が間違っている場合は、画像を見て正しく修正する
         - 表の構造（行数・列数）が間違っている場合は、画像を見て正しく修正する
         - 表に欠けているデータがある場合は、画像を見て補完する
      5. **欠けている部分を補完**:
         - 画像化されたタイトル、ロゴ、装飾文字
         - 背景画像として配置された文字、透かし文字
         - 小さな文字、注釈、脚注、コピーライト表記
         - その他、Stage E が見逃した文字を全て補完
      
      **検証・補完の原則:**
      - **full_text の文字数 >= Stage E のテキスト文字数** であるべき
      - もし full_text の文字数が少ない場合、文字を削除しています（許されません）
      - **Stage E のテキストを削除せず、欠けている文字を追加してください**
      - **Stage E に表が含まれている場合**:
        - 表の全データを `full_text` に含める（1文字も削除厳禁）
        - 表の構造を `layout_info.tables` に格納する
        - 表の情報を失うことは絶対に許されません
      

    default: |
      あなたはドキュメントから視覚情報を抽出する専門家です。

      この画像/PDFファイルから、人間が見たままの視覚情報をそのまま抽出してください。

      ## 抽出する情報

      1. **全テキスト（OCR）**
         - 画像内のすべてのテキストを抽出
         - レイアウトと位置関係を保持
         - 表、リスト、見出しなどの構造を維持

      2. **レイアウト情報**
         - セクション区切り
         - 表構造（行・列）
         - リスト項目
         - 見出しレベル

      3. **視覚的要素**
         - 強調されているテキスト（太字、色付きなど）
         - 図表のキャプション
         - 注釈やコメント

      4. **構造化データの抽出**
         - 視覚的に表として表示されていなくても、文章中に含まれる構造化データを識別して表形式に変換してください
         - 例: 「100円のリンゴが20％オフで80円です」→ 表形式: [["商品", "通常価格", "割引", "販売価格"], ["リンゴ", "100円", "20％オフ", "80円"]]
         - 価格情報、数量情報、日時情報、メタデータなど、構造化可能な情報は全て表形式に変換してください
         - データソース情報（例: 投稿文、OCRテキスト、統合テキストの文字数など）も表形式で記録してください

      ## 出力形式

      JSON形式で以下の構造で出力してください：

      ```json
      {{
        "full_text": "抽出した全テキスト（改行や空白を保持）",
        "layout_info": {{
          "sections": [
            {{
              "type": "heading/paragraph/table/list",
              "content": "セクションの内容",
              "level": 1
            }}
          ],
          "tables": [
            {{
              "table_title": "表のタイトル（任意）",
              "table_type": "visual_table/structured_data/metadata/pricing など",
              "rows": [["ヘッダー1", "ヘッダー2"], ["データ1", "データ2"]],
              "caption": "表の説明"
            }}
          ]
        }},
        "visual_elements": {{
          "emphasized_text": ["強調されているテキスト"],
          "notes": ["注釈・コメント"]
        }}
      }}
      ```

      **重要:**
      - すべてのテキストを漏れなく抽出してください
      - 文字が省略されたり、間引かれたりしないように注意してください
      - レイアウト構造を正確に把握してください
      - 構造化可能なデータは全て `layout_info.tables` に表形式で含めてください
      - 表には `table_type` フィールドを追加して、表の種類を明示してください（visual_table, structured_data, metadata, pricing など）
      

    flyer: |
      あなたはスーパーマーケットのチラシから視覚情報を抽出する専門家です。
      
      このチラシ画像から、人間が見たままの視覚情報をそのまま抽出してください。
      
      ## 抽出する情報
      
      1. **全テキスト（OCR）**
         - チラシ内のすべてのテキストを抽出
         - 商品名、価格、単位、説明文など全て
         - レイアウトと位置関係を保持
         - 表、リスト、見出しなどの構造を維持
      
      2. **レイアウト情報**
         - セクション区切り（カテゴリ別など）
         - 商品グループ
         - 価格表示の位置
         - 特売・セール情報の強調部分

      3. **視覚的要素**
         - 強調されているテキスト（赤字、大文字など）
         - 価格の表示形式（税込/税抜）
         - 商品画像の位置とキャプション
         - セール期間、注釈、ポイント情報

      4. **構造化データの抽出**
         - 商品情報（商品名、価格、割引、最終価格など）を表形式で抽出してください
         - 例: 「100円のリンゴが20％オフで80円」→ 表形式: [["商品", "通常価格", "割引", "販売価格"], ["リンゴ", "100円", "20％オフ", "80円"]]
         - カテゴリ別の商品リスト、セール情報、価格比較なども表形式に変換してください
         - データソース情報（例: 抽出したテキストの文字数、商品数など）も表形式で記録してください
      
      ## 出力形式
      
      JSON形式で以下の構造で出力してください：
      
      ```json
      {{
        "full_text": "抽出した全テキスト（改行や空白を保持）",
        "layout_info": {{
          "sections": [
            {{
              "type": "heading/product_group/sale_section",
              "content": "セクションの内容",
              "level": 1
            }}
          ],
          "tables": [
            {{
              "table_title": "商品リスト（カテゴリ名など）",
              "table_type": "product_list/pricing/sale_items など",
              "rows": [["商品名", "通常価格", "割引", "販売価格"], ["リンゴ", "100円", "20％オフ", "80円"]],
              "caption": "表の説明"
            }}
          ],
          "product_items": [
            {{
              "position": "左上",
              "text": "商品名と価格のテキスト"
            }}
          ]
        }},
        "visual_elements": {{
          "emphasized_prices": ["198円", "特価380円"],
          "sale_periods": ["3/15～3/20"],
          "notes": ["ポイント5倍", "数量限定"]
        }}
      }}
      ```

      **重要:**
      - 価格情報は絶対に省略しないでください
      - 商品名は完全な形で抽出してください
      - 税込/税抜の表記も必ず含めてください
      - セール期間や条件も正確に抽出してください
      - **商品情報は `layout_info.tables` に表形式で含めてください**
      - 表には `table_type` フィールドを追加して、表の種類を明示してください（product_list, pricing, sale_items など）
      

    kakeibo: |
      あなたはレシート画像からすべてのテキスト情報を抽出する専門家です。
      
      この画像に印字されているすべての文字・数字・記号を、一字一句漏らさず抽出してください。
      
      ## 最重要原則
      
      このタスクの目的は「人間の目の代わり」として機能することです。
      人間がレシートを見て読み取れるすべての情報を抽出してください。
      
      ## 抽出する情報
      
      ### 1. 全テキスト（上から下へ、順番に）
      - 店舗名
      - 住所、電話番号
      - 営業時間、店舗コード
      - すべての商品名（略語もそのまま）
      - すべての数量、単価、金額
      - 税率マーク（※、★、8%、10%など）
      - **【重要】税額情報（特に注意して正確に読み取る）:**
        - 小計の金額
        - 「外税 8%対象額」または「8%対象額」の行とその金額
        - 「外税 8%」または「8%税」の行とその金額
        - 「外税10%対象額」または「10%対象額」の行とその金額
        - 「外税10%」または「10%税」の行とその金額
        - 内税の場合は「内税額」「うち消費税」などの行とその金額
      - 合計金額
      - お預かり、お釣り
      - レジ番号、取引番号、レシート番号
      - 日付、時刻
      - バーコード番号
      - ポイント情報、キャンペーン情報
      - その他、すべての印字テキスト
      
      ### 2. レイアウト情報
      - 各行の位置（左寄せ/中央/右寄せ）
      - フォントサイズ（大/中/小）
      - 強調表示（太字、二重線など）
      - 行の区切り、空行
      
      ### 3. 数値情報
      - すべての数値をそのまま記録
      - 推測・計算は一切しない
      - カンマ、小数点もそのまま

      ### 4. 構造化データの抽出
      - 商品リスト（商品名、数量、単価、金額）を表形式で抽出してください
      - 税額情報（税率、対象額、税額）を表形式で抽出してください
      - 支払情報（小計、税額、合計、お預かり、お釣り）を表形式で抽出してください

      ## 出力形式

      JSON形式で以下の構造で出力してください：

      ```json
      {
        "raw_text": "レシート全体のテキスト（改行・スペースを含む、一字一句そのまま）",
        "lines": [
          {
            "line_number": 1,
            "text": "この行のテキスト",
            "alignment": "left/center/right",
            "font_size": "large/normal/small",
            "emphasized": true/false
          }
        ],
        "structured_tables": [
          {
            "table_title": "購入商品リスト",
            "table_type": "item_list",
            "rows": [["商品名", "数量", "単価", "金額"], ["商品A", "1", "100", "100"]]
          },
          {
            "table_title": "税額情報",
            "table_type": "tax_info",
            "rows": [["税率", "対象額", "税額"], ["8%", "1000", "80"], ["10%", "500", "50"]]
          },
          {
            "table_title": "支払情報",
            "table_type": "payment_info",
            "rows": [["項目", "金額"], ["小計", "1500"], ["消費税", "130"], ["合計", "1630"], ["お預かり", "2000"], ["お釣り", "370"]]
          }
        ],
        "detected_elements": {
          "has_store_name": true,
          "has_address": true,
          "has_phone": true,
          "has_store_code": true,
          "has_date": true,
          "has_time": true,
          "has_items": true,
          "has_subtotal": true,
          "has_tax_8": true,
          "has_tax_10": true,
          "has_total": true,
          "has_payment_info": true,
          "has_receipt_number": true
        },
        "notes": "読み取りにくい箇所、不鮮明な箇所があればここに記載"
      }
      ```
      
      ## 重要な注意

      - **すべての文字を抽出**：省略、間引き、推測は一切禁止
      - **レイアウトを保持**：位置関係、行の区切りを保持
      - **数値はそのまま**：計算しない、推測しない
      - **税額欄の数値は特に慎重に**：
        - 右寄せの小さい数字も正確に読み取る
        - 例: 「¥55」は55であり、4や5ではない
        - 例: 「¥68」は68であり、118や8ではない
        - 桁数を正確に（1桁、2桁、3桁など）
      - **税率マークを保持**：※、★、8%、10% などのマークをそのまま記録
      - **不明な文字**：読み取れない場合は「[不明]」と記載
      - **構造化テーブルを作成**：商品リスト、税額情報、支払情報を `structured_tables` に表形式で含めてください
      
      ## エラー処理
      
      - 複数のレシートが写っている場合：`{"error": "multiple_receipts"}`
      - レシート以外の画像の場合：`{"error": "not_a_receipt"}`
      - 画像が不鮮明で読み取れない場合：`{"error": "unreadable", "details": "不鮮明な箇所の説明"}`
      

  stage_g:
    classroom: |
      # Stage G: Text Formatting - Classroom Documents
      
      あなたはGoogle Classroom課題ドキュメントのテキスト整形を専門とするAIアシスタントです。
      
      ## タスク
      
      以下は Stage F（視覚解析）で抽出された生のOCR結果とレイアウト情報です：
      
      ---
      $vision_raw
      ---
      
      このテキストをAIが読みやすく、次のStage H（構造化）で処理しやすい形式に整形してください。
      
      ## 整形タスク
      
      1. **省略された文字の補完**
         - OCRで欠けた文字を文脈から推測して補完
         - 不完全な単語を完全な形に修正
      
      2. **レイアウト情報を使った文章の再構成**
         - 改行や空白を適切に配置
         - セクション区切りを明確化
         - 見出しと本文を区別
      
      3. **表構造の整形**
         - 表を読みやすい形式に整形
         - 行と列の関係を明確化
      
      4. **読みやすい文章への変換**
         - 不自然な改行を削除
         - 段落を適切に分割
         - リスト項目を整理
      
      ## 出力形式
      
      整形されたMarkdown形式のテキストを出力してください：
      
      ```markdown
      # 課題タイトル
      
      **期限:** YYYY-MM-DD HH:MM
      
      ## 課題内容
      
      本文テキスト...
      
      ### 提出方法
      
      - リスト項目1
      - リスト項目2
      
      ### 表: タイトル
      | 列1 | 列2 | 列3 |
      |-----|-----|-----|
      | データ1 | データ2 | データ3 |
      ```
      
      ## 注意事項
      
      - Classroom UIのボタン名やメニューは削除
      - 課題内容に関する情報のみを残す
      - 日付は必ず YYYY-MM-DD 形式に統一
      - 不要な空白や改行を削除
      
      **重要:**
      - **元のテキストの意味を変えないでください**
      - **情報を失わないでください**
      - **次のステージ（構造化）で処理しやすい形式にしてください**
      - 課題内容に関わるテキストは1文字も省略しないでください
      

    default: |
      あなたはOCRで抽出された生テキストを整形する専門家です。
      
      以下は Stage F（視覚解析）で抽出された生のOCR結果とレイアウト情報です：
      
      ---
      $vision_raw
      ---
      
      このテキストをAIが読みやすく、構造化しやすい形式に整形してください。
      
      ## 整形タスク
      
      1. **省略された文字の補完**
         - OCRで欠けた文字を文脈から推測して補完
         - 不完全な単語を完全な形に修正
      
      2. **レイアウト情報を使った文章の再構成**
         - 改行や空白を適切に配置
         - セクション区切りを明確化
         - 見出しと本文を区別
      
      3. **表構造の整形**
         - 表を読みやすい形式に整形
         - 行と列の関係を明確化
      
      4. **読みやすい文章への変換**
         - 不自然な改行を削除
         - 段落を適切に分割
         - リスト項目を整理
      
      ## 出力形式
      
      整形されたテキストを以下の形式で出力してください：
      
      ```
      # 見出し1
      
      本文テキスト...
      
      ## 見出し2
      
      本文テキスト...
      
      ### 表: タイトル
      | 列1 | 列2 | 列3 |
      |-----|-----|-----|
      | データ1 | データ2 | データ3 |
      
      - リスト項目1
      - リスト項目2
      ```
      
      **重要:**
      - 元のテキストの意味を変えないでください
      - 情報を失わないでください
      - 次のステージ（構造化）で処理しやすい形式にしてください
      

    flyer: |
      あなたはチラシのOCRテキストを整形する専門家です。
      
      以下は Stage F（視覚解析）で抽出されたチラシの生のOCR結果とレイアウト情報です：
      
      ---
      $vision_raw
      ---
      
      このテキストを商品情報抽出（Stage H）に最適な形式に整形してください。
      
      ## 整形タスク
      
      1. **商品名・価格の正規化**
         - OCRで欠けた文字を補完
         - 価格表記を統一（例: "¥198" → "198円"）
         - 単位を明確化（g, ml, 個など）
      
      2. **商品グループの構造化**
         - カテゴリごとにセクション分け
         - 商品ごとに改行
         - 価格と商品名の対応関係を明確化
      
      3. **セール情報の整理**
         - セール期間を明記
         - 特売条件を整理
         - ポイント情報を明確化
      
      4. **表形式の整形**
         - 商品リストを表形式に変換
         - 列: 商品名、価格、単位、備考
      
      ## 出力形式
      
      以下のような構造化されたテキストを出力してください：
      
      ```
      # 店舗名: XXXスーパー
      チラシ期間: 3/15～3/20
      
      ## カテゴリ: 野菜・果物
      
      | 商品名 | 価格 | 単位 | 備考 |
      |--------|------|------|------|
      | キャベツ | 98円 | 1玉 | 国産 |
      | りんご | 198円 | 1袋 | 青森産 |
      
      ## カテゴリ: 肉・魚
      
      | 商品名 | 価格 | 単位 | 備考 |
      |--------|------|------|------|
      | 豚バラ肉 | 158円 | 100g | 特価 |
      | さば | 298円 | 1尾 | ノルウェー産 |
      
      ## セール情報
      - ポイント5倍デー: 3/17
      - 数量限定商品あり
      ```
      
      **重要:**
      - 価格情報は絶対に省略・変更しないでください
      - 税込/税抜の表記も保持してください
      - 商品名は正確に補完してください
      - 次のステージ（商品情報抽出）で処理しやすい形式にしてください
      

    kakeibo: |
      あなたはレシートのOCR結果から構造化データを生成する専門家です。
      
      以下は Stage F（OCR）で抽出されたレシート全文です：
      
      ---
      $vision_raw
      ---
      
      このテキストから、レシート情報を構造化してJSON形式で出力してください。
      
      ## 構造化タスク
      
      ### 1. 店舗情報の抽出
      - 店舗名
      - 住所
      - 電話番号
      - 店舗コード（記載されていれば）
      
      ### 2. 取引情報の抽出
      - 日付（YYYY-MM-DD形式）
      - 時刻（HH:MM:SS形式、記載されていれば）
      - レジ番号
      - レシート番号
      - 取引番号
      
      ### 3. 商品明細の抽出
      各商品について：
      - 行番号（レシート記載の順序）
      - 行の種類（ITEM=商品、DISCOUNT=値引き、SUBTOTAL=小計、TAX=税額など）
      - 商品名（レシート記載のまま）
      - 数量
      - 単価
      - 金額（値引きの場合は負の値）
      - 税率マーク（※、★など）
      - 値引きテキスト（「2割引」「半額」「10%引」など、記載されていれば）
      - 値引き適用先（この値引きがどの商品に適用されるか、文脈から判断できれば行番号を記載）
      
      ### 4. 金額情報の抽出
      **重要：レシートに記載されている数値をそのまま抽出（計算・推測禁止）**
      
      レシートの記載形式に応じて抽出してください：
      
      #### 外税レシートの場合（「税抜額」+「消費税」が別記載）
      
      **以下の行を探して、記載されている数値をそのまま抽出してください：**
      
      1. **小計**（税抜の合計額）
         - 「小計」「計」などの行
      
      2. **8%税率の情報**（以下の2つを必ず探す）
         - **8%対象額（税抜）**: 「外税 8%対象額」「8%対象」などの行の金額
         - **8%消費税額**: 「外税 8%」「8%税」などの行の金額
      
      3. **10%税率の情報**（以下の2つを必ず探す）
         - **10%対象額（税抜）**: 「外税10%対象額」「10%対象」などの行の金額
         - **10%消費税額**: 「外税10%」「10%税」などの行の金額
      
      4. **合計**（税込支払額）
         - 「合計」「お支払」などの行
      
      #### 内税レシートの場合（「税込額」+「内税額」が記載）
      
      **以下の行を探して、記載されている数値をそのまま抽出してください：**
      
      1. **小計**（税込の合計額）
         - 「小計」「計」などの行
      
      2. **8%税率の情報**（以下の2つを必ず探す）
         - **8%対象計（税込）**: 「8%対象計」「軽減税率対象計」などの行の金額
         - **8%内税額**: 「(内税 ○○円)」「8%内税」などの行の金額
      
      3. **10%税率の情報**（以下の2つを必ず探す）
         - **10%対象計（税込）**: 「10%対象計」などの行の金額
         - **10%内税額**: 「(内税 ○○円)」「10%内税」などの行の金額
      
      4. **合計**（税込支払額）
         - 「合計」「お支払」などの行
      
      #### 共通項目
      - お預かり（記載されていれば）
      - お釣り（記載されていれば）
      
      **数値の読み取りに注意：**
      - 右寄せの数値は、¥マークの後ろの数字を正確に読み取る
      - カンマ区切りの数値もそのまま読み取る
      - 「¥55」は55、「¥68」は68と読み取る（4や118と誤読しない）
      
      ### 5. 支払情報
      - 支払方法（現金/カード/電子マネー等）
      - カード情報（記載されていれば）
      
      ### 6. その他情報
      - ポイント情報
      - キャンペーン情報
      - バーコード番号
      - その他の特記事項
      
      ## 出力形式
      
      以下のJSON形式で出力してください。
      
      ### 外税レシートの例：
      
      ```json
      {
        "shop_info": {
          "name": "店舗名",
          "address": "住所（記載があれば）",
          "phone": "電話番号（記載があれば）",
          "store_code": "店舗コード（記載があれば）"
        },
        "transaction_info": {
          "date": "YYYY-MM-DD",
          "time": "HH:MM:SS（記載があれば）",
          "register_number": "レジ番号（記載があれば）",
          "receipt_number": "レシート番号（記載があれば）",
          "transaction_number": "取引番号（記載があれば）"
        },
        "items": [
          {
            "line_number": 1,
            "line_type": "ITEM",
            "line_text": "レシートのこの行のテキストそのまま",
            "product_name": "商品名",
            "quantity": 1,
            "unit_price": 100,
            "amount": 100,
            "tax_mark": "※または★またはなし",
            "discount_text": null,
            "discount_applied_to": null
          },
          {
            "line_number": 2,
            "line_type": "DISCOUNT",
            "line_text": "▲値引 20%",
            "product_name": "値引",
            "quantity": 1,
            "unit_price": null,
            "amount": -20,
            "tax_mark": null,
            "discount_text": "20%",
            "discount_applied_to": 1
          }
        ],
        "amounts": {
          "subtotal": 1377,
          "tax_8_base": 0,
          "tax_8_amount": 0,
          "tax_10_base": 1377,
          "tax_10_amount": 123,
          "total_tax": 123,
          "total": 1500,
          "received": 2000,
          "change": 500,
          "tax_display_type": "excluded"
        },
        "payment": {
          "method": "現金",
          "card_info": null
        },
        "other_info": {
          "points": "ポイント情報",
          "campaign": "キャンペーン情報",
          "barcode": "バーコード番号",
          "notes": "その他特記事項"
        }
      }
      ```
      
      ### 内税レシートの例：
      
      ```json
      {
        "shop_info": {
          "name": "サイゼリヤ",
          "address": "イトーヨーカドー武蔵小杉駅前",
          "phone": "044-711-6451",
          "store_code": null
        },
        "transaction_info": {
          "date": "2025-11-01",
          "time": "18:13:00",
          "register_number": "001",
          "receipt_number": "0320",
          "transaction_number": "5264"
        },
        "items": [
          {
            "line_number": 1,
            "line_type": "ITEM",
            "line_text": "04633 チキンのサラダ",
            "product_name": "チキンのサラダ",
            "quantity": 1,
            "unit_price": 350,
            "amount": 350,
            "tax_mark": null,
            "discount_text": null,
            "discount_applied_to": null
          },
          {
            "line_number": 2,
            "line_type": "ITEM",
            "line_text": "02626 アロスティチーニ",
            "product_name": "アロスティチーニ",
            "quantity": 1,
            "unit_price": 400,
            "amount": 400,
            "tax_mark": null,
            "discount_text": null,
            "discount_applied_to": null
          }
        ],
        "amounts": {
          "subtotal": 3980,
          "tax_8_base": null,
          "tax_8_amount": null,
          "tax_10_base": 3980,
          "tax_10_amount": 361,
          "total_tax": 361,
          "total": 3980,
          "received": null,
          "change": null,
          "tax_display_type": "included"
        },
        "payment": {
          "method": "クレジット",
          "card_info": null
        },
        "other_info": {
          "points": null,
          "campaign": null,
          "barcode": null,
          "notes": "※印は軽減税率対象品目です"
        }
      }
      ```
      
      ## 重要な注意事項
      
      1. **数値は必ずレシート記載のまま**
         - 計算しない、推測しない
         - 記載がない項目は null にする
      
      2. **内税・外税の判別**
         - **外税レシート**: 「小計」+「消費税」=「合計」の形式。tax_display_type: "excluded"
         - **内税レシート**: 「合計」に税込で、「(内税額 ○○円)」と記載。tax_display_type: "included"
         - レシートに「内税額」「うち消費税」などの記載があれば内税レシート
         - 「○%対象計」という記載は税込額を指す（内税の場合）
      
      3. **8%/10%税率の区分**
         - レシートに税率の記載（※、★、8%、10%マーク）があればそれを使う
         - 記載がない場合は null にする（推測しない）
      
      4. **税額サマリーの抽出**
         - 外税：「8%対象 ○○円（税 △△円）」→ tax_8_base: ○○, tax_8_amount: △△
         - 内税：「8%対象計 ○○円 (内税額 △△円)」→ tax_8_base: ○○, tax_8_amount: △△
         - **重要**: 内税の場合、tax_8_base は税込額、tax_8_amount は内税額
         - 外税の場合、tax_8_base は税抜額、tax_8_amount は消費税額
      
      5. **記載がない項目**
         - 記載がない項目は null にする
         - 空文字列や0ではなく null を使う
      
      6. **値引きの取り扱い**
         - 値引き行（「▲値引」「割引」など）は line_type: "DISCOUNT" として別の明細行にする
         - 値引き金額は負の値（例：-20）で記録
         - 可能であれば discount_applied_to にどの商品の値引きかを記載（行番号）
         - 値引き率や割引内容は discount_text に記載（例：「20%」「半額」）
         - 値引き適用先が不明な場合は discount_applied_to: null にする
      
      ## エラー処理
      
      - OCR結果が不完全な場合：`{"error": "incomplete_ocr", "details": "不完全な箇所の説明"}`
      - レシートとして認識できない場合：`{"error": "not_a_receipt"}`
      

  stage_h:
    classroom: |
      # Stage H: 構造化（Google Classroom ドキュメント）
      
      あなたは Google Classroom のドキュメントから構造化データを抽出する専門家です。
      
      ## 入力情報
      
      - **ファイル名**: $file_name
      - **ドキュメントタイプ**: $doc_type
      - **ワークスペース**: $workspace
      - **現在日付**: $current_date
      
      ## テキスト
      
      ```
      $combined_text
      ```
      
      ## タスク
      
      このGoogle Classroomのテキストから、投稿文と添付ファイルの**全ての文章を余すところなく**抽出し、**text_blocks**と**表構造**に仕分けてJSON形式で出力してください。
      
      ## ⚠️ Stage Hの役割の明確化（最重要）
      
      **Stage Hの仕事は「振り分け」であり、「要約」でも「解釈」でも「選別」でもありません。**
      
      ### Stage Hの3つの原則:
      
      1. **全ての情報を集める**
         - combined_text（添付ファイルのテキスト）
         - 投稿者、件名、投稿日時などのメタデータ
         - **全ての情報源から、1文字も漏らさず収集**
      
      2. **適切に振り分ける**
         - 表データ → structured_tables, weekly_schedule
         - テキストブロック → text_blocks
         - 基本情報 → basic_info
         - その他 → other_text
         - **どこかに必ず振り分ける（捨てない）**
      
      3. **要約・解釈は一切しない**
         - ❌ 「保護者会が開催されます」（要約）
         - ✅ 「来週の木曜日、午後2時より体育館にて保護者会を開催いたします。ご多忙とは存じますが、ぜひご参加くださいますようお願い申し上げます。」（原文そのまま）
         - **原文を一字一句そのまま振り分ける**
      
      ### 振り分けの検証:
      
      **元のテキストの文字数 = 振り分け後の全文字数の合計**
      
      この等式が成立しない場合、情報が失われています。必ず再確認してください。
      
      ### 抽出項目
      
      1. **document_date** (文字列, YYYY-MM-DD形式, または null)
         - 投稿日時、または課題の期限日
         - 優先順位: 期限日 > 投稿日
      
      2. **tags** (文字列の配列)
         - 検索用のタグ 5-10個
         - 科目名、投稿タイプ、優先度、関連トピックなど
      
      3. **metadata** (オブジェクト)
         - 汎用スキーマに基づく構造化データ
         - Classroom固有の情報も含める
      
      ## metadata フィールドの構造
      
      ★★★ データ振り分けの基本原則 ★★★
      **投稿文と添付ファイルの全ての文章を余すところなく抽出し、text_blocks と表構造に仕分けてください。**
      
      ### metadataフィールドの構造:
      
      ```json
      {
        "basic_info": {
          "post_date": "投稿日時 (YYYY-MM-DD)",
          "author": "投稿者名",
          "subject": "件名",
          "post_type": "投稿タイプ（assignment/announcement/material/question）",
          "deadline": "期限（YYYY-MM-DD）",
          "priority": "優先度（high/medium/low）",
          "related_class": "関連する授業・科目名"
        },
        "text_blocks": [
          {
            "title": "見出し（例: 課題の説明、お知らせ、連絡事項）",
            "content": "本文（原文そのまま、一切省略せず）"
          }
        ],
        "weekly_schedule": [
          {
            "date": "YYYY-MM-DD",
            "day_of_week": "曜日",
            "events": ["行事1", "行事2"],
            "note": "持ち物や連絡事項"
          }
        ],
        "structured_tables": [
          {
            "table_title": "表のタイトル",
            "table_type": "requirements/schedule/grades など",
            "headers": ["列1", "列2", "列3"],
            "rows": [
              {"列1": "値1", "列2": "値2", "列3": "値3"}
            ]
          }
        ],
        "special_events": ["特別イベント"],
        "attachments": ["添付ファイル名"],
        "other_text": [
          {
            "type": "greeting/contact_info/signature/note/footer/misc",
            "content": "その他のテキスト（時候の挨拶、連絡先、署名、注釈など）"
          }
        ]
      }
      ```
      
      ### データ振り分けルール（必ず守ること）
      
      1. **text_blocks**: すべてのテキストコンテンツをトピックごとに分けたもの
         - 課題の説明、お知らせ本文、連絡事項、コメント、質問など
         - **すべてのテキスト**（長文でも短い箇条書きでも）を text_blocks に含めてください
         - 見出しがない場合は適切なタイトルをつけてください（例: 「課題の説明」「お知らせ」「連絡事項」）
         - title（見出し）と content（本文全文）のペアで記録
         - **content は一切省略せず、原文そのまま全文を記録**
      
         **【重要】text_blocks の粒度ガイドライン（ベクトル検索精度向上のため）**
         - **原則: 1トピック = 1ブロック**
         - 長い説明文は、サブトピックごとに分割してください
         - 各ブロックは **100-300文字を目安** に（厳密でなくてもOK、トピックの境界を優先）
         - 例: 「課題の説明」が長い場合:
           - ✅ 「課題の目的」「提出方法」「評価基準」「注意事項」に分割
           - ❌ 全体を1つのブロックにまとめる（500文字以上になる場合）
         - 理由: ベクトル検索では「提出方法は？」という質問に対して、該当ブロックのみがヒットする必要がある
      
      2. **weekly_schedule**: 日ごとの時間割・スケジュール（時間割表、週間予定表）
         - 日付、曜日、その日の行事・イベントが記載された表
         - 授業科目が時限ごとに記載された時間割
         - **【重要】表内の全ての日付行・時限行を抽出すること**
         - 【必須フィールド】: date, day_of_week
         - 【任意フィールド】: events (行事), class_schedules (クラス別時間割), note
         - クラス別時間割がある場合は class_schedules 配列を使用
         - **月間予定表も weekly_schedule へ**
      
      3. **structured_tables**: 上記1-2に当てはまらないその他の表データ
         - 成績表、提出物リスト、スケジュール表、ルーブリックなど
         - table_title（表のタイトル）、table_type（種類）、headers（列名）、rows（行データ）で構造化
         - **【重要】表内の全ての行を抽出すること**
      
      4. **basic_info**: Classroom固有の基本情報
         - 投稿日時、投稿者、件名、投稿タイプ、期限、優先度、関連科目
         - Classroom特有の情報を優先的に抽出
      
      5. **attachments**: 添付ファイル名の配列
      
      6. **other_text**: 上記1-5に該当しない、その他の全てのテキスト
         - 時候の挨拶（「いつもお世話になっております」等）
         - お問い合わせ窓口・連絡先（「お問い合わせ: 03-1234-5678」等）
         - 署名・落款（「○○学校 △△先生」等）
         - 注釈・補足（「※印の文章」「補足説明」等）
         - フッター情報（「本メールは自動送信です」等）
         - その他、分類しづらいが存在する全てのテキスト
         - **重要**: 「本題と関係ない」と判断して削除してはいけません
         - **理由**: 利用するかどうかの判断は人間が行います。AIは全ての情報を忠実に抽出することに専念してください
      
      ## 表構造抽出ガイドライン
      
      ### 表の検出（AIの腕の見せ所）
      
      文書内に以下のパターンがあれば**表として認識**してください:
      
      - **列区切り**: 空白またはタブで区切られた複数の列
      - **行区切り**: 改行で区切られた複数の行
      - **ヘッダー行**: 1行目または最初の数行が見出し
      - **データ行**: ヘッダー以降が実際のデータ
      - **罫線なしでもOK**: 視覚的な罫線がなくても、列区切りと行区切りがあれば表として扱う
      - **key-valueペア**: 「項目名: 値」の繰り返しも表として構造化
      
      **例: 罫線なしの表**:
      ```
      日付    曜日   1限      2限      3限
      11/18   月     国語     算数     理科
      11/19   火     算数     国語     社会
      ```
      
      **例: key-valueペアを表組に**:
      ```
      提出期限: 2025-01-15
      提出方法: Classroom
      評価配分: 50点
      注意事項: 解答過程も記載すること
      ```
      → structured_tables で以下のように構造化:
      ```json
      {
        "table_title": "課題詳細",
        "table_type": "requirements",
        "headers": ["項目", "内容"],
        "rows": [
          {"項目": "提出期限", "内容": "2025-01-15"},
          {"項目": "提出方法", "内容": "Classroom"},
          {"項目": "評価配分", "内容": "50点"},
          {"項目": "注意事項", "内容": "解答過程も記載すること"}
        ]
      }
      ```
      
      ### データの正規化ルール
      
      #### ケース1: 時間割表（weekly_schedule の periods 配列）
      
      **元のテキスト:** "1-2限: 体育"
      
      **✅ 正解: 時限ごとに展開**
      ```json
      {
        "class_schedules": [{
          "class": "5A",
          "periods": [
            {"period": 1, "subject": "体育"},
            {"period": 2, "subject": "体育"}
          ]
        }]
      }
      ```
      
      **❌ 誤り: "1-2限"を文字列として保持**
      ```json
      {
        "periods": [
          {"period": "1-2限", "subject": "体育"}  // NG: periodは数値であるべき
        ]
      }
      ```
      
      #### ケース2: イベント名や連絡事項（text_blocks, note フィールド）
      
      **元のテキスト:** "1-2限は体育館で集会"
      
      **✅ 正解: 原文のまま保持**
      ```json
      {
        "text_blocks": [
          {"title": "連絡事項", "content": "1-2限は体育館で集会"}
        ]
      }
      ```
      
      **❌ 誤り: 不必要に分解**
      ```json
      {
        "text_blocks": [
          {"title": "連絡事項", "content": "1限は体育館で集会"},
          {"title": "連絡事項", "content": "2限は体育館で集会"}
        ]
      }
      ```
      
      ### 重要な注意事項
      
      **✅ すべきこと:**
      1. **原文を尊重**: 文書に書かれている通りに抽出
      2. **構造を明示**: JSON形式で構造化
      3. **全量抽出**: 表の全ての行、テキストの全量を抽出
      
      **❌ してはいけないこと:**
      1. **推測**: 記載されていない情報を補完しない
      2. **分解**: "1-2限" を "1限", "2限" に分けない
      3. **省略**: 一部だけを代表例として抽出し、残りを省略する
      4. **要約**: テキストを要約したり言い換えたりしない
      
      ## ⚠️ 絶対原則：全量抽出
      
      **文書内の全てのテキストを、必ずどこかに分類してください**
      
      ### 分類フロー:
      
      1. text_blocks に該当する → text_blocks へ
      2. 表として構造化できる → structured_tables へ
      3. スケジュール情報 → weekly_schedule へ
      4. **上記のどれにも該当しない** → other_text へ
      
      ### 重要事項:
      
      - **情報の欠損・省略は一切禁止**
      - 投稿文と添付ファイルの**全ての文章を余すところなく**抽出
      - **要約・言い換えは厳禁**（特に text_blocks の content、note フィールド）
      - **【配列フィールドの完全抽出】**: text_blocks, structured_tables などの配列フィールドは、**全ての行**を抽出すること
      - 日付は必ず YYYY-MM-DD 形式で統一
      - 見つからない情報は null または空のリスト [] を設定
      - **表にできるものは表に仕立てる**（AIの腕の見せ所）
      
      ### 検証ステップ（必須）
      
      抽出後、以下を確認してください：
      
      1. **元のテキストを読み直す**
      2. **抽出結果に含まれていないテキストがないか確認**
      3. **漏れがあれば、適切なカテゴリ（またはother_text）に追加**
      4. **文字数の合計がほぼ一致するまで繰り返す**
      
      例：
      - 元のテキスト: 1500文字
      - 抽出結果の合計: 1480文字 → ✅ 許容範囲（98%以上）
      - 抽出結果の合計: 1200文字 → ❌ 300文字（20%）漏れている！再確認必須
      
      **全てのテキストが、最終的にどこかのチャンク（text_blocks, structured_tables, weekly_schedule, other_text のいずれか）に含まれていることを保証してください。**
      
      ## 出力形式
      
      ```json
      {
        "document_date": "2025-03-20",
        "tags": ["数学", "課題", "期限あり", "高優先度"],
        "metadata": {
          "basic_info": {
            "post_date": "2025-03-15",
            "author": "田中先生",
            "subject": "第5回課題：微分の応用",
            "post_type": "assignment",
            "deadline": "2025-03-20",
            "priority": "high",
            "related_class": "数学"
          },
          "text_blocks": [
            {"title": "課題の目的", "content": "今回の課題では...（100-300文字程度）"},
            {"title": "提出方法", "content": "Classroomから提出してください..."},
            {"title": "評価基準", "content": "以下の基準で評価します..."}
          ],
          "weekly_schedule": [],
          "structured_tables": [],
          "special_events": [],
          "attachments": ["課題5.pdf"],
          "other_text": [
            {"type": "greeting", "content": "いつもお世話になっております"},
            {"type": "signature", "content": "数学科 田中太郎"}
          ]
        }
      }
      ```
      
      **重要:**
      - **必ずJSON形式で出力してください**（```json ブロック内に記述）
      - document_date は期限日を優先、なければ投稿日、それも不明なら null
      - JSON構文エラー（カンマ、括弧、引用符の不一致）に十分注意してください
      
      ## ⚠️ Gemini特有の注意事項（必ず守ること）
      
      ### 推測・補完の厳禁（AI推論エンジンよりも厳格）
      - **原文に記載されていない情報は、一切推測・補完しないこと**
      - 確信がない場合は、必ず null または空配列 [] を使用
      - 「おそらく〜」「〜と思われる」といった推測は一切行わない
      - **Geminiは文脈から情報を補完する傾向があるため、特に注意**
      
      ### 要約・言い換えの厳禁（学年便り等の文章系ドキュメント）
      - **学年便り、連絡事項、お知らせなどの文章は、一切要約せず原文のまま全文を抽出**
      - content フィールドには、**原文をそのまま一字一句正確に記録**
      - **Geminiは文章を要約・言い換える傾向があるため、これを厳重に禁止**
      - 例：
        - ❌ 「保護者会が開催されます」（要約）
        - ✅ 「来週の木曜日、午後2時より体育館にて保護者会を開催いたします。ご多忙とは存じますが、ぜひご参加くださいますようお願い申し上げます。」（原文そのまま）
      
      ### 論理的処理の活用（スケジュール・時間割等）
      - スケジュール抽出時は、日付・時限の整合性を論理的に検証
      - 時間割の時限数が正しいか確認（例：6時限制なのに7限目があればエラー）
      - 曜日と日付の整合性を確認（例：2025-01-06は月曜日）
      - **Geminiの論理的思考能力を活用し、データの整合性を保証**
      

    default: |
      # Stage H: 構造化（Structuring）
      
      あなたは非構造化テキストから構造化データを抽出する専門家です。
      
      ## 入力情報
      
      - **ファイル名**: $file_name
      - **ドキュメントタイプ**: $doc_type
      - **ワークスペース**: $workspace
      - **現在日付**: $current_date
      
      ## テキスト
      
      ```
      $combined_text
      ```
      
      ## タスク
      
      このテキストから、投稿文と添付ファイルの**全ての文章を余すところなく**抽出し、**text_blocks**と**表構造**に仕分けてJSON形式で出力してください。
      
      ### 抽出項目
      
      1. **document_date** (文字列, YYYY-MM-DD形式, または null)
         - ドキュメントの基準日（作成日、発行日、イベント日など）
      
      2. **tags** (文字列の配列)
         - 検索用のタグ 5-10個
         - カテゴリ、トピック、キーワードなど
      
      3. **metadata** (オブジェクト)
         - 汎用スキーマに基づく構造化データ
         - 以下の構造に従ってください
      
      ## metadata フィールドの構造
      
      ★★★ データ振り分けの基本原則 ★★★
      **投稿文と添付ファイルの全ての文章を余すところなく抽出し、text_blocks と表構造に仕分けてください。**
      
      ### metadataフィールドの構造:
      
      ```json
      {
        "basic_info": {
          "document_title": "文書タイトル",
          "issue_date": "発行日（YYYY-MM-DD）",
          "period": "対象期間（例: 2024年11月18日-21日）",
          "document_number": "文書番号（例: 第12号）"
        },
        "text_blocks": [
          {
            "title": "見出し（例: 朝会「マナーとルールについて」）",
            "content": "本文（原文そのまま、一切省略せず）"
          }
        ],
        "weekly_schedule": [
          {
            "date": "YYYY-MM-DD",
            "day": "曜日（月、火など）",
            "day_of_week": "曜日フル（月曜日など）",
            "events": ["行事1", "行事2"],
            "class_schedules": [
              {
                "class": "5A",
                "subjects": ["1限:国語", "2限:算数", "3限:理科"],
                "periods": [
                  {"period": 1, "subject": "国語", "time": "8:45-9:30"},
                  {"period": 2, "subject": "算数", "time": "9:40-10:25"}
                ]
              }
            ],
            "note": "持ち物や連絡事項（原文そのまま）"
          }
        ],
        "structured_tables": [
          {
            "table_title": "表のタイトル",
            "table_type": "requirements/events/scores/contacts など",
            "headers": ["列1", "列2", "列3"],
            "rows": [
              {"列1": "値1", "列2": "値2", "列3": "値3"}
            ]
          }
        ],
        "special_events": [
          "特別イベント1"
        ],
        "other_text": [
          {
            "type": "greeting/contact_info/signature/note/footer/misc",
            "content": "その他のテキスト（時候の挨拶、連絡先、署名、注釈など）"
          }
        ]
      }
      ```
      
      ### データ振り分けルール（必ず守ること）
      
      1. **text_blocks**: すべてのテキストコンテンツをトピックごとに分けたもの
         - 朝会の話、今日のふりかえり、道徳の内容、先生からのメッセージ、連絡事項、お知らせなど
         - 学級通信や学年通信の記事、お知らせ本文、箇条書きの連絡事項など
         - **すべてのテキスト**（長文でも短い箇条書きでも）を text_blocks に含めてください
         - 見出しがない場合は適切なタイトルをつけてください（例: 「連絡事項」「お知らせ」「持ち物について」）
         - title（見出し）と content（本文全文）のペアで記録
         - **content は一切省略せず、原文そのまま全文を記録**
      
         **【重要】text_blocks の粒度ガイドライン（ベクトル検索精度向上のため）**
         - **原則: 1トピック = 1ブロック**
         - 長い説明文は、サブトピックごとに分割してください
         - 各ブロックは **100-300文字を目安** に（厳密でなくてもOK、トピックの境界を優先）
         - 例: 「お知らせ」が長い場合:
           - ✅ 「イベント概要」「参加方法」「注意事項」「持ち物」に分割
           - ❌ 全体を1つのブロックにまとめる（500文字以上になる場合）
         - 理由: ベクトル検索では「持ち物は？」という質問に対して、該当ブロックのみがヒットする必要がある
      
      2. **weekly_schedule**: 日ごとの時間割・スケジュール（時間割表、週間予定表）
         - 日付、曜日、その日の行事・イベントが記載された表
         - 授業科目が時限ごとに記載された時間割
         - **【重要】表内の全ての日付行・時限行を抽出すること**
         - 【必須フィールド】: date, day_of_week
         - 【任意フィールド】: events (行事), class_schedules (クラス別時間割), note
         - クラス別時間割がある場合は class_schedules 配列を使用
         - **月間予定表も weekly_schedule へ**
      
      3. **structured_tables**: 上記1-2に当てはまらないその他の表データ
         - 持ち物リスト、成績表、提出物リスト、制服価格表、給食献立、会場一覧など
         - 時間割・予定表でない汎用的な表はすべてこちら
         - table_title（表のタイトル）、table_type（種類）、headers（列名）、rows（行データ）で構造化
         - **【重要】表内の全ての行を抽出すること**
      
      4. **basic_info**: 文書の基本情報
         - 文書タイトル、発行日、対象期間などの基本情報を抽出
         - **注意**: basic_infoの中身は文書の種類に応じて柔軟に設定してください
         - 固定フィールド（school_name, gradeなど）にこだわらず、ドキュメントに適したフィールドを使用
      
      5. **special_events**: 特別イベント・行事
         - 通常授業以外の特別な予定
      
      6. **other_text**: 上記1-5に該当しない、その他の全てのテキスト
         - 時候の挨拶（「いつもお世話になっております」等）
         - お問い合わせ窓口・連絡先（「お問い合わせ: 03-1234-5678」等）
         - 署名・落款（「○○学校 △△先生」等）
         - 注釈・補足（「※印の文章」「補足説明」等）
         - フッター情報（「本メールは自動送信です」等）
         - その他、分類しづらいが存在する全てのテキスト
         - **重要**: 「本題と関係ない」と判断して削除してはいけません
         - **理由**: 利用するかどうかの判断は人間が行います。AIは全ての情報を忠実に抽出することに専念してください
      
      ## 表構造抽出ガイドライン
      
      ### 表の検出（AIの腕の見せ所）
      
      文書内に以下のパターンがあれば**表として認識**してください:
      
      - **列区切り**: 空白またはタブで区切られた複数の列
      - **行区切り**: 改行で区切られた複数の行
      - **ヘッダー行**: 1行目または最初の数行が見出し
      - **データ行**: ヘッダー以降が実際のデータ
      - **罫線なしでもOK**: 視覚的な罫線がなくても、列区切りと行区切りがあれば表として扱う
      - **key-valueペア**: 「項目名: 値」の繰り返しも表として構造化
      
      **例: 罫線なしの表**:
      ```
      日付    曜日   1限      2限      3限
      11/18   月     国語     算数     理科
      11/19   火     算数     国語     社会
      ```
      
      **例: key-valueペアを表組に**:
      ```
      連絡方法: 早稲田アカデミーOnline
      電話番号: 03-1234-5678
      受付時間: 平日9:00-17:00
      会場数: 44会場
      ```
      → structured_tables で以下のように構造化:
      ```json
      {
        "table_title": "連絡先情報",
        "table_type": "contact_info",
        "headers": ["項目", "内容"],
        "rows": [
          {"項目": "連絡方法", "内容": "早稲田アカデミーOnline"},
          {"項目": "電話番号", "内容": "03-1234-5678"},
          {"項目": "受付時間", "内容": "平日9:00-17:00"},
          {"項目": "会場数", "内容": "44会場"}
        ]
      }
      ```
      
      ### データの正規化ルール
      
      #### ケース1: 時間割表（weekly_schedule の periods 配列）
      
      **元のテキスト:** "1-2限: 体育"
      
      **✅ 正解: 時限ごとに展開**
      ```json
      {
        "class_schedules": [{
          "class": "5A",
          "periods": [
            {"period": 1, "subject": "体育"},
            {"period": 2, "subject": "体育"}
          ]
        }]
      }
      ```
      
      **❌ 誤り: "1-2限"を文字列として保持**
      ```json
      {
        "periods": [
          {"period": "1-2限", "subject": "体育"}  // NG: periodは数値であるべき
        ]
      }
      ```
      
      #### ケース2: イベント名や連絡事項（text_blocks, note フィールド）
      
      **元のテキスト:** "1-2限は体育館で集会"
      
      **✅ 正解: 原文のまま保持**
      ```json
      {
        "text_blocks": [
          {"title": "連絡事項", "content": "1-2限は体育館で集会"}
        ]
      }
      ```
      
      **❌ 誤り: 不必要に分解**
      ```json
      {
        "text_blocks": [
          {"title": "連絡事項", "content": "1限は体育館で集会"},
          {"title": "連絡事項", "content": "2限は体育館で集会"}
        ]
      }
      ```
      
      ### 重要な注意事項
      
      **✅ すべきこと:**
      1. **原文を尊重**: 文書に書かれている通りに抽出
      2. **構造を明示**: JSON形式で構造化
      3. **全量抽出**: 表の全ての行、テキストの全量を抽出
      
      **❌ してはいけないこと:**
      1. **推測**: 記載されていない情報を補完しない
      2. **分解**: "1-2限" を "1限", "2限" に分けない
      3. **省略**: 一部だけを代表例として抽出し、残りを省略する
      4. **要約**: テキストを要約したり言い換えたりしない
      
      ## ⚠️ 絶対原則：全量抽出
      
      **文書内の全てのテキストを、必ずどこかに分類してください**
      
      ### 分類フロー:
      
      1. text_blocks に該当する → text_blocks へ
      2. 表として構造化できる → structured_tables へ
      3. スケジュール情報 → weekly_schedule へ
      4. **上記のどれにも該当しない** → other_text へ
      
      ### 重要事項:
      
      - **情報の欠損・省略は一切禁止**
      - 投稿文と添付ファイルの**全ての文章を余すところなく**抽出
      - **要約・言い換えは厳禁**（特に text_blocks の content、note フィールド）
      - **【配列フィールドの完全抽出】**: text_blocks, structured_tables などの配列フィールドは、**全ての行**を抽出すること
      - 日付は必ず YYYY-MM-DD 形式で統一
      - 見つからない情報は null または空のリスト [] を設定
      - **表にできるものは表に仕立てる**（AIの腕の見せ所）
      
      ### 検証ステップ（必須）
      
      抽出後、以下を確認してください：
      
      1. **元のテキストを読み直す**
      2. **抽出結果に含まれていないテキストがないか確認**
      3. **漏れがあれば、適切なカテゴリ（またはother_text）に追加**
      4. **文字数の合計がほぼ一致するまで繰り返す**
      
      例：
      - 元のテキスト: 1500文字
      - 抽出結果の合計: 1480文字 → ✅ 許容範囲（98%以上）
      - 抽出結果の合計: 1200文字 → ❌ 300文字（20%）漏れている！再確認必須
      
      **全てのテキストが、最終的にどこかのチャンク（text_blocks, structured_tables, weekly_schedule, other_text のいずれか）に含まれていることを保証してください。**
      
      ## 出力形式
      
      ```json
      {
        "document_date": "YYYY-MM-DD",
        "tags": ["タグ1", "タグ2", "タグ3"],
        "metadata": {
          "basic_info": {
            "document_title": "◯◯のご案内",
            "issue_date": "YYYY-MM-DD"
          },
          "text_blocks": [
            {"title": "イベント概要", "content": "今週は...（100-300文字程度）"},
            {"title": "参加方法", "content": "参加を希望される方は..."},
            {"title": "持ち物", "content": "当日は以下をご持参ください..."}
          ],
          "weekly_schedule": [],
          "structured_tables": [],
          "special_events": [],
          "other_text": [
            {"type": "greeting", "content": "いつもお世話になっております"},
            {"type": "contact_info", "content": "お問い合わせ: 03-1234-5678"}
          ]
        }
      }
      ```
      
      **重要:**
      - **必ずJSON形式で出力してください**（```json ブロック内に記述）
      - JSON構文エラー（カンマ、括弧、引用符の不一致）に十分注意してください
      
      ## ⚠️ Gemini特有の注意事項（必ず守ること）
      
      ### 推測・補完の厳禁（AI推論エンジンよりも厳格）
      - **原文に記載されていない情報は、一切推測・補完しないこと**
      - 確信がない場合は、必ず null または空配列 [] を使用
      - 「おそらく〜」「〜と思われる」といった推測は一切行わない
      - **Geminiは文脈から情報を補完する傾向があるため、特に注意**
      
      ### 要約・言い換えの厳禁（文章系ドキュメント）
      - **お知らせ、連絡事項、記事などの文章は、一切要約せず原文のまま全文を抽出**
      - content フィールドには、**原文をそのまま一字一句正確に記録**
      - **Geminiは文章を要約・言い換える傾向があるため、これを厳重に禁止**
      - 例：
        - ❌ 「イベントが開催されます」（要約）
        - ✅ 「来月15日、午前10時より市民ホールにてイベントを開催いたします。参加ご希望の方は事前にお申し込みください。」（原文そのまま）
      
      ### 論理的処理の活用（表・スケジュール等）
      - 表データ抽出時は、行数・列数の整合性を論理的に検証
      - 日付と曜日の整合性を確認（例：2025-01-06は月曜日）
      - 数値データの合計・範囲が妥当か確認
      - **Geminiの論理的思考能力を活用し、データの整合性を保証**
      

    flyer: |
      # Stage H: 構造化（チラシ商品情報抽出）
      
      あなたはチラシから商品情報を構造化して抽出する専門家です。
      
      ## 入力情報
      
      - **ファイル名**: $file_name
      - **ドキュメントタイプ**: $doc_type
      - **ワークスペース**: $workspace
      - **現在日付**: $current_date
      
      ## テキスト
      
      ```
      $combined_text
      ```
      
      ## タスク
      
      整形されたチラシテキストから、商品情報を構造化して抽出してJSON形式で出力してください。
      
      ### 抽出項目
      
      1. **document_date** (文字列, YYYY-MM-DD形式, または null)
         - チラシの有効期間開始日、またはセール開始日
         - 例: "2025-03-15"
      
      2. **tags** (文字列の配列)
         - 検索用のタグ 5-10個
         - 店舗名、商品カテゴリ、セール情報など
         - 例: ["XXXスーパー", "野菜", "特価", "ポイント5倍"]
      
      3. **metadata** (オブジェクト)
         - **store_name**: 店舗名
         - **valid_period**: チラシ有効期間 {{"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}}
         - **products**: 商品情報の配列
           - name: 商品名
           - price: 価格（数値）
           - unit: 単位
           - category: カテゴリ
           - notes: 備考（特価、産地など）
         - **sale_info**: セール情報の配列
         - その他、テキストから抽出できる有用な情報
      
      ## 出力形式
      
      ```json
      {{
        "document_date": "2025-03-15",
        "tags": ["XXXスーパー", "野菜・果物", "精肉", "特価セール", "ポイント5倍"],
        "metadata": {{
          "store_name": "XXXスーパー",
          "valid_period": {{
            "start": "2025-03-15",
            "end": "2025-03-20"
          }},
          "products": [
            {{
              "name": "キャベツ",
              "price": 98,
              "unit": "1玉",
              "category": "野菜・果物",
              "notes": "国産"
            }},
            {{
              "name": "豚バラ肉",
              "price": 198,
              "unit": "100g",
              "category": "精肉",
              "notes": "特価"
            }}
          ],
          "sale_info": [
            "ポイント5倍デー: 3/17",
            "数量限定商品あり"
          ]
        }}
      }}
      ```
      
      ## ⚠️ 絶対原則：全量抽出
      
      **チラシ内の全ての商品情報を、必ず抽出してください**
      
      ### 検証ステップ（必須）
      
      抽出後、以下を確認してください：
      
      1. **元のチラシを読み直す**
      2. **抽出結果に含まれていない商品がないか確認**
      3. **漏れがあれば products 配列に追加**
      4. **商品数が一致するまで繰り返す**
      
      例：
      - 元のチラシ: 25商品掲載
      - 抽出結果: 25商品 → ✅ 完璧
      - 抽出結果: 20商品 → ❌ 5商品漏れている！再確認必須
      
      **全ての商品が products 配列に含まれていることを保証してください。**
      
      ## 重要な注意事項
      
      - **必ずJSON形式で出力してください**（```json ブロック内に記述）
      - document_date は有効期間の開始日を使用、不明なら null
      - tags は必ず配列形式で出力（店舗名や主要商品カテゴリを含める）
      - metadata には、商品情報を詳細に含める
      - **商品の省略は厳禁**：全ての商品を products 配列に含めてください
      
      ## ⚠️ Gemini特有の注意事項（必ず守ること）
      
      ### 推測・補完の厳禁（AI推論エンジンよりも厳格）
      - **原文（チラシ）に記載されていない情報は、一切推測・補完しないこと**
      - 価格が不明な商品は price: null とする
      - 産地が不明な商品は notes に産地情報を含めない
      - 確信がない場合は、必ず null を使用
      - **Geminiは文脈から情報を補完する傾向があるため、特に注意**
      
      ### 論理的処理の活用（商品データ検証）
      - **Geminiの論理的思考能力を活用し、以下を検証**
      - チラシに記載された商品総数と、抽出した products 配列の要素数が一致するか確認
      - 同じ商品が重複して抽出されていないか確認
      - カテゴリ分類が論理的に正しいか確認（例：キャベツは「野菜・果物」）
      - 価格の単位（円、g、個）が正しく抽出されているか確認
      

    kakeibo: |
      あなたは家計簿の商品をカテゴリ分類する専門家です。
      
      以下の商品名をカテゴリ分類してください。
      
      ---
      商品名：{product_name}
      ---
      
      ## 分類基準
      
      ### 食品（軽減税率8%対象）
      - 生鮮食品（肉、魚、野菜、果物）
      - 加工食品（パン、麺、冷凍食品、調味料）
      - 飲料（水、お茶、ジュース、牛乳）
      - 菓子類
      
      ### 日用品（標準税率10%）
      - 洗剤、掃除用品
      - ティッシュ、トイレットペーパー
      - シャンプー、石鹸、歯磨き粉
      - ゴミ袋、ラップ、アルミホイル
      - 電池、電球
      - 文房具
      
      ### 医薬品・衛生用品（標準税率10%）
      - 医薬品、サプリメント
      - マスク、消毒液
      - 生理用品、おむつ
      
      ### 酒類（標準税率10%）
      - ビール、日本酒、ワイン、焼酎
      - チューハイ、ハイボール
      
      ### 外食（標準税率10%）
      - 店内飲食
      - イートインスペース利用
      
      ### その他
      - 上記に該当しないもの
      
      ## 出力形式
      
      以下のJSON形式で出力してください：
      
      ```json
      {
        "category": "食品/日用品/医薬品/酒類/外食/その他",
        "subcategory": "具体的な分類（例：生鮮食品、洗剤、等）",
        "tax_rate": 8 or 10,
        "confidence": "high/medium/low"
      }
      ```
      
      ## ⚠️ 絶対原則：全量抽出
      
      **レシート内の全ての商品を、必ず分類してください**
      
      ### 検証ステップ（必須）
      
      1. **元のレシートを読み直す**
      2. **分類結果に含まれていない商品がないか確認**
      3. **漏れがあれば、該当する商品を分類**
      4. **商品数が一致するまで繰り返す**
      
      **全ての商品が分類されていることを保証してください。**
      
      **重要：**
      - レシートに税率マーク（※、★、8%、10%）がある場合、それを優先
      - 食品は基本的に8%、日用品・酒類は10%
      - みりん風調味料は10%（酒類扱い）
      - 本みりん（酒類）は10%
      - 迷った場合は confidence: "low" にする
      - **商品の省略は厳禁**：全ての商品を分類してください
      
      ## ⚠️ Gemini特有の注意事項（必ず守ること）
      
      ### 推測・補完の厳禁（AI推論エンジンよりも厳格）
      - **レシートに記載されていない情報は、一切推測・補完しないこと**
      - 税率マークがない場合、商品名から論理的に判断（食品=8%、日用品=10%）
      - 判断が困難な場合は confidence: "low" とし、推測はしない
      - **Geminiは文脈から情報を補完する傾向があるため、特に注意**
      
      ### 論理的処理の活用（金額検証）
      - **Geminiの計算能力を活用し、以下を必ず検証**
      - **全ての商品の金額を合計し、レシートの合計金額（Total）と一致するか確認**
      - 計算結果が一致しない場合、以下を再確認：
        1. 商品の抽出漏れがないか
        2. 金額の読み取りミスがないか
        3. 税込・税抜の解釈が正しいか
      - 検証ステップ例：
        - 商品1: 198円
        - 商品2: 298円
        - 商品3: 150円
        - 合計計算: 198 + 298 + 150 = 646円
        - レシート記載の合計: 646円
        - ✅ 一致確認 → 抽出完了
      - **計算が一致するまで抽出結果を見直すこと**
      

  stage_i:
    default: |
      # Stage I: Synthesis - Classroom Documents

      あなたはGoogle Classroom課題ドキュメントの要約とタグ生成を専門とするAIアシスタントです。
      
      ## 入力情報
      
      ### Stage H の構造化結果
      
      ```json
      {$stageH_result}
      ```
      
      ### 元のテキスト
      
      ```
      $combined_text
      ```
      
      ## タスク

      上記の情報を統合し、以下を生成してください：

      ### 1. タイトル (title)

      以下の優先順位でタイトルを生成：

      **優先順位1**: 添付ファイル名がある場合
      - Stage H の attachments 配列にファイル名がある場合、そのファイル名を元にテキスト整形
      - 拡張子を除去し、数字表記を適正化（例: "2025年12月19日" など）
      - 例: "ウインターコンサート日程のお知らせ.pdf" → "ウインターコンサート日程のお知らせ"

      **優先順位2**: 添付ファイル名が内容を反映していない場合（ファイルIDなど）
      - Stage H の text_blocks の最初の要素にタイトルらしき内容があれば、それを使用
      - なければ、ドキュメント全体を表す1行要約を生成（10-30文字）

      **優先順位3**: 添付ファイルがない場合（text_only など）
      - Stage H の basic_info.subject があれば、それを使用
      - なければ、投稿文のトップから1行で内容を表すタイトルを生成（10-30文字）

      ### タイトルに日付サフィックスを追加

      タイトル生成後、以下の優先順位で**書類の発行日・配布日**を抽出し、タイトルに追加：

      **日付抽出の優先順位**:
      1. **sended_date** (Stage H の metadata に含まれる場合): この日付を使用
      2. **本文ヘッダー部分の明示的な日付**: "発行日", "配布日", "作成日", "○○年○○月配布" などのラベル付き日付表記のみ
      3. **それ以外の場合**: 日付サフィックスを付けない（推測・捏造は絶対に禁止）

      **重要な制約**:
      - **推測や捏造は絶対に禁止**: 明示的な発行日・配布日の表記がない場合、日付を推測してはいけない
      - **文書の処理日を使用してはいけない**: ファイルの処理日や現在日は文書の内容と無関係
      - **イベント日・期限日は除外**: 以下は発行日ではないため使用しない
        - イベント日（「コンサート: 2025-03-15」など）
        - 提出期限（「提出期限: 2025-01-20」など）
        - テスト日（「テスト: 2025-02-10」など）
        - 保護者会・行事の開催日

      **日付サフィックスの書式**:
      - 年月日が分かる場合: `_(YYYY_MM_DD)`
      - 年月のみ分かる場合: `_(YYYY_MM)`
      - 年のみ分かる場合: `_(YYYY)`
      - **日付が不明な場合: サフィックスなし**

      **例**:
      - sended_date が "2025-01-15" の場合: "数学課題5: 二次方程式の解法_(2025_01_15)"
      - 本文ヘッダーに「配布日: 2025年1月」とある場合: "数学課題5: 二次方程式の解法_(2025_01)"
      - 本文ヘッダーに「発行: 2025年」とある場合: "数学課題5: 二次方程式の解法_(2025)"
      - **日付が不明な場合**: "数学課題5: 二次方程式の解法"

      **判断基準**:
      - 「発行日: 2025-12-04」→ _(2025_12_04) ✅
      - 「2025年1月配布」→ _(2025_01) ✅
      - 「2025」とだけ書いてある → サフィックスなし ❌ 推測禁止
      - イベント日しかない → サフィックスなし ❌ イベント日は使用不可
      - 日付表記がない → サフィックスなし ❌ 捏造禁止

      ### 2. 要約 (summary)
      - 課題全体の簡潔な要約（2-3文、100-200文字程度）
      - 課題内容、提出期限、重要事項を含める

      ### 3. 重要ポイント (key_points)
      - 課題の重要なポイントを箇条書き（3-5個）
      - 提出物の形式、評価基準、注意事項など

      ### 4. タグ (tags)
      - 検索用のタグを5-10個生成
      - Stage H のタグと重複しても良い（後で統合されます）
      - 以下のカテゴリを含める

      ### 5. 基準日付 (relevant_date)
      - 提出期限または関連日付（YYYY-MM-DD形式）
      - Stage H の document_date を参照しつつ、より適切な日付があれば上書き
      - 不明な場合は null

      ### 6. カレンダーイベント (calendar_events)
      - マスターカレンダーに登録すべきイベントを抽出
      - テスト日、保護者会、学校行事、特別授業、遠足、修学旅行など
      - **課題の提出期限はタスクとして抽出（カレンダーには含めない）**
      - 各イベント:
        - `event_date`: イベント日（YYYY-MM-DD）
        - `event_time`: 開始時刻（HH:MM、不明なら null）
        - `event_name`: イベント名（簡潔に）
        - `location`: 場所（不明なら null）
        - `description`: 詳細説明（持ち物、注意事項など）
        - `participants`: 参加者（["育哉", "絵麻"] など、該当者のみ）

      ### 7. タスク (tasks)
      - マスタータスクリストに登録すべきタスクを抽出
      - 課題提出、テスト勉強、プリント提出、準備物など
      - 各タスク:
        - `task_name`: タスク名（簡潔に、例: "数学課題5 提出"）
        - `deadline`: 期限（YYYY-MM-DD HH:MM、時刻不明なら23:59）
        - `priority`: 優先度（"high", "medium", "low"）
        - `category`: カテゴリ（"課題", "テスト", "提出物", "準備", "その他"）
        - `description`: 詳細説明（提出方法、評価基準など）
        - `checklist`: チェックリスト（サブタスク、["問題1-5を解く", "解答をPDF化", "Classroomに提出"] など）
        - `assignee`: 担当者（"育哉", "絵麻", または null）
      
      ## タグ生成ルール
      
      以下のカテゴリからタグを生成：
      
      ### 科目タグ
      - 数学、国語、英語、理科、社会、体育、音楽、美術、技術・家庭
      
      ### 課題種別タグ
      - 課題、テスト、プリント、提出物、復習、予習、宿題
      
      ### 重要度タグ
      - 期限あり、期限間近、重要、緊急、優先度高
      
      ### 対象者タグ
      - 育哉、絵麻（該当する場合のみ）
      
      ### トピックタグ
      - 課題内容に関連するキーワード（例: 二次方程式、作文、実験レポート）
      
      ## 出力形式

      ```json
      {{
        "title": "数学課題5: 二次方程式の解法_(2025_01_10)",
        "summary": "数学の二次方程式の解法に関する課題。2問の方程式を解いて提出。期限は2025-01-15 23:59。",
        "key_points": [
          "提出期限: 2025-01-15 23:59",
          "提出形式: PDFまたはWord形式",
          "2問の二次方程式を解く",
          "解答過程も記載すること"
        ],
        "tags": ["数学", "課題", "二次方程式", "期限あり", "育哉", "PDF提出"],
        "relevant_date": "2025-01-15",
        "calendar_events": [
          {{
            "event_date": "2025-01-20",
            "event_time": "14:00",
            "event_name": "数学テスト",
            "location": "第1教室",
            "description": "二次方程式の単元テスト。計算機持参必須。",
            "participants": ["育哉"]
          }}
        ],
        "tasks": [
          {{
            "task_name": "数学課題5 提出",
            "deadline": "2025-01-15 23:59",
            "priority": "high",
            "category": "課題",
            "description": "二次方程式2問を解いてPDFまたはWord形式で提出。解答過程も記載すること。",
            "checklist": [
              "問題1を解く",
              "問題2を解く",
              "解答過程を記載",
              "PDFまたはWordに変換",
              "Classroomに提出"
            ],
            "assignee": "育哉"
          }}
        ]
      }}
      ```
      
      ## 重要な注意事項
      
      - **必ずJSON形式で出力してください**（```json ブロック内に記述）
      - 要約は簡潔かつ正確に（100-200文字）
      - key_points は具体的で実用的な情報を含める
      - タグは検索に役立つものを選択
      - relevant_date は最も重要な日付を1つ選択
      - 推測や想像で情報を追加しないこと（テキストに明記されている情報のみ抽出）


    flyer: |
      # Stage I: 統合・要約（チラシ）
      
      チラシの商品情報を統合し、検索に最適な形式に整形してください。
      
      ## タスク
      
      1. **要約生成**
         - チラシ全体の簡潔な要約
         - 主要商品カテゴリと目玉商品
         - セール情報
      
      2. **タグ生成**
         - 店舗名
         - 商品カテゴリ
         - 特売キーワード（特価、セールなど）
         - 期間（例: "3月中旬"）
      
      3. **基準日付抽出**
         - チラシ有効期間の開始日
      
      ## 出力形式
      
      ```json
      {{
        "summary": "XXXスーパーの3/15-3/20のチラシ。野菜・果物を中心に特価商品多数。",
        "tags": ["XXXスーパー", "野菜", "果物", "特価", "3月中旬"],
        "relevant_date": "2025-03-15",
        "key_products": [
          "キャベツ 98円",
          "りんご 198円"
        ],
        "sale_info": "ポイント5倍デー: 3/17"
      }}
      ```
      
      **重要:**
      - 要約には店舗名と期間を必ず含める
      - タグには商品カテゴリと特売キーワードを含める
      - relevant_date はチラシ開始日
      

```

### shared\pipeline\config_loader.py

```py
"""
設定ローダー

models.yaml と pipeline_routes.yaml を読み込み、
doc_type や workspace に応じて適切なプロンプトとモデルを返す
"""
from pathlib import Path
from typing import Dict, Any, Optional
import yaml
from loguru import logger


class ConfigLoader:
    """パイプライン設定ローダー"""

    def __init__(self, config_dir: Optional[Path] = None):
        """
        初期化

        Args:
            config_dir: 設定ディレクトリ（デフォルト: G_unified_pipeline/config/）
        """
        if config_dir is None:
            config_dir = Path(__file__).parent / "config"

        self.config_dir = Path(config_dir)
        self.models_config = self._load_yaml(self.config_dir / "models.yaml")

        # パイプラインルーティング設定を読み込み
        pipeline_routing = self.config_dir / "pipeline_routing.yaml"
        if pipeline_routing.exists():
            self.routes_config = self._load_yaml(pipeline_routing)
            logger.info(f"✅ pipeline_routing.yaml を読み込みました")
        else:
            # フォールバック: 旧 pipeline_routes.yaml
            self.routes_config = self._load_yaml(self.config_dir / "pipeline_routes.yaml")
            logger.warning(f"⚠️ pipeline_routing.yaml が見つかりません。pipeline_routes.yaml を使用します")

        # プロンプト設定を読み込み
        prompts_file = self.config_dir / "prompts.yaml"
        if prompts_file.exists():
            prompts_data = self._load_yaml(prompts_file)
            self.prompts_config = prompts_data.get('prompts', {})
            logger.info(f"✅ prompts.yaml を読み込みました")
        else:
            self.prompts_config = {}
            logger.warning(f"⚠️ prompts.yaml が見つかりません。MDファイルから読み込みます")

        logger.info(f"✅ 設定ローダー初期化完了: {self.config_dir}")

    def _load_yaml(self, file_path: Path) -> Dict[str, Any]:
        """YAML ファイルを読み込む"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"YAML読み込みエラー: {file_path} - {e}")
            return {}

    def get_route_config(self, doc_type: str, workspace: Optional[str] = None) -> Dict[str, Any]:
        """
        doc_type と workspace に基づいてルート設定を取得

        Args:
            doc_type: ドキュメントタイプ
            workspace: ワークスペース（オプション）

        Returns:
            ルート設定（stages ごとの prompt_key, model_key）
        """
        routing = self.routes_config.get('routing', {})

        # 優先順位1: workspace ベースのルート
        if workspace:
            by_workspace = routing.get('by_workspace', {})
            if workspace in by_workspace:
                logger.debug(f"ルート選択: workspace={workspace}")
                return by_workspace[workspace]

        # 優先順位2: doc_type ベースのルート
        by_doc_type = routing.get('by_doc_type', {})
        if doc_type in by_doc_type:
            logger.debug(f"ルート選択: doc_type={doc_type}")
            return by_doc_type[doc_type]

        # 優先順位3: デフォルト
        logger.debug("ルート選択: default")
        return by_doc_type.get('default', {})

    def get_prompt(self, stage: str, prompt_key: str) -> str:
        """
        プロンプトを取得（prompts.yaml または MDファイルから）

        Args:
            stage: ステージ名 (stage_f, stage_g, stage_h, stage_i)
            prompt_key: プロンプトキー (default, flyer, classroom など)

        Returns:
            プロンプトテキスト
        """
        # prompts.yaml から読み込み
        if self.prompts_config and stage in self.prompts_config:
            if prompt_key in self.prompts_config[stage]:
                prompt = self.prompts_config[stage][prompt_key]
                logger.debug(f"プロンプト読み込み: {stage}/{prompt_key} ({len(prompt)}文字)")
                return prompt
            elif prompt_key != 'default' and 'default' in self.prompts_config[stage]:
                # フォールバック: default プロンプトを試す
                logger.warning(f"プロンプトキー '{prompt_key}' が見つかりません。default を使用します: {stage}")
                prompt = self.prompts_config[stage]['default']
                logger.debug(f"プロンプト読み込み: {stage}/default ({len(prompt)}文字)")
                return prompt

        # フォールバック: MDファイルから読み込み（後方互換性）
        prompt_file = self.config_dir / "prompts" / stage / f"{stage}_{prompt_key}.md"
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                prompt = f.read()
                logger.debug(f"プロンプト読み込み（MDファイル）: {stage}/{stage}_{prompt_key}.md ({len(prompt)}文字)")
                return prompt
        except FileNotFoundError:
            logger.warning(f"プロンプトが見つかりません: {stage}/{prompt_key}")
            # 最後のフォールバック: default プロンプトを試す
            if prompt_key != 'default':
                return self.get_prompt(stage, 'default')
            return ""
        except Exception as e:
            logger.error(f"プロンプト読み込みエラー: {prompt_file} - {e}")
            return ""

    def get_model(self, stage: str, model_key: str) -> str:
        """
        モデル名を取得

        Args:
            stage: ステージ名 (stage_f, stage_g, stage_h, stage_i, stage_k)
            model_key: モデルキー (default, flyer, classroom など)

        Returns:
            モデル名
        """
        models = self.models_config.get('models', {})
        stage_models = models.get(stage, {})

        model = stage_models.get(model_key)
        if model:
            logger.debug(f"モデル選択: {stage}/{model_key} → {model}")
            return model

        # フォールバック: default モデル
        default_model = stage_models.get('default')
        if default_model:
            logger.debug(f"モデル選択（フォールバック）: {stage}/default → {default_model}")
            return default_model

        logger.error(f"モデルが見つかりません: {stage}/{model_key}")
        return ""

    def get_stage_config(
        self,
        stage: str,
        doc_type: str,
        workspace: Optional[str] = None
    ) -> Dict[str, str]:
        """
        特定ステージの設定を取得（プロンプト + モデル + custom_handler）

        Args:
            stage: ステージ名 (stage_f, stage_g, stage_h, stage_i)
            doc_type: ドキュメントタイプ
            workspace: ワークスペース（オプション）

        Returns:
            {'prompt': str, 'model': str, 'custom_handler': str (optional), 'skip': bool (optional)}
        """
        route = self.get_route_config(doc_type, workspace)
        stages_config = route.get('stages', {})
        stage_config = stages_config.get(stage, {})

        prompt_key = stage_config.get('prompt_key', 'default')
        model_key = stage_config.get('model_key', 'default')

        result = {
            'prompt': self.get_prompt(stage, prompt_key),
            'model': self.get_model(stage, model_key)
        }

        # custom_handler がある場合は追加
        if 'custom_handler' in stage_config:
            result['custom_handler'] = stage_config['custom_handler']

        # skip フラグがある場合は追加
        if 'skip' in stage_config:
            result['skip'] = stage_config['skip']

        return result

    def get_hybrid_ocr_enabled(self, doc_type: str, workspace: Optional[str] = None) -> bool:
        """
        ハイブリッドOCR（Surya + PaddleOCR）が有効かどうかを取得

        Args:
            doc_type: ドキュメントタイプ
            workspace: ワークスペース（オプション）

        Returns:
            True: ハイブリッドOCR有効
            False: ハイブリッドOCR無効（Gemini Visionのみ）
        """
        hybrid_ocr_config = self.models_config.get('hybrid_ocr', {})

        # workspace ベースの設定を確認
        if workspace and workspace in hybrid_ocr_config:
            return hybrid_ocr_config[workspace]

        # doc_type ベースの設定を確認
        if doc_type in hybrid_ocr_config:
            return hybrid_ocr_config[doc_type]

        # デフォルト設定
        return hybrid_ocr_config.get('default', False)
```

### shared\pipeline\image_preprocessing.py

```py
"""
画像前処理ユーティリティ

PaddleOCRの認識精度向上のための画像前処理機能を提供
"""
import cv2
import numpy as np
from typing import Tuple
from loguru import logger


def preprocess_image_for_ocr(
    image: np.ndarray,
    apply_clahe: bool = True,
    apply_denoise: bool = True,
    apply_sharpen: bool = True,
    apply_binarize: bool = False
) -> Tuple[np.ndarray, dict]:
    """
    OCR認識精度向上のための画像前処理

    Args:
        image: 入力画像（numpy array、RGB or Grayscale）
        apply_clahe: CLAHEによるコントラスト調整を適用
        apply_denoise: ノイズ除去を適用
        apply_sharpen: シャープ化を適用
        apply_binarize: 二値化を適用（低品質画像向け）

    Returns:
        (processed_image, stats): 前処理済み画像と統計情報
    """
    stats = {
        'original_shape': image.shape,
        'applied_operations': []
    }

    # RGB → Grayscale 変換
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    else:
        gray = image.copy()

    processed = gray.copy()

    # 1. CLAHE（コントラスト制限適応ヒストグラム均等化）
    if apply_clahe:
        try:
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            processed = clahe.apply(processed)
            stats['applied_operations'].append('CLAHE')
        except Exception as e:
            logger.warning(f"CLAHE処理失敗: {e}")

    # 2. ノイズ除去（Non-local Means Denoising）
    if apply_denoise:
        try:
            processed = cv2.fastNlMeansDenoising(processed, None, h=10, templateWindowSize=7, searchWindowSize=21)
            stats['applied_operations'].append('Denoise')
        except Exception as e:
            logger.warning(f"ノイズ除去処理失敗: {e}")

    # 3. シャープ化（Unsharp Masking）
    if apply_sharpen:
        try:
            # ガウシアンブラーでぼかし画像を作成
            blurred = cv2.GaussianBlur(processed, (0, 0), 3)
            # オリジナル - ぼかし = シャープマスク
            processed = cv2.addWeighted(processed, 1.5, blurred, -0.5, 0)
            stats['applied_operations'].append('Sharpen')
        except Exception as e:
            logger.warning(f"シャープ化処理失敗: {e}")

    # 4. 二値化（Otsuの閾値処理） - オプション
    if apply_binarize:
        try:
            _, processed = cv2.threshold(processed, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            stats['applied_operations'].append('Binarize')
        except Exception as e:
            logger.warning(f"二値化処理失敗: {e}")

    # 5. PaddleOCR互換性のためRGB形式に変換
    # PaddleOCRはRGB/カラー画像を期待するため、グレースケールから戻す
    if len(processed.shape) == 2:
        processed = cv2.cvtColor(processed, cv2.COLOR_GRAY2RGB)
        stats['applied_operations'].append('GrayToRGB')

    stats['final_shape'] = processed.shape
    stats['operations_count'] = len(stats['applied_operations'])

    return processed, stats


def adaptive_preprocess(
    image: np.ndarray,
    confidence_threshold: float = 0.7
) -> np.ndarray:
    """
    低信頼度領域に対する適応的前処理

    通常の前処理で認識精度が低い場合に、より強力な前処理を適用

    Args:
        image: 入力画像
        confidence_threshold: この閾値以下の場合、強力な前処理を適用

    Returns:
        前処理済み画像
    """
    # より強力な前処理: 二値化 + ノイズ除去 + シャープ化
    processed, _ = preprocess_image_for_ocr(
        image,
        apply_clahe=True,
        apply_denoise=True,
        apply_sharpen=True,
        apply_binarize=True  # 二値化を有効化
    )

    return processed


def calculate_image_quality_score(image: np.ndarray) -> float:
    """
    画像品質スコアを計算（0.0～1.0）

    ぼやけ具合やコントラストを評価し、前処理の必要性を判定

    Args:
        image: 入力画像

    Returns:
        品質スコア（高いほど高品質）
    """
    try:
        # グレースケール変換
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image

        # 1. ぼやけ検出（Laplacian分散）
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        blur_score = min(laplacian_var / 100.0, 1.0)  # 正規化

        # 2. コントラスト評価（標準偏差）
        contrast_score = min(gray.std() / 50.0, 1.0)

        # 総合スコア
        quality_score = (blur_score * 0.6 + contrast_score * 0.4)

        return quality_score

    except Exception as e:
        logger.warning(f"画像品質評価失敗: {e}")
        return 0.5  # デフォルト値
```

### shared\pipeline\ocr_config.py

```py
"""
OCRエンジン設定とキャッシング

PaddleOCRの設定、バージョン検出、結果キャッシング機能を提供
"""
import hashlib
import json
import pickle
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger
import time


class OCRConfig:
    """OCRエンジンの設定管理"""

    # 信頼度閾値
    CONFIDENCE_THRESHOLD_LOW = 0.5      # これ以下は無視
    CONFIDENCE_THRESHOLD_MID = 0.7      # これ以下は再処理
    CONFIDENCE_THRESHOLD_HIGH = 0.85    # これ以上は高品質

    # 並列処理設定
    ENABLE_PARALLEL = True               # 並列処理を有効化
    MAX_WORKERS = 4                      # 最大ワーカー数

    # キャッシング設定
    ENABLE_CACHE = True                  # 結果キャッシングを有効化
    CACHE_DIR = Path("cache/ocr_results")  # キャッシュディレクトリ
    CACHE_TTL = 86400                    # キャッシュ有効期限（秒）

    # OCRエンジン優先順位
    OCR_ENGINE_PRIORITY = [
        'paddleocr',  # 1. PaddleOCR（日本語に強い）
        'surya',      # 2. Surya（レイアウト解析）
        'gemini'      # 3. Gemini Vision（フォールバック）
    ]

    # 画像前処理設定
    ENABLE_PREPROCESSING = True          # 画像前処理を有効化
    PREPROCESSING_QUALITY_THRESHOLD = 0.7  # この品質以下で前処理適用

    # Geminiフォールバック設定
    ENABLE_GEMINI_FALLBACK = True        # 低信頼度時Geminiフォールバック
    GEMINI_FALLBACK_THRESHOLD = 0.6      # この信頼度以下でフォールバック


class OCRResultCache:
    """OCR結果のキャッシング"""

    def __init__(self, cache_dir: Path = OCRConfig.CACHE_DIR, ttl: int = OCRConfig.CACHE_TTL):
        """
        Args:
            cache_dir: キャッシュディレクトリ
            ttl: キャッシュ有効期限（秒）
        """
        self.cache_dir = cache_dir
        self.ttl = ttl
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.hits = 0
        self.misses = 0

    def _get_cache_key(self, image_data: bytes, config: Dict[str, Any]) -> str:
        """
        画像とconfigからキャッシュキーを生成

        Args:
            image_data: 画像のバイトデータ
            config: OCR設定

        Returns:
            SHA256ハッシュキー
        """
        config_str = json.dumps(config, sort_keys=True)
        combined = image_data + config_str.encode('utf-8')
        return hashlib.sha256(combined).hexdigest()

    def get(self, image_data: bytes, config: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        キャッシュから結果を取得

        Args:
            image_data: 画像のバイトデータ
            config: OCR設定

        Returns:
            キャッシュされた結果、または None
        """
        if not OCRConfig.ENABLE_CACHE:
            return None

        cache_key = self._get_cache_key(image_data, config)
        cache_file = self.cache_dir / f"{cache_key}.pkl"

        if not cache_file.exists():
            self.misses += 1
            return None

        try:
            # キャッシュファイルの有効期限確認
            file_age = time.time() - cache_file.stat().st_mtime
            if file_age > self.ttl:
                cache_file.unlink()  # 期限切れ削除
                self.misses += 1
                return None

            # キャッシュ読み込み
            with open(cache_file, 'rb') as f:
                result = pickle.load(f)

            self.hits += 1
            logger.debug(f"[Cache] HIT: {cache_key[:8]}... (age: {file_age:.0f}s)")
            return result

        except Exception as e:
            logger.warning(f"[Cache] 読み込み失敗: {e}")
            self.misses += 1
            return None

    def set(self, image_data: bytes, config: Dict[str, Any], result: Dict[str, Any]):
        """
        結果をキャッシュに保存

        Args:
            image_data: 画像のバイトデータ
            config: OCR設定
            result: OCR結果
        """
        if not OCRConfig.ENABLE_CACHE:
            return

        cache_key = self._get_cache_key(image_data, config)
        cache_file = self.cache_dir / f"{cache_key}.pkl"

        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(result, f)
            logger.debug(f"[Cache] SAVE: {cache_key[:8]}...")
        except Exception as e:
            logger.warning(f"[Cache] 保存失敗: {e}")

    def clear(self):
        """キャッシュをクリア"""
        try:
            for cache_file in self.cache_dir.glob("*.pkl"):
                cache_file.unlink()
            logger.info(f"[Cache] クリア完了: {self.cache_dir}")
        except Exception as e:
            logger.warning(f"[Cache] クリア失敗: {e}")

    def get_stats(self) -> Dict[str, Any]:
        """キャッシュ統計を取得"""
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0

        cache_files = list(self.cache_dir.glob("*.pkl"))
        total_size = sum(f.stat().st_size for f in cache_files)

        return {
            'hits': self.hits,
            'misses': self.misses,
            'total_requests': total,
            'hit_rate': hit_rate,
            'cache_files': len(cache_files),
            'total_size_mb': total_size / (1024 * 1024)
        }


class PaddleOCRVersionAdapter:
    """PaddleOCRバージョン互換性アダプター"""

    @staticmethod
    def detect_version() -> str:
        """PaddleOCRのバージョンを検出"""
        try:
            import paddleocr
            version = getattr(paddleocr, '__version__', 'unknown')
            logger.info(f"[PaddleOCR] Version detected: {version}")
            return version
        except Exception as e:
            logger.warning(f"[PaddleOCR] Version detection failed: {e}")
            return 'unknown'

    @staticmethod
    def extract_result(ocr_result: Any) -> tuple:
        """
        PaddleOCRの結果から統一的にテキストと信頼度を抽出

        Args:
            ocr_result: PaddleOCRの結果オブジェクト

        Returns:
            (texts: List[str], confidences: List[float])
        """
        texts = []
        confidences = []

        try:
            # PaddleOCR 3.x: 辞書ライクオブジェクト
            if isinstance(ocr_result, dict) or hasattr(ocr_result, '__getitem__'):
                rec_texts = ocr_result.get('rec_texts', []) if hasattr(ocr_result, 'get') else ocr_result.get('rec_texts', [])
                rec_scores = ocr_result.get('rec_scores', []) if hasattr(ocr_result, 'get') else ocr_result.get('rec_scores', [])

                if rec_texts:
                    texts = list(rec_texts)
                    confidences = list(rec_scores) if rec_scores else []

            # PaddleOCR 2.x: リスト形式
            elif isinstance(ocr_result, list):
                for line in ocr_result:
                    if line and len(line) >= 2 and line[1]:
                        texts.append(line[1][0])
                        confidences.append(line[1][1])

        except Exception as e:
            logger.warning(f"[PaddleOCR] Result extraction failed: {e}")

        return texts, confidences
```

### shared\pipeline\ocr_report.py

```py
"""
OCR認識精度レポート生成

OCR処理の詳細な統計とレポートを生成
"""
from typing import Dict, Any, List
from dataclasses import dataclass, field
from datetime import datetime
import json


@dataclass
class OCRRegionStats:
    """領域ごとの統計"""
    region_id: int
    bbox: List[int]
    text_length: int
    confidence: float
    preprocessing_applied: bool
    reprocessed: bool = False
    improvement: float = 0.0


@dataclass
class OCRProcessingReport:
    """OCR処理レポート"""
    # 基本情報
    file_name: str
    processing_time: float
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

    # 領域統計
    total_regions: int = 0
    recognized_regions: int = 0
    low_confidence_regions: int = 0
    reprocessed_regions: int = 0
    improved_regions: int = 0

    # 認識統計
    total_chars: int = 0
    avg_confidence: float = 0.0
    min_confidence: float = 1.0
    max_confidence: float = 0.0

    # 前処理統計
    preprocessed_regions: int = 0
    preprocessing_time: float = 0.0

    # 再処理統計
    reprocessing_time: float = 0.0
    avg_improvement: float = 0.0

    # キャッシュ統計
    cache_hits: int = 0
    cache_misses: int = 0

    # 領域詳細
    regions: List[OCRRegionStats] = field(default_factory=list)

    def add_region(self, region_stats: OCRRegionStats):
        """領域統計を追加"""
        self.regions.append(region_stats)
        self.total_regions += 1

        if region_stats.text_length > 0:
            self.recognized_regions += 1
            self.total_chars += region_stats.text_length

        if region_stats.confidence < 0.7:
            self.low_confidence_regions += 1

        if region_stats.preprocessing_applied:
            self.preprocessed_regions += 1

        if region_stats.reprocessed:
            self.reprocessed_regions += 1
            if region_stats.improvement > 0:
                self.improved_regions += 1

        # 信頼度統計更新
        if region_stats.confidence > 0:
            self.min_confidence = min(self.min_confidence, region_stats.confidence)
            self.max_confidence = max(self.max_confidence, region_stats.confidence)

    def calculate_final_stats(self):
        """最終統計を計算"""
        if self.regions:
            confidences = [r.confidence for r in self.regions if r.confidence > 0]
            self.avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0

            improvements = [r.improvement for r in self.regions if r.reprocessed and r.improvement > 0]
            self.avg_improvement = sum(improvements) / len(improvements) if improvements else 0.0

    def to_dict(self) -> Dict[str, Any]:
        """辞書形式に変換"""
        return {
            'file_name': self.file_name,
            'processing_time': self.processing_time,
            'timestamp': self.timestamp,
            'summary': {
                'total_regions': self.total_regions,
                'recognized_regions': self.recognized_regions,
                'recognition_rate': self.recognized_regions / self.total_regions if self.total_regions > 0 else 0,
                'low_confidence_regions': self.low_confidence_regions,
                'total_chars': self.total_chars,
            },
            'confidence': {
                'average': self.avg_confidence,
                'min': self.min_confidence,
                'max': self.max_confidence,
            },
            'preprocessing': {
                'preprocessed_regions': self.preprocessed_regions,
                'preprocessing_rate': self.preprocessed_regions / self.total_regions if self.total_regions > 0 else 0,
                'preprocessing_time': self.preprocessing_time,
            },
            'reprocessing': {
                'reprocessed_regions': self.reprocessed_regions,
                'improved_regions': self.improved_regions,
                'improvement_rate': self.improved_regions / self.reprocessed_regions if self.reprocessed_regions > 0 else 0,
                'avg_improvement': self.avg_improvement,
                'reprocessing_time': self.reprocessing_time,
            },
            'cache': {
                'cache_hits': self.cache_hits,
                'cache_misses': self.cache_misses,
                'cache_hit_rate': self.cache_hits / (self.cache_hits + self.cache_misses) if (self.cache_hits + self.cache_misses) > 0 else 0,
            }
        }

    def to_json(self, indent: int = 2) -> str:
        """JSON文字列に変換"""
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=indent)

    def print_summary(self):
        """サマリーを表示"""
        print("\n" + "=" * 60)
        print(f"OCR Processing Report: {self.file_name}")
        print("=" * 60)
        print(f"Total Regions: {self.total_regions}")
        print(f"Recognized: {self.recognized_regions}/{self.total_regions} ({self.recognized_regions/self.total_regions*100:.1f}%)")
        print(f"Total Characters: {self.total_chars}")
        print(f"Average Confidence: {self.avg_confidence:.2%}")
        print(f"Low Confidence Regions: {self.low_confidence_regions} (< 0.7)")
        print(f"\nPreprocessing:")
        print(f"  Preprocessed: {self.preprocessed_regions}/{self.total_regions} ({self.preprocessed_regions/self.total_regions*100:.1f}%)")
        print(f"  Time: {self.preprocessing_time:.2f}s")
        print(f"\nReprocessing:")
        print(f"  Reprocessed: {self.reprocessed_regions}")
        print(f"  Improved: {self.improved_regions}/{self.reprocessed_regions}")
        if self.reprocessed_regions > 0:
            print(f"  Improvement Rate: {self.improved_regions/self.reprocessed_regions*100:.1f}%")
            print(f"  Avg Improvement: +{self.avg_improvement:.2%}")
        print(f"  Time: {self.reprocessing_time:.2f}s")
        print(f"\nCache:")
        total_cache = self.cache_hits + self.cache_misses
        if total_cache > 0:
            print(f"  Hits: {self.cache_hits}/{total_cache} ({self.cache_hits/total_cache*100:.1f}%)")
        print(f"\nTotal Processing Time: {self.processing_time:.2f}s")
        print("=" * 60 + "\n")
```

### shared\pipeline\pipeline.py

```py
"""
統合ドキュメント処理パイプライン (Stage E-K) - 設定ベース版

設計書: DESIGN_UNIFIED_PIPELINE.md v2.0 に準拠
処理順序: Stage E → F → G → H → I → J → K

特徴:
- doc_type / workspace に応じて自動的にプロンプトとモデルを切り替え
- config/ 内の YAML と Markdown ファイルで設定管理
"""
import asyncio
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger

from shared.ai.llm_client.llm_client import LLMClient
from shared.common.database.client import DatabaseClient
from shared.common.connectors.google_drive import GoogleDriveConnector

from .config_loader import ConfigLoader
from .stage_e_preprocessing import StageEPreprocessor
from .stage_f_visual import StageFVisualAnalyzer
from .stage_h_structuring import StageHStructuring
from .stage_h_kakeibo import StageHKakeibo
from .stage_i_synthesis import StageISynthesis
from .stage_j_chunking import StageJChunking
from .stage_k_embedding import StageKEmbedding

# 家計簿専用のDB保存ハンドラー (オプショナル)
try:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from shared.kakeibo.kakeibo_db_handler import KakeiboDBHandler
    KAKEIBO_AVAILABLE = True
except ImportError:
    logger.warning("K_kakeibo module not available, kakeibo features will be disabled")
    KakeiboDBHandler = None
    KAKEIBO_AVAILABLE = False


class UnifiedDocumentPipeline:
    """統合ドキュメント処理パイプライン (Stage E-K) - 設定ベース版"""

    @staticmethod
    def _sanitize_text(text: str) -> str:
        """
        テキストからnull文字を除去

        Args:
            text: 入力テキスト

        Returns:
            サニタイズ済みテキスト
        """
        if not text:
            return text
        # null文字 (\u0000) を除去
        return text.replace('\u0000', '')

    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        db_client: Optional[DatabaseClient] = None,
        config_dir: Optional[Path] = None,
        enable_hybrid_ocr: Optional[bool] = None
    ):
        """
        Args:
            llm_client: LLMクライアント（Noneの場合は新規作成）
            db_client: データベースクライアント（Noneの場合は新規作成）
            config_dir: 設定ディレクトリ（デフォルト: G_unified_pipeline/config/）
            enable_hybrid_ocr: ハイブリッドOCR（Surya + PaddleOCR）を有効化（Noneの場合は設定ファイルから取得）
        """
        self.llm_client = llm_client or LLMClient()
        self.db = db_client or DatabaseClient(use_service_role=True)  # RLSバイパスのためService Role使用
        self.drive_connector = GoogleDriveConnector()  # Google Drive ファイル名更新用

        # 設定ローダーを初期化
        self.config = ConfigLoader(config_dir)

        # ハイブリッドOCRの有効/無効を決定
        if enable_hybrid_ocr is None:
            # 設定ファイルから取得（デフォルトの設定）
            enable_hybrid_ocr = self.config.get_hybrid_ocr_enabled('default')

        # 各ステージを初期化
        self.stage_e = StageEPreprocessor(self.llm_client)
        self.stage_f = StageFVisualAnalyzer(self.llm_client, enable_hybrid_ocr=enable_hybrid_ocr)
        self.stage_h = StageHStructuring(self.llm_client)
        self.stage_h_kakeibo = StageHKakeibo(self.db)  # 家計簿専用Stage H
        self.stage_i = StageISynthesis(self.llm_client)
        self.stage_j = StageJChunking()
        self.stage_k = StageKEmbedding(self.llm_client, self.db)

        # 家計簿専用のDB保存ハンドラー
        self.kakeibo_db_handler = KakeiboDBHandler(self.db) if KAKEIBO_AVAILABLE else None

        logger.info(f"✅ UnifiedDocumentPipeline 初期化完了（設定ベース, ハイブリッドOCR={'有効' if enable_hybrid_ocr else '無効'}）")

    async def process_document(
        self,
        file_path: Path,
        file_name: str,
        doc_type: str,
        workspace: str,
        mime_type: str,
        source_id: str,
        existing_document_id: Optional[str] = None,
        extra_metadata: Optional[Dict[str, Any]] = None,
        progress_callback=None
    ) -> Dict[str, Any]:
        """
        ドキュメントを処理（Stage E-K）

        Args:
            file_path: ファイルパス
            file_name: ファイル名
            doc_type: ドキュメントタイプ（設定ルーティングに使用）
            workspace: ワークスペース
            mime_type: MIMEタイプ
            source_id: ソースID
            existing_document_id: 更新する既存ドキュメントID（Noneの場合は新規作成）
            extra_metadata: 追加メタデータ（Classroom固有フィールドなど）

        Returns:
            処理結果 {'success': bool, 'document_id': str, ...}
        """
        try:
            logger.info(f"📄 ドキュメント処理開始: {file_name} (doc_type={doc_type}, workspace={workspace})")

            # ============================================
            # Stage E: Pre-processing
            # ============================================
            logger.info("[Stage E] Pre-processing開始...")
            if progress_callback:
                progress_callback("E1")

            # extra_metadata から既に抽出済みのテキスト（attachment_text）を取得
            # HTMLファイル等、Ingestion時にテキスト抽出済みの場合に使用
            pre_extracted_text = extra_metadata.get('attachment_text', '') if extra_metadata else ''

            stage_e_result = self.stage_e.extract_text(
                file_path,
                mime_type,
                pre_extracted_text=pre_extracted_text,
                workspace=workspace,
                progress_callback=progress_callback
            )

            # イベントループに制御を返す（並列タスク実行のため）
            await asyncio.sleep(0)

            # Stage E の結果をチェック
            if not stage_e_result.get('success'):
                error_msg = f"Stage E失敗: {stage_e_result.get('error', 'テキスト抽出エラー')}"
                logger.error(f"[Stage E失敗] {error_msg}")
                return {'success': False, 'error': error_msg}

            extracted_text = stage_e_result.get('content', '')
            # ログ出力は Stage E 内で既に実施済み

            # ============================================
            # Stage F: Visual Analysis (gemini-2.5-pro で完璧に仕上げる)
            # ============================================
            # 設定から Stage F のプロンプトとモデルを取得
            stage_f_config = self.config.get_stage_config('stage_f', doc_type, workspace)
            prompt_f = stage_f_config['prompt']
            model_f = stage_f_config['model']

            logger.info(f"[Stage F] Visual Analysis開始... (model={model_f})")
            if progress_callback:
                progress_callback("F")
            vision_raw = self.stage_f.process(
                file_path=file_path,
                prompt=prompt_f,
                model=model_f,
                extracted_text=extracted_text,
                workspace=workspace,
                progress_callback=progress_callback
            )
            logger.info(f"[Stage F完了] Vision結果: {len(vision_raw)}文字")

            # イベントループに制御を返す（並列タスク実行のため）
            await asyncio.sleep(0)

            # ============================================
            # Stage F 結果パース: JSON から構造化情報を取得
            # ============================================
            import json
            try:
                vision_json = json.loads(vision_raw)
                ocr_text = vision_json.get('full_text', '')
                stage_f_structure = {
                    'sections': vision_json.get('layout_info', {}).get('sections', []),
                    'tables': vision_json.get('layout_info', {}).get('tables', []),
                    'visual_elements': vision_json.get('visual_elements', {}),
                    'full_text': ocr_text
                }

                # combined_textの構築（複数ソースから統合）
                text_parts = []

                # 1. 投稿文テキスト（Classroom等のメタデータから）
                if extra_metadata:
                    display_post_text = extra_metadata.get('display_post_text', '')
                    if display_post_text and display_post_text.strip():
                        text_parts.append(f"[投稿文]\n{display_post_text}")
                        logger.info(f"[Stage F→H] display_post_text追加: {len(display_post_text)}文字")

                # 2. OCR抽出テキスト
                if ocr_text and ocr_text.strip():
                    text_parts.append(f"[OCR抽出テキスト]\n{ocr_text}")

                # 3. 画像の視覚的説明（visual_elements.notes）
                visual_elements = vision_json.get('visual_elements', {})
                notes = visual_elements.get('notes', [])
                if notes:
                    notes_text = '\n'.join(notes)
                    text_parts.append(f"[画像の視覚的説明]\n{notes_text}")
                    logger.info(f"[Stage F→H] visual_elements.notes追加: {len(notes_text)}文字")

                # 統合テキスト生成
                combined_text = '\n\n'.join(text_parts)

                logger.info(f"[Stage F→H] 構造化情報を抽出:")
                logger.info(f"  ├─ combined_text: {len(combined_text)}文字")
                logger.info(f"  ├─ OCR full_text: {len(ocr_text)}文字")
                logger.info(f"  ├─ sections: {len(stage_f_structure.get('sections', []))}個")
                logger.info(f"  └─ tables: {len(stage_f_structure.get('tables', []))}個")
            except json.JSONDecodeError as e:
                logger.warning(f"[Stage F→H] JSON解析失敗: {e}")
                combined_text = vision_raw
                stage_f_structure = None

            # 空のコンテンツをチェック（空のドキュメントは警告のみ、エラーではない）
            if not combined_text or not combined_text.strip():
                logger.warning(f"[Stage F→H] 統合テキストが空です（テキストのないドキュメントの可能性）")
                combined_text = ""  # 空文字列として継続

            # ============================================
            # Stage H: Structuring
            # ============================================
            # 設定から Stage H のプロンプトとモデルを取得
            stage_h_config = self.config.get_stage_config('stage_h', doc_type, workspace)
            custom_handler = stage_h_config.get('custom_handler')

            # 家計簿専用処理の場合
            if custom_handler == 'kakeibo':
                logger.info(f"[Stage H] 家計簿構造化開始... (custom_handler=kakeibo)")
                if progress_callback:
                    progress_callback("H")

                # Stage F の出力を辞書に変換（combined_text が JSON 文字列の場合）
                import json
                import re
                try:
                    # Markdownのコードブロック (```json ... ```) を除去
                    json_text = combined_text.strip()
                    if json_text.startswith('```'):
                        # 最初と最後の```を除去
                        json_text = re.sub(r'^```(?:json)?\s*\n', '', json_text)
                        json_text = re.sub(r'\n```\s*$', '', json_text)

                    logger.debug(f"[Stage H] JSON パース前の最初の500文字:\n{json_text[:500]}")
                    stage_f_output = json.loads(json_text)
                except (json.JSONDecodeError, TypeError) as e:
                    logger.error(f"[Stage H] combined_text が JSON 形式ではありません: {e}")
                    logger.error(f"[Stage H] combined_text の内容:\n{combined_text[:1000]}")
                    raise ValueError("Stage F output must be JSON for kakeibo processing")

                # 家計簿専用 Stage H で処理
                stageH_result = self.stage_h_kakeibo.process(stage_f_output)

                # イベントループに制御を返す（並列タスク実行のため）
                await asyncio.sleep(0)

                # 家計簿専用のDB保存
                if self.kakeibo_db_handler:
                    logger.info("[DB保存] 家計簿データをDBに保存...")
                    kakeibo_save_result = self.kakeibo_db_handler.save_receipt(
                        stage_h_output=stageH_result,
                        file_name=file_name,
                        drive_file_id=source_id,
                        model_name=stage_h_config['model'],
                        source_folder=workspace
                    )
                    logger.info(f"[DB保存完了] receipt_id={kakeibo_save_result['receipt_id']}")
                else:
                    logger.warning("K_kakeibo module not available, skipping kakeibo DB save")

                # 家計簿は Rawdata_FILE_AND_MAIL に保存せず、ここで終了
                return {
                    'success': True,
                    'receipt_id': kakeibo_save_result['receipt_id'],
                    'transaction_ids': kakeibo_save_result['transaction_ids'],
                    'log_id': kakeibo_save_result['log_id'],
                    'doc_type': 'kakeibo'
                }

            # 通常の Stage H 処理
            else:
                prompt_h = stage_h_config['prompt']
                model_h = stage_h_config['model']

                logger.info(f"[Stage H] 構造化開始... (model={model_h})")
                if progress_callback:
                    progress_callback("H")
                stageH_result = self.stage_h.process(
                    file_name=file_name,
                    doc_type=doc_type,
                    workspace=workspace,
                    combined_text=combined_text,
                    prompt=prompt_h,
                    model=model_h,
                    stage_f_structure=stage_f_structure  # 構造化情報を渡す
                )

                # イベントループに制御を返す（並列タスク実行のため）
                await asyncio.sleep(0)

                # Stage H の結果をチェック
                if not stageH_result or not isinstance(stageH_result, dict):
                    error_msg = "Stage H失敗: 構造化結果が不正です"
                    logger.error(f"[Stage H失敗] {error_msg}")
                    return {'success': False, 'error': error_msg}

                # フォールバック結果の処理（テキストが空のドキュメントの場合）
                stageH_metadata = stageH_result.get('metadata', {})
                if stageH_metadata.get('extraction_failed'):
                    logger.warning("[Stage H警告] テキストが空のドキュメントです（フォールバック結果を使用）")
                    # エラーではなく、空のメタデータとして継続

                document_date = stageH_result.get('document_date')
                tags = stageH_result.get('tags', [])
                logger.info(f"[Stage H完了]")

            # ============================================
            # Stage I: Synthesis
            # ============================================
            # 設定から Stage I のプロンプトとモデルを取得
            stage_i_config = self.config.get_stage_config('stage_i', doc_type, workspace)

            # skip フラグがある場合はスキップ
            if stage_i_config.get('skip'):
                logger.info("[Stage I] スキップ (skip=true)")
                summary = ""
                relevant_date = None
            else:
                prompt_i = stage_i_config['prompt']
                model_i = stage_i_config['model']

                logger.info(f"[Stage I] 統合・要約開始... (model={model_i})")
                if progress_callback:
                    progress_callback("I")
                stageI_result = self.stage_i.process(
                    combined_text=combined_text,
                    stageH_result=stageH_result,
                    prompt=prompt_i,
                    model=model_i
                )

                # イベントループに制御を返す（並列タスク実行のため）
                await asyncio.sleep(0)

                # Stage I の結果をチェック
                if not stageI_result or not isinstance(stageI_result, dict):
                    error_msg = "Stage I失敗: 統合・要約結果が不正です"
                    logger.error(f"[Stage I失敗] {error_msg}")
                    return {'success': False, 'error': error_msg}

                title = stageI_result.get('title', '')
                summary = stageI_result.get('summary', '')

                # フォールバック結果の処理（テキストが空のドキュメントの場合）
                if summary == '処理に失敗しました':
                    logger.warning("[Stage I警告] テキストが空のドキュメントです（フォールバック結果を使用）")
                    summary = ''  # 空の要約として継続

                relevant_date = stageI_result.get('relevant_date')

                # カレンダーイベントとタスクを取得
                calendar_events = stageI_result.get('calendar_events', [])
                tasks = stageI_result.get('tasks', [])

                # metadataに追加
                stageH_metadata['calendar_events'] = calendar_events
                stageH_metadata['tasks'] = tasks

                logger.info(f"[Stage I完了] calendar_events={len(calendar_events)}件, tasks={len(tasks)}件")

                # ============================================
                # Google Drive ファイル名更新（タイトルに基づく）
                # ============================================
                if title and source_id:
                    # ファイル名から拡張子を抽出
                    import os
                    file_extension = os.path.splitext(file_name)[1]  # 例: ".pdf"

                    # 新しいファイル名を生成（タイトル + 拡張子）
                    new_file_name = title + file_extension

                    # Google Drive のファイル名を更新
                    try:
                        self.drive_connector.rename_file(source_id, new_file_name)
                        logger.info(f"[Google Drive] ファイル名更新成功: {new_file_name}")
                    except Exception as e:
                        # ファイル名更新失敗はエラーログのみ（処理は継続）
                        logger.warning(f"[Google Drive] ファイル名更新失敗: {e}")

            # ============================================
            # Stage J: Chunking
            # ============================================
            logger.info("[Stage J] チャンク化開始...")
            if progress_callback:
                progress_callback("J")
            chunks = self.stage_j.process(
                display_subject=extra_metadata.get('display_subject', file_name) if extra_metadata else file_name,
                summary=summary,
                tags=tags,
                document_date=document_date,
                metadata=stageH_metadata
            )
            logger.info(f"[Stage J完了] チャンク数: {len(chunks)}")

            # イベントループに制御を返す（並列タスク実行のため）
            await asyncio.sleep(0)

            # ============================================
            # DB保存: Rawdata_FILE_AND_MAIL
            # ============================================
            document_id = existing_document_id
            try:
                # 既存ドキュメントの attachment_text, metadata, display_* フィールドを取得（nullで上書きしないため）
                existing_attachment_text = None
                existing_metadata = {}
                existing_display_fields = {}
                if existing_document_id:
                    try:
                        existing_doc = self.db.client.table('Rawdata_FILE_AND_MAIL').select(
                            'attachment_text, metadata, display_sender, display_sender_email, display_subject, display_sent_at, display_post_text'
                        ).eq('id', existing_document_id).execute()
                        if existing_doc.data and len(existing_doc.data) > 0:
                            doc = existing_doc.data[0]
                            existing_attachment_text = doc.get('attachment_text', '')
                            # 既存 metadata を保持（message_id, thread_id, subject など）
                            existing_metadata = doc.get('metadata', {})
                            if isinstance(existing_metadata, str):
                                import json
                                existing_metadata = json.loads(existing_metadata)
                            # display_* フィールドを保持
                            existing_display_fields = {
                                'display_sender': doc.get('display_sender'),
                                'display_sender_email': doc.get('display_sender_email'),
                                'display_subject': doc.get('display_subject'),
                                'display_sent_at': doc.get('display_sent_at'),
                                'display_post_text': doc.get('display_post_text')
                            }
                            logger.debug(f"[DB保存] 既存attachment_text取得: {len(existing_attachment_text or '')}文字")
                            logger.debug(f"[DB保存] 既存metadata取得: {list(existing_metadata.keys())}")
                            logger.debug(f"[DB保存] 既存display_*フィールド取得: sender={existing_display_fields.get('display_sender')}, subject={existing_display_fields.get('display_subject')}")
                    except Exception as e:
                        logger.warning(f"[DB保存警告] 既存フィールド取得失敗: {e}")

                # テキストフィールドをサニタイズ（null文字を除去）
                sanitized_combined_text = self._sanitize_text(combined_text)
                sanitized_summary = self._sanitize_text(summary)
                sanitized_extracted_text = self._sanitize_text(extracted_text)

                # Stage F の出力をパース（JSONから各要素を抽出）
                stage_f_text_ocr = None
                stage_f_layout_ocr = None
                stage_f_visual_elements = None
                try:
                    if vision_raw and stage_f_structure:
                        # full_text を text OCR として保存
                        stage_f_text_ocr = self._sanitize_text(stage_f_structure.get('full_text', ''))
                        # sections + tables を layout OCR として保存
                        import json
                        stage_f_layout_ocr = json.dumps({
                            'sections': stage_f_structure.get('sections', []),
                            'tables': stage_f_structure.get('tables', [])
                        }, ensure_ascii=False, indent=2)
                        # visual_elements をそのまま保存
                        stage_f_visual_elements = json.dumps(
                            stage_f_structure.get('visual_elements', {}),
                            ensure_ascii=False,
                            indent=2
                        )
                except Exception as e:
                    logger.warning(f"[DB保存警告] Stage F出力のパースに失敗: {e}")

                # Stage Eが空の場合、Stage Fのfull_textをE4/E5に使用
                if not sanitized_extracted_text and stage_f_text_ocr:
                    logger.info("[DB保存] Stage Eが空のため、Stage Fのfull_textをE4/E5に使用")
                    sanitized_extracted_text = stage_f_text_ocr

                # titleをサニタイズ
                sanitized_title = self._sanitize_text(title)

                # attachment_text の決定ロジック
                # - Stage Eが正当にテキストを抽出した場合（sanitized_combined_text が空でない）→ 使用（正当な上書き）
                # - Stage Eが失敗した場合（sanitized_combined_text が空）→ 既存値を保持（nullで上書きしない）
                final_attachment_text = sanitized_combined_text
                if not sanitized_combined_text and existing_attachment_text:
                    final_attachment_text = existing_attachment_text
                    logger.info(f"[DB保存] Stage Eが空のため、既存attachment_textを保持: {len(final_attachment_text)}文字")

                # metadata のマージロジック
                # 既存の metadata（message_id, thread_id, subject など）を保持しつつ、
                # Stage H の metadata（LLMが生成した構造化データ）を追加
                final_metadata = {}
                if existing_document_id and existing_metadata:
                    # 既存の metadata をベースにする
                    final_metadata = existing_metadata.copy()
                    logger.info(f"[DB保存] 既存metadataを保持: {list(existing_metadata.keys())}")
                # Stage H の metadata を追加・更新
                if stageH_metadata:
                    final_metadata.update(stageH_metadata)
                    logger.info(f"[DB保存] Stage H metadataをマージ: {list(stageH_metadata.keys())}")

                doc_data = {
                    'source_id': source_id,
                    'source_type': 'unified_pipeline',
                    'file_name': file_name,
                    'workspace': workspace,
                    'doc_type': doc_type,
                    'title': sanitized_title,
                    'attachment_text': final_attachment_text,
                    'summary': sanitized_summary,
                    'tags': tags,
                    'document_date': document_date,
                    'metadata': final_metadata,
                    'processing_status': 'completed',
                    # 各ステージの出力を保存
                    # E1-E3: 現在は未実装のため、E4と同じ値を保存（将来的に個別エンジンを実装予定）
                    'stage_e1_text': sanitized_extracted_text,  # Stage E-1: PyPDF2（未実装、E4の値を使用）
                    'stage_e2_text': sanitized_extracted_text,  # Stage E-2: pdfminer（未実装、E4の値を使用）
                    'stage_e3_text': sanitized_extracted_text,  # Stage E-3: PyMuPDF（未実装、E4の値を使用）
                    'stage_e4_text': sanitized_extracted_text,  # Stage E-4: pdfplumber/画像OCR
                    'stage_e5_text': sanitized_extracted_text,  # Stage E-5: 最終統合（現在はE4と同じ）
                    'stage_f_text_ocr': stage_f_text_ocr,        # Stage F: Text OCR
                    'stage_f_layout_ocr': stage_f_layout_ocr,    # Stage F: Layout OCR
                    'stage_f_visual_elements': stage_f_visual_elements,  # Stage F: Visual Elements
                    'stage_h_normalized': sanitized_combined_text,  # Stage H への入力テキスト
                    'stage_i_structured': json.dumps(stageH_result, ensure_ascii=False, indent=2) if stageH_result else None,  # Stage H の出力
                    'stage_j_chunks_json': json.dumps(chunks, ensure_ascii=False, indent=2)  # Stage J の出力
                }

                # 既存ドキュメントの場合、display_* フィールドを保持（Gmail ingestion時に設定された値を上書きしないため）
                if existing_document_id and existing_display_fields:
                    for key, value in existing_display_fields.items():
                        if value is not None:  # Noneでない値のみ保持
                            doc_data[key] = value
                    logger.debug(f"[DB保存] display_*フィールドを保持: {list(existing_display_fields.keys())}")

                # extra_metadata をマージ
                if extra_metadata:
                    # display_*フィールドは最上位フィールドとして保存
                    display_fields = ['display_subject', 'display_sender', 'display_sender_email', 'display_sent_at', 'display_post_text', 'display_type']
                    for field in display_fields:
                        if field in extra_metadata and extra_metadata[field] is not None:
                            doc_data[field] = extra_metadata[field]
                            logger.debug(f"[DB保存] extra_metadataから{field}を設定: {extra_metadata[field]}")

                    # display_*以外のフィールドはmetadataにマージ
                    other_metadata = {k: v for k, v in extra_metadata.items() if k not in display_fields}
                    if other_metadata:
                        if isinstance(doc_data['metadata'], dict):
                            doc_data['metadata'].update(other_metadata)
                        else:
                            doc_data['metadata'] = other_metadata

                # 既存ドキュメントを更新 or 新規作成
                if existing_document_id:
                    logger.info(f"[DB更新] 既存ドキュメント更新: {existing_document_id}")
                    # IDを除外してUPDATE（IDは変更不可）
                    update_data = {k: v for k, v in doc_data.items() if k != 'id'}
                    result = self.db.client.table('Rawdata_FILE_AND_MAIL').update(update_data).eq('id', existing_document_id).execute()
                    if result.data and len(result.data) > 0:
                        document_id = result.data[0]['id']
                        logger.info(f"[DB更新完了] Rawdata_FILE_AND_MAIL ID: {document_id}")
                    else:
                        logger.error("[DB更新エラー] ドキュメント更新失敗")
                        return {'success': False, 'error': 'Document update failed'}
                else:
                    logger.info("[DB保存] 新規ドキュメント作成")
                    result = self.db.client.table('Rawdata_FILE_AND_MAIL').insert(doc_data).execute()
                    if result.data and len(result.data) > 0:
                        document_id = result.data[0]['id']
                        logger.info(f"[DB保存] Rawdata_FILE_AND_MAIL ID: {document_id}")
                    else:
                        logger.error("[DB保存エラー] ドキュメント作成失敗")
                        return {'success': False, 'error': 'Document creation failed'}

            except Exception as e:
                logger.error(f"[DB保存エラー] {e}")
                return {'success': False, 'error': str(e)}

            # ============================================
            # Stage K: Embedding
            # ============================================
            logger.info("[Stage K] ベクトル化開始...")
            if progress_callback:
                progress_callback("K")

            # 既存ドキュメントの場合は、古いチャンクを削除
            if existing_document_id:
                try:
                    logger.info(f"[Stage K] 既存チャンク削除: document_id={document_id}")
                    self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
                except Exception as e:
                    logger.warning(f"[Stage K 警告] 既存チャンク削除エラー（継続）: {e}")

            # 新しいチャンクを保存
            stage_k_result = self.stage_k.embed_and_save(document_id, chunks)

            # Stage K の結果をチェック（厳格モード: 1つでも失敗したら全体失敗）
            if not stage_k_result.get('success'):
                error_msg = f"Stage K失敗: {stage_k_result.get('failed_count', 0)}/{len(chunks)}チャンク保存失敗"
                logger.error(f"[Stage K失敗] {error_msg}")
                return {'success': False, 'error': error_msg}

            # 部分的失敗は警告として扱う（一部のチャンクは保存済み）
            failed_count = stage_k_result.get('failed_count', 0)
            saved_count = stage_k_result.get('saved_count', 0)
            if failed_count > 0:
                logger.warning(f"[Stage K警告] 部分的な失敗: {failed_count}/{len(chunks)}チャンク保存失敗（{saved_count}チャンクは保存済み）")
                # 失敗したが、一部は成功しているので継続

            logger.info(f"[Stage K完了] {stage_k_result.get('saved_count', 0)}/{len(chunks)}チャンク保存")

            return {
                'success': True,
                'document_id': document_id,
                'summary': summary,
                'tags': tags,
                'chunks_count': stage_k_result.get('saved_count', 0)
            }

        except Exception as e:
            logger.error(f"[パイプラインエラー] {e}", exc_info=True)
            return {'success': False, 'error': str(e)}
```

### shared\pipeline\prompts\__init__.py

```py
"""
G_unified_pipeline prompts module
"""

# Stage G: レシート構造化プロンプト（家計簿用）
STAGE_G_FORMATTING_PROMPT = """あなたはレシートのOCR結果から構造化データを生成する専門家です。

以下は Stage F（OCR）で抽出されたレシート全文です：

---
{vision_raw}
---

このテキストから、レシート情報を構造化してJSON形式で出力してください。

## 構造化タスク

### 1. 店舗情報の抽出
- 店舗名
- 住所
- 電話番号
- 店舗コード（記載されていれば）

### 2. 取引情報の抽出
- 日付（YYYY-MM-DD形式）
- 時刻（HH:MM:SS形式、記載されていれば）
- レジ番号
- レシート番号
- 取引番号

### 3. 商品明細の抽出
各商品について：
- 行番号（レシート記載の順序）
- 商品名（レシート記載のまま）
- 数量
- 単価
- 金額
- **税率マーク（最重要！）**
  - 商品名の横や行内にある記号を必ず抽出してください
  - 例: `*`, `※`, `★`, `◆`, `8%`, `10%`, `(軽)` など
  - マークがない場合は null
  - **注意**: `*`（アスタリスク）も`※`（米印）もすべて抽出すること

**重要な注意事項（数量情報行の処理）**：
- 括弧で囲まれた数量情報（例: `(2個 X 単128)`, `(3パック X 単200)`）は**独立した商品ではない**
- これらは**直前の商品の数量詳細**なので、独立した行として抽出しないこと
- 数量情報は直前の商品の quantity と unit_price に反映させること

**重要な注意事項（小計・合計行の識別）**：
- 以下の行は**商品ではなく、集計行**です。独立した商品として抽出しないでください：
  - 「小計」「小 計」
  - 「合計」「合 計」
  - 「お買上」「お買上げ」「お買い上げ」
  - 「計」
  - これらの行には `line_type: "SUBTOTAL"` または `line_type: "TOTAL"` を設定してください

### 4. 金額情報の抽出
**重要：レシートに記載されている数値をそのまま抽出（計算・推測禁止）**

- 小計（税抜小計、記載されていれば）
- 8%対象額（税抜）
- 8%消費税額
- 10%対象額（税抜）
- 10%消費税額
- 消費税合計
- 合計（支払額）
- お預かり（記載されていれば）
- お釣り（記載されていれば）

### 5. 支払情報
- 支払方法（現金/カード/電子マネー等）
- カード情報（記載されていれば）

### 6. その他情報
- ポイント情報
- キャンペーン情報
- バーコード番号
- **税率説明**（重要！）
  - レシート下部の税率に関する説明文を抽出
  - 例: 「*印は軽減税率（8%）適用商品」「※は軽減税率対象」など
  - 記載がない場合は null
- その他の特記事項

## 出力形式

以下のJSON形式で出力してください：

```json
{{
  "shop_info": {{
    "name": "店舗名",
    "address": "住所（記載があれば）",
    "phone": "電話番号（記載があれば）",
    "store_code": "店舗コード（記載があれば）"
  }},
  "transaction_info": {{
    "date": "YYYY-MM-DD",
    "time": "HH:MM:SS（記載があれば）",
    "register_number": "レジ番号（記載があれば）",
    "receipt_number": "レシート番号（記載があれば）",
    "transaction_number": "取引番号（記載があれば）"
  }},
  "items": [
    {{
      "line_number": 1,
      "line_text": "レシートのこの行のテキストそのまま",
      "product_name": "商品名",
      "quantity": 1,
      "unit_price": 100,
      "amount": 100,
      "tax_mark": "※または★またはなし"
    }}
  ],
  "amounts": {{
    "subtotal": 1377,
    "tax_8_base": 0,
    "tax_8_amount": 0,
    "tax_10_base": 1377,
    "tax_10_amount": 123,
    "total_tax": 123,
    "total": 1500,
    "received": 2000,
    "change": 500
  }},
  "payment": {{
    "method": "現金",
    "card_info": null
  }},
  "other_info": {{
    "points": "ポイント情報",
    "campaign": "キャンペーン情報",
    "barcode": "バーコード番号",
    "tax_rate_note": "*印は軽減税率（8%）適用商品",
    "notes": "その他特記事項"
  }}
}}
```

## 重要な注意事項

1. **数値は必ずレシート記載のまま**
   - 計算しない、推測しない
   - 記載がない項目は null にする

2. **8%/10%税率の区分（最重要！）**
   - **各商品行の税率マークを必ず確認して抽出すること**
   - マークの例: `*`, `※`, `★`, `◆`, `8%`, `10%`, `(軽)` など
   - **`*`（アスタリスク）と`※`（米印）は別の文字として扱うこと**
   - 商品名の横、行の末尾、数量の後など、どこにあっても抽出すること
   - レシート下部に「*印は軽減税率（8%）適用商品」のような説明があれば、other_info.tax_rate_note に抽出すること
   - 記載がない場合のみ null にする（推測しない）

3. **小計・消費税・合計の関係**
   - レシートに「小計」「消費税」「合計」が明記されている場合、その数値をそのまま使う
   - 外税レシート：小計 + 消費税 = 合計
   - 内税レシート：合計のみ（小計は記載なしの場合あり）

4. **税額サマリー**
   - 「8%対象 ○○円（税 △△円）」のような記載があれば必ず抽出
   - 「10%対象 ○○円（税 △△円）」のような記載があれば必ず抽出

5. **記載がない項目**
   - 記載がない項目は null にする
   - 空文字列や0ではなく null を使う

## エラー処理

- OCR結果が不完全な場合：`{{"error": "incomplete_ocr", "details": "不完全な箇所の説明"}}`
- レシートとして認識できない場合：`{{"error": "not_a_receipt"}}`
"""
```

### shared\pipeline\stage_e_preprocessing.py

```py
"""
Stage E: Pre-processing (前処理)

全ファイルタイプ共通のE1-E5ステージ:
- E-1: テキスト抽出（ライブラリベース）
- E-2: 表抽出
- E-3: 統合（テキスト + 表）
- E-4: Gemini Vision差分検出
- E-5: Vision OCR結果適用

対応ファイル:
- PDF: pdfplumber + Vision補完
- Office: python-docx, openpyxl, python-pptx + Vision補完
- 画像: Vision OCR（E-4で全文字拾い）
"""
from pathlib import Path
from typing import Dict, Any, List, Optional
from loguru import logger

from shared.common.processors.pdf import PDFProcessor
from shared.common.processors.office import OfficeProcessor
from shared.ai.llm_client.llm_client import LLMClient


class StageEPreprocessor:
    """Stage E: 前処理（全ファイルタイプ共通のE1-E5ステージ）"""

    def __init__(self, llm_client: LLMClient):
        """
        Args:
            llm_client: LLMクライアント
        """
        self.llm_client = llm_client
        self.pdf_processor = PDFProcessor(llm_client=llm_client)
        self.office_processor = OfficeProcessor()

    def extract_text(
        self,
        file_path: Path,
        mime_type: str,
        pre_extracted_text: Optional[str] = None,
        workspace: Optional[str] = None,
        progress_callback=None
    ) -> Dict[str, Any]:
        """
        ファイルからテキストを抽出（E1-E5ステージ）

        Args:
            file_path: ファイルパス
            mime_type: MIMEタイプ
            pre_extracted_text: 既に抽出済みのテキスト（HTML→PNG等の場合）
            workspace: ワークスペース（gmail判定に使用）

        Returns:
            {
                'success': bool,
                'content': str,  # 完全なテキスト（単一）
                'char_count': int,
                'method': str  # 'pdf', 'docx', 'xlsx', 'pptx', 'image', 'none'
            }
        """
        logger.info("=" * 60)
        logger.info("[Stage E] Pre-processing開始...")
        logger.info(f"  ├─ ファイル: {file_path.name if isinstance(file_path, Path) else file_path}")
        logger.info(f"  └─ MIMEタイプ: {mime_type}")

        content = ""
        method = "none"

        try:
            # PDF処理（E1-E5は pdf.py 内で実行）
            if mime_type == 'application/pdf':
                result = self.pdf_processor.extract_text(str(file_path), progress_callback=progress_callback)
                if result.get('success'):
                    content = result.get('content', '')
                    method = 'pdf'

            # Office文書処理（E1-E5ログ付き）
            elif mime_type in [
                'application/vnd.openxmlformats-officedocument.wordprocessingml.document',  # .docx
                'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',      # .xlsx
                'application/vnd.openxmlformats-officedocument.presentationml.presentation'  # .pptx
            ]:
                content, method = self._process_office_with_stages(file_path, mime_type, progress_callback)

            # 画像ファイル処理（E1-E5ログ付き）
            # HTML→PNG の場合も含む（mime_type='text/html' だがファイルはPNG）
            elif mime_type.startswith('image/') or mime_type == 'text/html':
                content, method = self._process_image_with_stages(file_path, pre_extracted_text, workspace, progress_callback)

            # 最終ログ
            logger.info("=" * 60)
            logger.info(f"[Stage E完了] 最終結果:")
            logger.info(f"  ├─ 処理方式: {method}")
            logger.info(f"  └─ 最終テキスト: {len(content)} 文字")
            logger.info("=" * 60)

            return {
                'success': True,
                'content': content,
                'char_count': len(content),
                'method': method
            }

        except Exception as e:
            logger.error(f"[Stage E エラー] テキスト抽出失敗: {e}", exc_info=True)
            return {
                'success': False,
                'content': '',
                'char_count': 0,
                'method': 'error',
                'error': str(e)
            }

    def _process_office_with_stages(self, file_path: Path, mime_type: str, progress_callback=None) -> tuple:
        """
        Officeファイル処理（E1-E5ステージログ付き）

        Args:
            file_path: ファイルパス
            mime_type: MIMEタイプ

        Returns:
            (content, method)
        """
        file_path = Path(file_path)

        # ファイルタイプ判定
        if 'word' in mime_type:
            file_type = 'docx'
        elif 'sheet' in mime_type:
            file_type = 'xlsx'
        elif 'presentation' in mime_type:
            file_type = 'pptx'
        else:
            file_type = 'office'

        logger.info(f"[Stage E] Office処理開始 (type: {file_type})")

        # ============================================
        # E-1: テキスト抽出（ライブラリベース）
        # ============================================
        logger.info(f"[E-1] テキスト抽出開始 (engine: python-{file_type})")

        result = self.office_processor.extract_text(str(file_path))
        e1_text = result.get('content', '') if result.get('success') else ''
        e1_chars = len(e1_text)

        logger.info(f"[E-1] テキスト抽出完了:")
        logger.info(f"  └─ 抽出テキスト: {e1_chars} 文字")

        # ============================================
        # E-2: 表抽出
        # ============================================
        logger.info(f"[E-2] 表抽出:")
        # Officeファイルの場合、表はテキスト抽出時に含まれる
        # （python-docx/openpyxl/python-pptxが表を含めて抽出）
        table_count = e1_text.count('|') // 2 if '|' in e1_text else 0  # 簡易カウント
        logger.info(f"  └─ 表データ: テキストに含まれる (推定 {table_count} セル)")

        # ============================================
        # E-3: 統合
        # ============================================
        logger.info(f"[E-3] 統合:")
        e3_text = e1_text  # Officeは既に統合済み
        e3_chars = len(e3_text)
        logger.info(f"  └─ 統合テキスト: {e3_chars} 文字")

        # ============================================
        # E-4: Gemini Vision差分検出
        # ============================================
        if progress_callback:
            progress_callback("E4")
        logger.info(f"[E-4] Vision差分検出:")

        e4_text = ""
        if self.llm_client and e3_chars < 100:
            # テキストが少ない場合のみVision補完を実行
            logger.info(f"  ├─ テキスト量が少ない ({e3_chars}文字) → Vision補完を実行")

            # PPTXの場合、スライドを画像化してVision処理
            if file_type == 'pptx':
                try:
                    from pptx import Presentation
                    from PIL import Image
                    import tempfile
                    import os

                    # スライド数を取得
                    prs = Presentation(str(file_path))
                    slide_count = len(prs.slides)
                    logger.info(f"  ├─ スライド数: {slide_count}")

                    # 各スライドをVision処理（最大5スライド）
                    vision_texts = []
                    for i, slide in enumerate(prs.slides[:5]):
                        logger.info(f"  ├─ スライド {i+1} をVision処理中...")
                        # Note: PPTXを画像化するには追加ライブラリが必要
                        # ここでは簡略化してスキップ

                    if vision_texts:
                        e4_text = "\n\n".join(vision_texts)
                        logger.info(f"  └─ Vision補完結果: {len(e4_text)} 文字")
                    else:
                        logger.info(f"  └─ Vision補完: スキップ（画像化未対応）")

                except Exception as e:
                    logger.warning(f"  └─ Vision補完失敗: {e}")
            else:
                logger.info(f"  └─ Vision補完: スキップ（{file_type}は対象外）")
        else:
            logger.info(f"  └─ Vision補完: 不要 (十分なテキスト量: {e3_chars}文字)")

        # ============================================
        # E-5: Vision OCR結果適用
        # ============================================
        if progress_callback:
            progress_callback("E5")
        logger.info(f"[E-5] Vision OCR結果適用:")

        if e4_text:
            # Vision補完がある場合は追加
            e5_text = e3_text + "\n\n---\n\n## Vision OCR 補完情報\n\n" + e4_text
            logger.info(f"  ├─ E-3テキスト: {e3_chars} 文字")
            logger.info(f"  ├─ E-4補完: +{len(e4_text)} 文字")
            logger.info(f"  └─ E-5最終: {len(e5_text)} 文字")
        else:
            e5_text = e3_text
            logger.info(f"  └─ E-5最終: {len(e5_text)} 文字 (Vision補完なし)")

        return e5_text, file_type

    def _process_image_with_stages(
        self,
        file_path: Path,
        pre_extracted_text: Optional[str] = None,
        workspace: Optional[str] = None,
        progress_callback=None
    ) -> tuple:
        """
        画像ファイル処理（E1-E5ステージログ付き）

        Args:
            file_path: ファイルパス
            pre_extracted_text: 既に抽出済みのテキスト（HTML→PNG等の場合）
            workspace: ワークスペース（gmail判定に使用）

        Returns:
            (content, method)
        """
        file_path = Path(file_path)

        logger.info(f"[Stage E] 画像処理開始")

        # ============================================
        # E-1: テキスト抽出（画像なのでなし、ただしHTML→PNG等の場合は既抽出済み）
        # ============================================
        logger.info(f"[E-1] テキスト抽出:")
        if pre_extracted_text:
            e1_chars = len(pre_extracted_text)
            logger.info(f"  └─ 既抽出テキスト (Ingestion時): {e1_chars} 文字")
        else:
            e1_chars = 0
            logger.info(f"  └─ 画像ファイルのため、ライブラリ抽出: 0 文字")

        # ============================================
        # E-2: 表抽出（画像なのでなし）
        # ============================================
        logger.info(f"[E-2] 表抽出:")
        logger.info(f"  └─ 画像ファイルのため、表抽出: 0 個")

        # ============================================
        # E-3: 統合（画像なのでなし、ただしHTML→PNG等の場合は既抽出テキストを使用）
        # ============================================
        logger.info(f"[E-3] 統合:")
        if pre_extracted_text:
            e3_text = pre_extracted_text
            e3_chars = len(e3_text)
            logger.info(f"  └─ E-1既抽出テキスト: {e3_chars} 文字")
        else:
            e3_text = ""
            e3_chars = 0
            logger.info(f"  └─ E-1 + E-2 統合: 0 文字")

        # ============================================
        # E-4: Gemini Vision OCR（画像のメイン処理）
        # ============================================
        if progress_callback:
            progress_callback("E4")
        # Gmail処理の場合はコスト削減のためflash-liteを使用
        is_gmail = workspace == 'gmail' if workspace else False
        vision_model = "gemini-2.5-flash-lite" if is_gmail else "gemini-2.5-flash"
        logger.info(f"[E-4] Vision OCR処理 (model: {vision_model}):")

        e4_text = ""
        if self.llm_client:
            vision_result = self.llm_client.transcribe_image(
                image_path=file_path,
                model=vision_model,
                prompt="""この画像から、全ての文字を徹底的に拾い尽くしてください。

【あなたの役割】
画像から全ての文字を漏らさず拾ってください。

【文字拾いの徹底指示】
- **小さな文字**: 注釈、脚注、コピーライト表記なども全て拾う
- **ロゴ化された文字**: 画像として埋め込まれたタイトル、会社名、ブランド名なども全て読み取る
- **装飾された文字**: 太字、斜体、色付きなど、装飾に関わらず全て拾う
- **背景に埋もれた文字**: 薄い色、透かし文字なども可能な限り読み取る
- **手書き文字**: 判読可能な範囲で全て拾う
- **表構造**: 表がある場合は、Markdown table形式で出力

【出力形式】
画像内の全てのテキストを、Markdown形式で構造化して出力してください。
表がある場合は必ずMarkdown table形式で出力してください。

**重要**: 1文字も見逃さないでください。"""
            )

            if vision_result.get('success'):
                e4_text = vision_result.get('content', '')
                logger.info(f"  ├─ Vision OCR成功")
                logger.info(f"  └─ 抽出テキスト: {len(e4_text)} 文字")
            else:
                logger.warning(f"  └─ Vision OCR失敗: {vision_result.get('error', 'Unknown error')}")
        else:
            logger.warning(f"  └─ LLMクライアント未設定のためスキップ")

        # ============================================
        # E-5: Vision OCR結果適用（画像は E-4 がそのまま最終、ただし既抽出テキストがあれば統合）
        # ============================================
        if progress_callback:
            progress_callback("E5")
        logger.info(f"[E-5] Vision OCR結果適用:")

        # E-3テキストとE-4 Vision OCRを統合
        if e4_text:
            if e3_text:
                # 既抽出テキスト + Vision OCR
                e5_text = e3_text + "\n\n---\n\n## Vision OCR 補完情報\n\n" + e4_text
            else:
                # Vision OCRのみ
                e5_text = e4_text
        else:
            # 既抽出テキストのみ
            e5_text = e3_text

        logger.info(f"  ├─ E-3テキスト: {e3_chars} 文字")
        logger.info(f"  ├─ E-4 Vision OCR: {len(e4_text)} 文字")
        logger.info(f"  └─ E-5最終: {len(e5_text)} 文字")

        return e5_text, 'image'

    def process(self, file_path: Path, mime_type: str) -> str:
        """
        ファイルからテキストを抽出（process() エイリアス）

        Args:
            file_path: ファイルパス
            mime_type: MIMEタイプ

        Returns:
            extracted_text: 抽出されたテキスト
        """
        result = self.extract_text(file_path, mime_type)
        return result.get('content', '') if result.get('success') else ''
```

### shared\pipeline\stage_f_visual.py

```py
"""
Stage F: Visual Analysis (視覚解析)

Hybrid方式: Surya + PaddleOCR + Gemini Vision（3段階）
- Step 1 (F-1): PaddleOCR で表構造を抽出
- Step 2 (F-2～F-5): Surya + PaddleOCR で高精度テキスト認識
  - F-2: Suryaでレイアウト解析・Bounding Box取得
  - F-3: 画像切り出し
  - F-4: PaddleOCRで日本語テキスト認識
  - F-5: テキスト統合
- Step 3 (F-6～F-10): Gemini Vision で全体解析・統合
  - F-6: プロンプト構築
  - F-7: Gemini Vision API呼び出し
  - F-8: JSON クリーニング
  - F-9: 全結果マージ
  - F-10: 最終検証・出力

- 役割: 人間が見たままの視覚情報をそのまま捉える（OCR、レイアウト認識）
- モデル: 設定ファイルで指定（config/models.yaml）
- プロンプト: 設定ファイルで指定（config/prompts/stage_f/*.md）
- 出力: 3種類のデータ（full_text, layout_info, visual_elements）
"""
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from loguru import logger
import json
import time
import numpy as np
from PIL import Image

from shared.ai.llm_client.llm_client import LLMClient
from shared.ai.llm_client.exceptions import MaxTokensExceededError
import cv2
from .image_preprocessing import preprocess_image_for_ocr, calculate_image_quality_score
from .ocr_config import OCRConfig, OCRResultCache, PaddleOCRVersionAdapter
from .ocr_report import OCRProcessingReport, OCRRegionStats

# Surya のインポート（オプショナル）
try:
    from surya.detection import DetectionPredictor
    from surya.layout import LayoutPredictor
    from surya.foundation import FoundationPredictor
    SURYA_AVAILABLE = True
except ImportError:
    SURYA_AVAILABLE = False
    logger.warning("[Hybrid OCR] Surya not installed - Surya mode disabled")

# PaddleOCR のインポート（オプショナル）
try:
    from paddleocr import PPStructureV3 as PPStructure, PaddleOCR
    PADDLEOCR_AVAILABLE = True
except ImportError:
    PADDLEOCR_AVAILABLE = False
    logger.warning("[Hybrid OCR] PaddleOCR not installed - PaddleOCR mode disabled")


class StageFVisualAnalyzer:
    """Stage F: 視覚解析（Surya + PaddleOCR + Gemini Vision のハイブリッド）"""

    def __init__(self, llm_client: LLMClient, enable_hybrid_ocr: bool = False):
        """
        Args:
            llm_client: LLMクライアント
            enable_hybrid_ocr: ハイブリッドOCR（Surya + PaddleOCR）を有効化
        """
        self.llm_client = llm_client
        self.enable_hybrid_ocr = enable_hybrid_ocr

        
        # OCR結果キャッシュ
        self.ocr_cache = OCRResultCache() if enable_hybrid_ocr else None
        
        # Hybrid OCR engines (lazy loading)
        self.surya_detector = None
        self.surya_layout = None
        self.paddle_ocr = None
        self.paddle_structure = None  # 表抽出用

        if enable_hybrid_ocr:
            self._initialize_hybrid_ocr_engines()

    def should_run(self, mime_type: str, extracted_text_length: int) -> bool:
        """
        Stage F を実行すべきか判定

        発動条件:
        1. 画像ファイル
        2. Pre-processing でテキストがほとんど抽出できなかった（100文字未満）

        Args:
            mime_type: MIMEタイプ
            extracted_text_length: Stage E で抽出したテキストの長さ

        Returns:
            True: Stage F を実行すべき
        """
        # 条件1: 画像ファイル
        if mime_type and mime_type.startswith('image/'):
            logger.info("[Stage F] 画像ファイルを検出 → Vision処理実行")
            return True

        # 条件2: テキストがほとんど抽出できなかった
        if extracted_text_length < 100:
            logger.info(f"[Stage F] テキスト量が少ない({extracted_text_length}文字) → Vision処理実行")
            return True

        return False

    def analyze(self, file_path: Path) -> Dict[str, Any]:
        """
        画像/PDFから視覚情報を抽出（廃止予定メソッド）

        Args:
            file_path: ファイルパス

        Returns:
            {
                'success': bool,
                'vision_raw': str,
                'vision_json': dict,
                'char_count': int
            }
        """
        logger.info("[Stage F] Visual Analysis開始...")

        if not file_path.exists():
            logger.error(f"[Stage F エラー] ファイルが存在しません: {file_path}")
            return {
                'success': False,
                'vision_raw': '',
                'vision_json': None,
                'char_count': 0,
                'error': 'File not found'
            }

        try:
            # NOTE: この analyze() メソッドは廃止予定
            # 代わりに process() メソッドを使用してください
            vision_raw = self.llm_client.generate_with_vision(
                prompt="<deprecated>",
                image_path=str(file_path),
                model="gemini-2.5-flash",
                response_format="json"
            )

            logger.info(f"[Stage F完了] Vision結果: {len(vision_raw)}文字")

            vision_json = None
            try:
                vision_json = json.loads(vision_raw)
            except json.JSONDecodeError as e:
                logger.warning(f"[Stage F] JSON解析失敗: {e}")

            return {
                'success': True,
                'vision_raw': vision_raw,
                'vision_json': vision_json,
                'char_count': len(vision_raw)
            }

        except Exception as e:
            logger.error(f"[Stage F エラー] Vision処理失敗: {e}", exc_info=True)
            return {
                'success': False,
                'vision_raw': '',
                'vision_json': None,
                'char_count': 0,
                'error': str(e)
            }

    def process(
        self,
        file_path: Path,
        prompt: str,
        model: str,
        extracted_text: str = "",
        workspace: str = "default",
        progress_callback=None
    ) -> str:
        """
        画像/PDFから視覚情報を抽出（F-1～F-10の完全フロー）

        Args:
            file_path: ファイルパス
            prompt: プロンプトテキスト（config/prompts/stage_f/*.md から読み込み）
            model: モデル名（config/models.yaml から取得）
            extracted_text: Stage E で抽出した完全なテキスト
            workspace: ワークスペース名（gmail の場合は表抽出をスキップ）

        Returns:
            vision_raw: 3つの情報（full_text, layout_info, visual_elements）のJSONテキスト
        """
        total_start_time = time.time()

        logger.info("=" * 60)
        logger.info(f"[Stage F] ハイブリッドOCR処理開始 (model={model})")
        logger.info(f"  ├─ ファイル: {file_path.name}")
        logger.info(f"  ├─ Stage Eテキスト: {len(extracted_text)}文字")
        logger.info(f"  └─ ハイブリッドモード: {'有効' if self.enable_hybrid_ocr else '無効（Geminiのみ）'}")
        logger.info("=" * 60)

        if not file_path.exists():
            logger.error(f"[Stage F エラー] ファイルが存在しません: {file_path}")
            return ""

        # Gemini Vision APIがサポートしていないファイルタイプをスキップ
        unsupported_extensions = {'.pptx', '.ppt', '.doc', '.docx', '.xls', '.xlsx'}
        if file_path.suffix.lower() in unsupported_extensions:
            logger.info(f"[Stage F] スキップ: {file_path.suffix} はVision APIでサポートされていません")
            return ""

        try:
            # ============================================
            # [F-1] PaddleOCR 表構造抽出
            # ============================================
            if progress_callback:
                progress_callback("F-1")
            paddle_tables = []
            paddle_text_chars = 0
            total_cells = 0

            # Gmail処理では表抽出をスキップ（ハング問題回避）
            if workspace == 'gmail':
                logger.info("[F-1] PaddleOCR表抽出: スキップ（Gmail処理のため無効化）")
            elif self.enable_hybrid_ocr and PADDLEOCR_AVAILABLE:
                f1_start = time.time()
                logger.info("[F-1] PaddleOCR 表構造抽出開始...")

                paddle_tables = self._extract_tables_with_paddleocr(file_path)

                # 統計計算
                for table in paddle_tables:
                    rows = table.get('rows', [])
                    for row in rows:
                        total_cells += len(row)
                        paddle_text_chars += sum(len(cell) for cell in row)

                f1_elapsed = time.time() - f1_start
                logger.info(f"[F-1完了] PaddleOCR 表抽出:")
                logger.info(f"  ├─ 検出表数: {len(paddle_tables)}個")
                logger.info(f"  ├─ 総セル数: {total_cells}個")
                logger.info(f"  ├─ 抽出文字数: {paddle_text_chars}文字")
                logger.info(f"  └─ 処理時間: {f1_elapsed:.2f}秒")
            else:
                logger.info("[F-1] PaddleOCR表抽出: スキップ（ハイブリッドモード無効）")

            # ============================================
            # [F-2] Surya レイアウト解析
            # ============================================
            if progress_callback:
                progress_callback("F-2")
            text_boxes = []
            img_width = 0
            img_height = 0
            avg_bbox_size = 0

            if self.enable_hybrid_ocr and self.surya_detector:
                f2_start = time.time()
                logger.info("[F-2] Suryaレイアウト解析開始...")

                # 画像読み込み（PDFの場合は最初のページを画像に変換）
                file_ext = str(file_path).lower().split('.')[-1]
                if file_ext == 'pdf':
                    import fitz  # PyMuPDF
                    doc = fitz.open(file_path)
                    page = doc[0]  # 最初のページ
                    # 高解像度でレンダリング (300 DPI)
                    mat = fitz.Matrix(300/72, 300/72)
                    pix = page.get_pixmap(matrix=mat)
                    image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                    doc.close()
                    logger.info(f"  ├─ PDF→画像変換完了: {pix.width}x{pix.height}px")
                else:
                    image = Image.open(file_path).convert("RGB")
                img_width, img_height = image.size

                # 画像サイズ制限（Suryaのメモリ問題対策）
                # 文字認識は別で元画像を使うのでリサイズOK
                MAX_DIMENSION = 2000  # 最大辺を2000pxに制限
                if max(img_width, img_height) > MAX_DIMENSION:
                    scale = MAX_DIMENSION / max(img_width, img_height)
                    new_width = int(img_width * scale)
                    new_height = int(img_height * scale)
                    image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
                    logger.info(f"  ├─ 画像リサイズ（Surya用）: {img_width}x{img_height} → {new_width}x{new_height}px")
                    img_width, img_height = new_width, new_height

                # Suryaでレイアウト検出
                detection_results = self.surya_detector([image])
                # PolygonBoxオブジェクトからbbox（[x1,y1,x2,y2]）を抽出
                text_boxes = [box.bbox for box in detection_results[0].bboxes] if detection_results and detection_results[0].bboxes else []

                # 平均領域サイズ計算
                if text_boxes:
                    bbox_sizes = [(bbox[2] - bbox[0]) * (bbox[3] - bbox[1]) for bbox in text_boxes]
                    avg_bbox_size = sum(bbox_sizes) / len(bbox_sizes)

                f2_elapsed = time.time() - f2_start
                logger.info(f"[F-2完了] Suryaレイアウト解析:")
                logger.info(f"  ├─ 検出領域数: {len(text_boxes)}個")
                logger.info(f"  ├─ 画像サイズ: {img_width}x{img_height}px")
                logger.info(f"  ├─ 平均領域サイズ: {avg_bbox_size:.1f}px²")
                logger.info(f"  └─ 処理時間: {f2_elapsed:.2f}秒")
            else:
                logger.info("[F-2] Suryaレイアウト解析: スキップ（ハイブリッドモード無効）")
                # Geminiのみの場合でも画像サイズは取得
                try:
                    file_ext = str(file_path).lower().split('.')[-1]
                    if file_ext == 'pdf':
                        import fitz  # PyMuPDF
                        doc = fitz.open(file_path)
                        page = doc[0]
                        mat = fitz.Matrix(300/72, 300/72)
                        pix = page.get_pixmap(matrix=mat)
                        image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                        img_width, img_height = pix.width, pix.height
                        doc.close()
                    else:
                        image = Image.open(file_path).convert("RGB")
                        img_width, img_height = image.size
                except:
                    pass

            # ============================================
            # [F-3] 画像切り出し
            # ============================================
            if progress_callback:
                progress_callback("F-3")
            cropped_regions = []
            min_w = min_h = max_w = max_h = avg_w = avg_h = 0

            # Gmail はレイアウト検出のみ、切り出し＆OCRはスキップ
            if workspace == 'gmail':
                logger.info("[F-3] 画像切り出し: スキップ（Gmail軽量モード: Suryaレイアウトのみ使用）")
            elif self.enable_hybrid_ocr and text_boxes:
                f3_start = time.time()
                logger.info("[F-3] 画像切り出し開始...")

                img_array = np.array(image)
                widths = []
                heights = []

                for idx, bbox in enumerate(text_boxes):
                    x1, y1, x2, y2 = map(int, [bbox[0], bbox[1], bbox[2], bbox[3]])
                    cropped = img_array[y1:y2, x1:x2]

                    w, h = x2 - x1, y2 - y1
                    widths.append(w)
                    heights.append(h)

                    cropped_regions.append({
                        'bbox': [x1, y1, x2, y2],
                        'image': cropped,
                        'region_id': idx
                    })

                # 統計計算
                if widths and heights:
                    min_w, max_w = min(widths), max(widths)
                    min_h, max_h = min(heights), max(heights)
                    avg_w = sum(widths) / len(widths)
                    avg_h = sum(heights) / len(heights)

                f3_elapsed = time.time() - f3_start
                logger.info(f"[F-3完了] 画像切り出し:")
                logger.info(f"  ├─ 切り出し領域数: {len(cropped_regions)}個")
                logger.info(f"  ├─ 最小領域サイズ: {min_w}x{min_h}px")
                logger.info(f"  ├─ 最大領域サイズ: {max_w}x{max_h}px")
                logger.info(f"  ├─ 平均領域サイズ: {avg_w:.0f}x{avg_h:.0f}px")
                logger.info(f"  └─ 処理時間: {f3_elapsed:.2f}秒")
            else:
                logger.info("[F-3] 画像切り出し: スキップ（ハイブリッドモード無効）")

            # ============================================
            # [F-4] PaddleOCR テキスト認識
            # ============================================
            if progress_callback:
                progress_callback("F-4")
            regions = []
            recognized_regions = 0
            total_chars = 0
            confidence_scores = []
            low_conf_count = 0

            if self.enable_hybrid_ocr and cropped_regions and self.paddle_ocr:
                f4_start = time.time()
                logger.info(f"[F-4] PaddleOCRテキスト認識開始... ({len(cropped_regions)}領域)")

                for idx, region_data in enumerate(cropped_regions):
                    try:
                        # PaddleOCR 3.xではcls引数は廃止、use_textline_orientation初期化時に設定済み
                        # 画像品質評価
                        quality_score = calculate_image_quality_score(region_data['image'])
                        
                        # 画像前処理（品質スコアに応じて適用）
                        preprocessed_image = region_data['image']
                        if quality_score < 0.7:
                            preprocessed_image, preprocess_stats = preprocess_image_for_ocr(
                                region_data['image'],
                                apply_clahe=True,
                                apply_denoise=True,
                                apply_sharpen=True,
                                apply_binarize=False
                            )
                        else:
                            preprocessed_image, preprocess_stats = preprocess_image_for_ocr(
                                region_data['image'],
                                apply_clahe=True,
                                apply_denoise=False,
                                apply_sharpen=True,
                                apply_binarize=False
                            )
                        
                        result = self.paddle_ocr.ocr(preprocessed_image)
                        text_lines = []
                        region_confidences = []

                        # PaddleOCR 3.x: OCRResultオブジェクトを処理
                        if result and len(result) > 0:
                            ocr_result = result[0]
                            # PaddleOCR 3.x: OCRResultは辞書ライクオブジェクト
                            # rec_texts, rec_scoresは属性ではなく辞書キーでアクセス
                            if isinstance(ocr_result, dict) or hasattr(ocr_result, '__getitem__'):
                                rec_texts = ocr_result.get('rec_texts', []) if hasattr(ocr_result, 'get') else ocr_result['rec_texts'] if 'rec_texts' in ocr_result else []
                                rec_scores = ocr_result.get('rec_scores', []) if hasattr(ocr_result, 'get') else ocr_result['rec_scores'] if 'rec_scores' in ocr_result else []
                                if rec_texts:
                                    text_lines = list(rec_texts)
                                    region_confidences = list(rec_scores) if rec_scores else []
                            # 旧API互換（リスト形式）
                            elif isinstance(ocr_result, list):
                                for line in ocr_result:
                                    if line and len(line) >= 2 and line[1]:
                                        text_lines.append(line[1][0])
                                        region_confidences.append(line[1][1])

                        text = "\n".join(text_lines)
                        avg_confidence = sum(region_confidences) / len(region_confidences) if region_confidences else 0.0

                        if text.strip():
                            recognized_regions += 1
                            total_chars += len(text)
                            confidence_scores.append(avg_confidence)

                            if avg_confidence < 0.7:
                                low_conf_count += 1

                        regions.append({
                            'bbox': region_data['bbox'],
                            'text': text,
                            'confidence': avg_confidence,
                            'region_id': idx
                        })

                        # 進捗表示
                        if (idx + 1) % 10 == 0:
                            logger.info(f"[F-4] 進捗: {idx + 1}/{len(cropped_regions)}領域処理完了 ({(idx + 1)/len(cropped_regions)*100:.1f}%)")

                    except Exception as e:
                        logger.warning(f"[F-4] 領域 {idx} のOCR失敗: {e}")


                avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0

                f4_elapsed = time.time() - f4_start
                logger.info(f"[F-4完了] PaddleOCRテキスト認識:")
                logger.info(f"  ├─ 認識領域数: {recognized_regions}/{len(cropped_regions)}個")
                logger.info(f"  ├─ 認識文字数: {total_chars}文字")
                logger.info(f"  ├─ 平均信頼度: {avg_confidence:.2%}")
                logger.info(f"  ├─ 低信頼度領域: {low_conf_count}個 (< 0.7)")
                logger.info(f"  └─ 処理時間: {f4_elapsed:.2f}秒")
            else:
                logger.info("[F-4] PaddleOCRテキスト認識: スキップ（ハイブリッドモード無効）")

            # ============================================
            # [F-5] テキスト統合
            # ============================================
            if progress_callback:
                progress_callback("F-5")
            surya_full_text = ""
            before_regions = len(regions)
            after_regions = 0
            avg_line_length = 0

            if self.enable_hybrid_ocr and regions:
                f5_start = time.time()
                logger.info("[F-5] テキスト統合開始（読み順ソート）...")

                surya_full_text = self._combine_hybrid_results(regions)

                # 統計計算
                after_regions = len([r for r in regions if r['text'].strip()])
                lines = surya_full_text.split('\n')
                avg_line_length = sum(len(line) for line in lines) / len(lines) if lines else 0

                f5_elapsed = time.time() - f5_start
                logger.info(f"[F-5完了] テキスト統合:")
                logger.info(f"  ├─ 統合前領域数: {before_regions}個")
                logger.info(f"  ├─ 統合後領域数: {after_regions}個")
                logger.info(f"  ├─ 全文テキスト: {len(surya_full_text)}文字")
                logger.info(f"  ├─ 平均行長: {avg_line_length:.1f}文字/行")
                logger.info(f"  └─ 処理時間: {f5_elapsed:.2f}秒")
            else:
                logger.info("[F-5] テキスト統合: スキップ（ハイブリッドモード無効）")

            # ============================================
            # [F-6] プロンプト構築
            # ============================================
            if progress_callback:
                progress_callback("F-6")
            f6_start = time.time()
            logger.info("[F-6] プロンプト構築開始...")

            base_prompt_chars = len(prompt)
            full_prompt = prompt
            stage_e_chars = 0
            paddle_chars = 0
            surya_chars = 0

            # Stage E のテキストを追加
            if extracted_text:
                full_prompt += "\n\n---\n\n【Stage E で抽出したテキスト】\n"
                full_prompt += f"```\n{extracted_text}\n```\n\n"
                stage_e_chars = len(extracted_text)

            # PaddleOCR の表を追加
            if paddle_tables:
                full_prompt += "\n\n---\n\n【PaddleOCR で抽出した表】\n"
                full_prompt += f"{len(paddle_tables)}個の表を検出しました：\n\n"
                for i, table in enumerate(paddle_tables, 1):
                    full_prompt += f"**表{i}:**\n"
                    full_prompt += "```\n"
                    for row in table['rows']:
                        full_prompt += " | ".join(row) + "\n"
                    full_prompt += "```\n\n"
                paddle_chars = paddle_text_chars

            # Suryaテキストを追加
            if surya_full_text:
                full_prompt += "\n\n---\n\n【Surya + PaddleOCR で抽出したテキスト】\n"
                full_prompt += f"```\n{surya_full_text}\n```\n\n"
                surya_chars = len(surya_full_text)

            # Suryaレイアウト情報を追加（Gmail軽量モード用）
            elif text_boxes and workspace == 'gmail':
                full_prompt += "\n\n---\n\n【Surya で検出したレイアウト構造】\n"
                full_prompt += f"{len(text_boxes)}個のテキスト領域を検出しました。\n"
                full_prompt += "画像を見て、これらの領域の内容とレイアウト構造を正確に記述してください。\n\n"

            # 役割説明を追加
            if extracted_text or paddle_tables or surya_full_text:
                full_prompt += """【あなたの役割】
上記の Stage E のテキスト、PaddleOCR の表、Surya のテキストを統合し、画像を詳細に見て完璧な結果を作成してください：

1. **ベース**: Stage E で抽出したテキストを `full_text` のベースとして使用する
2. **補完**: Surya/PaddleOCR で見つかった追加のテキスト、表、レイアウト情報を追加する
3. **検証**: 画像を見て、全ての情報が正しいか確認する
4. **強化**: 両方で欠けている部分を補完する（画像化されたタイトル、ロゴ、装飾文字、見逃した表など）

**重要な優先順位**:
- `full_text`: **Stage E のテキストをベースに**、Surya のテキストで補完・追加する
- `layout_info.tables`: PaddleOCR が抽出した表を必ず含め、さらに見逃した表があれば追加する
- `layout_info.sections`: Surya のレイアウト情報を活用し、セクション構造を正確に記述する
- 画像を詳細に見て、全ての文字と要素を漏らさず拾ってください
"""
            elif text_boxes and workspace == 'gmail':
                full_prompt += """【あなたの役割】
上記の Stage E のテキストと Surya のレイアウト構造を活用し、画像を見て完璧な結果を作成してください：

1. **ベース**: Stage E で抽出したテキストを `full_text` のベースとして使用する
2. **レイアウト**: Surya が検出したレイアウト構造を `layout_info.sections` に記述する
3. **視覚要素**: 画像、チャート、強調されたテキストなどを `visual_elements` に記述する
4. **検証**: 画像を見て、テキストとレイアウトが正確か確認する

**注意**: これはGmail HTMLメールのため、テキストは既に抽出済みです。視覚的な構造と要素に集中してください。
"""
            else:
                full_prompt += "\n\n【注意】Stage E でテキストを抽出できませんでした。画像から全ての文字と表を拾い尽くしてください。\n"

            total_prompt_chars = len(full_prompt)

            f6_elapsed = time.time() - f6_start
            logger.info(f"[F-6完了] プロンプト構築:")
            logger.info(f"  ├─ 基本プロンプト: {base_prompt_chars}文字")
            logger.info(f"  ├─ Stage Eテキスト: {stage_e_chars}文字")
            logger.info(f"  ├─ PaddleOCR表: {len(paddle_tables)}個 ({paddle_chars}文字)")
            logger.info(f"  ├─ Suryaテキスト: {surya_chars}文字")
            logger.info(f"  ├─ 最終プロンプト: {total_prompt_chars}文字")
            logger.info(f"  └─ 処理時間: {f6_elapsed:.2f}秒")

            # ============================================
            # [F-7] Gemini Vision API呼び出し
            # ============================================
            if progress_callback:
                progress_callback("F-7")
            f7_start = time.time()
            logger.info(f"[F-7] Gemini Vision API呼び出し開始 (model={model}, max_tokens=65536)...")

            try:
                vision_raw = self.llm_client.generate_with_vision(
                    prompt=full_prompt,
                    image_path=str(file_path),
                    model=model,
                    max_tokens=65536,
                    response_format="json"
                )

                f7_elapsed = time.time() - f7_start
                estimated_tokens = len(vision_raw) // 4  # 概算
                chars_per_sec = len(vision_raw) / f7_elapsed if f7_elapsed > 0 else 0

                logger.info(f"[F-7完了] Gemini Vision API応答受信:")
                logger.info(f"  ├─ 応答サイズ: {len(vision_raw)}文字")
                logger.info(f"  ├─ 推定トークン数: ~{estimated_tokens}トークン")
                logger.info(f"  ├─ 処理時間: {f7_elapsed:.2f}秒")
                logger.info(f"  └─ レート: {chars_per_sec:.0f}文字/秒")

                logger.debug(f"[F-7] Gemini生応答（最初の500文字）: {vision_raw[:500]}")
                logger.debug(f"[F-7] Gemini生応答（最後の500文字）: {vision_raw[-500:]}")

            except MaxTokensExceededError as e:
                # MAX_TOKENSエラー: 途中で切れた出力をファイルに保存してエラーとして記録
                f7_elapsed = time.time() - f7_start

                # 途中で切れた出力をファイルに保存
                error_output_dir = Path("logs/max_tokens_errors")
                error_output_dir.mkdir(parents=True, exist_ok=True)
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                error_file = error_output_dir / f"max_tokens_error_{file_path.stem}_{timestamp}.txt"

                try:
                    with open(error_file, 'w', encoding='utf-8') as f:
                        f.write(f"=== MAX_TOKENS ERROR ===\n")
                        f.write(f"File: {file_path}\n")
                        f.write(f"Model: {model}\n")
                        f.write(f"Output size: {len(e.partial_output)} characters\n")
                        f.write(f"Estimated tokens: ~{len(e.partial_output) // 4}\n")
                        f.write(f"Finish reason: {e.finish_reason_name}\n")
                        f.write(f"Processing time: {f7_elapsed:.2f}s\n")
                        f.write(f"\n=== PARTIAL OUTPUT (FULL) ===\n\n")
                        f.write(e.partial_output)
                    logger.info(f"[F-7] 途中で切れた出力を保存しました: {error_file}")
                except Exception as save_error:
                    logger.error(f"[F-7] 出力ファイル保存失敗: {save_error}")

                logger.error("=" * 80)
                logger.error(f"[F-7 MAX_TOKENS エラー] 出力が途中で切れました")
                logger.error(f"  ├─ エラー内容: {e}")
                logger.error(f"  ├─ 途中で切れた出力サイズ: {len(e.partial_output)}文字")
                logger.error(f"  ├─ 推定トークン数: ~{len(e.partial_output) // 4}トークン")
                logger.error(f"  ├─ finish_reason: {e.finish_reason_name}")
                logger.error(f"  ├─ 処理時間: {f7_elapsed:.2f}秒")
                logger.error(f"  ├─ 全文保存先: {error_file}")
                logger.error(f"  └─ 対処法: プロンプトを短くするか、max_tokensを増やす")
                logger.error("=" * 80)
                logger.error(f"[F-7] 途中で切れた出力（最初の1000文字）:")
                logger.error(e.partial_output[:1000])
                logger.error(f"[F-7] 途中で切れた出力（最後の1000文字）:")
                logger.error(e.partial_output[-1000:])
                logger.error("=" * 80)

                # 途中で切れた出力を返さず、空文字を返してエラーとして扱う
                raise

            # ============================================
            # [F-8] JSON クリーニング
            # ============================================
            if progress_callback:
                progress_callback("F-8")
            f8_start = time.time()
            logger.info("[F-8] JSONクリーニング開始...")

            vision_cleaned = self._clean_json_response(vision_raw)

            reduction_rate = (1 - len(vision_cleaned) / len(vision_raw)) * 100 if len(vision_raw) > 0 else 0
            is_valid_json = False
            try:
                json.loads(vision_cleaned)
                is_valid_json = True
            except:
                pass

            f8_elapsed = time.time() - f8_start
            logger.info(f"[F-8完了] JSONクリーニング:")
            logger.info(f"  ├─ クリーニング前: {len(vision_raw)}文字")
            logger.info(f"  ├─ クリーニング後: {len(vision_cleaned)}文字")
            logger.info(f"  ├─ 削減率: {reduction_rate:.1f}%")
            logger.info(f"  ├─ JSON形式: {'有効' if is_valid_json else '無効'}")
            logger.info(f"  └─ 処理時間: {f8_elapsed:.2f}秒")

            # ============================================
            # [F-9] 全結果マージ
            # ============================================
            if progress_callback:
                progress_callback("F-9")
            f9_start = time.time()
            logger.info("[F-9] 全結果マージ開始...")

            # PaddleOCR表とVision結果をマージ
            paddle_table_count = len(paddle_tables)
            gemini_table_count = 0
            merged_table_count = 0
            duplicates = 0

            if paddle_tables:
                vision_cleaned = self._merge_paddle_and_vision(vision_cleaned, paddle_tables)

                # 統計取得
                try:
                    vision_data = json.loads(vision_cleaned)
                    merged_tables = vision_data.get('layout_info', {}).get('tables', [])
                    merged_table_count = len(merged_tables)
                    gemini_table_count = merged_table_count - paddle_table_count
                    duplicates = paddle_table_count + gemini_table_count - merged_table_count
                except:
                    pass

            # full_textの統合
            stage_e_len = len(extracted_text)
            surya_len = len(surya_full_text)
            gemini_len = 0
            total_len = 0
            merged_full_text = ""

            try:
                vision_data = json.loads(vision_cleaned)
                gemini_full_text = vision_data.get('full_text', '')
                gemini_len = len(gemini_full_text)

                # 最適なfull_textを選択してマージ
                if surya_full_text and gemini_full_text:
                    # Geminiのテキストに Surya のテキストが含まれていれば、Geminiを使用
                    if surya_full_text in gemini_full_text:
                        merged_full_text = gemini_full_text
                        total_len = gemini_len
                    else:
                        # Suryaのテキストを優先（より詳細な可能性）
                        merged_full_text = surya_full_text
                        total_len = surya_len
                elif surya_full_text:
                    merged_full_text = surya_full_text
                    total_len = surya_len
                elif gemini_full_text:
                    merged_full_text = gemini_full_text
                    total_len = gemini_len
                elif extracted_text:
                    # Stage Eのテキストをフォールバックとして使用
                    merged_full_text = extracted_text
                    total_len = stage_e_len

                # vision_dataのfull_textを更新
                vision_data['full_text'] = merged_full_text
                vision_cleaned = json.dumps(vision_data, ensure_ascii=False)

            except Exception as e:
                logger.warning(f"[F-9] full_textマージ失敗: {e}")
                total_len = len(vision_cleaned)

            f9_elapsed = time.time() - f9_start
            logger.info(f"[F-9完了] 全結果マージ:")
            logger.info(f"  ├─ PaddleOCR表: {paddle_table_count}個")
            logger.info(f"  ├─ Gemini表: {gemini_table_count}個")
            logger.info(f"  ├─ マージ後表数: {merged_table_count}個 (重複{duplicates}個削除)")
            logger.info(f"  ├─ full_text結合: Stage E({stage_e_len}) + Surya({surya_len}) + Gemini({gemini_len}) = {total_len}文字")
            logger.info(f"  └─ 処理時間: {f9_elapsed:.2f}秒")

            # ============================================
            # [F-10] 最終検証・出力
            # ============================================
            if progress_callback:
                progress_callback("F-10")
            f10_start = time.time()
            logger.info("[F-10] 最終検証開始...")

            # 最終データ検証
            full_text_chars = 0
            sections_count = 0
            tables_count = 0
            layout_info_size = 0
            images_count = 0
            charts_count = 0
            visual_elements_size = 0
            total_json_size = len(vision_cleaned)

            try:
                vision_json = json.loads(vision_cleaned)
                full_text = vision_json.get('full_text', '')
                layout_info = vision_json.get('layout_info', {})
                visual_elements = vision_json.get('visual_elements', {})

                full_text_chars = len(full_text)
                sections_count = len(layout_info.get('sections', []))
                tables_count = len(layout_info.get('tables', []))
                layout_info_size = len(json.dumps(layout_info, ensure_ascii=False))

                images_count = len(visual_elements.get('images', []))
                charts_count = len(visual_elements.get('charts', []))
                visual_elements_size = len(json.dumps(visual_elements, ensure_ascii=False))

            except Exception as e:
                logger.warning(f"[F-10] JSON解析失敗: {e}")
                logger.debug(f"[F-10] クリーニング後（最初の1000文字）: {vision_cleaned[:1000]}")
                logger.debug(f"[F-10] クリーニング後（最後の500文字）: {vision_cleaned[-500:]}")

            f10_elapsed = time.time() - f10_start
            total_elapsed = time.time() - total_start_time

            logger.info(f"[F-10完了] 最終検証・出力:")
            logger.info(f"  ├─ full_text: {full_text_chars}文字")
            logger.info(f"  ├─ layout_info:")
            logger.info(f"  │   ├─ sections: {sections_count}個")
            logger.info(f"  │   ├─ tables: {tables_count}個")
            logger.info(f"  │   └─ 合計: {layout_info_size}文字 (JSON)")
            logger.info(f"  ├─ visual_elements:")
            logger.info(f"  │   ├─ images: {images_count}個")
            logger.info(f"  │   ├─ charts: {charts_count}個")
            logger.info(f"  │   └─ 合計: {visual_elements_size}文字 (JSON)")
            logger.info(f"  ├─ 最終JSONサイズ: {total_json_size}文字")
            logger.info(f"  ├─ F-10処理時間: {f10_elapsed:.2f}秒")
            logger.info(f"  └─ 総処理時間: {total_elapsed:.2f}秒")

            # ============================================
            # [Stage F 完了] 総括
            # ============================================
            logger.info("=" * 60)
            logger.info("[Stage F完了] ハイブリッドOCR処理完了")
            logger.info(f"  ├─ 入力: {file_path.name}")
            logger.info(f"  ├─ 出力: 3種類のデータ (full_text + layout_info + visual_elements)")
            logger.info(f"  ├─ 総文字数: {full_text_chars}文字")
            logger.info(f"  └─ 総処理時間: {total_elapsed:.2f}秒")
            logger.info("=" * 60)

            return vision_cleaned

        except Exception as e:
            logger.error(f"[Stage F エラー] Vision処理失敗: {e}", exc_info=True)
            return ""

    def _initialize_hybrid_ocr_engines(self):
        """ハイブリッドOCRエンジン（Surya + PaddleOCR）を初期化"""
        if not SURYA_AVAILABLE or not PADDLEOCR_AVAILABLE:
            logger.error("[Hybrid OCR] Required packages not installed")
            self.enable_hybrid_ocr = False
            return

        try:
            logger.info("[Hybrid OCR] Initializing Surya Foundation Model...")
            self.surya_foundation = FoundationPredictor()

            logger.info("[Hybrid OCR] Initializing Surya Detection...")
            self.surya_detector = DetectionPredictor()

            logger.info("[Hybrid OCR] Initializing Surya Layout...")
            self.surya_layout = LayoutPredictor(self.surya_foundation)

            logger.info("[Hybrid OCR] Initializing PaddleOCR (lang=japan) for text recognition...")
            self.paddle_ocr = PaddleOCR(lang='japan', use_textline_orientation=True)

            # PPStructureはキャッシュせず、都度初期化する（メモリリーク対策）
            self.paddle_structure = None
            logger.info("[Hybrid OCR] PPStructure: 都度初期化モード（メモリリーク対策）")

            logger.info("[Hybrid OCR] All engines initialized successfully!")
            
            # PaddleOCRバージョン検出
            paddle_version = PaddleOCRVersionAdapter.detect_version()


        except Exception as e:
            logger.error(f"[Hybrid OCR] Initialization failed: {e}")
            self.enable_hybrid_ocr = False

    def _extract_tables_with_paddleocr(self, file_path: Path) -> List[Dict[str, Any]]:
        """
        PaddleOCR で表構造を抽出（毎回初期化・削除でメモリリーク対策）

        Args:
            file_path: 画像/PDFファイルパス

        Returns:
            抽出された表のリスト [{"rows": [[]], "caption": ""}]
        """
        import gc
        import psutil
        import os
        import shutil
        import tempfile
        from PIL import Image

        # 毎回新しいPPStructureインスタンスを作成
        paddle_structure = None
        try:
            # ====== 診断情報収集 ======
            logger.info("=" * 60)
            logger.info("[F-1 診断] 詳細診断開始")

            # 1. ファイル情報
            file_exists = file_path.exists()
            file_size = file_path.stat().st_size if file_exists else 0
            file_name = file_path.name
            file_str = str(file_path)
            has_japanese = any(ord(c) > 127 for c in file_name)

            logger.info(f"[F-1 診断] ファイル情報:")
            logger.info(f"  ├─ パス: {file_str}")
            logger.info(f"  ├─ ファイル名: {file_name}")
            logger.info(f"  ├─ 存在: {file_exists}")
            logger.info(f"  ├─ サイズ: {file_size:,} bytes")
            logger.info(f"  ├─ 日本語文字含む: {has_japanese}")
            logger.info(f"  └─ 拡張子: {file_path.suffix}")

            # 2. 画像情報
            try:
                img = Image.open(file_path)
                img_size = img.size
                img_mode = img.mode
                img_format = img.format
                logger.info(f"[F-1 診断] 画像情報:")
                logger.info(f"  ├─ サイズ: {img_size[0]}x{img_size[1]}px")
                logger.info(f"  ├─ モード: {img_mode}")
                logger.info(f"  └─ フォーマット: {img_format}")
                img.close()
            except Exception as e:
                logger.error(f"[F-1 診断] 画像読み込み失敗: {e}")

            # 3. システムリソース
            process = psutil.Process(os.getpid())
            mem_info = process.memory_info()
            cpu_percent = process.cpu_percent(interval=0.1)
            logger.info(f"[F-1 診断] システムリソース:")
            logger.info(f"  ├─ メモリ使用量: {mem_info.rss / 1024 / 1024:.1f} MB")
            logger.info(f"  └─ CPU使用率: {cpu_percent:.1f}%")

            logger.info("=" * 60)

            # ====== メモリ診断開始 ======
            import time
            process = psutil.Process(os.getpid())

            mem_start = process.memory_info().rss / 1024 / 1024
            logger.info(f"[F-1 メモリ] 処理開始時: {mem_start:.1f} MB")

            # ====== PPStructure初期化 ======
            logger.debug("[F-1] PPStructure初期化開始...")
            init_start = time.time()
            paddle_structure = PPStructure(lang='japan', device='cpu')
            init_time = time.time() - init_start

            mem_after_init = process.memory_info().rss / 1024 / 1024
            mem_init_increase = mem_after_init - mem_start
            logger.debug(f"[F-1] PPStructure初期化完了 ({init_time:.2f}秒)")
            logger.info(f"[F-1 メモリ] 初期化後: {mem_after_init:.1f} MB (+{mem_init_increase:.1f} MB)")

            # ====== 診断テスト: ASCII名でコピー ======
            logger.info("[F-1 診断] テスト1: ASCII名で一時ファイル作成")
            with tempfile.NamedTemporaryFile(suffix=file_path.suffix, delete=False) as tmp:
                tmp_path = tmp.name
                shutil.copy(file_path, tmp_path)
                logger.info(f"  └─ 一時ファイル: {tmp_path}")

            # ====== predict実行（タイムアウト監視） ======
            logger.info("[F-1 診断] predict実行開始（ASCII名で）...")
            predict_start = time.time()

            try:
                result = list(paddle_structure.predict(tmp_path))
                predict_time = time.time() - predict_start

                mem_after_predict = process.memory_info().rss / 1024 / 1024
                mem_predict_increase = mem_after_predict - mem_after_init

                logger.info(f"[F-1 診断] predict成功！ ({predict_time:.2f}秒)")
                logger.info(f"  └─ 結果件数: {len(result)}件")
                logger.info(f"[F-1 メモリ] predict後: {mem_after_predict:.1f} MB (+{mem_predict_increase:.1f} MB)")

                # 一時ファイル削除
                os.unlink(tmp_path)

            except Exception as predict_error:
                predict_time = time.time() - predict_start
                logger.error(f"[F-1 診断] predict失敗 ({predict_time:.2f}秒): {predict_error}")
                os.unlink(tmp_path)
                raise

            tables = []
            for page_result in result:
                # 新しいAPI: page_result['table_res_list']から表を抽出
                table_list = page_result.get('table_res_list', []) if isinstance(page_result, dict) else getattr(page_result, 'table_res_list', [])
                for table_res in table_list:
                    # table_resから'html'属性を取得
                    html_content = table_res.get('html', '') if isinstance(table_res, dict) else getattr(table_res, 'html', '')
                    if html_content:
                        table_data = self._parse_html_table(html_content)
                        if table_data:
                            tables.append({
                                'rows': table_data,
                                'caption': f"PaddleOCR抽出表{len(tables) + 1}"
                            })

            return tables

        except Exception as e:
            logger.warning(f"[F-1] PaddleOCR 表抽出失敗: {e}")
            return []

        finally:
            # 必ずリソースを解放（メモリリーク対策）
            if paddle_structure is not None:
                logger.debug("[F-1] PPStructure リソース解放開始...")

                mem_before_cleanup = process.memory_info().rss / 1024 / 1024

                del paddle_structure
                gc.collect()  # ガベージコレクション強制実行

                mem_after_cleanup = process.memory_info().rss / 1024 / 1024
                mem_freed = mem_before_cleanup - mem_after_cleanup
                mem_leaked = mem_after_cleanup - mem_start

                logger.debug("[F-1] PPStructure リソース解放完了")
                logger.info(f"[F-1 メモリ] cleanup後: {mem_after_cleanup:.1f} MB (解放: {mem_freed:.1f} MB)")
                logger.info(f"[F-1 メモリ] 最終リーク量: {mem_leaked:.1f} MB")
                logger.info("=" * 60)

    def _parse_html_table(self, html: str) -> List[List[str]]:
        """
        HTML形式の表を行列に変換

        Args:
            html: HTML形式の表

        Returns:
            行列データ [[cell, cell], [cell, cell]]
        """
        try:
            from bs4 import BeautifulSoup

            soup = BeautifulSoup(html, 'html.parser')
            table = soup.find('table')
            if not table:
                return []

            rows = []
            for tr in table.find_all('tr'):
                cells = []
                for td in tr.find_all(['td', 'th']):
                    cells.append(td.get_text(strip=True))
                if cells:
                    rows.append(cells)

            return rows

        except Exception as e:
            logger.warning(f"HTML表パース失敗: {e}")
            return []

    def _combine_hybrid_results(self, regions: List[Dict[str, Any]]) -> str:
        """
        ハイブリッドOCRの結果を統合（読み順に並び替え）

        Args:
            regions: 領域ごとの認識結果

        Returns:
            読み順に並べられた全テキスト
        """
        if not regions:
            return ""

        # Y座標でソート（上から下）、同じY範囲ならX座標でソート（左から右）
        sorted_regions = sorted(regions, key=lambda r: (r['bbox'][1], r['bbox'][0]))

        # テキストを結合
        texts = [r['text'] for r in sorted_regions if r['text'].strip()]
        return "\n\n".join(texts)

    def _merge_paddle_and_vision(
        self,
        vision_json_str: str,
        paddle_tables: List[Dict[str, Any]]
    ) -> str:
        """
        PaddleOCR の表と Vision API の結果をマージ

        Args:
            vision_json_str: Vision API の結果（JSON文字列）
            paddle_tables: PaddleOCR が抽出した表のリスト

        Returns:
            マージされたJSON文字列
        """
        try:
            vision_data = json.loads(vision_json_str)

            layout_info = vision_data.get('layout_info', {})
            vision_tables = layout_info.get('tables', [])

            # PaddleOCR の表を先頭に追加（高精度なので優先）
            merged_tables = paddle_tables + vision_tables

            # 重複削除（同じ内容の表を削除）
            unique_tables = []
            seen = set()
            for table in merged_tables:
                table_str = json.dumps(table.get('rows', []), sort_keys=True)
                if table_str not in seen:
                    seen.add(table_str)
                    unique_tables.append(table)

            layout_info['tables'] = unique_tables
            vision_data['layout_info'] = layout_info

            return json.dumps(vision_data, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.warning(f"[F-9] マージ失敗: {e}")
            return vision_json_str

    def _clean_json_response(self, response: str) -> str:
        """
        Gemini の応答からJSONを抽出してクリーニング

        Args:
            response: Gemini の生の応答

        Returns:
            クリーニングされたJSON文字列
        """
        import re

        # パターン1: ```json ... ``` で囲まれている場合
        json_match = re.search(r'```json\s*\n(.*?)\n```', response, re.DOTALL)
        if json_match:
            return json_match.group(1).strip()

        # パターン2: ``` ... ``` で囲まれている場合
        code_match = re.search(r'```\s*\n(.*?)\n```', response, re.DOTALL)
        if code_match:
            return code_match.group(1).strip()

        # パターン3: { ... } を探す（最初の{から最後の}まで）
        first_brace = response.find('{')
        last_brace = response.rfind('}')
        if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
            return response[first_brace:last_brace + 1].strip()

        # パターン4: そのまま返す
        return response.strip()
```

### shared\pipeline\stage_h_kakeibo.py

```py
"""
Stage H: Kakeibo Structuring (家計簿構造化)

家計簿レシート専用のStage H処理
- 税額按分計算
- 商品分類
- マスタデータとの紐付け
"""

from typing import Dict, Any, List
from loguru import logger
from datetime import date

from shared.common.database.client import DatabaseClient


class StageHKakeibo:
    """家計簿専用のStage H（税額按分・分類）"""

    def __init__(self, db_client: DatabaseClient):
        """
        Args:
            db_client: データベースクライアント
        """
        self.db = db_client

        # マスタデータをロード
        self.aliases = self._load_aliases()
        self.product_dict = self._load_product_dictionary()
        self.situations = self._load_situations()
        self.categories = self._load_categories()

    def process(
        self,
        stage_g_output: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Stage Gの出力から最終データを生成

        Args:
            stage_g_output: Stage Gの構造化データ

        Returns:
            Dict: 最終データ（DB保存可能な形式）
        """
        try:
            # 【重要】レシート記載の金額は改ざんしない
            # 割引行は別行としてそのまま保持（マイナス金額）
            items = stage_g_output.get("items", [])

            # 割引を商品にリンク（税込価計算用）
            items = self._link_discounts_to_items(items)

            # 1. 商品を正規化（マスタとの紐付け）
            normalized_items = []
            for item in items:
                # 値引き行も含めて処理（金額はそのまま）
                if item.get("line_type") == "DISCOUNT":
                    normalized_items.append({
                        "raw_item": item,
                        "normalized": {
                            "product_name": item.get("product_name", "値引"),
                            "category_id": None,
                            "tax_rate": self._get_discount_tax_rate(item, items),
                            "tax_rate_source": "discount",
                            "is_discount": True
                        }
                    })
                    continue

                normalized = self._normalize_item(
                    item,
                    stage_g_output["shop_info"]["name"],
                    stage_g_output.get("amounts", {})
                )
                normalized_items.append({
                    "raw_item": item,
                    "normalized": normalized
                })

            # 2. シチュエーション判定
            trans_date = date.fromisoformat(stage_g_output["transaction_info"]["date"])
            situation_id = self._determine_situation(trans_date)

            # 3. 税額を按分計算
            items_with_tax, tax_subtotals = self._calculate_and_distribute_tax(
                normalized_items,
                stage_g_output.get("amounts", {})
            )

            # 4. 最終データを構築
            result = {
                "receipt": {
                    **stage_g_output["shop_info"],
                    **stage_g_output["transaction_info"],
                    **stage_g_output.get("amounts", {}),
                    **tax_subtotals,  # 税対象額を追加
                    "situation_id": situation_id
                },
                "items": items_with_tax,
                "payment": stage_g_output.get("payment", {}),
                "other_info": stage_g_output.get("other_info", {})
            }

            logger.info(f"Stage H completed: {len(items_with_tax)} items processed")
            return result

        except Exception as e:
            logger.error(f"Stage H failed: {e}")
            raise

    def _get_discount_tax_rate(self, discount_item: Dict, all_items: List[Dict]) -> int:
        """
        割引行の税率を判定（適用先商品から推定）

        Args:
            discount_item: 割引行データ
            all_items: 全明細行リスト

        Returns:
            int: 税率（8 or 10）
        """
        # 行番号でインデックスを作成
        items_by_line = {item.get("line_number"): item for item in all_items}

        # 明示的に値引き適用先が指定されている場合
        applied_to_line = discount_item.get("discount_applied_to")
        if applied_to_line and applied_to_line in items_by_line:
            target = items_by_line[applied_to_line]
            tax_mark = target.get("tax_mark")
            if tax_mark and (
                tax_mark in ["*", "※", "◆"] or
                "8%" in str(tax_mark) or
                "8" in str(tax_mark)
            ):
                return 8
            return 10

        # 直前の商品から推定
        discount_line_num = discount_item.get("line_number")
        if discount_line_num:
            for i in range(discount_line_num - 1, 0, -1):
                if i in items_by_line and items_by_line[i].get("line_type") != "DISCOUNT":
                    target = items_by_line[i]
                    tax_mark = target.get("tax_mark")
                    if tax_mark and (
                        tax_mark in ["*", "※", "◆"] or
                        "8%" in str(tax_mark) or
                        "8" in str(tax_mark)
                    ):
                        return 8
                    return 10

        # デフォルト10%
        return 10

    def _link_discounts_to_items(self, items: List[Dict]) -> List[Dict]:
        """
        割引行を商品にリンク（税込価計算用）

        各商品に linked_discount フィールドを追加
        割引の適用先が明示されていない場合は直前の商品に適用

        Args:
            items: Stage Gで抽出された全明細行

        Returns:
            List[Dict]: リンク情報が追加された明細行リスト
        """
        # 行番号でインデックスを作成
        items_by_line = {item.get("line_number"): item for item in items}

        # 各商品のlinked_discountを初期化
        for item in items:
            if item.get("line_type") != "DISCOUNT":
                item["linked_discount"] = 0

        # 割引を適用先にリンク
        for item in items:
            if item.get("line_type") != "DISCOUNT":
                continue

            discount_amount = item.get("amount", 0)  # 負の値
            applied_to_line = item.get("discount_applied_to")

            target = None
            if applied_to_line and applied_to_line in items_by_line:
                # 明示的に適用先が指定されている場合
                target = items_by_line[applied_to_line]
            else:
                # 直前の商品を探す
                discount_line_num = item.get("line_number")
                if discount_line_num:
                    for i in range(discount_line_num - 1, 0, -1):
                        if i in items_by_line and items_by_line[i].get("line_type") != "DISCOUNT":
                            target = items_by_line[i]
                            break

            if target and target.get("line_type") != "DISCOUNT":
                target["linked_discount"] = target.get("linked_discount", 0) + discount_amount
                logger.info(f"Linked discount {discount_amount}円 to {target.get('product_name')}")

        return items

    def _normalize_item(self, item: Dict, shop_name: str, amounts: Dict = None) -> Dict:
        """
        商品名を正規化し、カテゴリ・税率を判定

        Args:
            item: 商品データ（Stage Gの出力）
            shop_name: 店舗名
            amounts: レシート全体の金額情報（税率判定に使用）

        Returns:
            Dict: {"product_name": "正規化後", "category_id": "...", "tax_rate": 10}
        """
        # 商品名を取得（空文字列もNoneとして扱う）
        product_name = item.get("product_name") or item.get("line_text") or item.get("ocr_raw_text") or "不明"
        # 空文字列の場合は「不明」に
        if not product_name or not product_name.strip():
            product_name = "不明"

        receipt_tax_mark = item.get("tax_mark")  # レシートの税率マーク

        # レシート全体の税率情報を確認（最優先）
        receipt_level_tax_rate = None
        if amounts:
            tax_8_amount = amounts.get("tax_8_amount") or 0
            tax_10_amount = amounts.get("tax_10_amount") or 0

            # 8%のみの場合
            if tax_8_amount > 0 and tax_10_amount == 0:
                receipt_level_tax_rate = 8
                logger.debug(f"Receipt has only 8% tax, setting all items to 8%")
            # 10%のみの場合
            elif tax_10_amount > 0 and tax_8_amount == 0:
                receipt_level_tax_rate = 10
                logger.debug(f"Receipt has only 10% tax, setting all items to 10%")
            # 混在の場合は個別判定に進む

        # レシート全体の税率が判定できた場合はそれを使用（最優先）
        if receipt_level_tax_rate is not None:
            return {
                "product_name": product_name,
                "category_id": None,
                "tax_rate": receipt_level_tax_rate,
                "tax_rate_source": "receipt_level",
                "tax_amount": None
            }

        # 1. エイリアス変換
        product_name = self.aliases.get(product_name.lower(), product_name)

        # 2. 商品辞書マッチング
        for entry in self.product_dict:
            if entry["raw_keyword"].lower() in product_name.lower():
                return {
                    "product_name": entry["official_name"],
                    "category_id": entry["category_id"],
                    "tax_rate": entry["tax_rate"],
                    "tax_rate_source": "master",  # マスタから取得
                    "tax_amount": None  # 後で計算
                }

        # 3. 商品名から税率パターンを検出（「外8」「内8」などのレシート記載パターン）
        if "外8" in product_name or "内8" in product_name or "外 8" in product_name or "内 8" in product_name:
            tax_rate = 8
            tax_rate_source = "product_name_pattern"
            # 商品名から税率パターンを削除
            product_name = product_name.replace("外8", "").replace("内8", "").replace("外 8", "").replace("内 8", "").strip()
            # 空文字列になった場合は「不明」に
            if not product_name:
                product_name = "不明"
        elif "外10" in product_name or "内10" in product_name or "外 10" in product_name or "内 10" in product_name:
            tax_rate = 10
            tax_rate_source = "product_name_pattern"
            # 商品名から税率パターンを削除
            product_name = product_name.replace("外10", "").replace("内10", "").replace("外 10", "").replace("内 10", "").strip()
            # 空文字列になった場合は「不明」に
            if not product_name:
                product_name = "不明"
        # 4. レシートのマークから税率を判定
        # 8%マークの判定（複数パターン対応）
        elif receipt_tax_mark and (
            receipt_tax_mark in ["*", "※", "◆"] or  # よくある軽減税率マーク
            "8%" in str(receipt_tax_mark) or
            "8" in str(receipt_tax_mark) or
            "(軽)" in str(receipt_tax_mark) or
            "外8" in str(receipt_tax_mark) or  # 外税8%のパターン
            "内8" in str(receipt_tax_mark)  # 内税8%のパターン
        ):
            tax_rate = 8
            tax_rate_source = "receipt_mark"
        # 10%マークの判定
        elif receipt_tax_mark and (
            receipt_tax_mark in ["★", "☆"] or  # よくある標準税率マーク
            "10%" in str(receipt_tax_mark) or
            "10" in str(receipt_tax_mark) or
            "外10" in str(receipt_tax_mark) or  # 外税10%のパターン
            "内10" in str(receipt_tax_mark)  # 内税10%のパターン
        ):
            tax_rate = 10
            tax_rate_source = "receipt_mark"
        else:
            # デフォルト10%（あとで要レビュー）
            tax_rate = 10
            tax_rate_source = "default"

        return {
            "product_name": product_name,
            "category_id": None,
            "tax_rate": tax_rate,
            "tax_rate_source": tax_rate_source,
            "tax_amount": None
        }

    def _determine_situation(self, trans_date: date) -> str:
        """
        取引日からシチュエーションを判定

        Args:
            trans_date: 取引日

        Returns:
            str: シチュエーションID
        """
        weekday = trans_date.weekday()  # 0=月曜, 6=日曜

        # 土日
        if weekday >= 5:
            for s in self.situations:
                if s["name"] == "週末":
                    return s["id"]

        # 平日
        for s in self.situations:
            if s["name"] == "平日":
                return s["id"]

        # デフォルト（最初のシチュエーション）
        return self.situations[0]["id"] if self.situations else None

    def _calculate_and_distribute_tax(
        self,
        normalized_items: List[Dict],
        amounts: Dict
    ) -> List[Dict]:
        """
        税額を按分計算（内税・外税対応）

        Args:
            normalized_items: 正規化済み商品リスト
            amounts: Stage Gで抽出した金額情報

        Returns:
            List[Dict]: 税額が計算された商品リスト
        """
        # 【重要】内税・外税の判定
        # Stage Gで判定済みの場合はそれを優先
        tax_type = amounts.get("tax_display_type")

        if tax_type:
            logger.info(f"Stage Gで判定済み: tax_display_type={tax_type}")
        else:
            # フォールバック: 小計と合計の比較で判定
            subtotal = amounts.get("subtotal")
            total = amounts.get("total")

            if subtotal is not None and total is not None and subtotal < total:
                tax_type = "excluded"  # 外税
                logger.info(f"外税レシート検出: 小計={subtotal}円 < 合計={total}円")
            else:
                tax_type = "included"  # 内税
                logger.info(f"内税レシート検出: 小計={subtotal}円 = 合計={total}円")

        # 商品を8%と10%にグループ化
        items_8 = []
        items_10 = []

        for item_data in normalized_items:
            if item_data["normalized"]["tax_rate"] == 8:
                items_8.append(item_data)
            else:
                items_10.append(item_data)

        # レシート記載の税額を使用（優先）
        tax_8_amount = amounts.get("tax_8_amount") or 0
        tax_10_amount = amounts.get("tax_10_amount") or 0

        # レシート記載がない場合のみ逆算（通常は記載されている）
        if tax_8_amount == 0:
            total_8 = sum(item["raw_item"]["amount"] or 0 for item in items_8)
            if total_8 > 0:
                if tax_type == "excluded":
                    tax_8_amount = round(total_8 * 8 / 100)  # 外税：本体価格×税率
                else:
                    tax_8_amount = round(total_8 * 8 / 108)  # 内税：税込額から逆算
                logger.warning(f"8% tax not in receipt, calculated: {tax_8_amount}円 (type={tax_type})")

        if tax_10_amount == 0:
            total_10 = sum(item["raw_item"]["amount"] or 0 for item in items_10)
            if total_10 > 0:
                if tax_type == "excluded":
                    tax_10_amount = round(total_10 * 10 / 100)  # 外税：本体価格×税率
                else:
                    tax_10_amount = round(total_10 * 10 / 110)  # 内税：税込額から逆算
                logger.warning(f"10% tax not in receipt, calculated: {tax_10_amount}円 (type={tax_type})")

        # 各商品に税額を按分（内外タイプを渡す）
        self._distribute_tax_to_items(items_8, tax_8_amount, tax_type)
        self._distribute_tax_to_items(items_10, tax_10_amount, tax_type)

        # 税対象額を計算（税抜額）
        total_8 = sum(item["raw_item"]["amount"] or 0 for item in items_8)
        total_10 = sum(item["raw_item"]["amount"] or 0 for item in items_10)

        # 税対象額を返す（内税の場合は税抜額、外税の場合も税抜額）
        if tax_type == "included":
            # 内税：税込額から税額を引いて税抜額を計算
            tax_8_subtotal = total_8 - tax_8_amount if total_8 > 0 else 0
            tax_10_subtotal = total_10 - tax_10_amount if total_10 > 0 else 0
        else:
            # 外税：表示額がそのまま税抜額
            tax_8_subtotal = total_8
            tax_10_subtotal = total_10

        tax_subtotals = {
            "tax_8_subtotal": tax_8_subtotal,
            "tax_10_subtotal": tax_10_subtotal
        }

        return normalized_items, tax_subtotals

    def _distribute_tax_to_items(self, items: List[Dict], total_tax: int, tax_type: str):
        """
        商品データの7要素を設定
        1. 数量
        2. 表示額
        3. 外or内
        4. 税率
        5. 本体価
        6. 税額
        7. 税込価

        Args:
            items: 商品リスト
            total_tax: グループ全体の税額
            tax_type: "included"（内税）or "excluded"（外税）
        """
        if not items:
            return

        # 金額0円の商品（セット内訳行など）を除外
        items_with_amount = [item for item in items if (item["raw_item"].get("amount") or 0) != 0]
        items_zero_amount = [item for item in items if (item["raw_item"].get("amount") or 0) == 0]

        # 金額0円の商品には税額0を設定
        for item in items_zero_amount:
            quantity = item["raw_item"].get("quantity", 1)
            displayed_amount = 0
            item["normalized"]["quantity"] = quantity
            item["normalized"]["displayed_amount"] = displayed_amount
            item["normalized"]["tax_display_type"] = tax_type
            item["normalized"]["base_price"] = 0
            item["normalized"]["tax_amount"] = 0
            item["normalized"]["tax_included_amount"] = 0
            logger.debug(f"Zero-amount item excluded from tax distribution: {item['raw_item'].get('product_name')}")

        # 金額がある商品のみで税額按分を行う
        if not items_with_amount:
            return

        if total_tax == 0:
            # 税額が0の場合も7要素を設定（割引は考慮）
            for item in items_with_amount:
                quantity = item["raw_item"].get("quantity", 1)
                displayed_amount = item["raw_item"].get("amount") or 0
                linked_discount = item["raw_item"].get("linked_discount", 0)

                # 税込価を計算（表示額 + 割引）
                tax_included_amount = displayed_amount + linked_discount

                # 7要素を設定
                item["normalized"]["quantity"] = quantity  # 1. 数量
                item["normalized"]["displayed_amount"] = displayed_amount  # 2. 表示額
                item["normalized"]["tax_display_type"] = tax_type  # 3. 外or内
                # 4. 税率 は _normalize_item で既に設定済み
                item["normalized"]["tax_amount"] = 0  # 6. 税額

                if tax_type == "excluded":
                    # 外税：表示額 = 本体価
                    item["normalized"]["base_price"] = displayed_amount + linked_discount  # 5. 本体価
                    item["normalized"]["tax_included_amount"] = displayed_amount + linked_discount  # 7. 税込価
                else:
                    # 内税：表示額 = 税込額
                    item["normalized"]["tax_included_amount"] = tax_included_amount  # 7. 税込価
                    item["normalized"]["base_price"] = tax_included_amount  # 5. 本体価
            return

        # Step 1: 各商品の税込価を計算（金額がある商品のみ）
        tax_included_amounts = []
        for item in items_with_amount:
            displayed_amount = item["raw_item"].get("amount") or 0
            linked_discount = item["raw_item"].get("linked_discount", 0)
            tax_included_amount = displayed_amount + linked_discount
            tax_included_amounts.append(tax_included_amount)

        # Step 2: 各商品の理論税額を計算（小数のまま保持）
        theoretical_taxes_float = []
        for i, item in enumerate(items_with_amount):
            tax_included_amount = tax_included_amounts[i]
            tax_rate = item["normalized"].get("tax_rate", 10)
            line_type = item["raw_item"].get("line_type", "ITEM")

            # 割引行は税額0（商品行にすでに割引後の税額が含まれているため）
            if line_type == "DISCOUNT":
                theoretical_tax = 0.0
            elif tax_type == "excluded":
                # 外税：理論税額 = 税抜額 × 税率 / 100
                theoretical_tax = tax_included_amount * tax_rate / 100
            else:
                # 内税：理論税額 = 税込価 - (税込価 / (1 + 税率/100))
                theoretical_tax = tax_included_amount - (tax_included_amount / (1 + tax_rate / 100))

            theoretical_taxes_float.append(theoretical_tax)

        # Step 3: 理論税額の合計（小数）とレシート記載税額の差分
        total_theoretical_tax = sum(theoretical_taxes_float)
        remainder = total_tax - total_theoretical_tax

        # Step 4: 各商品の理論税額を四捨五入し、端数を按分
        theoretical_taxes_rounded = [round(tax) for tax in theoretical_taxes_float]
        total_rounded = sum(theoretical_taxes_rounded)
        final_remainder = total_tax - total_rounded

        # Step 5: 最終端数を税込価の大きい順に1円ずつ配分
        distributed_tax = theoretical_taxes_rounded.copy()

        if final_remainder != 0:
            # 税込価の絶対値でソート（インデックスを保持）
            indexed_amounts = [(i, abs(tax_included_amounts[i])) for i in range(len(items_with_amount))]
            indexed_amounts.sort(key=lambda x: x[1], reverse=True)

            # 端数を1円ずつ配分
            for j in range(abs(final_remainder)):
                idx = indexed_amounts[j % len(items_with_amount)][0]
                if final_remainder > 0:
                    distributed_tax[idx] += 1
                else:
                    distributed_tax[idx] -= 1

        # 各商品に7要素を設定（金額がある商品のみ）
        for i, item in enumerate(items_with_amount):
            quantity = item["raw_item"].get("quantity", 1)
            displayed_amount = item["raw_item"].get("amount") or 0
            linked_discount = item["raw_item"].get("linked_discount", 0)  # リンクされた割引（負の値）

            # 税込価を計算（表示額 + 割引）
            # 割引は負の値なので加算すると減算になる
            tax_included_amount = displayed_amount + linked_discount

            # 按分された税額を使用
            tax_amount = distributed_tax[i]

            # 税率から本体価を計算
            tax_rate = item["normalized"].get("tax_rate", 10)
            if tax_type == "excluded":
                # 外税：表示額 = 本体価、税込価 = 本体価 + 税額
                base_price = displayed_amount + linked_discount
                tax_included_amount = base_price + tax_amount
            else:
                # 内税：税込価 - 按分税額 = 本体価
                base_price = tax_included_amount - tax_amount

            # 7要素を設定
            item["normalized"]["quantity"] = quantity  # 1. 数量
            item["normalized"]["displayed_amount"] = displayed_amount  # 2. 表示額
            item["normalized"]["tax_display_type"] = tax_type  # 3. 外or内
            # 4. 税率 は _normalize_item で既に設定済み
            item["normalized"]["base_price"] = base_price  # 5. 本体価
            item["normalized"]["tax_amount"] = tax_amount  # 6. 税額
            item["normalized"]["tax_included_amount"] = tax_included_amount  # 7. 税込価

            if linked_discount != 0:
                logger.info(f"{item['raw_item'].get('product_name')}: 表示額={displayed_amount}, 割引={linked_discount}, 税込価={tax_included_amount}, 本体価={base_price}, 税額={tax_amount}")

        logger.debug(f"Distributed tax ({tax_type})")

    # ========================================
    # マスタデータ読み込み
    # ========================================

    def _load_aliases(self) -> Dict[str, str]:
        """エイリアステーブルを読み込み"""
        result = self.db.client.table("MASTER_Rules_transaction_dict").select("*").execute()
        # product_name → official_name のマッピング
        aliases = {}
        for row in result.data:
            if row.get("product_name") and row.get("official_name"):
                aliases[row["product_name"].lower()] = row["official_name"]
        return aliases

    def _load_product_dictionary(self) -> List[Dict]:
        """商品辞書を読み込み"""
        result = self.db.client.table("MASTER_Product_classify").select("*").execute()
        return result.data

    def _load_situations(self) -> List[Dict]:
        """シチュエーションマスタを読み込み（名目）"""
        result = self.db.client.table("MASTER_Categories_purpose").select("*").execute()
        return result.data

    def _load_categories(self) -> List[Dict]:
        """カテゴリマスタを読み込み（商品カテゴリ）"""
        result = self.db.client.table("MASTER_Categories_product").select("*").execute()
        return result.data
```

### shared\pipeline\stage_h_structuring.py

```py
"""
Stage H: Structuring (構造化)

非構造化テキストから、意味のあるデータ(JSON)を抽出
- 役割: 指定スキーマに基づくJSON抽出、表データの正規化
- モデル: 設定ファイルで指定（Gemini, OpenAI等のLLM）
- 重要: doc_typeを判定するのではなく、渡されたdoc_typeのスキーマを使ってデータを抽出

F_stage_c_extractor から完全移行
"""
import re
import json
import json_repair
from typing import Dict, Any, Optional
from pathlib import Path
from string import Template
from loguru import logger
from datetime import datetime

from shared.ai.llm_client.llm_client import LLMClient


class StageHStructuring:
    """Stage H: 構造化（設定ベース版）"""

    def __init__(self, llm_client: LLMClient):
        """
        Args:
            llm_client: LLMクライアント
        """
        self.llm = llm_client

    def process(
        self,
        file_name: str,
        doc_type: str,
        workspace: str,
        combined_text: str,
        prompt: str,
        model: str,
        stage_f_structure: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        構造化（設定ベース版）

        Args:
            file_name: ファイル名
            doc_type: ドキュメントタイプ
            workspace: ワークスペース
            combined_text: 統合テキスト（Stage E + G の結果）
            prompt: プロンプト（config/prompts/stage_h/*.md から読み込み）
            model: モデル名
            stage_f_structure: Stage F の構造化情報（tables, sections, visual_elements）

        Returns:
            {
                'document_date': str,
                'tags': List[str],
                'metadata': Dict[str, Any]
            }
        """
        logger.info(f"[Stage H] 構造化開始... (doc_type={doc_type}, model={model})")

        # Stage F の構造化情報がある場合はログ出力
        if stage_f_structure:
            logger.info(f"[Stage H] Stage F の構造化情報を受信: tables={len(stage_f_structure.get('tables', []))}, sections={len(stage_f_structure.get('sections', []))}")

        if not combined_text or not combined_text.strip():
            logger.warning("[Stage H] 入力テキストが空です")
            return self._get_fallback_result(doc_type)

        try:
            # プロンプト構築
            logger.info("[Stage H] プロンプト構築中...")
            full_prompt = self._build_prompt(
                prompt_template=prompt,
                file_name=file_name,
                doc_type=doc_type,
                workspace=workspace,
                combined_text=combined_text
            )
            logger.info(f"[Stage H] プロンプト構築完了 ({len(full_prompt)}文字)")

            # LLM呼び出し
            logger.info(f"[Stage H] LLM呼び出し中... (model={model})")
            response = self.llm.call_model(
                tier="default",
                prompt=full_prompt,
                model_name=model
            )
            logger.info(f"[Stage H] LLM応答受信: success={response.get('success')}")

            if not response.get("success"):
                logger.error(f"[Stage H エラー] LLM呼び出し失敗: {response.get('error')}")
                return self._get_fallback_result(doc_type)

            # JSON抽出（リトライ機能付き）
            content = response.get("content", "")
            logger.info(f"[Stage H] ===== LLMレスポンス全文 =====\n{content}\n[Stage H] ===== レスポンス終了 =====")
            result = self._extract_json_with_retry(content, model=model, max_retries=2)

            # Stage F の構造化情報をマージ
            if stage_f_structure:
                result = self._merge_stage_f_structure(result, stage_f_structure)

            # 結果の整形
            return {
                'document_date': result.get('document_date'),
                'tags': result.get('tags', []),
                'metadata': result.get('metadata', {})
            }

        except Exception as e:
            logger.error(f"[Stage H エラー] 構造化失敗: {e}", exc_info=True)
            return self._get_fallback_result(doc_type)

    def _build_prompt(
        self,
        prompt_template: str,
        file_name: str,
        doc_type: str,
        workspace: str,
        combined_text: str
    ) -> str:
        """
        プロンプトを構築

        Args:
            prompt_template: プロンプトテンプレート
            file_name: ファイル名
            doc_type: ドキュメントタイプ
            workspace: ワークスペース
            combined_text: 統合テキスト

        Returns:
            構築されたプロンプト
        """
        # string.Templateを使用してテンプレート変数を置換（JSONの{}と競合しない）
        template = Template(prompt_template)
        prompt = template.substitute(
            file_name=file_name,
            doc_type=doc_type,
            workspace=workspace,
            combined_text=combined_text,
            current_date=datetime.now().strftime("%Y-%m-%d")
        )

        return prompt

    def _extract_json_with_retry(
        self,
        content: str,
        model: str,
        max_retries: int = 2
    ) -> Dict[str, Any]:
        """
        JSON抽出（リトライ機能付き）

        Args:
            content: LLMの出力
            model: モデル名
            max_retries: 最大リトライ回数

        Returns:
            抽出されたJSON
        """
        for attempt in range(max_retries + 1):
            try:
                result = self._extract_json(content)
                logger.debug(f"[Stage H] JSON抽出成功 (試行{attempt + 1}/{max_retries + 1})")
                return result

            except Exception as e:
                if attempt < max_retries:
                    logger.warning(f"[Stage H] JSON抽出失敗 (試行{attempt + 1}/{max_retries + 1}): {e}")
                    # リトライ: LLMにJSON修正を依頼
                    content = self._retry_json_extraction(content, str(e), model)
                else:
                    logger.error(f"[Stage H] JSON抽出失敗（最終試行）: {e}")
                    raise

        # ここには到達しないはずだが、念のため
        return {}

    def _extract_json(self, content: str) -> Dict[str, Any]:
        """
        コンテンツからJSONを抽出

        Args:
            content: LLMの出力

        Returns:
            抽出されたJSON

        Raises:
            Exception: JSON抽出に失敗した場合
        """
        # 複数のパターンでJSONブロックを探す
        patterns = [
            r'```json\s*(.*?)```',  # ```json ... ``` (改行を柔軟に)
            r'```\s*(.*?)```',      # ``` ... ```
            r'\{[\s\S]*?\}',        # { ... } (非貪欲)
        ]

        json_str = None
        for pattern in patterns:
            match = re.search(pattern, content, re.DOTALL)
            if match:
                json_str = match.group(1) if match.lastindex else match.group(0)
                json_str = json_str.strip()
                # { で始まるか確認
                if json_str.startswith('{'):
                    break
                else:
                    json_str = None

        if not json_str:
            # パターンマッチしない場合、{ ... } を直接探す
            match = re.search(r'\{[\s\S]*\}', content, re.DOTALL)
            if match:
                json_str = match.group(0).strip()
            else:
                json_str = content.strip()

        # JSONパース
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            # json_repair で修復を試みる
            logger.error(f"[Stage H] JSON解析失敗: {e}")
            logger.error(f"[Stage H] 抽出されたJSON（最初の500文字）:\n{json_str[:500]}")
            try:
                return json_repair.loads(json_str)
            except Exception as repair_error:
                logger.error(f"[Stage H] JSON修復も失敗: {repair_error}")
                logger.error(f"[Stage H] 修復失敗したJSON全文:\n{json_str}")
                raise

    def _retry_json_extraction(
        self,
        failed_content: str,
        error_message: str,
        model: str
    ) -> str:
        """
        JSON抽出失敗時、LLMにJSON修正を依頼

        Args:
            failed_content: 失敗したコンテンツ
            error_message: エラーメッセージ
            model: モデル名

        Returns:
            修正されたJSON文字列
        """
        prompt = f"""以下のJSONにエラーがあります。修正してください。

エラー: {error_message}

元のJSON:
```
{failed_content}
```

修正されたJSONを ```json ブロックで出力してください。
"""

        try:
            response = self.llm.call_model(
                tier="default",
                prompt=prompt,
                model_name=model
            )

            if response.get("success"):
                return response.get("content", "")
            else:
                logger.error(f"[Stage H] JSON修正失敗: {response.get('error')}")
                return failed_content

        except Exception as e:
            logger.error(f"[Stage H] JSON修正エラー: {e}")
            return failed_content

    def _merge_stage_f_structure(self, result: Dict[str, Any], stage_f_structure: Dict[str, Any]) -> Dict[str, Any]:
        """
        Stage F の構造化情報を Stage H の結果にマージ

        Args:
            result: Stage H の抽出結果
            stage_f_structure: Stage F の構造化情報

        Returns:
            マージ済みの結果
        """
        metadata = result.get('metadata', {})

        # Stage F の tables を優先的に使用（Stage H で tables が抽出されていない場合、または少ない場合）
        stage_f_tables = stage_f_structure.get('tables', [])
        stage_h_tables = metadata.get('structured_tables', [])

        if stage_f_tables and len(stage_f_tables) > len(stage_h_tables):
            logger.info(f"[Stage H] Stage F の tables を優先使用: {len(stage_f_tables)}個（Stage H: {len(stage_h_tables)}個）")

            # Stage F の tables を Stage H の形式に変換
            converted_tables = []
            for i, table in enumerate(stage_f_tables):
                rows = table.get('rows', [])
                if rows:
                    # 最初の行をヘッダーとして扱う
                    headers = rows[0] if rows else []
                    data_rows = rows[1:] if len(rows) > 1 else []

                    # 辞書形式に変換
                    converted_rows = []
                    for row in data_rows:
                        row_dict = {}
                        for j, cell in enumerate(row):
                            header = headers[j] if j < len(headers) else f"列{j+1}"
                            row_dict[header] = cell
                        converted_rows.append(row_dict)

                    converted_tables.append({
                        'table_title': table.get('caption', f'表{i+1}'),
                        'table_type': 'ocr_extracted',
                        'headers': headers,
                        'rows': converted_rows
                    })

            metadata['structured_tables'] = converted_tables

        # Stage F の sections を text_blocks に統合（text_blocks が少ない場合）
        stage_f_sections = stage_f_structure.get('sections', [])
        stage_h_text_blocks = metadata.get('text_blocks', [])

        if stage_f_sections and len(stage_h_text_blocks) < len(stage_f_sections):
            logger.info(f"[Stage H] Stage F の sections を text_blocks に変換: {len(stage_f_sections)}個")

            converted_blocks = []
            current_heading = None
            current_content_parts = []

            for section in stage_f_sections:
                section_type = section.get('type', '')
                content = section.get('content', '')

                if section_type == 'heading':
                    # 前のブロックを保存
                    if current_heading and current_content_parts:
                        converted_blocks.append({
                            'title': current_heading,
                            'content': '\n'.join(current_content_parts)
                        })
                    # 新しいヘッディング開始
                    current_heading = content
                    current_content_parts = []
                else:
                    # paragraph, list など
                    if content:
                        # contentがlistの場合は改行で結合して文字列化
                        if isinstance(content, list):
                            current_content_parts.append('\n'.join(str(item) for item in content))
                        else:
                            current_content_parts.append(str(content))

            # 最後のブロックを保存
            if current_heading and current_content_parts:
                converted_blocks.append({
                    'title': current_heading,
                    'content': '\n'.join(current_content_parts)
                })

            # Stage H の text_blocks より多ければ置き換え
            if len(converted_blocks) > len(stage_h_text_blocks):
                metadata['text_blocks'] = converted_blocks

        # Stage F の visual_elements からデッドライン情報を抽出
        visual_elements = stage_f_structure.get('visual_elements', {})
        if visual_elements:
            deadline_info = visual_elements.get('deadline_info')
            if deadline_info and not result.get('document_date'):
                logger.info(f"[Stage H] Stage F の deadline_info を document_date として使用: {deadline_info}")
                result['document_date'] = deadline_info

            # 強調されたテキストをタグに追加
            emphasized_text = visual_elements.get('emphasized_text', [])
            if emphasized_text:
                existing_tags = result.get('tags', [])
                for text in emphasized_text:
                    if text and text not in existing_tags:
                        existing_tags.append(text)
                result['tags'] = existing_tags
                logger.info(f"[Stage H] Stage F の emphasized_text をタグに追加: {emphasized_text}")

        result['metadata'] = metadata
        return result

    def _get_fallback_result(self, doc_type: str) -> Dict[str, Any]:
        """
        フォールバック結果を返す

        Args:
            doc_type: ドキュメントタイプ

        Returns:
            最小限の結果
        """
        return {
            'document_date': None,
            'tags': [],
            'metadata': {
                'doc_type': doc_type,
                'extraction_failed': True
            }
        }
```

### shared\pipeline\stage_i_synthesis.py

```py
"""
Stage I: Synthesis (統合・要約)

抽出されたデータと元のテキストを統合し、人間と検索エンジンに最適な形に整形
- 役割: 全情報を統合し、要約・タグ生成・基準日付抽出
- モデル: 設定ファイルで指定（デフォルト: Gemini 2.5 Flash）

D_stage_a_classifier から完全移行
"""
import json
from typing import Dict, Any, Optional
from pathlib import Path
from string import Template
from loguru import logger

from shared.ai.llm_client.llm_client import LLMClient


class StageISynthesis:
    """Stage I: 統合・要約（設定ベース版）"""

    def __init__(self, llm_client: LLMClient):
        """
        Args:
            llm_client: LLMクライアント
        """
        self.llm = llm_client

    def process(
        self,
        combined_text: str,
        stageH_result: Dict[str, Any],
        prompt: str,
        model: str
    ) -> Dict[str, Any]:
        """
        統合・要約（設定ベース版）

        Args:
            combined_text: 統合テキスト
            stageH_result: Stage H の結果
            prompt: プロンプト（config/prompts/stage_i/*.md から読み込み）
            model: モデル名

        Returns:
            {
                'summary': str,
                'relevant_date': str,
                'tags': List[str]
            }
        """
        logger.info(f"[Stage I] 統合・要約開始... (model={model})")

        if not combined_text or not combined_text.strip():
            logger.warning("[Stage I] 入力テキストが空です")
            return self._get_fallback_result(stageH_result)

        try:
            # プロンプト構築
            full_prompt = self._build_prompt(
                prompt_template=prompt,
                combined_text=combined_text,
                stageH_result=stageH_result
            )

            # LLM呼び出し
            response = self.llm.call_model(
                tier="default",
                prompt=full_prompt,
                model_name=model
            )

            if not response.get("success"):
                logger.error(f"[Stage I エラー] LLM呼び出し失敗: {response.get('error')}")
                return self._get_fallback_result(stageH_result)

            # 結果をJSON形式で取得
            content = response.get("content", "")
            logger.info(f"[Stage I] ===== LLMレスポンス全文 ===== {content[:500]}")
            result = self._parse_result(content)

            # Stage H のタグとマージ
            stageH_tags = stageH_result.get('tags', [])
            stageI_tags = result.get('tags', [])
            merged_tags = list(set(stageH_tags + stageI_tags))  # 重複削除

            return {
                'title': result.get('title', ''),
                'summary': result.get('summary', ''),
                'relevant_date': result.get('relevant_date') or stageH_result.get('document_date'),
                'tags': merged_tags,
                'calendar_events': result.get('calendar_events', []),
                'tasks': result.get('tasks', [])
            }

        except Exception as e:
            logger.error(f"[Stage I エラー] 統合・要約失敗: {e}", exc_info=True)
            return self._get_fallback_result(stageH_result)

    def _build_prompt(
        self,
        prompt_template: str,
        combined_text: str,
        stageH_result: Dict[str, Any]
    ) -> str:
        """
        プロンプトを構築

        Args:
            prompt_template: プロンプトテンプレート
            combined_text: 統合テキスト
            stageH_result: Stage H の結果

        Returns:
            構築されたプロンプト
        """
        # Stage H の結果をJSON文字列化
        stageH_json = json.dumps(stageH_result, ensure_ascii=False, indent=2)

        # string.Templateを使用してテンプレート変数を置換（JSONの{}と競合しない）
        template = Template(prompt_template)
        prompt = template.substitute(
            combined_text=combined_text,
            stageH_result=stageH_json
        )

        return prompt

    def _parse_result(self, content: str) -> Dict[str, Any]:
        """
        LLM出力から結果を抽出

        Args:
            content: LLMの出力

        Returns:
            抽出された結果
        """
        # JSON形式で出力されている場合
        try:
            # ```json ブロックを探す
            import re
            json_match = re.search(r'```json\s*\n(.*?)\n```', content, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                result = json.loads(json_str)
                logger.info(f"[Stage I] JSON解析成功")
                return result

            # ```json ブロックがない場合、直接JSON文字列を探す
            # { で始まる部分を探す
            json_start = content.find('{')
            json_end = content.rfind('}')
            if json_start >= 0 and json_end > json_start:
                json_str = content[json_start:json_end + 1]
                result = json.loads(json_str)
                logger.info(f"[Stage I] JSON解析成功（直接抽出）")
                return result
        except json.JSONDecodeError as e:
            logger.warning(f"[Stage I] JSON解析失敗（フォールバックに移行）: {e}")
        except Exception as e:
            logger.warning(f"[Stage I] JSON抽出失敗（フォールバックに移行）: {e}")

        # JSON形式でない場合、テキストから抽出
        result = {
            'summary': '',
            'tags': [],
            'relevant_date': None
        }

        # 要約を抽出（最初の段落または全体）
        lines = content.split('\n')
        summary_lines = []
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#') and not line.startswith('-'):
                summary_lines.append(line)
                if len(summary_lines) >= 3:  # 最大3行
                    break

        result['summary'] = ' '.join(summary_lines) if summary_lines else content[:200]

        # タグを抽出（例: タグ: tag1, tag2, tag3）
        import re
        tags_match = re.search(r'タグ[:：]\s*(.+)', content, re.IGNORECASE)
        if tags_match:
            tags_str = tags_match.group(1)
            result['tags'] = [t.strip() for t in tags_str.split(',')]

        # 日付を抽出（YYYY-MM-DD形式）
        date_match = re.search(r'(\d{4}-\d{2}-\d{2})', content)
        if date_match:
            result['relevant_date'] = date_match.group(1)

        return result

    def _get_fallback_result(self, stageH_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        フォールバック結果を返す

        Args:
            stageH_result: Stage H の結果

        Returns:
            最小限の結果
        """
        return {
            'summary': '処理に失敗しました',
            'relevant_date': stageH_result.get('document_date'),
            'tags': stageH_result.get('tags', []),
            'calendar_events': [],
            'tasks': []
        }
```

### shared\pipeline\stage_j_chunking.py

```py
"""
Stage J: Chunking (チャンク化)

メタデータからチャンクを生成
- 役割: 検索用チャンクの作成
- 処理: MetadataChunker でメタデータチャンク生成
"""
from typing import Dict, Any, List
from loguru import logger

from shared.common.processing.metadata_chunker import MetadataChunker


class StageJChunking:
    """Stage J: チャンク化"""

    def __init__(self):
        """初期化"""
        self.chunker = MetadataChunker()

    def create_chunks(
        self,
        display_subject: str,
        summary: str,
        tags: List[str],
        document_date: str,
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        メタデータからチャンクを生成

        Args:
            display_subject: 件名/ファイル名
            summary: 要約
            tags: タグリスト
            document_date: ドキュメント日付
            metadata: 構造化メタデータ

        Returns:
            チャンクリスト [
                {
                    'chunk_text': str,
                    'chunk_type': str,
                    'search_weight': float
                },
                ...
            ]
        """
        logger.info("[Stage J] チャンク化開始...")

        try:
            # metadata から構造化データを展開
            document_data = {
                'file_name': display_subject,
                'summary': summary,
                'tags': tags,
                'document_date': document_date,
                # metadata の中身を直接展開
                'persons': metadata.get('persons', []) if isinstance(metadata, dict) else [],
                'organizations': metadata.get('organizations', []) if isinstance(metadata, dict) else [],
                'people': metadata.get('people', []) if isinstance(metadata, dict) else [],
                # Stage H の構造化データを追加
                'text_blocks': metadata.get('text_blocks', []) if isinstance(metadata, dict) else [],
                'structured_tables': metadata.get('structured_tables', []) if isinstance(metadata, dict) else [],
                'weekly_schedule': metadata.get('weekly_schedule', []) if isinstance(metadata, dict) else [],
                'other_text': metadata.get('other_text', []) if isinstance(metadata, dict) else [],
                # Stage I の抽出データを追加
                'calendar_events': metadata.get('calendar_events', []) if isinstance(metadata, dict) else [],
                'tasks': metadata.get('tasks', []) if isinstance(metadata, dict) else [],
                # basic_info も展開（あれば）
                'doc_type': metadata.get('basic_info', {}).get('related_class', '') if isinstance(metadata, dict) else ''
            }

            chunks = self.chunker.create_metadata_chunks(document_data)

            logger.info(f"[Stage J完了] チャンク数: {len(chunks)}")

            return chunks

        except Exception as e:
            logger.error(f"[Stage J エラー] チャンク化失敗: {e}", exc_info=True)
            return []

    def process(
        self,
        display_subject: str,
        summary: str,
        tags: List[str],
        document_date: Any,
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        チャンク生成（process() エイリアス）

        Args:
            display_subject: 表示件名
            summary: 要約
            tags: タグ
            document_date: ドキュメント日付
            metadata: メタデータ

        Returns:
            chunks: チャンクリスト
        """
        return self.create_chunks(
            display_subject=display_subject,
            summary=summary,
            tags=tags,
            document_date=document_date,
            metadata=metadata
        )
```

### shared\pipeline\stage_k_embedding.py

```py
"""
Stage K: Embedding (ベクトル化)

チャンクをベクトル化して search_index に保存
- 役割: チャンクをベクトル化
- モデル: OpenAI text-embedding-3-small (1536次元)
"""
from typing import Dict, Any, List
from loguru import logger

from shared.ai.llm_client.llm_client import LLMClient
from shared.common.database.client import DatabaseClient


class StageKEmbedding:
    """Stage K: ベクトル化"""

    def __init__(self, llm_client: LLMClient, db_client: DatabaseClient):
        """
        Args:
            llm_client: LLMクライアント
            db_client: データベースクライアント
        """
        self.llm_client = llm_client
        self.db = db_client

    def embed_and_save(
        self,
        document_id: str,
        chunks: List[Dict[str, Any]],
        delete_existing: bool = False
    ) -> Dict[str, Any]:
        """
        チャンクをベクトル化して search_index に保存

        Args:
            document_id: ドキュメントID
            chunks: チャンクリスト
            delete_existing: 既存のチャンクを削除するか

        Returns:
            {
                'success': bool,
                'saved_count': int,
                'failed_count': int
            }
        """
        logger.info("[Stage K] ベクトル化 + search_index保存開始...")

        # 既存ドキュメントの場合は、古いチャンクを削除
        if delete_existing:
            try:
                logger.info(f"[Stage K] 既存チャンク削除: document_id={document_id}")
                self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
            except Exception as e:
                logger.warning(f"[Stage K 警告] 既存チャンク削除エラー（継続）: {e}")

        saved_count = 0
        failed_count = 0

        for chunk in chunks:
            try:
                # null文字を除去
                chunk_text = chunk['chunk_text'].replace('\u0000', '') if chunk['chunk_text'] else ''

                # Embedding生成
                embedding = self.llm_client.generate_embedding(chunk_text)

                # search_indexに保存
                chunk_data = {
                    'document_id': document_id,
                    'chunk_content': chunk_text,
                    'chunk_size': len(chunk_text),
                    'chunk_type': chunk['chunk_type'],
                    'embedding': embedding,
                    'search_weight': chunk.get('search_weight', 1.0),
                    'chunk_index': chunk.get('chunk_index', 0),
                    'chunk_metadata': chunk.get('metadata')  # 構造化データを保存
                }

                self.db.client.table('10_ix_search_index').insert(chunk_data).execute()
                saved_count += 1

            except Exception as e:
                logger.error(f"[Stage K エラー] チャンク保存失敗: {e}")
                failed_count += 1

        logger.info(f"[Stage K完了] {saved_count}/{len(chunks)}チャンクを保存 (失敗: {failed_count})")

        # chunk_countを更新
        if saved_count > 0:
            try:
                self.db.client.table('Rawdata_FILE_AND_MAIL')\
                    .update({'chunk_count': saved_count})\
                    .eq('id', document_id)\
                    .execute()
                logger.debug(f"[Stage K] chunk_count更新: {saved_count}個")
            except Exception as e:
                logger.warning(f"[Stage K 警告] chunk_count更新エラー（継続）: {e}")

        # 成功条件: 最低1チャンク以上保存 & 失敗なし
        is_success = saved_count > 0 and failed_count == 0

        return {
            'success': is_success,
            'saved_count': saved_count,
            'failed_count': failed_count
        }

    def process(self, chunks: List[Dict[str, Any]], document_id: str) -> None:
        """
        チャンクをベクトル化して保存（process() エイリアス）

        Args:
            chunks: チャンクリスト
            document_id: ドキュメントID
        """
        self.embed_and_save(document_id, chunks)
```

### tests\stage_e_text.txt

```txt
================================================================================
Stage E - Extracted Text
================================================================================
Length: 145 chars



---

# Page 1

foodium/武蔵⼩杉の最新チラシ‧店舗情報｜無料で⾒られるShufoo!（シュフー） 2026/01/02 23:37

https://www.shufoo.net/pntweb/shopDetail/9876/43205746288978/ 1/1
```

### tests\stage_f_full_text.txt

```txt
================================================================================
Stage F - Full Text (Hybrid OCR)
================================================================================
Length: 2159 chars

2026/01/02 23:37
foodium/武蔵小杉の最新チラシ・店舗情報｜無料で⾒られるShufoo!（シュフー）
daici ダイエー / AEON FOOD STYLE
年始の家計応援! の市
このコーナーの広告商品売り出し日 1/3(水) 7(日)
お肉の よりどりセール
よりどり3パック 本体 1,050円 税込1,134円
牛肉 豚肉 鶏肉
※当社指定商品に限ります。
掲載商品の写真はイメージです

1月3日限り
イオン農場 長ねぎ (1束2本入)
本体 158円 税込170.64円
フジパン ネオバターロール ●ネオレーズンバターロール ●ネオ黒糖ロール (各6コ入)
本体各 148円 税込各159.84円
味の素 鍋キューブ ・うま辛キムチ・濃厚白湯 ・鶏だし・うま塩 (各7コ入)
本体各 158円 税込各170.64円
森永乳業 牧場の大地 (1000ml)
AEONクーポン ご利用時 本体各 155円 税込167.40円
60円引きで
伊藤ハム The GRAND アルトバイエルン (117g×2袋)
本体 358円 税込386.64円
宮崎県産 生産者にこだわった きゅうり (1パック3本入)
本体 228円 税込246.24円
チリ産 冷銀さけ 切身(養殖) (1切当り)
本体 95円 税込102.60円
国産 豚肉 小間切れ (100g当り)
本体 398円 税込429.84円
大阪王将 ぷるもち水餃子 (272g)
本体 258円 税込278.64円

1月4日限り
ヤマザキ ランチパック ・グラタンコロッケとナポリタン ・ハムチーズとエッグ(ケチャップ) ・ピーナッツ ・たまご (各2コ入)
本体各 198円 税込各213.84円
森永乳業 ビヒダスBB536 ・プレーンヨーグルト ・プレーンヨーグルト脂肪0 (各400g)
本体各 138円 税込各149.04円
よりどり3コ 本体各 278円 税込300.24円
日清シスコ ココナッツサブレ ●ココナッツサブレトリプルナッツ (各16枚入)
AEONクーポン ご利用時 本体各 98円 税込各105.84円
40円引きで
ハウス カリー屋カレー ・甘口・中辛・辛口 (各180g)
AEONクーポン ご利用時 本体各 298円 税込321.84円
50円引きで
日清フーズ マ・マー 早ゆでスパゲッティ (600g)
AEONクーポン ご利用時 本体各 208円 税込各224.64円
50円引きで
ニチレイ 焼おにぎり (10コ入)
AEONクーポン ご利用時 本体各 248円 税込各267.84円
50円引きで
熊本県産他 国内産 トマト (1箱)
男前豆腐店 京まろとうふ (100g×4コ入)
本体 138円 税込149.04円
AEONクーポン ご利用時 本体各 88円 税込各95.04円
20円引きで
赤城 ミルクレア ●ミルクレアベルギーチョコレート (40ml×5本入) ●チョコミント (60ml×6本入)

TOPVALU ロシア産他 MSC認証 ひと塩たら切身 (100g当り)
本体 298円 税込321.84円
エクアドル産他 解凍えび (特大)養殖 (10尾)
本体 598円 税込645.84円
キユーピー ●ゼロノンコレステロール (310g) ●マヨネーズ ●ハーフ (各400g)
本体各 258円 税込各278.64円
1会計のみ2点限り
日清オイリオ ●ヘルシークリア (800g) ●日清キャノーラ油におい少ない (900g)
本体各 258円 税込各278.64円
1会計のみ計3点限り
サントリー 天然水 2L (6本入)
本体 560円 税込604.80円
1本あたり 本体約93.4円
1会計のみ2点限り
シマダヤ 昔なつかしの 本生ラーメン 味噌味 醤油味 (各3食入)
本体各 258円 税込各278.64円
日清シスコ ホットケーキミックス (600g)
本体 158円 税込170.64円
炭火焼かつおたたき スライス (生食用) (100g当り)
本体 178円 税込192.24円

広告実施店舗は裏面の広告商品の取扱い店舗をご確認ください。
●税込価格は、対象商品別にそれぞれ標準税率10%と軽減税率8%で記載しています。飲食料品(酒類・一部の食品を除きます)はお持ち帰りいただくことを基本に軽減税率8%で記載しています。
●割引き・値引き表示は「本体価格」からの割引き・値引きとなります。
合計金額に消費税を加えた金額(1円未満は切り捨て)をお支払いください。
●広告の品が万一、売り切れの場合はご容赦ください。商品によっては数量制限させていただく場合もございます。
●ダイエーでは、当社基準に基づいて「鮮度・衛生管理」に十分配慮しています。●CO2削減・環境保全のためお買物袋持参運動にご協力をお願いします。
AEONクーポン 新規ダウンロードはこちらより お気に入り登録がまだの方へ
daici
https://www.shufoo.net/pntweb/shopDetail/9876/43205746288978/ 1/1
```

### tests\stage_f_result.json

```json
{
  "full_text": "2026/01/02 23:37\nfoodium/武蔵小杉の最新チラシ・店舗情報｜無料で⾒られるShufoo!（シュフー）\ndaici ダイエー / AEON FOOD STYLE\n年始の家計応援! の市\nこのコーナーの広告商品売り出し日 1/3(水) 7(日)\nお肉の よりどりセール\nよりどり3パック 本体 1,050円 税込1,134円\n牛肉 豚肉 鶏肉\n※当社指定商品に限ります。\n掲載商品の写真はイメージです\n\n1月3日限り\nイオン農場 長ねぎ (1束2本入)\n本体 158円 税込170.64円\nフジパン ネオバターロール ●ネオレーズンバターロール ●ネオ黒糖ロール (各6コ入)\n本体各 148円 税込各159.84円\n味の素 鍋キューブ ・うま辛キムチ・濃厚白湯 ・鶏だし・うま塩 (各7コ入)\n本体各 158円 税込各170.64円\n森永乳業 牧場の大地 (1000ml)\nAEONクーポン ご利用時 本体各 155円 税込167.40円\n60円引きで\n伊藤ハム The GRAND アルトバイエルン (117g×2袋)\n本体 358円 税込386.64円\n宮崎県産 生産者にこだわった きゅうり (1パック3本入)\n本体 228円 税込246.24円\nチリ産 冷銀さけ 切身(養殖) (1切当り)\n本体 95円 税込102.60円\n国産 豚肉 小間切れ (100g当り)\n本体 398円 税込429.84円\n大阪王将 ぷるもち水餃子 (272g)\n本体 258円 税込278.64円\n\n1月4日限り\nヤマザキ ランチパック ・グラタンコロッケとナポリタン ・ハムチーズとエッグ(ケチャップ) ・ピーナッツ ・たまご (各2コ入)\n本体各 198円 税込各213.84円\n森永乳業 ビヒダスBB536 ・プレーンヨーグルト ・プレーンヨーグルト脂肪0 (各400g)\n本体各 138円 税込各149.04円\nよりどり3コ 本体各 278円 税込300.24円\n日清シスコ ココナッツサブレ ●ココナッツサブレトリプルナッツ (各16枚入)\nAEONクーポン ご利用時 本体各 98円 税込各105.84円\n40円引きで\nハウス カリー屋カレー ・甘口・中辛・辛口 (各180g)\nAEONクーポン ご利用時 本体各 298円 税込321.84円\n50円引きで\n日清フーズ マ・マー 早ゆでスパゲッティ (600g)\nAEONクーポン ご利用時 本体各 208円 税込各224.64円\n50円引きで\nニチレイ 焼おにぎり (10コ入)\nAEONクーポン ご利用時 本体各 248円 税込各267.84円\n50円引きで\n熊本県産他 国内産 トマト (1箱)\n男前豆腐店 京まろとうふ (100g×4コ入)\n本体 138円 税込149.04円\nAEONクーポン ご利用時 本体各 88円 税込各95.04円\n20円引きで\n赤城 ミルクレア ●ミルクレアベルギーチョコレート (40ml×5本入) ●チョコミント (60ml×6本入)\n\nTOPVALU ロシア産他 MSC認証 ひと塩たら切身 (100g当り)\n本体 298円 税込321.84円\nエクアドル産他 解凍えび (特大)養殖 (10尾)\n本体 598円 税込645.84円\nキユーピー ●ゼロノンコレステロール (310g) ●マヨネーズ ●ハーフ (各400g)\n本体各 258円 税込各278.64円\n1会計のみ2点限り\n日清オイリオ ●ヘルシークリア (800g) ●日清キャノーラ油におい少ない (900g)\n本体各 258円 税込各278.64円\n1会計のみ計3点限り\nサントリー 天然水 2L (6本入)\n本体 560円 税込604.80円\n1本あたり 本体約93.4円\n1会計のみ2点限り\nシマダヤ 昔なつかしの 本生ラーメン 味噌味 醤油味 (各3食入)\n本体各 258円 税込各278.64円\n日清シスコ ホットケーキミックス (600g)\n本体 158円 税込170.64円\n炭火焼かつおたたき スライス (生食用) (100g当り)\n本体 178円 税込192.24円\n\n広告実施店舗は裏面の広告商品の取扱い店舗をご確認ください。\n●税込価格は、対象商品別にそれぞれ標準税率10%と軽減税率8%で記載しています。飲食料品(酒類・一部の食品を除きます)はお持ち帰りいただくことを基本に軽減税率8%で記載しています。\n●割引き・値引き表示は「本体価格」からの割引き・値引きとなります。\n合計金額に消費税を加えた金額(1円未満は切り捨て)をお支払いください。\n●広告の品が万一、売り切れの場合はご容赦ください。商品によっては数量制限させていただく場合もございます。\n●ダイエーでは、当社基準に基づいて「鮮度・衛生管理」に十分配慮しています。●CO2削減・環境保全のためお買物袋持参運動にご協力をお願いします。\nAEONクーポン 新規ダウンロードはこちらより お気に入り登録がまだの方へ\ndaici\nhttps://www.shufoo.net/pntweb/shopDetail/9876/43205746288978/ 1/1",
  "layout_info": {
    "sections": [
      {
        "title": "Header",
        "text_content": "2026/01/02 23:37\nfoodium/武蔵小杉の最新チラシ・店舗情報｜無料で⾒られるShufoo!（シュフー）\ndaici ダイエー / AEON FOOD STYLE\n年始の家計応援! の市"
      },
      {
        "title": "Meat Special Offer",
        "text_content": "このコーナーの広告商品売り出し日 1/3(水) 7(日)\nお肉の よりどりセール\nよりどり3パック 本体 1,050円 税込1,134円\n牛肉 豚肉 鶏肉\n※当社指定商品に限ります。\n掲載商品の写真はイメージです"
      },
      {
        "title": "Jan 3rd Limited Offers",
        "text_content": "1月3日限り\nイオン農場 長ねぎ (1束2本入) 本体 158円 税込170.64円\nフジパン ネオバターロール ●ネオレーズンバターロール ●ネオ黒糖ロール (各6コ入) 本体各 148円 税込各159.84円\n味の素 鍋キューブ ・うま辛キムチ・濃厚白湯 ・鶏だし・うま塩 (各7コ入) 本体各 158円 税込各170.64円\n森永乳業 牧場の大地 (1000ml) AEONクーポン ご利用時 本体各 155円 税込167.40円 60円引きで\n伊藤ハム The GRAND アルトバイエルン (117g×2袋) 本体 358円 税込386.64円\n宮崎県産 生産者にこだわった きゅうり (1パック3本入) 本体 228円 税込246.24円\nチリ産 冷銀さけ 切身(養殖) (1切当り) 本体 95円 税込102.60円\n国産 豚肉 小間切れ (100g当り) 本体 398円 税込429.84円\n大阪王将 ぷるもち水餃子 (272g) 本体 258円 税込278.64円"
      },
      {
        "title": "Jan 4th Limited Offers",
        "text_content": "1月4日限り\nヤマザキ ランチパック ・グラタンコロッケとナポリタン ・ハムチーズとエッグ(ケチャップ) ・ピーナッツ ・たまご (各2コ入) 本体各 198円 税込各213.84円\n森永乳業 ビヒダスBB536 ・プレーンヨーグルト ・プレーンヨーグルト脂肪0 (各400g) 本体各 138円 税込各149.04円\nよりどり3コ 本体各 278円 税込300.24円 日清シスコ ココナッツサブレ ●ココナッツサブレトリプルナッツ (各16枚入)\nAEONクーポン ご利用時 本体各 98円 税込各105.84円 40円引きで ハウス カリー屋カレー ・甘口・中辛・辛口 (各180g)\nAEONクーポン ご利用時 本体各 298円 税込321.84円 50円引きで 日清フーズ マ・マー 早ゆでスパゲッティ (600g)\nAEONクーポン ご利用時 本体各 208円 税込各224.64円 50円引きで ニチレイ 焼おにぎり (10コ入)\nAEONクーポン ご利用時 本体各 248円 税込各267.84円 50円引きで 熊本県産他 国内産 トマト (1箱)\n男前豆腐店 京まろとうふ (100g×4コ入) 本体 138円 税込149.04円\nAEONクーポン ご利用時 本体各 88円 税込各95.04円 20円引きで 赤城 ミルクレア ●ミルクレアベルギーチョコレート (40ml×5本入) ●チョコミント (60ml×6本入)"
      },
      {
        "title": "General Groceries & Daily Essentials",
        "text_content": "TOPVALU ロシア産他 MSC認証 ひと塩たら切身 (100g当り) 本体 298円 税込321.84円\nエクアドル産他 解凍えび (特大)養殖 (10尾) 本体 598円 税込645.84円\nキユーピー ●ゼロノンコレステロール (310g) ●マヨネーズ ●ハーフ (各400g) 本体各 258円 税込各278.64円 1会計のみ2点限り\n日清オイリオ ●ヘルシークリア (800g) ●日清キャノーラ油におい少ない (900g) 本体各 258円 税込各278.64円 1会計のみ計3点限り\nサントリー 天然水 2L (6本入) 本体 560円 税込604.80円 1本あたり 本体約93.4円 1会計のみ2点限り\nシマダヤ 昔なつかしの 本生ラーメン 味噌味 醤油味 (各3食入) 本体各 258円 税込各278.64円\n日清シスコ ホットケーキミックス (600g) 本体 158円 税込170.64円\n炭火焼かつおたたき スライス (生食用) (100g当り) 本体 178円 税込192.24円"
      },
      {
        "title": "Footer",
        "text_content": "広告実施店舗は裏面の広告商品の取扱い店舗をご確認ください。\n●税込価格は、対象商品別にそれぞれ標準税率10%と軽減税率8%で記載しています。飲食料品(酒類・一部の食品を除きます)はお持ち帰りいただくことを基本に軽減税率8%で記載しています。\n●割引き・値引き表示は「本体価格」からの割引き・値引きとなります。\n合計金額に消費税を加えた金額(1円未満は切り捨て)をお支払いください。\n●広告の品が万一、売り切れの場合はご容赦ください。商品によっては数量制限させていただく場合もございます。\n●ダイエーでは、当社基準に基づいて「鮮度・衛生管理」に十分配慮しています。●CO2削減・環境保全のためお買物袋持参運動にご協力をお願いします。\nAEONクーポン 新規ダウンロードはこちらより お気に入り登録がまだの方へ\ndaici\nhttps://www.shufoo.net/pntweb/shopDetail/9876/43205746288978/ 1/1"
      }
    ],
    "tables": [
      {
        "header": "お肉のよりどりセール",
        "rows": [
          ["牛肉"],
          ["豚肉"],
          ["鶏肉"]
        ],
        "footer": "よりどり3パック 本体 1,050円 税込1,134円\n※当社指定商品に限ります。"
      }
    ]
  },
  "visual_elements": {
    "images": [
      {
        "description": "daici ダイエー / AEON FOOD STYLE logo"
      },
      {
        "description": "年始の家計応援! の市 promotional graphic"
      },
      {
        "description": "Assortment of meat (beef, pork, chicken)"
      },
      {
        "description": "Long green onions (長ねぎ)"
      },
      {
        "description": "Fuji Pan Neo Butter Roll products"
      },
      {
        "description": "Ajinomoto Nabe Cube products"
      },
      {
        "description": "Morinaga Milk Farm's Earth milk carton"
      },
      {
        "description": "Ito Ham The GRAND Alt Bayern sausages"
      },
      {
        "description": "Cucumbers"
      },
      {
        "description": "Chili-sourced chilled silver salmon fillet"
      },
      {
        "description": "Domestic pork slices"
      },
      {
        "description": "Osaka Ohsho Puru Mochi Gyoza (dumplings)"
      },
      {
        "description": "Yamazaki Lunch Pack products"
      },
      {
        "description": "Morinaga Milk Bifidus BB536 yogurt products"
      },
      {
        "description": "Nissin Cisco Coconut Sable products"
      },
      {
        "description": "House Curry-ya Curry products"
      },
      {
        "description": "Nissin Foods Ma・Ma Early-boiled Spaghetti"
      },
      {
        "description": "Nichirei Baked Onigiri (rice balls)"
      },
      {
        "description": "Domestic tomatoes"
      },
      {
        "description": "Otoko-mae Tofu Kyoto Maro Tofu"
      },
      {
        "description": "Akagi Milcrea ice cream products"
      },
      {
        "description": "TOPVALU MSC certified salted cod fillet"
      },
      {
        "description": "Ecuador-sourced thawed large shrimp"
      },
      {
        "description": "Kewpie mayonnaise products (Zero, Mayonnaise, Half)"
      },
      {
        "description": "Nissin Oillio cooking oil products (Healthy Clear, Canola Oil)"
      },
      {
        "description": "Suntory Natural Water bottles"
      },
      {
        "description": "Shimadaya nostalgic raw ramen (Miso, Soy Sauce)"
      },
      {
        "description": "Nissin Cisco Hotcake Mix"
      },
      {
        "description": "Charcoal-grilled seared bonito slices"
      },
      {
        "description": "AEON Coupon QR code and logo"
      }
    ],
    "charts": []
  }
}
```

### tests\summary.txt

```txt
================================================================================
Test Summary
================================================================================
Stage E text: 145 chars
Stage F full_text: 2159 chars
Improvement: +2014 chars
Tables: 1 items
Sections: 6 items
Images: 30 items

Files saved:
- stage_e_text.txt: Stage E extracted text
- stage_f_full_text.txt: Stage F full text
- stage_f_result.json: Complete Stage F JSON result
- summary.txt: This summary
```

