# Pipeline Snapshot (Lightweight)

Generated: 2026-01-30T13:53:13.336036
Files: 32

## Files

- shared/pipeline/__init__.py (24 lines)
- shared/pipeline/archive/stage_h_structuring.py (496 lines)
- shared/pipeline/archive/stage_i_synthesis.py (262 lines)
- shared/pipeline/config_loader.py (222 lines)
- shared/pipeline/constants.py (73 lines)
- shared/pipeline/image_preprocessing.py (215 lines)
- shared/pipeline/ocr_config.py (219 lines)
- shared/pipeline/ocr_report.py (161 lines)
- shared/pipeline/pipeline.py (964 lines)
- shared/pipeline/prompts/__init__.py (171 lines)
- shared/pipeline/stage_e_preprocessing.py (344 lines)
- shared/pipeline/stage_f_visual.py (2883 lines)
- shared/pipeline/stage_g1_table_refiner.py (744 lines)
- shared/pipeline/stage_g2_text_refiner.py (760 lines)
- shared/pipeline/stage_g_gate.py (662 lines)
- shared/pipeline/stage_g_refiner.py (701 lines)
- shared/pipeline/stage_h1_table.py (438 lines)
- shared/pipeline/stage_h2_text.py (574 lines)
- shared/pipeline/stage_h_kakeibo.py (617 lines)
- shared/pipeline/stage_hi_combined.py (432 lines)
- shared/pipeline/stage_j_chunking.py (112 lines)
- shared/pipeline/stage_k_embedding.py (144 lines)
- shared/pipeline/utils/__init__.py (7 lines)
- shared/pipeline/utils/table_parser.py (144 lines)
- shared/ai/__init__.py (1 lines)
- shared/ai/embeddings/__init__.py (1 lines)
- shared/ai/embeddings/embeddings.py (63 lines)
- shared/ai/llm_client/__init__.py (1 lines)
- shared/ai/llm_client/exceptions.py (19 lines)
- shared/ai/llm_client/llm_client.py (664 lines)
- shared/__init__.py (2 lines)
- pyproject.toml (17 lines)

Total: 12137 lines

---

## shared/pipeline/__init__.py

```python
"""
G_unified_pipeline: 統合ドキュメント処理パイプライン

Stage E-K を統合した、堅牢かつ高精度なドキュメント処理フロー

使用方法:
    from shared.pipeline import UnifiedDocumentPipeline

    pipeline = UnifiedDocumentPipeline()
    result = await pipeline.process_document(
        file_path=Path("document.pdf"),
        file_name="document.pdf",
        doc_type="invoice",
        workspace="personal",
        mime_type="application/pdf",
        source_id="drive_file_id"
    )
"""

from .pipeline import UnifiedDocumentPipeline

__all__ = ['UnifiedDocumentPipeline']
__version__ = '1.0.0'
```

## shared/pipeline/archive/stage_h_structuring.py

```python
"""
Stage H: Structuring (構造化)

非構造化テキストから、意味のあるデータ(JSON)を抽出
- 役割: 指定スキーマに基づくJSON抽出、表データの正規化
- モデル: 設定ファイルで指定（Gemini, OpenAI等のLLM）
- 重要: doc_typeを判定するのではなく、渡されたdoc_typeのスキーマを使ってデータを抽出

F_stage_c_extractor から完全移行
"""
import re
import json
import json_repair
from typing import Dict, Any, Optional
from pathlib import Path
from string import Template
from loguru import logger
from datetime import datetime

from shared.ai.llm_client.llm_client import LLMClient
from .constants import STAGE_H_INPUT_SCHEMA_VERSION


class StageHStructuring:
    """Stage H: 構造化（設定ベース版）"""

    def __init__(self, llm_client: LLMClient):
        """
        Args:
            llm_client: LLMクライアント
        """
        self.llm = llm_client

    def process(
        self,
        file_name: str,
        doc_type: str,
        workspace: str,
        combined_text: str,
        prompt: str,
        model: str,
        stage_f_structure: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        構造化（設定ベース版）

        Args:
            file_name: ファイル名
            doc_type: ドキュメントタイプ
            workspace: ワークスペース
            combined_text: 統合テキスト（Stage E + G の結果）
            prompt: プロンプト（config/prompts/stage_h/*.md から読み込み）
            model: モデル名
            stage_f_structure: Stage F の構造化情報（tables, sections, visual_elements）

        Returns:
            {
                'document_date': str,
                'tags': List[str],
                'metadata': Dict[str, Any]
            }
        """
        logger.info(f"[Stage H] 構造化開始... (doc_type={doc_type}, model={model})")

        # ============================================
        # Stage H 入力スキーマ v1.1 検証（契約固定）
        # ============================================
        h_warnings = []
        if stage_f_structure:
            # 1. schema_version 検証
            schema_ver = stage_f_structure.get('schema_version', 'unknown')
            if schema_ver != STAGE_H_INPUT_SCHEMA_VERSION:
                h_warnings.append(f"H_SCHEMA_VERSION_MISMATCH: expected stage_h_input.v1.1, got {schema_ver}")
                logger.warning(f"[Stage H] schema_version不一致: {schema_ver}")
            else:
                logger.info(f"[Stage H] schema_version=stage_h_input.v1.1 ✓")

            # 2. post_body.text 検証
            post_body = stage_f_structure.get('post_body', {})
            post_body_text = post_body.get('text', '')
            if not post_body_text or not post_body_text.strip():
                h_warnings.append("H_POST_BODY_EMPTY: post_body.text is empty")
                logger.warning("[Stage H] post_body.text が空です")
            else:
                logger.info(f"[Stage H] post_body: {len(post_body_text)}文字 (source: {post_body.get('source', 'unknown')})")

            # 3. text_blocks[0].block_type == "post_body" 検証
            text_blocks = stage_f_structure.get('text_blocks', [])
            if text_blocks:
                first_block = text_blocks[0]
                if first_block.get('block_type') != 'post_body':
                    h_warnings.append(f"H_FIRST_BLOCK_NOT_POST_BODY: text_blocks[0].block_type={first_block.get('block_type')}")
                    logger.warning(f"[Stage H] text_blocks[0]がpost_bodyではありません: {first_block.get('block_type')}")
                else:
                    logger.info(f"[Stage H] text_blocks[0]=post_body ({first_block.get('char_count', 0)}文字) ✓")

            # 構造化情報ログ
            logger.info(f"[Stage H] Stage F 構造化情報: tables={len(stage_f_structure.get('tables', []))}, text_blocks={len(text_blocks)}")
            if h_warnings:
                logger.warning(f"[Stage H] 入力検証warnings: {len(h_warnings)}件")
        else:
            logger.info("[Stage H] stage_f_structure なし（レガシーモード）")

        if not combined_text or not combined_text.strip():
            logger.warning("[Stage H] 入力テキストが空です")
            return self._get_fallback_result(doc_type)

        try:
            # プロンプト構築
            logger.info("[Stage H] プロンプト構築中...")
            full_prompt = self._build_prompt(
                prompt_template=prompt,
                file_name=file_name,
                doc_type=doc_type,
                workspace=workspace,
                combined_text=combined_text
            )
            logger.info(f"[Stage H] プロンプト構築完了 ({len(full_prompt)}文字)")

            # LLM呼び出し
            logger.info(f"[Stage H] LLM呼び出し中... (model={model})")
            response = self.llm.call_model(
                tier="default",
                prompt=full_prompt,
                model_name=model
            )
            logger.info(f"[Stage H] LLM応答受信: success={response.get('success')}")

            if not response.get("success"):
                logger.error(f"[Stage H エラー] LLM呼び出し失敗: {response.get('error')}")
                return self._get_fallback_result(doc_type)

            # JSON抽出（リトライ機能付き）
            content = response.get("content", "")
            logger.info(f"[Stage H] ===== LLMレスポンス全文 =====\n{content}\n[Stage H] ===== レスポンス終了 =====")
            result = self._extract_json_with_retry(content, model=model, max_retries=2)

            # Stage F の構造化情報をマージ
            if stage_f_structure:
                result = self._merge_stage_f_structure(result, stage_f_structure)

            # 結果の整形
            final_metadata = result.get('metadata', {})
            # h_warnings を metadata に合流（ログだけでなく実データとして保持）
            if h_warnings:
                final_metadata.setdefault('warnings', []).extend(h_warnings)
                final_metadata['schema_validation'] = {
                    'version_checked': STAGE_H_INPUT_SCHEMA_VERSION,
                    'warnings_count': len(h_warnings)
                }
            return {
                'document_date': result.get('document_date'),
                'tags': result.get('tags', []),
                'metadata': final_metadata
            }

        except Exception as e:
            logger.error(f"[Stage H エラー] 構造化失敗: {e}", exc_info=True)
            return self._get_fallback_result(doc_type)

    def _build_prompt(
        self,
        prompt_template: str,
        file_name: str,
        doc_type: str,
        workspace: str,
        combined_text: str
    ) -> str:
        """
        プロンプトを構築

        Args:
            prompt_template: プロンプトテンプレート
            file_name: ファイル名
            doc_type: ドキュメントタイプ
            workspace: ワークスペース
            combined_text: 統合テキスト

        Returns:
            構築されたプロンプト
        """
        # string.Templateを使用してテンプレート変数を置換（JSONの{}と競合しない）
        template = Template(prompt_template)
        prompt = template.substitute(
            file_name=file_name,
            doc_type=doc_type,
            workspace=workspace,
            combined_text=combined_text,
            current_date=datetime.now().strftime("%Y-%m-%d")
        )

        return prompt

    def _extract_json_with_retry(
        self,
        content: str,
        model: str,
        max_retries: int = 2
    ) -> Dict[str, Any]:
        """
        JSON抽出（リトライ機能付き）

        Args:
            content: LLMの出力
            model: モデル名
            max_retries: 最大リトライ回数

        Returns:
            抽出されたJSON
        """
        for attempt in range(max_retries + 1):
            try:
                result = self._extract_json(content)
                logger.debug(f"[Stage H] JSON抽出成功 (試行{attempt + 1}/{max_retries + 1})")
                return result

            except Exception as e:
                if attempt < max_retries:
                    logger.warning(f"[Stage H] JSON抽出失敗 (試行{attempt + 1}/{max_retries + 1}): {e}")
                    # リトライ: LLMにJSON修正を依頼
                    content = self._retry_json_extraction(content, str(e), model)
                else:
                    logger.error(f"[Stage H] JSON抽出失敗（最終試行）: {e}")
                    raise

        # ここには到達しないはずだが、念のため
        return {}

    def _extract_json(self, content: str) -> Dict[str, Any]:
        """
        コンテンツからJSONを抽出

        Args:
            content: LLMの出力

        Returns:
            抽出されたJSON

        Raises:
            Exception: JSON抽出に失敗した場合
        """
        # 複数のパターンでJSONブロックを探す
        patterns = [
            r'```json\s*(.*?)```',  # ```json ... ``` (改行を柔軟に)
            r'```\s*(.*?)```',      # ``` ... ```
            r'\{[\s\S]*?\}',        # { ... } (非貪欲)
        ]

        json_str = None
        for pattern in patterns:
            match = re.search(pattern, content, re.DOTALL)
            if match:
                json_str = match.group(1) if match.lastindex else match.group(0)
                json_str = json_str.strip()
                # { で始まるか確認
                if json_str.startswith('{'):
                    break
                else:
                    json_str = None

        if not json_str:
            # パターンマッチしない場合、{ ... } を直接探す
            match = re.search(r'\{[\s\S]*\}', content, re.DOTALL)
            if match:
                json_str = match.group(0).strip()
            else:
                json_str = content.strip()

        # JSONパース
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            # json_repair で修復を試みる
            logger.error(f"[Stage H] JSON解析失敗: {e}")
            logger.error(f"[Stage H] 抽出されたJSON（最初の500文字）:\n{json_str[:500]}")
            try:
                return json_repair.loads(json_str)
            except Exception as repair_error:
                logger.error(f"[Stage H] JSON修復も失敗: {repair_error}")
                logger.error(f"[Stage H] 修復失敗したJSON全文:\n{json_str}")
                raise

    def _retry_json_extraction(
        self,
        failed_content: str,
        error_message: str,
        model: str
    ) -> str:
        """
        JSON抽出失敗時、LLMにJSON修正を依頼

        Args:
            failed_content: 失敗したコンテンツ
            error_message: エラーメッセージ
            model: モデル名

        Returns:
            修正されたJSON文字列
        """
        prompt = f"""以下のJSONにエラーがあります。修正してください。

エラー: {error_message}

元のJSON:
```
{failed_content}
```

修正されたJSONを ```json ブロックで出力してください。
"""

        try:
            response = self.llm.call_model(
                tier="default",
                prompt=prompt,
                model_name=model
            )

            if response.get("success"):
                return response.get("content", "")
            else:
                logger.error(f"[Stage H] JSON修正失敗: {response.get('error')}")
                return failed_content

        except Exception as e:
            logger.error(f"[Stage H] JSON修正エラー: {e}")
            return failed_content

    def _merge_stage_f_structure(self, result: Dict[str, Any], stage_f_structure: Dict[str, Any]) -> Dict[str, Any]:
        """
        Stage F の構造化情報を Stage H の結果にマージ

        Args:
            result: Stage H の抽出結果
            stage_f_structure: Stage F の構造化情報

        Returns:
            マージ済みの結果
        """
        metadata = result.get('metadata', {})

        # v1.1契約の判定
        schema_ver = stage_f_structure.get('schema_version', '')
        is_v1_1 = (schema_ver == STAGE_H_INPUT_SCHEMA_VERSION)

        # v1.1: tables/sections の取得場所を適切に
        if is_v1_1:
            # v1.1: tables はトップレベル、sections は layout_info
            stage_f_tables = stage_f_structure.get('tables', [])
            stage_f_sections = stage_f_structure.get('layout_info', {}).get('sections', [])
            # v1.1では text_blocks が Stage F で生成済み → _raw_text_blocks に保存（LLM出力を優先）
            stage_f_text_blocks = stage_f_structure.get('text_blocks', [])
            if stage_f_text_blocks:
                # 生のtext_blocksは隠しフィールドに保存
                metadata['_raw_text_blocks'] = stage_f_text_blocks
                logger.info(f"[Stage H] v1.1: Stage F text_blocks を _raw_text_blocks に保存 ({len(stage_f_text_blocks)}ブロック)")

                # LLMが articles を生成していればそれを優先、なければ text_blocks をチェック
                llm_articles = metadata.get('articles', [])
                llm_text_blocks = metadata.get('text_blocks', [])

                if llm_articles:
                    # LLMが articles を生成済み → text_blocks は不要
                    logger.info(f"[Stage H] LLM生成の articles を優先 ({len(llm_articles)}件)")
                    if 'text_blocks' in metadata:
                        del metadata['text_blocks']
                elif llm_text_blocks:
                    # LLMが text_blocks を生成済み → そのまま使用
                    logger.info(f"[Stage H] LLM生成の text_blocks を使用 ({len(llm_text_blocks)}件)")
                else:
                    # LLM出力が空 → フォールバックとして _raw_text_blocks を text_blocks に
                    metadata['text_blocks'] = stage_f_text_blocks
                    logger.info(f"[Stage H] フォールバック: _raw_text_blocks を text_blocks に採用")
        else:
            # レガシー: 直下または layout_info から取得
            stage_f_tables = stage_f_structure.get('tables', []) or stage_f_structure.get('layout_info', {}).get('tables', [])
            stage_f_sections = stage_f_structure.get('sections', []) or stage_f_structure.get('layout_info', {}).get('sections', [])

        # Stage F の tables を優先的に使用（Stage H で tables が抽出されていない場合、または少ない場合）
        stage_h_tables = metadata.get('structured_tables', [])

        if stage_f_tables and len(stage_f_tables) > len(stage_h_tables):
            logger.info(f"[Stage H] Stage F の tables を優先使用: {len(stage_f_tables)}個（Stage H: {len(stage_h_tables)}個）")

            # Stage F の tables を Stage H の形式に変換
            converted_tables = []
            for i, table in enumerate(stage_f_tables):
                rows = table.get('rows', [])
                if rows:
                    # 最初の行をヘッダーとして扱う
                    headers = rows[0] if rows else []
                    data_rows = rows[1:] if len(rows) > 1 else []

                    # 辞書形式に変換
                    converted_rows = []
                    for row in data_rows:
                        row_dict = {}
                        for j, cell in enumerate(row):
                            header = headers[j] if j < len(headers) else f"列{j+1}"
                            row_dict[header] = cell
                        converted_rows.append(row_dict)

                    converted_tables.append({
                        'table_title': table.get('caption', f'表{i+1}'),
                        'table_type': 'ocr_extracted',
                        'headers': headers,
                        'rows': converted_rows
                    })

            metadata['structured_tables'] = converted_tables

        # Stage F の sections を text_blocks に統合（text_blocks が少ない場合）
        # v1.1では text_blocks が正なのでこの変換はスキップ
        stage_h_text_blocks = metadata.get('text_blocks', [])

        if not is_v1_1 and stage_f_sections and len(stage_h_text_blocks) < len(stage_f_sections):
            logger.info(f"[Stage H] Stage F の sections を text_blocks に変換: {len(stage_f_sections)}個")

            converted_blocks = []
            current_heading = None
            current_content_parts = []

            for section in stage_f_sections:
                section_type = section.get('type', '')
                content = section.get('content', '')

                if section_type == 'heading':
                    # 前のブロックを保存
                    if current_heading and current_content_parts:
                        converted_blocks.append({
                            'title': current_heading,
                            'content': '\n'.join(current_content_parts)
                        })
                    # 新しいヘッディング開始
                    current_heading = content
                    current_content_parts = []
                else:
                    # paragraph, list など
                    if content:
                        # contentがlistの場合は改行で結合して文字列化
                        if isinstance(content, list):
                            current_content_parts.append('\n'.join(str(item) for item in content))
                        else:
                            current_content_parts.append(str(content))

            # 最後のブロックを保存
            if current_heading and current_content_parts:
                converted_blocks.append({
                    'title': current_heading,
                    'content': '\n'.join(current_content_parts)
                })

            # Stage H の text_blocks より多ければ置き換え
            if len(converted_blocks) > len(stage_h_text_blocks):
                metadata['text_blocks'] = converted_blocks

        # Stage F の visual_elements からデッドライン情報を抽出
        visual_elements = stage_f_structure.get('visual_elements', {})
        if visual_elements:
            deadline_info = visual_elements.get('deadline_info')
            if deadline_info and not result.get('document_date'):
                logger.info(f"[Stage H] Stage F の deadline_info を document_date として使用: {deadline_info}")
                result['document_date'] = deadline_info

            # 強調されたテキストをタグに追加
            emphasized_text = visual_elements.get('emphasized_text', [])
            if emphasized_text:
                existing_tags = result.get('tags', [])
                for text in emphasized_text:
                    if text and text not in existing_tags:
                        existing_tags.append(text)
                result['tags'] = existing_tags
                logger.info(f"[Stage H] Stage F の emphasized_text をタグに追加: {emphasized_text}")

        result['metadata'] = metadata
        return result

    def _get_fallback_result(self, doc_type: str) -> Dict[str, Any]:
        """
        フォールバック結果を返す

        Args:
            doc_type: ドキュメントタイプ

        Returns:
            最小限の結果
        """
        return {
            'document_date': None,
            'tags': [],
            'metadata': {
                'doc_type': doc_type,
                'extraction_failed': True
            }
        }
```

## shared/pipeline/archive/stage_i_synthesis.py

```python
"""
Stage I: Synthesis (統合・要約)

抽出されたデータと元のテキストを統合し、人間と検索エンジンに最適な形に整形
- 役割: 全情報を統合し、要約・タグ生成・基準日付抽出
- モデル: 設定ファイルで指定（デフォルト: Gemini 2.5 Flash）

D_stage_a_classifier から完全移行
"""
import json
from typing import Dict, Any, Optional
from pathlib import Path
from string import Template
from loguru import logger

from shared.ai.llm_client.llm_client import LLMClient


class StageISynthesis:
    """Stage I: 統合・要約（設定ベース版）"""

    def __init__(self, llm_client: LLMClient):
        """
        Args:
            llm_client: LLMクライアント
        """
        self.llm = llm_client

    def process(
        self,
        combined_text: str,
        stageH_result: Dict[str, Any],
        prompt: str,
        model: str
    ) -> Dict[str, Any]:
        """
        統合・要約（設定ベース版）

        Args:
            combined_text: 統合テキスト
            stageH_result: Stage H の結果
            prompt: プロンプト（config/prompts/stage_i/*.md から読み込み）
            model: モデル名

        Returns:
            {
                'summary': str,
                'relevant_date': str,
                'tags': List[str]
            }
        """
        logger.info(f"[Stage I] 統合・要約開始... (model={model})")

        if not combined_text or not combined_text.strip():
            logger.warning("[Stage I] 入力テキストが空です")
            return self._get_fallback_result(stageH_result)

        try:
            # プロンプト構築
            full_prompt = self._build_prompt(
                prompt_template=prompt,
                combined_text=combined_text,
                stageH_result=stageH_result
            )

            # LLM呼び出し
            response = self.llm.call_model(
                tier="default",
                prompt=full_prompt,
                model_name=model
            )

            if not response.get("success"):
                logger.error(f"[Stage I エラー] LLM呼び出し失敗: {response.get('error')}")
                return self._get_fallback_result(stageH_result)

            # 結果をJSON形式で取得
            content = response.get("content", "")
            logger.info(f"[Stage I] ===== LLMレスポンス全文 ===== {content[:500]}")
            result = self._parse_result(content)

            # Stage H のタグとマージ
            stageH_tags = stageH_result.get('tags', [])
            stageI_tags = result.get('tags', [])
            merged_tags = list(set(stageH_tags + stageI_tags))  # 重複削除

            return {
                'title': result.get('title', ''),
                'summary': result.get('summary', ''),
                'relevant_date': result.get('relevant_date') or stageH_result.get('document_date'),
                'tags': merged_tags,
                'calendar_events': result.get('calendar_events', []),
                'tasks': result.get('tasks', [])
            }

        except Exception as e:
            logger.error(f"[Stage I エラー] 統合・要約失敗: {e}", exc_info=True)
            return self._get_fallback_result(stageH_result)

    def _build_prompt(
        self,
        prompt_template: str,
        combined_text: str,
        stageH_result: Dict[str, Any]
    ) -> str:
        """
        プロンプトを構築

        Args:
            prompt_template: プロンプトテンプレート
            combined_text: 統合テキスト
            stageH_result: Stage H の結果

        Returns:
            構築されたプロンプト
        """
        # Stage H の結果をJSON文字列化
        stageH_json = json.dumps(stageH_result, ensure_ascii=False, indent=2)

        # string.Templateを使用してテンプレート変数を置換（JSONの{}と競合しない）
        template = Template(prompt_template)
        prompt = template.substitute(
            combined_text=combined_text,
            stageH_result=stageH_json
        )

        return prompt

    def _sanitize_llm_json(self, text: str) -> str:
        """
        P1-2: LLM出力をJSONパース前にサニタイズ

        処理順序:
        1. None → "" / strip()
        2. コードフェンス除去 (```json ... ```)
        3. 先頭の json / JSON: ラベル除去
        4. 先頭の {{ → { に縮退
        5. 最初の { から最後の } までを切り出し
        """
        import re

        # 1. None対策とstrip
        if text is None:
            return ""
        text = text.strip()
        if not text:
            return ""

        # 2. コードフェンス除去
        # ```json や ```JSON など
        text = re.sub(r'^```(?:json|JSON)?\s*\n?', '', text)
        text = re.sub(r'\n?```\s*$', '', text)

        # 3. 先頭の json / JSON: ラベル除去
        text = re.sub(r'^(?:json|JSON)\s*[:：]?\s*', '', text.strip())

        # 4. 先頭の {{ → { に縮退（先頭のみ）
        if text.startswith('{{'):
            text = text[1:]

        # 5. 最初の { から最後の } までを切り出し
        first_brace = text.find('{')
        last_brace = text.rfind('}')
        if first_brace >= 0 and last_brace > first_brace:
            text = text[first_brace:last_brace + 1]

        return text

    def _parse_result(self, content: str, doc_id: str = None) -> Dict[str, Any]:
        """
        P1-2/P1-3: LLM出力から結果を抽出（サニタイズ + ログ強化）

        Args:
            content: LLMの出力
            doc_id: ドキュメントID（ログ用）

        Returns:
            抽出された結果
        """
        raw = content or ""
        raw_head = raw[:200] if raw else "(empty)"

        # P1-2: サニタイズ適用
        clean = self._sanitize_llm_json(raw)
        clean_head = clean[:200] if clean else "(empty)"

        # JSON形式で出力されている場合
        try:
            if clean:
                result = json.loads(clean)
                logger.info(f"[Stage I] JSON解析成功 (doc_id={doc_id})")
                return result
        except json.JSONDecodeError as e:
            logger.warning(f"[P1-2] JSON parse failed attempt=1 (doc_id={doc_id}): {e}")
            logger.warning(f"  raw_head: {raw_head}")
            logger.warning(f"  clean_head: {clean_head}")

        # P1-3: フォールバック（テキストから抽出）
        logger.info(f"[Stage I] JSONパース失敗 → テキスト抽出フォールバック (doc_id={doc_id})")
        return self._extract_from_text(raw)

    def _extract_from_text(self, content: str) -> Dict[str, Any]:
        """
        JSON形式でない場合、テキストから情報を抽出（フォールバック）
        """
        import re

        result = {
            'title': '',
            'summary': '',
            'tags': [],
            'relevant_date': None,
            'calendar_events': [],
            'tasks': []
        }

        if not content:
            return result

        # 要約を抽出（最初の段落または全体）
        lines = content.split('\n')
        summary_lines = []
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#') and not line.startswith('-'):
                summary_lines.append(line)
                if len(summary_lines) >= 3:  # 最大3行
                    break

        result['summary'] = ' '.join(summary_lines) if summary_lines else content[:200]

        # タグを抽出（例: タグ: tag1, tag2, tag3）
        tags_match = re.search(r'タグ[:：]\s*(.+)', content, re.IGNORECASE)
        if tags_match:
            tags_str = tags_match.group(1)
            result['tags'] = [t.strip() for t in tags_str.split(',')]

        # 日付を抽出（YYYY-MM-DD形式）
        date_match = re.search(r'(\d{4}-\d{2}-\d{2})', content)
        if date_match:
            result['relevant_date'] = date_match.group(1)

        return result

    def _get_fallback_result(self, stageH_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        フォールバック結果を返す

        Args:
            stageH_result: Stage H の結果

        Returns:
            最小限の結果
        """
        return {
            'summary': '処理に失敗しました',
            'relevant_date': stageH_result.get('document_date'),
            'tags': stageH_result.get('tags', []),
            'calendar_events': [],
            'tasks': []
        }
```

## shared/pipeline/config_loader.py

```python
"""
設定ローダー

models.yaml と pipeline_routes.yaml を読み込み、
doc_type や workspace に応じて適切なプロンプトとモデルを返す
"""
from pathlib import Path
from typing import Dict, Any, Optional
import yaml
from loguru import logger


class ConfigLoader:
    """パイプライン設定ローダー"""

    def __init__(self, config_dir: Optional[Path] = None):
        """
        初期化

        Args:
            config_dir: 設定ディレクトリ（デフォルト: G_unified_pipeline/config/）
        """
        if config_dir is None:
            config_dir = Path(__file__).parent / "config"

        self.config_dir = Path(config_dir)
        self.models_config = self._load_yaml(self.config_dir / "models.yaml")

        # パイプラインルーティング設定を読み込み
        pipeline_routing = self.config_dir / "pipeline_routing.yaml"
        if pipeline_routing.exists():
            self.routes_config = self._load_yaml(pipeline_routing)
            logger.info(f"✅ pipeline_routing.yaml を読み込みました")
        else:
            # フォールバック: 旧 pipeline_routes.yaml
            self.routes_config = self._load_yaml(self.config_dir / "pipeline_routes.yaml")
            logger.warning(f"⚠️ pipeline_routing.yaml が見つかりません。pipeline_routes.yaml を使用します")

        # プロンプト設定を読み込み
        prompts_file = self.config_dir / "prompts.yaml"
        if prompts_file.exists():
            prompts_data = self._load_yaml(prompts_file)
            self.prompts_config = prompts_data.get('prompts', {})
            logger.info(f"✅ prompts.yaml を読み込みました")
        else:
            self.prompts_config = {}
            logger.warning(f"⚠️ prompts.yaml が見つかりません。MDファイルから読み込みます")

        logger.info(f"✅ 設定ローダー初期化完了: {self.config_dir}")

    def _load_yaml(self, file_path: Path) -> Dict[str, Any]:
        """YAML ファイルを読み込む"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"YAML読み込みエラー: {file_path} - {e}")
            return {}

    def get_route_config(self, doc_type: str, workspace: Optional[str] = None) -> Dict[str, Any]:
        """
        doc_type と workspace に基づいてルート設定を取得

        Args:
            doc_type: ドキュメントタイプ
            workspace: ワークスペース（オプション）

        Returns:
            ルート設定（stages ごとの prompt_key, model_key）
        """
        routing = self.routes_config.get('routing', {})

        # 優先順位1: workspace ベースのルート
        if workspace:
            by_workspace = routing.get('by_workspace', {})
            if workspace in by_workspace:
                logger.debug(f"ルート選択: workspace={workspace}")
                return by_workspace[workspace]

        # 優先順位2: doc_type ベースのルート
        by_doc_type = routing.get('by_doc_type', {})
        if doc_type in by_doc_type:
            logger.debug(f"ルート選択: doc_type={doc_type}")
            return by_doc_type[doc_type]

        # 優先順位3: デフォルト
        logger.debug("ルート選択: default")
        return by_doc_type.get('default', {})

    def get_prompt(self, stage: str, prompt_key: str) -> str:
        """
        プロンプトを取得（prompts.yaml または MDファイルから）

        Args:
            stage: ステージ名 (stage_f, stage_g, stage_h, stage_i)
            prompt_key: プロンプトキー (default, flyer, classroom など)

        Returns:
            プロンプトテキスト
        """
        # prompts.yaml から読み込み
        if self.prompts_config and stage in self.prompts_config:
            if prompt_key in self.prompts_config[stage]:
                prompt = self.prompts_config[stage][prompt_key]
                logger.debug(f"プロンプト読み込み: {stage}/{prompt_key} ({len(prompt)}文字)")
                return prompt
            elif prompt_key != 'default' and 'default' in self.prompts_config[stage]:
                # フォールバック: default プロンプトを試す
                logger.warning(f"プロンプトキー '{prompt_key}' が見つかりません。default を使用します: {stage}")
                prompt = self.prompts_config[stage]['default']
                logger.debug(f"プロンプト読み込み: {stage}/default ({len(prompt)}文字)")
                return prompt

        # フォールバック: MDファイルから読み込み（後方互換性）
        prompt_file = self.config_dir / "prompts" / stage / f"{stage}_{prompt_key}.md"
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                prompt = f.read()
                logger.debug(f"プロンプト読み込み（MDファイル）: {stage}/{stage}_{prompt_key}.md ({len(prompt)}文字)")
                return prompt
        except FileNotFoundError:
            logger.warning(f"プロンプトが見つかりません: {stage}/{prompt_key}")
            # 最後のフォールバック: default プロンプトを試す
            if prompt_key != 'default':
                return self.get_prompt(stage, 'default')
            return ""
        except Exception as e:
            logger.error(f"プロンプト読み込みエラー: {prompt_file} - {e}")
            return ""

    def get_model(self, stage: str, model_key: str) -> str:
        """
        モデル名を取得

        Args:
            stage: ステージ名 (stage_f, stage_g, stage_h, stage_i, stage_k)
            model_key: モデルキー (default, flyer, classroom など)

        Returns:
            モデル名
        """
        models = self.models_config.get('models', {})
        stage_models = models.get(stage, {})

        model = stage_models.get(model_key)
        if model:
            logger.debug(f"モデル選択: {stage}/{model_key} → {model}")
            return model

        # フォールバック: default モデル
        default_model = stage_models.get('default')
        if default_model:
            logger.debug(f"モデル選択（フォールバック）: {stage}/default → {default_model}")
            return default_model

        logger.error(f"モデルが見つかりません: {stage}/{model_key}")
        return ""

    def get_stage_config(
        self,
        stage: str,
        doc_type: str,
        workspace: Optional[str] = None
    ) -> Dict[str, str]:
        """
        特定ステージの設定を取得（プロンプト + モデル + custom_handler）

        Args:
            stage: ステージ名 (stage_f, stage_g, stage_h, stage_i)
            doc_type: ドキュメントタイプ
            workspace: ワークスペース（オプション）

        Returns:
            {'prompt': str, 'model': str, 'custom_handler': str (optional), 'skip': bool (optional)}
        """
        route = self.get_route_config(doc_type, workspace)
        stages_config = route.get('stages', {})
        stage_config = stages_config.get(stage, {})

        prompt_key = stage_config.get('prompt_key', 'default')
        model_key = stage_config.get('model_key', 'default')

        result = {
            'prompt': self.get_prompt(stage, prompt_key),
            'model': self.get_model(stage, model_key)
        }

        # custom_handler がある場合は追加
        if 'custom_handler' in stage_config:
            result['custom_handler'] = stage_config['custom_handler']

        # skip フラグがある場合は追加
        if 'skip' in stage_config:
            result['skip'] = stage_config['skip']

        return result

    def get_hybrid_ocr_enabled(self, doc_type: str, workspace: Optional[str] = None) -> bool:
        """
        ハイブリッドOCR（Surya + PaddleOCR）が有効かどうかを取得

        Args:
            doc_type: ドキュメントタイプ
            workspace: ワークスペース（オプション）

        Returns:
            True: ハイブリッドOCR有効
            False: ハイブリッドOCR無効（Gemini Visionのみ）
        """
        hybrid_ocr_config = self.models_config.get('hybrid_ocr', {})

        # workspace ベースの設定を確認
        if workspace and workspace in hybrid_ocr_config:
            return hybrid_ocr_config[workspace]

        # doc_type ベースの設定を確認
        if doc_type in hybrid_ocr_config:
            return hybrid_ocr_config[doc_type]

        # デフォルト設定
        return hybrid_ocr_config.get('default', False)
```

## shared/pipeline/constants.py

```python
"""
Stage F / Stage H 共通定数

v1.1 契約で使用するスキーマバージョンおよび定数を一元管理

【設計 2026-01-26】新Stage F: 10ステップ構成対応
"""

# Stage H 入力スキーマバージョン
STAGE_H_INPUT_SCHEMA_VERSION = "stage_h_input.v1.1"

# Stage F 出力スキーマバージョン
STAGE_F_OUTPUT_SCHEMA_VERSION = "stage_f_output.v2.0"

# block_type の許可値（v1.1）
BLOCK_TYPES_V1_1 = [
    "post_body",      # 投稿本文（最優先文脈、必ず先頭）
    "heading",        # 見出し
    "paragraph",      # 段落
    "list_item",      # 箇条書き
    "table",          # 表（Markdown形式）
    "table_text",     # 表内テキスト
    "note",           # 注記
]

# ============================================
# Stage F: 10ステップ構成の定数
# ============================================

# F-1: Image Normalization
F1_TARGET_DPI = 300  # 統一DPI

# F-2: Surya Block Detection
SURYA_MAX_DIM = 2000  # Suryaリサイズ上限

# F-3: Coordinate Quantization（座標量子化）
QUANTIZE_GRID_SIZE = 1000  # 1000×1000 グリッド

# F-6 OCR 上限（レガシー互換）
MAX_OCR_CALLS = 20
MAX_CROP_LONG_EDGE = 1000  # リサイズ閾値
PER_PAGE_MAX_UNION_ROI = 3
MIN_ROI_AREA = 2000  # 最小ROI面積
UNION_PADDING = 20  # union ROIのpadding (px)

# F-7: Dual Read - Path A
F7_MODEL_IMAGE = "gemini-2.0-flash"  # 画像用（テキストの鬼）
F7_MODEL_AV = "gemini-2.5-flash-lite"  # 音声/動画用

# F-8: Dual Read - Path B
F8_MODEL = "gemini-2.5-flash"  # 構造解析（視覚の鬼）

# F-7/F-8 共通
F7_F8_MAX_TOKENS = 65536
F7_F8_TEMPERATURE = 0.0

# チャンク処理（MAX_TOKENSエラー回避）
# gemini-2.0-flash の出力上限は 8,192 トークンのため、1ページ単位で処理
CHUNK_SIZE_PAGES = 1  # 1ページごとに分割処理

# ============================================
# Stage G / H1 / H2 モデル定義
# ============================================

# Stage G: Integration Refiner
G_MODEL = "gemini-2.5-flash-lite"

# Stage H1: Table Specialist
H1_MODEL = "gemini-2.5-flash-lite"

# Stage H2: Text Specialist
H2_MODEL = "gemini-2.5-flash"
```

## shared/pipeline/image_preprocessing.py

```python
"""
画像前処理ユーティリティ

PaddleOCRの認識精度向上のための画像前処理機能を提供
"""
import cv2
import numpy as np
from typing import Tuple
from loguru import logger


def preprocess_image_for_ocr(
    image: np.ndarray,
    apply_clahe: bool = True,
    apply_denoise: bool = True,
    apply_sharpen: bool = True,
    apply_binarize: bool = False
) -> Tuple[np.ndarray, dict]:
    """
    OCR認識精度向上のための画像前処理

    Args:
        image: 入力画像（numpy array、RGB or Grayscale）
        apply_clahe: CLAHEによるコントラスト調整を適用
        apply_denoise: ノイズ除去を適用
        apply_sharpen: シャープ化を適用
        apply_binarize: 二値化を適用（低品質画像向け）

    Returns:
        (processed_image, stats): 前処理済み画像と統計情報
    """
    stats = {
        'original_shape': image.shape,
        'applied_operations': []
    }

    # RGB → Grayscale 変換
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    else:
        gray = image.copy()

    processed = gray.copy()

    # 1. CLAHE（コントラスト制限適応ヒストグラム均等化）
    if apply_clahe:
        try:
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            processed = clahe.apply(processed)
            stats['applied_operations'].append('CLAHE')
        except Exception as e:
            logger.warning(f"CLAHE処理失敗: {e}")

    # 2. ノイズ除去（Non-local Means Denoising）
    if apply_denoise:
        try:
            processed = cv2.fastNlMeansDenoising(processed, None, h=10, templateWindowSize=7, searchWindowSize=21)
            stats['applied_operations'].append('Denoise')
        except Exception as e:
            logger.warning(f"ノイズ除去処理失敗: {e}")

    # 3. シャープ化（Unsharp Masking）
    if apply_sharpen:
        try:
            # ガウシアンブラーでぼかし画像を作成
            blurred = cv2.GaussianBlur(processed, (0, 0), 3)
            # オリジナル - ぼかし = シャープマスク
            processed = cv2.addWeighted(processed, 1.5, blurred, -0.5, 0)
            stats['applied_operations'].append('Sharpen')
        except Exception as e:
            logger.warning(f"シャープ化処理失敗: {e}")

    # 4. 二値化（Otsuの閾値処理） - オプション
    if apply_binarize:
        try:
            _, processed = cv2.threshold(processed, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            stats['applied_operations'].append('Binarize')
        except Exception as e:
            logger.warning(f"二値化処理失敗: {e}")

    # 5. PaddleOCR互換性のためRGB形式に変換
    # PaddleOCRはRGB/カラー画像を期待するため、グレースケールから戻す
    if len(processed.shape) == 2:
        processed = cv2.cvtColor(processed, cv2.COLOR_GRAY2RGB)
        stats['applied_operations'].append('GrayToRGB')

    stats['final_shape'] = processed.shape
    stats['operations_count'] = len(stats['applied_operations'])

    return processed, stats


def adaptive_preprocess(
    image: np.ndarray,
    confidence_threshold: float = 0.7
) -> np.ndarray:
    """
    低信頼度領域に対する適応的前処理

    通常の前処理で認識精度が低い場合に、より強力な前処理を適用

    Args:
        image: 入力画像
        confidence_threshold: この閾値以下の場合、強力な前処理を適用

    Returns:
        前処理済み画像
    """
    # より強力な前処理: 二値化 + ノイズ除去 + シャープ化
    processed, _ = preprocess_image_for_ocr(
        image,
        apply_clahe=True,
        apply_denoise=True,
        apply_sharpen=True,
        apply_binarize=True  # 二値化を有効化
    )

    return processed


def preprocess_for_ppstructure(
    image: np.ndarray,
    enhance_contrast: bool = True,
    sharpen: bool = True
) -> Tuple[np.ndarray, dict]:
    """
    P2-1: PPStructure表検出用の画像前処理

    表罫線を強調し、PPStructureの検出精度を向上させる

    Args:
        image: 入力画像（numpy array、RGB）
        enhance_contrast: コントラスト強調を適用
        sharpen: シャープ化を適用

    Returns:
        (processed_image, stats): 前処理済み画像と統計情報
    """
    stats = {
        'original_shape': image.shape,
        'applied_operations': []
    }

    processed = image.copy()

    # 1. コントラスト強調（LABカラースペースで輝度のみ調整）
    if enhance_contrast:
        try:
            # RGB → LAB
            lab = cv2.cvtColor(processed, cv2.COLOR_RGB2LAB)
            l, a, b = cv2.split(lab)

            # CLAHE を L チャンネルに適用
            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
            l = clahe.apply(l)

            # LAB → RGB
            lab = cv2.merge([l, a, b])
            processed = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
            stats['applied_operations'].append('CLAHE_LAB')
        except Exception as e:
            logger.warning(f"[P2-1] コントラスト強調失敗: {e}")

    # 2. シャープ化（表罫線を強調）
    if sharpen:
        try:
            # Unsharp Masking
            blurred = cv2.GaussianBlur(processed, (0, 0), 2)
            processed = cv2.addWeighted(processed, 1.8, blurred, -0.8, 0)
            stats['applied_operations'].append('Sharpen')
        except Exception as e:
            logger.warning(f"[P2-1] シャープ化失敗: {e}")

    stats['final_shape'] = processed.shape
    stats['operations_count'] = len(stats['applied_operations'])
    stats['preproc'] = 'on' if stats['operations_count'] > 0 else 'off'

    return processed, stats


def calculate_image_quality_score(image: np.ndarray) -> float:
    """
    画像品質スコアを計算（0.0～1.0）

    ぼやけ具合やコントラストを評価し、前処理の必要性を判定

    Args:
        image: 入力画像

    Returns:
        品質スコア（高いほど高品質）
    """
    try:
        # グレースケール変換
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image

        # 1. ぼやけ検出（Laplacian分散）
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        blur_score = min(laplacian_var / 100.0, 1.0)  # 正規化

        # 2. コントラスト評価（標準偏差）
        contrast_score = min(gray.std() / 50.0, 1.0)

        # 総合スコア
        quality_score = (blur_score * 0.6 + contrast_score * 0.4)

        return quality_score

    except Exception as e:
        logger.warning(f"画像品質評価失敗: {e}")
        return 0.5  # デフォルト値
```

## shared/pipeline/ocr_config.py

```python
"""
OCRエンジン設定とキャッシング

PaddleOCRの設定、バージョン検出、結果キャッシング機能を提供
"""
import hashlib
import json
import pickle
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger
import time


class OCRConfig:
    """OCRエンジンの設定管理"""

    # 信頼度閾値
    CONFIDENCE_THRESHOLD_LOW = 0.5      # これ以下は無視
    CONFIDENCE_THRESHOLD_MID = 0.7      # これ以下は再処理
    CONFIDENCE_THRESHOLD_HIGH = 0.85    # これ以上は高品質

    # 並列処理設定
    ENABLE_PARALLEL = True               # 並列処理を有効化
    MAX_WORKERS = 4                      # 最大ワーカー数

    # キャッシング設定
    ENABLE_CACHE = True                  # 結果キャッシングを有効化
    CACHE_DIR = Path("cache/ocr_results")  # キャッシュディレクトリ
    CACHE_TTL = 86400                    # キャッシュ有効期限（秒）

    # OCRエンジン優先順位
    OCR_ENGINE_PRIORITY = [
        'paddleocr',  # 1. PaddleOCR（日本語に強い）
        'surya',      # 2. Surya（レイアウト解析）
        'gemini'      # 3. Gemini Vision（フォールバック）
    ]

    # 画像前処理設定
    ENABLE_PREPROCESSING = True          # 画像前処理を有効化
    PREPROCESSING_QUALITY_THRESHOLD = 0.7  # この品質以下で前処理適用

    # Geminiフォールバック設定
    ENABLE_GEMINI_FALLBACK = True        # 低信頼度時Geminiフォールバック
    GEMINI_FALLBACK_THRESHOLD = 0.6      # この信頼度以下でフォールバック


class OCRResultCache:
    """OCR結果のキャッシング"""

    def __init__(self, cache_dir: Path = OCRConfig.CACHE_DIR, ttl: int = OCRConfig.CACHE_TTL):
        """
        Args:
            cache_dir: キャッシュディレクトリ
            ttl: キャッシュ有効期限（秒）
        """
        self.cache_dir = cache_dir
        self.ttl = ttl
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.hits = 0
        self.misses = 0

    def _get_cache_key(self, image_data: bytes, config: Dict[str, Any]) -> str:
        """
        画像とconfigからキャッシュキーを生成

        Args:
            image_data: 画像のバイトデータ
            config: OCR設定

        Returns:
            SHA256ハッシュキー
        """
        config_str = json.dumps(config, sort_keys=True)
        combined = image_data + config_str.encode('utf-8')
        return hashlib.sha256(combined).hexdigest()

    def get(self, image_data: bytes, config: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        キャッシュから結果を取得

        Args:
            image_data: 画像のバイトデータ
            config: OCR設定

        Returns:
            キャッシュされた結果、または None
        """
        if not OCRConfig.ENABLE_CACHE:
            return None

        cache_key = self._get_cache_key(image_data, config)
        cache_file = self.cache_dir / f"{cache_key}.pkl"

        if not cache_file.exists():
            self.misses += 1
            return None

        try:
            # キャッシュファイルの有効期限確認
            file_age = time.time() - cache_file.stat().st_mtime
            if file_age > self.ttl:
                cache_file.unlink()  # 期限切れ削除
                self.misses += 1
                return None

            # キャッシュ読み込み
            with open(cache_file, 'rb') as f:
                result = pickle.load(f)

            self.hits += 1
            logger.debug(f"[Cache] HIT: {cache_key[:8]}... (age: {file_age:.0f}s)")
            return result

        except Exception as e:
            logger.warning(f"[Cache] 読み込み失敗: {e}")
            self.misses += 1
            return None

    def set(self, image_data: bytes, config: Dict[str, Any], result: Dict[str, Any]):
        """
        結果をキャッシュに保存

        Args:
            image_data: 画像のバイトデータ
            config: OCR設定
            result: OCR結果
        """
        if not OCRConfig.ENABLE_CACHE:
            return

        cache_key = self._get_cache_key(image_data, config)
        cache_file = self.cache_dir / f"{cache_key}.pkl"

        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(result, f)
            logger.debug(f"[Cache] SAVE: {cache_key[:8]}...")
        except Exception as e:
            logger.warning(f"[Cache] 保存失敗: {e}")

    def clear(self):
        """キャッシュをクリア"""
        try:
            for cache_file in self.cache_dir.glob("*.pkl"):
                cache_file.unlink()
            logger.info(f"[Cache] クリア完了: {self.cache_dir}")
        except Exception as e:
            logger.warning(f"[Cache] クリア失敗: {e}")

    def get_stats(self) -> Dict[str, Any]:
        """キャッシュ統計を取得"""
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0

        cache_files = list(self.cache_dir.glob("*.pkl"))
        total_size = sum(f.stat().st_size for f in cache_files)

        return {
            'hits': self.hits,
            'misses': self.misses,
            'total_requests': total,
            'hit_rate': hit_rate,
            'cache_files': len(cache_files),
            'total_size_mb': total_size / (1024 * 1024)
        }


class PaddleOCRVersionAdapter:
    """PaddleOCRバージョン互換性アダプター"""

    @staticmethod
    def detect_version() -> str:
        """PaddleOCRのバージョンを検出"""
        try:
            import paddleocr
            version = getattr(paddleocr, '__version__', 'unknown')
            logger.info(f"[PaddleOCR] Version detected: {version}")
            return version
        except Exception as e:
            logger.warning(f"[PaddleOCR] Version detection failed: {e}")
            return 'unknown'

    @staticmethod
    def extract_result(ocr_result: Any) -> tuple:
        """
        PaddleOCRの結果から統一的にテキストと信頼度を抽出

        Args:
            ocr_result: PaddleOCRの結果オブジェクト

        Returns:
            (texts: List[str], confidences: List[float])
        """
        texts = []
        confidences = []

        try:
            # PaddleOCR 3.x: 辞書ライクオブジェクト
            if isinstance(ocr_result, dict) or hasattr(ocr_result, '__getitem__'):
                rec_texts = ocr_result.get('rec_texts', []) if hasattr(ocr_result, 'get') else ocr_result.get('rec_texts', [])
                rec_scores = ocr_result.get('rec_scores', []) if hasattr(ocr_result, 'get') else ocr_result.get('rec_scores', [])

                if rec_texts:
                    texts = list(rec_texts)
                    confidences = list(rec_scores) if rec_scores else []

            # PaddleOCR 2.x: リスト形式
            elif isinstance(ocr_result, list):
                for line in ocr_result:
                    if line and len(line) >= 2 and line[1]:
                        texts.append(line[1][0])
                        confidences.append(line[1][1])

        except Exception as e:
            logger.warning(f"[PaddleOCR] Result extraction failed: {e}")

        return texts, confidences
```

## shared/pipeline/ocr_report.py

```python
"""
OCR認識精度レポート生成

OCR処理の詳細な統計とレポートを生成
"""
from typing import Dict, Any, List
from dataclasses import dataclass, field
from datetime import datetime
import json


@dataclass
class OCRRegionStats:
    """領域ごとの統計"""
    region_id: int
    bbox: List[int]
    text_length: int
    confidence: float
    preprocessing_applied: bool
    reprocessed: bool = False
    improvement: float = 0.0


@dataclass
class OCRProcessingReport:
    """OCR処理レポート"""
    # 基本情報
    file_name: str
    processing_time: float
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

    # 領域統計
    total_regions: int = 0
    recognized_regions: int = 0
    low_confidence_regions: int = 0
    reprocessed_regions: int = 0
    improved_regions: int = 0

    # 認識統計
    total_chars: int = 0
    avg_confidence: float = 0.0
    min_confidence: float = 1.0
    max_confidence: float = 0.0

    # 前処理統計
    preprocessed_regions: int = 0
    preprocessing_time: float = 0.0

    # 再処理統計
    reprocessing_time: float = 0.0
    avg_improvement: float = 0.0

    # キャッシュ統計
    cache_hits: int = 0
    cache_misses: int = 0

    # 領域詳細
    regions: List[OCRRegionStats] = field(default_factory=list)

    def add_region(self, region_stats: OCRRegionStats):
        """領域統計を追加"""
        self.regions.append(region_stats)
        self.total_regions += 1

        if region_stats.text_length > 0:
            self.recognized_regions += 1
            self.total_chars += region_stats.text_length

        if region_stats.confidence < 0.7:
            self.low_confidence_regions += 1

        if region_stats.preprocessing_applied:
            self.preprocessed_regions += 1

        if region_stats.reprocessed:
            self.reprocessed_regions += 1
            if region_stats.improvement > 0:
                self.improved_regions += 1

        # 信頼度統計更新
        if region_stats.confidence > 0:
            self.min_confidence = min(self.min_confidence, region_stats.confidence)
            self.max_confidence = max(self.max_confidence, region_stats.confidence)

    def calculate_final_stats(self):
        """最終統計を計算"""
        if self.regions:
            confidences = [r.confidence for r in self.regions if r.confidence > 0]
            self.avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0

            improvements = [r.improvement for r in self.regions if r.reprocessed and r.improvement > 0]
            self.avg_improvement = sum(improvements) / len(improvements) if improvements else 0.0

    def to_dict(self) -> Dict[str, Any]:
        """辞書形式に変換"""
        return {
            'file_name': self.file_name,
            'processing_time': self.processing_time,
            'timestamp': self.timestamp,
            'summary': {
                'total_regions': self.total_regions,
                'recognized_regions': self.recognized_regions,
                'recognition_rate': self.recognized_regions / self.total_regions if self.total_regions > 0 else 0,
                'low_confidence_regions': self.low_confidence_regions,
                'total_chars': self.total_chars,
            },
            'confidence': {
                'average': self.avg_confidence,
                'min': self.min_confidence,
                'max': self.max_confidence,
            },
            'preprocessing': {
                'preprocessed_regions': self.preprocessed_regions,
                'preprocessing_rate': self.preprocessed_regions / self.total_regions if self.total_regions > 0 else 0,
                'preprocessing_time': self.preprocessing_time,
            },
            'reprocessing': {
                'reprocessed_regions': self.reprocessed_regions,
                'improved_regions': self.improved_regions,
                'improvement_rate': self.improved_regions / self.reprocessed_regions if self.reprocessed_regions > 0 else 0,
                'avg_improvement': self.avg_improvement,
                'reprocessing_time': self.reprocessing_time,
            },
            'cache': {
                'cache_hits': self.cache_hits,
                'cache_misses': self.cache_misses,
                'cache_hit_rate': self.cache_hits / (self.cache_hits + self.cache_misses) if (self.cache_hits + self.cache_misses) > 0 else 0,
            }
        }

    def to_json(self, indent: int = 2) -> str:
        """JSON文字列に変換"""
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=indent)

    def print_summary(self):
        """サマリーを表示"""
        print("\n" + "=" * 60)
        print(f"OCR Processing Report: {self.file_name}")
        print("=" * 60)
        print(f"Total Regions: {self.total_regions}")
        print(f"Recognized: {self.recognized_regions}/{self.total_regions} ({self.recognized_regions/self.total_regions*100:.1f}%)")
        print(f"Total Characters: {self.total_chars}")
        print(f"Average Confidence: {self.avg_confidence:.2%}")
        print(f"Low Confidence Regions: {self.low_confidence_regions} (< 0.7)")
        print(f"\nPreprocessing:")
        print(f"  Preprocessed: {self.preprocessed_regions}/{self.total_regions} ({self.preprocessed_regions/self.total_regions*100:.1f}%)")
        print(f"  Time: {self.preprocessing_time:.2f}s")
        print(f"\nReprocessing:")
        print(f"  Reprocessed: {self.reprocessed_regions}")
        print(f"  Improved: {self.improved_regions}/{self.reprocessed_regions}")
        if self.reprocessed_regions > 0:
            print(f"  Improvement Rate: {self.improved_regions/self.reprocessed_regions*100:.1f}%")
            print(f"  Avg Improvement: +{self.avg_improvement:.2%}")
        print(f"  Time: {self.reprocessing_time:.2f}s")
        print(f"\nCache:")
        total_cache = self.cache_hits + self.cache_misses
        if total_cache > 0:
            print(f"  Hits: {self.cache_hits}/{total_cache} ({self.cache_hits/total_cache*100:.1f}%)")
        print(f"\nTotal Processing Time: {self.processing_time:.2f}s")
        print("=" * 60 + "\n")
```

## shared/pipeline/pipeline.py

```python
"""
統合ドキュメント処理パイプライン (Stage E-K) - 設定ベース版

設計書: DESIGN_UNIFIED_PIPELINE.md v2.0 に準拠
処理順序: Stage E → F → G → H1 → H2 → J → K

Stage概要:
- Stage E: Pre-processing（テキスト抽出）
- Stage F: Visual Analysis（視覚解析、gemini-2.5-pro）
         - 物理的OCR抽出、JSON出力（カラムナ形式）
- Stage G: Logical Refinement（論理的精錬、gemini-2.0-flash-lite）
         - 重複排除、REF_ID付与、unified_text生成
- Stage H1: Table Specialist（表処理専門）
         - 定型表・構造化表を先に処理
         - カラムナ形式→辞書リスト変換
         - H2への入力量削減のため表テキストを抽出
- Stage H2: Text Specialist（テキスト処理専門、gemini-2.0-flash）
         - 軽量化されたテキストで構造化 + 要約
         - calendar_events, tasks, title, summary を生成
         - audit_canonical_text（監査用正本）を生成
- Stage J: Chunking（チャンク化）
- Stage K: Embedding（ベクトル化）

特徴:
- doc_type / workspace に応じて自動的にプロンプトとモデルを切り替え
- config/ 内の YAML と Markdown ファイルで設定管理
- Stage G で REF_ID付き目録を生成し、後続ステージが参照可能
- H1 + H2 分割によりトークン消費を削減
"""
import asyncio
import json
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger

from shared.ai.llm_client.llm_client import LLMClient
from shared.common.database.client import DatabaseClient
from shared.common.connectors.google_drive import GoogleDriveConnector

from .config_loader import ConfigLoader
from .stage_e_preprocessing import StageEPreprocessor
from .stage_f_visual import StageFVisualAnalyzer
from .stage_g_refiner import StageGRefiner  # Stage G: 論理的精錬
from .stage_hi_combined import StageHICombined  # Stage H+I: 統合版（後方互換）
from .stage_h1_table import StageH1Table  # Stage H1: 表処理専門
from .stage_h2_text import StageH2Text  # Stage H2: テキスト処理専門
from .stage_h_kakeibo import StageHKakeibo  # 家計簿専用
from .stage_j_chunking import StageJChunking
from .stage_k_embedding import StageKEmbedding
from .constants import STAGE_H_INPUT_SCHEMA_VERSION

# Phase 5: Execution versioning
from shared.processing.execution_manager import ExecutionManager, ExecutionContext

# 家計簿専用のDB保存ハンドラー (オプショナル)
try:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from shared.kakeibo.kakeibo_db_handler import KakeiboDBHandler
    KAKEIBO_AVAILABLE = True
except ImportError:
    logger.warning("K_kakeibo module not available, kakeibo features will be disabled")
    KakeiboDBHandler = None
    KAKEIBO_AVAILABLE = False


# ============================================
# v1.1 契約: post_body は Rawdata_FILE_AND_MAIL.display_post_text から取得
# ============================================
def _build_post_body(raw_doc: dict | None) -> dict:
    """
    post_body を Rawdata_FILE_AND_MAIL.display_post_text から直接取得。
    GAS で classroom/gmail/drive 全てこのカラムに本文を保存している。

    Returns:
        { "text": str, "source": str, "char_count": int }
    """
    if not isinstance(raw_doc, dict):
        return {"text": "", "source": "no_raw_doc", "char_count": 0}

    text = (raw_doc.get("display_post_text") or "").strip()
    if text:
        return {"text": text, "source": "rawdata.display_post_text", "char_count": len(text)}

    return {"text": "", "source": "empty", "char_count": 0}


class UnifiedDocumentPipeline:
    """統合ドキュメント処理パイプライン (Stage E-K) - 設定ベース版"""

    @staticmethod
    def _sanitize_text(text: str) -> str:
        """
        テキストからnull文字を除去

        Args:
            text: 入力テキスト

        Returns:
            サニタイズ済みテキスト
        """
        if not text:
            return text
        # null文字 (\u0000) を除去
        return text.replace('\u0000', '')

    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        db_client: Optional[DatabaseClient] = None,
        config_dir: Optional[Path] = None,
        enable_hybrid_ocr: Optional[bool] = None
    ):
        """
        Args:
            llm_client: LLMクライアント（Noneの場合は新規作成）
            db_client: データベースクライアント（Noneの場合は新規作成）
            config_dir: 設定ディレクトリ（デフォルト: G_unified_pipeline/config/）
            enable_hybrid_ocr: ハイブリッドOCR（Surya + PaddleOCR）を有効化（Noneの場合は設定ファイルから取得）
        """
        self.llm_client = llm_client or LLMClient()
        self.db = db_client or DatabaseClient(use_service_role=True)  # RLSバイパスのためService Role使用
        self.drive_connector = GoogleDriveConnector()  # Google Drive ファイル名更新用

        # 設定ローダーを初期化
        self.config = ConfigLoader(config_dir)

        # ハイブリッドOCRの有効/無効を決定
        if enable_hybrid_ocr is None:
            # 設定ファイルから取得（デフォルトの設定）
            enable_hybrid_ocr = self.config.get_hybrid_ocr_enabled('default')

        # 各ステージを初期化
        self.stage_e = StageEPreprocessor(self.llm_client)
        self.stage_f = StageFVisualAnalyzer(self.llm_client, enable_surya=enable_hybrid_ocr)
        self.stage_g = StageGRefiner(self.llm_client)  # Stage G: 論理的精錬
        self.stage_hi = StageHICombined(self.llm_client)  # Stage H+I: 統合版（後方互換）
        self.stage_h1 = StageH1Table(self.llm_client)  # Stage H1: 表処理専門
        self.stage_h2 = StageH2Text(self.llm_client)  # Stage H2: テキスト処理専門
        self.stage_h_kakeibo = StageHKakeibo(self.db)  # 家計簿専用
        self.stage_j = StageJChunking()
        self.stage_k = StageKEmbedding(self.llm_client, self.db)

        # 家計簿専用のDB保存ハンドラー
        self.kakeibo_db_handler = KakeiboDBHandler(self.db) if KAKEIBO_AVAILABLE else None

        logger.info(f"✅ UnifiedDocumentPipeline 初期化完了（E→F→G→H1→H2→J→K, ハイブリッドOCR={'有効' if enable_hybrid_ocr else '無効'}）")

    async def process_document(
        self,
        file_path: Path,
        file_name: str,
        doc_type: str,
        workspace: str,
        mime_type: str,
        source_id: str,
        existing_document_id: Optional[str] = None,
        extra_metadata: Optional[Dict[str, Any]] = None,
        progress_callback=None,
        owner_id: Optional[str] = None,
        enable_execution_tracking: bool = False
    ) -> Dict[str, Any]:
        """
        ドキュメントを処理（Stage E-K）

        Args:
            file_path: ファイルパス
            file_name: ファイル名
            doc_type: ドキュメントタイプ（設定ルーティングに使用）
            workspace: ワークスペース
            mime_type: MIMEタイプ
            source_id: ソースID
            existing_document_id: 更新する既存ドキュメントID（Noneの場合は新規作成）
            extra_metadata: 追加メタデータ（Classroom固有フィールドなど）
            progress_callback: 進捗コールバック
            owner_id: オーナーID（Phase 3 必須 for kakeibo）
            enable_execution_tracking: Phase 5 execution versioning を有効化

        Returns:
            処理結果 {'success': bool, 'document_id': str, ...}
        """
        # Phase 5: Execution tracking 初期化
        execution_context: Optional[ExecutionContext] = None
        execution_manager: Optional[ExecutionManager] = None
        start_time = None

        if enable_execution_tracking:
            import time
            start_time = time.time()
            execution_manager = ExecutionManager(self.db)

        try:
            logger.info(f"📄 ドキュメント処理開始: {file_name} (doc_type={doc_type}, workspace={workspace})")

            # ============================================
            # Stage E: Pre-processing
            # ============================================
            logger.info("[Stage E] Pre-processing開始...")
            if progress_callback:
                progress_callback("E1")

            # extra_metadata から既に抽出済みのテキスト（attachment_text）を取得
            # HTMLファイル等、Ingestion時にテキスト抽出済みの場合に使用
            pre_extracted_text = extra_metadata.get('attachment_text', '') if extra_metadata else ''

            stage_e_result = self.stage_e.extract_text(
                file_path,
                mime_type,
                pre_extracted_text=pre_extracted_text,
                workspace=workspace,
                progress_callback=progress_callback
            )

            # イベントループに制御を返す（並列タスク実行のため）
            await asyncio.sleep(0)

            # Stage E の結果をチェック
            if not stage_e_result.get('success'):
                error_msg = f"Stage E失敗: {stage_e_result.get('error', 'テキスト抽出エラー')}"
                logger.error(f"[Stage E失敗] {error_msg}")
                return {'success': False, 'error': error_msg}

            extracted_text = stage_e_result.get('content', '')
            # P2-2: E-2で検出した表のbbox情報を取得
            stage_e_metadata = stage_e_result.get('metadata', {})
            e2_table_bboxes = stage_e_metadata.get('table_bboxes', [])
            # ログ出力は Stage E 内で既に実施済み

            # ============================================
            # Stage F: Visual Analysis (gemini-2.5-pro で完璧に仕上げる)
            # ============================================
            # post_body 作成（投稿本文 = Stage H 最優先文脈）
            # 【v1.1契約】Rawdata_FILE_AND_MAIL から本文を優先取得
            raw_doc = None
            if existing_document_id:
                try:
                    r = self.db.client.table("Rawdata_FILE_AND_MAIL").select(
                        "id, display_post_text, attachment_text"
                    ).eq("id", existing_document_id).limit(1).execute()
                    if r and getattr(r, "data", None):
                        raw_doc = r.data[0]
                        logger.info(f"[Stage F] raw_doc取得: id={existing_document_id}")
                except Exception as e:
                    logger.warning(f"[Stage F] raw_doc取得失敗: {e.__class__.__name__}: {e}")

            post_body = _build_post_body(raw_doc)
            logger.info(f"[Stage F] post_body作成: {post_body['char_count']}文字 (source: {post_body['source']})")

            # P0-4: Stage F 直前の存在チェック（ファイルがある場合のみ）
            if file_path is not None and not file_path.exists():
                error_msg = f"[P0-4] TEMP_PDF_MISSING: Stage F 入力ファイルが存在しません: {file_path}"
                logger.error(error_msg)
                return {
                    'success': False,
                    'error': error_msg,
                    'failure_stage': 'F',
                    'failure_reason': 'TEMP_PDF_MISSING'
                }

            # 設定から Stage F のプロンプトとモデルを取得
            stage_f_config = self.config.get_stage_config('stage_f', doc_type, workspace)
            prompt_f = stage_f_config['prompt']
            model_f = stage_f_config['model']

            # P0-1: 明示的に file_path を渡す（state 参照禁止）
            logger.info(f"[Stage F] Visual Analysis開始... (model={model_f})")
            if file_path is not None:
                logger.info(f"[P0-1] 入力ファイル: {file_path} (exists={file_path.exists()})")
            else:
                logger.info("[P0-1] 入力ファイル: なし（テキストのみ）")
            # P2-2: E-2のtable_bboxes情報をログ出力
            if e2_table_bboxes:
                logger.info(f"[P2-2] Stage Fへ渡す E-2 table_bboxes: {len(e2_table_bboxes)}個")

            if progress_callback:
                progress_callback("F")

            # Stage Eの判定結果を直接使用（再計算しない）
            requires_vision = stage_e_result.get('requires_vision', False)
            requires_transcription = stage_e_result.get('requires_transcription', False)

            # Stage F 呼び出し（正攻法: 全引数を正しく渡す）
            stage_f_result = self.stage_f.process(
                file_path=file_path,
                mime_type=mime_type or '',
                requires_vision=requires_vision,
                requires_transcription=requires_transcription,
                post_body=post_body,
                progress_callback=progress_callback,
                # YAMLから読み込んだ設定を渡す
                prompt=prompt_f,
                model=model_f,
                extracted_text=extracted_text,
                workspace=workspace,
                e2_table_bboxes=e2_table_bboxes
            )
            logger.info(f"[Stage F完了] Vision結果: {type(stage_f_result).__name__}")

            # イベントループに制御を返す（並列タスク実行のため）
            await asyncio.sleep(0)

            # ============================================
            # Stage F 結果処理（Dict型を直接使用 - dumps/loads排除）
            # ============================================
            try:
                # Stage F は Dict を直接返す（JSONの往復変換を排除）
                vision_json = stage_f_result
                # DB保存用にJSON文字列も保持（vision_raw）
                vision_raw = json.dumps(stage_f_result, ensure_ascii=False)

                # v1.1契約: Stage F payload をそのまま stage_f_structure として使用（再構成禁止）
                stage_f_structure = vision_json
                schema_ver = vision_json.get('schema_version', '')
                is_v1_1 = (schema_ver == STAGE_H_INPUT_SCHEMA_VERSION)

                if is_v1_1:
                    # v1.1: full_text をそのまま使用（混ぜ物合成禁止）
                    combined_text = vision_json.get('full_text', '')
                    post_body = vision_json.get('post_body', {})
                    text_blocks = vision_json.get('text_blocks', [])

                    logger.info(f"[Stage F→H] v1.1契約モード:")
                    logger.info(f"  ├─ schema_version: {schema_ver}")
                    logger.info(f"  ├─ full_text: {len(combined_text)}文字")
                    logger.info(f"  ├─ post_body: {post_body.get('char_count', 0)}文字 (source: {post_body.get('source', 'unknown')})")
                    logger.info(f"  ├─ text_blocks: {len(text_blocks)}ブロック")
                    logger.info(f"  ├─ text_blocks[0]: {text_blocks[0].get('block_type') if text_blocks else 'N/A'}")
                    logger.info(f"  └─ tables: {len(vision_json.get('tables', []))}個")
                else:
                    # レガシー: 従来の合成ロジック（後方互換）
                    ocr_text = vision_json.get('full_text', '')
                    text_parts = []

                    # 1. 投稿文テキスト（Classroom等のメタデータから）
                    if extra_metadata:
                        display_post_text = extra_metadata.get('display_post_text', '')
                        if display_post_text and display_post_text.strip():
                            text_parts.append(f"[投稿文]\n{display_post_text}")
                            logger.info(f"[Stage F→H] display_post_text追加: {len(display_post_text)}文字")

                    # 2. OCR抽出テキスト
                    if ocr_text and ocr_text.strip():
                        text_parts.append(f"[OCR抽出テキスト]\n{ocr_text}")

                    # 3. 画像の視覚的説明（visual_elements.notes）
                    visual_elements = vision_json.get('visual_elements', {})
                    notes = visual_elements.get('notes', [])
                    if notes:
                        notes_text = '\n'.join(notes)
                        text_parts.append(f"[画像の視覚的説明]\n{notes_text}")
                        logger.info(f"[Stage F→H] visual_elements.notes追加: {len(notes_text)}文字")

                    combined_text = '\n\n'.join(text_parts)

                    logger.info(f"[Stage F→H] レガシーモード:")
                    logger.info(f"  ├─ combined_text: {len(combined_text)}文字")
                    logger.info(f"  ├─ OCR full_text: {len(ocr_text)}文字")
                    logger.info(f"  ├─ sections: {len(vision_json.get('layout_info', {}).get('sections', []))}個")
                    logger.info(f"  └─ tables: {len(vision_json.get('layout_info', {}).get('tables', []))}個")
            except json.JSONDecodeError as e:
                logger.warning(f"[Stage F→H] JSON解析失敗: {e}")
                combined_text = vision_raw
                stage_f_structure = None

            # 空のコンテンツをチェック（空のドキュメントは警告のみ、エラーではない）
            if not combined_text or not combined_text.strip():
                logger.warning(f"[Stage F→H] 統合テキストが空です（テキストのないドキュメントの可能性）")
                combined_text = ""  # 空文字列として継続

            # ============================================
            # Stage G: 論理的精錬（Logical Refinement）
            # ============================================
            # Stage F の出力を整理し、REF_ID付き目録を作成
            stage_g_result = None
            stage_g_config = self.config.get_stage_config('stage_g', doc_type, workspace)

            # Stage G スキップ判定（家計簿や skip 設定がある場合）
            skip_stage_g = stage_g_config.get('skip', False) or doc_type == 'kakeibo'

            if stage_f_structure and not skip_stage_g:
                model_g = stage_g_config.get('model', 'gemini-2.0-flash-lite')
                logger.info(f"[Stage G] 論理的精錬開始... (model={model_g})")
                if progress_callback:
                    progress_callback("G")

                try:
                    # Stage E + Stage F を Stage G に渡す（v2.0: G-Gate + G1 + G2）
                    stage_g_result = self.stage_g.process(
                        stage_e_result=stage_e_result,
                        stage_f_payload=stage_f_structure,
                        post_body=post_body,
                        model=model_g,
                        workspace=workspace
                    )

                    # Stage G の出力をログ
                    logger.info(f"[Stage G完了] ref_count={stage_g_result.get('ref_count', 0)}, mode={stage_g_result.get('processing_mode', 'unknown')}")

                    # Stage G の unified_text を combined_text として使用（後続に渡す）
                    if stage_g_result.get('unified_text'):
                        combined_text = stage_g_result['unified_text']
                        logger.info(f"[Stage G→H] unified_text: {len(combined_text)}文字")

                    # Stage G の source_inventory を stage_f_structure に追加（Stage Hで参照可能に）
                    if stage_g_result.get('source_inventory'):
                        stage_f_structure['source_inventory'] = stage_g_result['source_inventory']
                        logger.info(f"[Stage G→H] source_inventory: {len(stage_g_result['source_inventory'])}件")

                    if stage_g_result.get('table_inventory'):
                        stage_f_structure['table_inventory'] = stage_g_result['table_inventory']
                        logger.info(f"[Stage G→H] table_inventory: {len(stage_g_result['table_inventory'])}件")

                    # 警告があれば出力
                    for warning in stage_g_result.get('warnings', []):
                        logger.warning(f"[Stage G警告] {warning}")

                except Exception as e:
                    logger.warning(f"[Stage G] 処理失敗、スキップして続行: {e}")
                    # Stage G が失敗しても Stage H は続行可能

                # イベントループに制御を返す（並列タスク実行のため）
                await asyncio.sleep(0)
            elif skip_stage_g:
                logger.info(f"[Stage G] スキップ (doc_type={doc_type}, skip={stage_g_config.get('skip', False)})")

            # ============================================
            # Stage H+I: 構造化 + 統合・要約
            # ============================================
            # custom_handler の確認（ルート設定から直接取得、model は取得しない）
            route_config = self.config.get_route_config(doc_type, workspace)
            stage_h_routing = route_config.get('stages', {}).get('stage_h', {})
            custom_handler = stage_h_routing.get('custom_handler')

            # 家計簿専用処理の場合（統合版は使わない）
            if custom_handler == 'kakeibo':
                # 家計簿の場合のみ stage_h_config を取得
                stage_h_config = self.config.get_stage_config('stage_h', doc_type, workspace)
                logger.info(f"[Stage H] 家計簿構造化開始... (custom_handler=kakeibo)")
                if progress_callback:
                    progress_callback("H")

                # Stage F の出力を辞書に変換（combined_text が JSON 文字列の場合）
                # ※ json, re はモジュールレベルでインポート済み
                try:
                    # Markdownのコードブロック (```json ... ```) を除去
                    json_text = combined_text.strip()
                    if json_text.startswith('```'):
                        # 最初と最後の```を除去
                        json_text = re.sub(r'^```(?:json)?\s*\n', '', json_text)
                        json_text = re.sub(r'\n```\s*$', '', json_text)

                    logger.debug(f"[Stage H] JSON パース前の最初の500文字:\n{json_text[:500]}")
                    stage_f_output = json.loads(json_text)
                except (json.JSONDecodeError, TypeError) as e:
                    logger.error(f"[Stage H] combined_text が JSON 形式ではありません: {e}")
                    logger.error(f"[Stage H] combined_text の内容:\n{combined_text[:1000]}")
                    raise ValueError("Stage F output must be JSON for kakeibo processing")

                # 家計簿専用 Stage H で処理
                stageH_result = self.stage_h_kakeibo.process(stage_f_output)

                # イベントループに制御を返す（並列タスク実行のため）
                await asyncio.sleep(0)

                # 家計簿専用のDB保存
                if self.kakeibo_db_handler:
                    # Phase 3: owner_id 必須チェック
                    if not owner_id:
                        raise ValueError("owner_id is required for kakeibo processing (Phase 3)")

                    logger.info("[DB保存] 家計簿データをDBに保存...")
                    kakeibo_save_result = self.kakeibo_db_handler.save_receipt(
                        stage_h_output=stageH_result,
                        file_name=file_name,
                        drive_file_id=source_id,
                        model_name=stage_h_config['model'],
                        source_folder=workspace,
                        owner_id=owner_id
                    )
                    logger.info(f"[DB保存完了] receipt_id={kakeibo_save_result['receipt_id']}")
                else:
                    logger.warning("K_kakeibo module not available, skipping kakeibo DB save")

                # 家計簿は Rawdata_FILE_AND_MAIL に保存せず、ここで終了
                return {
                    'success': True,
                    'receipt_id': kakeibo_save_result['receipt_id'],
                    'transaction_ids': kakeibo_save_result['transaction_ids'],
                    'log_id': kakeibo_save_result['log_id'],
                    'doc_type': 'kakeibo'
                }

            # ============================================
            # Stage H1 + H2: 分割処理（トークン消費削減版）
            # ============================================
            else:
                stage_hi_config = self.config.get_stage_config('stage_hi', doc_type, workspace)
                prompt_hi = stage_hi_config['prompt']
                model_hi = stage_hi_config['model']

                # -----------------------------------------
                # アンカーベースのH1/H2ルーティング
                # -----------------------------------------
                # Stage F からアンカー配列を取得（新形式）
                anchors = stage_f_structure.get('anchors', []) if stage_f_structure else []

                routing_result = self.stage_g.route_anchors_to_stages(
                    stage_g_result=stage_g_result or {},
                    anchors=anchors
                )

                h1_payload = routing_result.get('h1_payload', {})
                h2_payload = routing_result.get('h2_payload', {})
                anchor_map = routing_result.get('anchor_map', {})

                logger.info(f"[Stage G→H] ルーティング完了: H1={len(h1_payload.get('heavy_tables', []))}表, H2テキスト={len(h2_payload.get('text_anchors', []))}件")

                # -----------------------------------------
                # Stage H1: 表処理専門（重い表のみ）
                # -----------------------------------------
                heavy_tables = h1_payload.get('heavy_tables', [])
                table_inventory = stage_g_result.get('table_inventory', []) if stage_g_result else []

                logger.info(f"[Stage H1] 表処理開始... (重い表: {len(heavy_tables)}件, 全表: {len(table_inventory)}件)")
                if progress_callback:
                    progress_callback("H1")

                h1_result = self.stage_h1.process(
                    table_inventory=table_inventory,
                    doc_type=doc_type,
                    workspace=workspace,
                    unified_text=combined_text
                )

                # H1の結果をログ
                h1_stats = h1_result.get('statistics', {})
                logger.info(f"[Stage H1完了] processed={h1_stats.get('processed', 0)}, "
                           f"extracted_meta_keys={list(h1_result.get('extracted_metadata', {}).keys())}")

                # イベントループに制御を返す
                await asyncio.sleep(0)

                # -----------------------------------------
                # H2用にテキストを軽量化（アンカーベース）
                # -----------------------------------------
                # 方法1: Stage G のルーティング結果を使用
                reduced_text = h2_payload.get('reduced_text', '')

                # 方法2: フォールバック（従来のフラグメントベース）
                if not reduced_text:
                    reduced_text = combined_text
                    table_text_fragments = h1_result.get('table_text_fragments', [])

                    if table_text_fragments:
                        reduced_text = self.stage_h1.remove_table_text_from_unified(
                            combined_text, table_text_fragments
                        )

                logger.info(f"[Stage H1→H2] テキスト軽量化: {len(combined_text)}→{len(reduced_text)}文字 (-{(len(combined_text) - len(reduced_text)) * 100 // max(len(combined_text), 1)}%)")

                # -----------------------------------------
                # Stage H2: テキスト処理専門
                # -----------------------------------------
                logger.info(f"[Stage H2] テキスト処理開始... (model={model_hi})")
                if progress_callback:
                    progress_callback("H2")

                stageHI_result = self.stage_h2.process(
                    file_name=file_name,
                    doc_type=doc_type,
                    workspace=workspace,
                    reduced_text=reduced_text,
                    prompt=prompt_hi,
                    model=model_hi,
                    h1_result=h1_result,
                    stage_f_structure=stage_f_structure,
                    stage_g_result=stage_g_result
                )

                # イベントループに制御を返す（並列タスク実行のため）
                await asyncio.sleep(0)

                # Stage H2 の結果をチェック
                if not stageHI_result or not isinstance(stageHI_result, dict):
                    error_msg = "Stage H2失敗: 結果が不正です"
                    logger.error(f"[Stage H2失敗] {error_msg}")
                    return {'success': False, 'error': error_msg}

                # 結果を変数に展開（後続処理との互換性のため）
                document_date = stageHI_result.get('document_date')
                tags = stageHI_result.get('tags', [])
                stageH_metadata = stageHI_result.get('metadata', {})
                title = stageHI_result.get('title', '')
                summary = stageHI_result.get('summary', '')
                relevant_date = stageHI_result.get('document_date')

                # カレンダーイベントとタスクを取得
                calendar_events = stageHI_result.get('calendar_events', [])
                tasks = stageHI_result.get('tasks', [])

                # metadataに追加
                stageH_metadata['calendar_events'] = calendar_events
                stageH_metadata['tasks'] = tasks

                # audit_canonical_text があれば metadata に追加
                audit_text = stageHI_result.get('audit_canonical_text', '')
                if audit_text:
                    stageH_metadata['audit_canonical_text'] = audit_text

                # H1処理統計を追加
                stageH_metadata['_h1_h2_split'] = True
                stageH_metadata['_h1_statistics'] = h1_stats

                logger.info(f"[Stage H1+H2完了] title={title[:30] if title else 'N/A'}..., "
                           f"calendar_events={len(calendar_events)}件, tasks={len(tasks)}件")

                # Stage H 互換の結果オブジェクトを作成
                stageH_result = {
                    'document_date': document_date,
                    'tags': tags,
                    'metadata': stageH_metadata
                }

            # ============================================
            # Google Drive ファイル名更新（タイトルに基づく）
            # ============================================
            if title and source_id and file_name:
                # ファイル名から拡張子を抽出
                import os
                file_extension = os.path.splitext(file_name)[1]  # 例: ".pdf"

                # 新しいファイル名を生成（タイトル + 拡張子）
                new_file_name = title + file_extension

                # Google Drive のファイル名を更新
                try:
                    self.drive_connector.rename_file(source_id, new_file_name)
                    logger.info(f"[Google Drive] ファイル名更新成功: {new_file_name}")
                except Exception as e:
                    # ファイル名更新失敗はエラーログのみ（処理は継続）
                    logger.warning(f"[Google Drive] ファイル名更新失敗: {e}")

            # ============================================
            # Stage J: Chunking
            # ============================================
            logger.info("[Stage J] チャンク化開始...")
            if progress_callback:
                progress_callback("J")
            chunks = self.stage_j.process(
                display_subject=extra_metadata.get('display_subject', file_name) if extra_metadata else file_name,
                summary=summary,
                tags=tags,
                document_date=document_date,
                metadata=stageH_metadata
            )
            logger.info(f"[Stage J完了] チャンク数: {len(chunks)}")

            # イベントループに制御を返す（並列タスク実行のため）
            await asyncio.sleep(0)

            # ============================================
            # DB保存: Rawdata_FILE_AND_MAIL
            # ============================================
            document_id = existing_document_id
            try:
                # 既存ドキュメントの attachment_text, metadata, display_* フィールドを取得（nullで上書きしないため）
                existing_attachment_text = None
                existing_metadata = {}
                existing_display_fields = {}
                if existing_document_id:
                    try:
                        existing_doc = self.db.client.table('Rawdata_FILE_AND_MAIL').select(
                            'attachment_text, metadata, display_sender, display_sender_email, display_subject, display_sent_at, display_post_text'
                        ).eq('id', existing_document_id).execute()
                        if existing_doc.data and len(existing_doc.data) > 0:
                            doc = existing_doc.data[0]
                            existing_attachment_text = doc.get('attachment_text', '')
                            # 既存 metadata を保持（message_id, thread_id, subject など）
                            existing_metadata = doc.get('metadata', {})
                            if isinstance(existing_metadata, str):
                                existing_metadata = json.loads(existing_metadata)
                            # display_* フィールドを保持
                            existing_display_fields = {
                                'display_sender': doc.get('display_sender'),
                                'display_sender_email': doc.get('display_sender_email'),
                                'display_subject': doc.get('display_subject'),
                                'display_sent_at': doc.get('display_sent_at'),
                                'display_post_text': doc.get('display_post_text')
                            }
                            logger.debug(f"[DB保存] 既存attachment_text取得: {len(existing_attachment_text or '')}文字")
                            logger.debug(f"[DB保存] 既存metadata取得: {list(existing_metadata.keys())}")
                            logger.debug(f"[DB保存] 既存display_*フィールド取得: sender={existing_display_fields.get('display_sender')}, subject={existing_display_fields.get('display_subject')}")
                    except Exception as e:
                        logger.warning(f"[DB保存警告] 既存フィールド取得失敗: {e}")

                # テキストフィールドをサニタイズ（null文字を除去）
                sanitized_combined_text = self._sanitize_text(combined_text)
                sanitized_summary = self._sanitize_text(summary)
                sanitized_extracted_text = self._sanitize_text(extracted_text)

                # Stage F の出力をパース（JSONから各要素を抽出）
                stage_f_text_ocr = None
                stage_f_layout_ocr = None
                stage_f_visual_elements = None
                try:
                    if vision_raw and stage_f_structure:
                        # full_text を text OCR として保存
                        stage_f_text_ocr = self._sanitize_text(stage_f_structure.get('full_text', ''))

                        # v1.1契約: sections/tables の取得場所を適切に
                        sf_schema = stage_f_structure.get('schema_version', '')
                        if sf_schema == STAGE_H_INPUT_SCHEMA_VERSION:
                            # v1.1: layout_info.sections, トップレベル tables
                            layout_info = stage_f_structure.get('layout_info', {})
                            sections = layout_info.get('sections', [])
                            tables = stage_f_structure.get('tables', [])
                        else:
                            # レガシー: 直下または layout_info から取得（後方互換）
                            sections = stage_f_structure.get('sections', []) or stage_f_structure.get('layout_info', {}).get('sections', [])
                            tables = stage_f_structure.get('tables', []) or stage_f_structure.get('layout_info', {}).get('tables', [])

                        stage_f_layout_ocr = json.dumps({
                            'sections': sections,
                            'tables': tables
                        }, ensure_ascii=False, indent=2)

                        # visual_elements をそのまま保存
                        stage_f_visual_elements = json.dumps(
                            stage_f_structure.get('visual_elements', {}),
                            ensure_ascii=False,
                            indent=2
                        )
                except Exception as e:
                    logger.warning(f"[DB保存警告] Stage F出力のパースに失敗: {e}")

                # Stage Eが空の場合、Stage Fのfull_textを使用
                if not sanitized_extracted_text and stage_f_text_ocr:
                    logger.info("[DB保存] Stage Eが空のため、Stage Fのfull_textを使用")
                    sanitized_extracted_text = stage_f_text_ocr

                # Stage F アンカー配列を取得
                stage_f_anchors = None
                if stage_f_structure and 'anchors' in stage_f_structure:
                    stage_f_anchors = stage_f_structure.get('anchors', [])

                # Stage G 結果を取得
                stage_g_result_json = None
                if stage_g_result:
                    stage_g_result_json = {
                        'source_inventory': stage_g_result.get('source_inventory', []),
                        'table_inventory': stage_g_result.get('table_inventory', []),
                        'cross_validation': stage_g_result.get('cross_validation', {}),
                        'processing_mode': stage_g_result.get('processing_mode', '')
                    }

                # Stage H1 結果を取得
                stage_h1_tables_json = None
                if 'h1_result' in dir() and h1_result:
                    stage_h1_tables_json = {
                        'processed_tables': h1_result.get('processed_tables', []),
                        'extracted_metadata': h1_result.get('extracted_metadata', {}),
                        'statistics': h1_result.get('statistics', {})
                    }

                # titleをサニタイズ
                sanitized_title = self._sanitize_text(title)

                # attachment_text の決定ロジック
                # - Stage Eが正当にテキストを抽出した場合（sanitized_combined_text が空でない）→ 使用（正当な上書き）
                # - Stage Eが失敗した場合（sanitized_combined_text が空）→ 既存値を保持（nullで上書きしない）
                final_attachment_text = sanitized_combined_text
                if not sanitized_combined_text and existing_attachment_text:
                    final_attachment_text = existing_attachment_text
                    logger.info(f"[DB保存] Stage Eが空のため、既存attachment_textを保持: {len(final_attachment_text)}文字")

                # metadata のマージロジック
                # 既存の metadata（message_id, thread_id, subject など）を保持しつつ、
                # Stage H の metadata（LLMが生成した構造化データ）を追加
                final_metadata = {}
                if existing_document_id and existing_metadata:
                    # 既存の metadata をベースにする
                    final_metadata = existing_metadata.copy()
                    logger.info(f"[DB保存] 既存metadataを保持: {list(existing_metadata.keys())}")
                # Stage H の metadata を追加・更新
                if stageH_metadata:
                    final_metadata.update(stageH_metadata)
                    logger.info(f"[DB保存] Stage H metadataをマージ: {list(stageH_metadata.keys())}")

                doc_data = {
                    'source_id': source_id,
                    'source_type': 'unified_pipeline',
                    'file_name': file_name,
                    'workspace': workspace,
                    'doc_type': doc_type,
                    'title': sanitized_title,
                    'attachment_text': final_attachment_text,
                    'summary': sanitized_summary,
                    'tags': tags,
                    'document_date': document_date,
                    'metadata': final_metadata,
                    'processing_status': 'completed',
                    # 各ステージの出力を保存（新スキーマ 2026-01-27）
                    'stage_e_text': sanitized_extracted_text,  # Stage E: 物理抽出テキスト（E-1〜E-3統合）
                    'stage_f_text_ocr': stage_f_text_ocr,        # Stage F: Path A テキスト抽出
                    'stage_f_layout_ocr': stage_f_layout_ocr,    # Stage F: レイアウト情報
                    'stage_f_visual_elements': stage_f_visual_elements,  # Stage F: 視覚要素
                    'stage_f_anchors': json.dumps(stage_f_anchors, ensure_ascii=False) if stage_f_anchors else None,  # Stage F: アンカー配列
                    'stage_g_result': json.dumps(stage_g_result_json, ensure_ascii=False) if stage_g_result_json else None,  # Stage G: 統合精錬結果
                    'stage_h_normalized': reduced_text if 'reduced_text' in dir() else sanitized_combined_text,  # Stage H2: 軽量化済み入力
                    'stage_h1_tables': json.dumps(stage_h1_tables_json, ensure_ascii=False) if stage_h1_tables_json else None,  # Stage H1: 処理済み表
                    'stage_h_result': json.dumps(stageH_result, ensure_ascii=False, indent=2) if stageH_result else None,  # Stage H2: 構造化結果
                    'stage_j_chunks_json': json.dumps(chunks, ensure_ascii=False, indent=2)  # Stage J: チャンク化結果
                }

                # 既存ドキュメントの場合、display_* フィールドを保持（Gmail ingestion時に設定された値を上書きしないため）
                if existing_document_id and existing_display_fields:
                    for key, value in existing_display_fields.items():
                        if value is not None:  # Noneでない値のみ保持
                            doc_data[key] = value
                    logger.debug(f"[DB保存] display_*フィールドを保持: {list(existing_display_fields.keys())}")

                # extra_metadata をマージ
                if extra_metadata:
                    # display_*フィールドは最上位フィールドとして保存
                    display_fields = ['display_subject', 'display_sender', 'display_sender_email', 'display_sent_at', 'display_post_text', 'display_type']
                    for field in display_fields:
                        if field in extra_metadata and extra_metadata[field] is not None:
                            doc_data[field] = extra_metadata[field]
                            logger.debug(f"[DB保存] extra_metadataから{field}を設定: {extra_metadata[field]}")

                    # display_*以外のフィールドはmetadataにマージ
                    other_metadata = {k: v for k, v in extra_metadata.items() if k not in display_fields}
                    if other_metadata:
                        if isinstance(doc_data['metadata'], dict):
                            doc_data['metadata'].update(other_metadata)
                        else:
                            doc_data['metadata'] = other_metadata

                # 既存ドキュメントを更新 or 新規作成
                if existing_document_id:
                    logger.info(f"[DB更新] 既存ドキュメント更新: {existing_document_id}")
                    # IDを除外してUPDATE（IDは変更不可）
                    update_data = {k: v for k, v in doc_data.items() if k != 'id'}
                    result = self.db.client.table('Rawdata_FILE_AND_MAIL').update(update_data).eq('id', existing_document_id).execute()
                    if result.data and len(result.data) > 0:
                        document_id = result.data[0]['id']
                        logger.info(f"[DB更新完了] Rawdata_FILE_AND_MAIL ID: {document_id}")
                    else:
                        logger.error("[DB更新エラー] ドキュメント更新失敗")
                        return {'success': False, 'error': 'Document update failed'}
                else:
                    logger.info("[DB保存] 新規ドキュメント作成")
                    result = self.db.client.table('Rawdata_FILE_AND_MAIL').insert(doc_data).execute()
                    if result.data and len(result.data) > 0:
                        document_id = result.data[0]['id']
                        logger.info(f"[DB保存] Rawdata_FILE_AND_MAIL ID: {document_id}")
                    else:
                        logger.error("[DB保存エラー] ドキュメント作成失敗")
                        return {'success': False, 'error': 'Document creation failed'}

            except Exception as e:
                logger.error(f"[DB保存エラー] {e}")
                return {'success': False, 'error': str(e)}

            # ============================================
            # Stage K: Embedding
            # ============================================
            logger.info("[Stage K] ベクトル化開始...")
            if progress_callback:
                progress_callback("K")

            # 既存ドキュメントの場合は、古いチャンクを削除
            if existing_document_id:
                try:
                    logger.info(f"[Stage K] 既存チャンク削除: document_id={document_id}")
                    self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
                except Exception as e:
                    logger.warning(f"[Stage K 警告] 既存チャンク削除エラー（継続）: {e}")

            # 新しいチャンクを保存
            stage_k_result = self.stage_k.embed_and_save(document_id, chunks)

            # Stage K の結果をチェック（厳格モード: 1つでも失敗したら全体失敗）
            if not stage_k_result.get('success'):
                error_msg = f"Stage K失敗: {stage_k_result.get('failed_count', 0)}/{len(chunks)}チャンク保存失敗"
                logger.error(f"[Stage K失敗] {error_msg}")
                return {'success': False, 'error': error_msg}

            # 部分的失敗は警告として扱う（一部のチャンクは保存済み）
            failed_count = stage_k_result.get('failed_count', 0)
            saved_count = stage_k_result.get('saved_count', 0)
            if failed_count > 0:
                logger.warning(f"[Stage K警告] 部分的な失敗: {failed_count}/{len(chunks)}チャンク保存失敗（{saved_count}チャンクは保存済み）")
                # 失敗したが、一部は成功しているので継続

            logger.info(f"[Stage K完了] {stage_k_result.get('saved_count', 0)}/{len(chunks)}チャンク保存")

            # Phase 5: Execution tracking - 成功時
            if enable_execution_tracking and execution_manager and owner_id and document_id:
                import time
                duration_ms = int((time.time() - start_time) * 1000) if start_time else None

                # execution 作成（処理完了後に作成、即座に succeeded）
                try:
                    exec_ctx = execution_manager.create_execution(
                        document_id=document_id,
                        owner_id=owner_id,
                        input_text=combined_text if 'combined_text' in dir() else '',
                        model_version=stage_h_config.get('model') if 'stage_h_config' in dir() else None,
                        normalized_text=combined_text if 'combined_text' in dir() else ''
                    )
                    execution_manager.mark_succeeded(
                        execution_id=exec_ctx.execution_id,
                        result_data={
                            'summary': summary,
                            'tags': tags,
                            'document_date': document_date if 'document_date' in dir() else None,
                            'metadata': stageH_metadata if 'stageH_metadata' in dir() else {},
                            'chunks_count': stage_k_result.get('saved_count', 0)
                        },
                        processing_duration_ms=duration_ms
                    )
                    logger.info(f"[Phase 5] Execution 記録完了: {exec_ctx.execution_id[:8]}...")
                except Exception as exec_e:
                    logger.warning(f"[Phase 5] Execution 記録エラー（継続）: {exec_e}")

            return {
                'success': True,
                'document_id': document_id,
                'summary': summary,
                'tags': tags,
                'chunks_count': stage_k_result.get('saved_count', 0)
            }

        except Exception as e:
            logger.error(f"[パイプラインエラー] {e}", exc_info=True)

            # Phase 5: Execution tracking - 失敗時
            if enable_execution_tracking and execution_manager and owner_id:
                import time
                duration_ms = int((time.time() - start_time) * 1000) if start_time else None
                try:
                    # 既存 document_id がある場合のみ execution を記録
                    doc_id = existing_document_id or (document_id if 'document_id' in dir() else None)
                    if doc_id:
                        exec_ctx = execution_manager.create_execution(
                            document_id=doc_id,
                            owner_id=owner_id,
                            input_text='',  # 失敗時は入力が不明な場合がある
                            model_version=None
                        )
                        execution_manager.mark_failed(
                            execution_id=exec_ctx.execution_id,
                            error_code='PIPELINE_ERROR',
                            error_message=str(e),
                            processing_duration_ms=duration_ms
                        )
                        logger.info(f"[Phase 5] 失敗 Execution 記録: {exec_ctx.execution_id[:8]}...")
                except Exception as exec_e:
                    logger.warning(f"[Phase 5] 失敗 Execution 記録エラー（継続）: {exec_e}")

            return {'success': False, 'error': str(e)}
```

## shared/pipeline/prompts/__init__.py

```python
"""
G_unified_pipeline prompts module
"""

# Stage G: レシート構造化プロンプト（家計簿用）
STAGE_G_FORMATTING_PROMPT = """あなたはレシートのOCR結果から構造化データを生成する専門家です。

以下は Stage F（OCR）で抽出されたレシート全文です：

---
{vision_raw}
---

このテキストから、レシート情報を構造化してJSON形式で出力してください。

## 構造化タスク

### 1. 店舗情報の抽出
- 店舗名
- 住所
- 電話番号
- 店舗コード（記載されていれば）

### 2. 取引情報の抽出
- 日付（YYYY-MM-DD形式）
- 時刻（HH:MM:SS形式、記載されていれば）
- レジ番号
- レシート番号
- 取引番号

### 3. 商品明細の抽出
各商品について：
- 行番号（レシート記載の順序）
- 商品名（レシート記載のまま）
- 数量
- 単価
- 金額
- **税率マーク（最重要！）**
  - 商品名の横や行内にある記号を必ず抽出してください
  - 例: `*`, `※`, `★`, `◆`, `8%`, `10%`, `(軽)` など
  - マークがない場合は null
  - **注意**: `*`（アスタリスク）も`※`（米印）もすべて抽出すること

**重要な注意事項（数量情報行の処理）**：
- 括弧で囲まれた数量情報（例: `(2個 X 単128)`, `(3パック X 単200)`）は**独立した商品ではない**
- これらは**直前の商品の数量詳細**なので、独立した行として抽出しないこと
- 数量情報は直前の商品の quantity と unit_price に反映させること

**重要な注意事項（小計・合計行の識別）**：
- 以下の行は**商品ではなく、集計行**です。独立した商品として抽出しないでください：
  - 「小計」「小 計」
  - 「合計」「合 計」
  - 「お買上」「お買上げ」「お買い上げ」
  - 「計」
  - これらの行には `line_type: "SUBTOTAL"` または `line_type: "TOTAL"` を設定してください

### 4. 金額情報の抽出
**重要：レシートに記載されている数値をそのまま抽出（計算・推測禁止）**

- 小計（税抜小計、記載されていれば）
- 8%対象額（税抜）
- 8%消費税額
- 10%対象額（税抜）
- 10%消費税額
- 消費税合計
- 合計（支払額）
- お預かり（記載されていれば）
- お釣り（記載されていれば）

### 5. 支払情報
- 支払方法（現金/カード/電子マネー等）
- カード情報（記載されていれば）

### 6. その他情報
- ポイント情報
- キャンペーン情報
- バーコード番号
- **税率説明**（重要！）
  - レシート下部の税率に関する説明文を抽出
  - 例: 「*印は軽減税率（8%）適用商品」「※は軽減税率対象」など
  - 記載がない場合は null
- その他の特記事項

## 出力形式

以下のJSON形式で出力してください：

```json
{{
  "shop_info": {{
    "name": "店舗名",
    "address": "住所（記載があれば）",
    "phone": "電話番号（記載があれば）",
    "store_code": "店舗コード（記載があれば）"
  }},
  "transaction_info": {{
    "date": "YYYY-MM-DD",
    "time": "HH:MM:SS（記載があれば）",
    "register_number": "レジ番号（記載があれば）",
    "receipt_number": "レシート番号（記載があれば）",
    "transaction_number": "取引番号（記載があれば）"
  }},
  "items": [
    {{
      "line_number": 1,
      "line_text": "レシートのこの行のテキストそのまま",
      "product_name": "商品名",
      "quantity": 1,
      "unit_price": 100,
      "amount": 100,
      "tax_mark": "※または★またはなし"
    }}
  ],
  "amounts": {{
    "subtotal": 1377,
    "tax_8_base": 0,
    "tax_8_amount": 0,
    "tax_10_base": 1377,
    "tax_10_amount": 123,
    "total_tax": 123,
    "total": 1500,
    "received": 2000,
    "change": 500
  }},
  "payment": {{
    "method": "現金",
    "card_info": null
  }},
  "other_info": {{
    "points": "ポイント情報",
    "campaign": "キャンペーン情報",
    "barcode": "バーコード番号",
    "tax_rate_note": "*印は軽減税率（8%）適用商品",
    "notes": "その他特記事項"
  }}
}}
```

## 重要な注意事項

1. **数値は必ずレシート記載のまま**
   - 計算しない、推測しない
   - 記載がない項目は null にする

2. **8%/10%税率の区分（最重要！）**
   - **各商品行の税率マークを必ず確認して抽出すること**
   - マークの例: `*`, `※`, `★`, `◆`, `8%`, `10%`, `(軽)` など
   - **`*`（アスタリスク）と`※`（米印）は別の文字として扱うこと**
   - 商品名の横、行の末尾、数量の後など、どこにあっても抽出すること
   - レシート下部に「*印は軽減税率（8%）適用商品」のような説明があれば、other_info.tax_rate_note に抽出すること
   - 記載がない場合のみ null にする（推測しない）

3. **小計・消費税・合計の関係**
   - レシートに「小計」「消費税」「合計」が明記されている場合、その数値をそのまま使う
   - 外税レシート：小計 + 消費税 = 合計
   - 内税レシート：合計のみ（小計は記載なしの場合あり）

4. **税額サマリー**
   - 「8%対象 ○○円（税 △△円）」のような記載があれば必ず抽出
   - 「10%対象 ○○円（税 △△円）」のような記載があれば必ず抽出

5. **記載がない項目**
   - 記載がない項目は null にする
   - 空文字列や0ではなく null を使う

## エラー処理

- OCR結果が不完全な場合：`{{"error": "incomplete_ocr", "details": "不完全な箇所の説明"}}`
- レシートとして認識できない場合：`{{"error": "not_a_receipt"}}`
"""
```

## shared/pipeline/stage_e_preprocessing.py

```python
"""
Stage E: Pre-processing (前処理) - 物理抽出専用（AI排除版）

【設計 2026-01-26】コスト最適化のためAIを完全排除

役割: ファイルからの「物理的テキスト抽出」のみ
- PDF: pdfplumber / PyMuPDF でテキスト抽出
- Office: OfficeProcessor でテキスト抽出
- テキスト: エンコーディング検出して読み込み
- 画像/音声/動画: raw_text="" で返す（Stage F-7で処理）

処理フロー（E-3で終了）:
- E-1: ファイル検証
- E-2: MIMEタイプルーティング
- E-3: 物理テキスト抽出（AI不使用）

【重要】
- 添付なしの場合は Stage E をスキップし、Stage G からスタート
- 画像/音声/動画は raw_text="" で返し、Stage F-7 で AI 処理
"""
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger

from shared.common.processors.pdf import PDFProcessor
from shared.common.processors.office import OfficeProcessor


class StageEPreprocessor:
    """Stage E: 前処理（物理抽出専用、AI排除版）"""

    # MIMEタイプ分類
    DOCUMENT_MIME_TYPES = {
        'application/pdf',
        'text/plain',
        'text/html',
        'text/csv',
        'text/markdown',
        'application/vnd.openxmlformats-officedocument.wordprocessingml.document',  # .docx
        'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',        # .xlsx
        'application/vnd.openxmlformats-officedocument.presentationml.presentation',  # .pptx
        'application/msword',  # .doc
        'application/vnd.ms-excel',  # .xls
        'application/vnd.ms-powerpoint',  # .ppt
    }

    IMAGE_MIME_TYPES = {
        'image/png',
        'image/jpeg',
        'image/jpg',
        'image/gif',
        'image/webp',
        'image/bmp',
        'image/tiff',
    }

    AUDIO_MIME_TYPES = {
        'audio/mpeg',
        'audio/mp3',
        'audio/wav',
        'audio/x-wav',
        'audio/ogg',
        'audio/flac',
        'audio/aac',
        'audio/m4a',
        'audio/x-m4a',
        'audio/webm',
    }

    VIDEO_MIME_TYPES = {
        'video/mp4',
        'video/mpeg',
        'video/quicktime',
        'video/x-msvideo',
        'video/webm',
        'video/x-matroska',
        'video/avi',
        'video/mov',
    }

    def __init__(self, llm_client=None):
        """
        Args:
            llm_client: LLMクライアント（後方互換性のため残すが使用しない）
        """
        # AI排除版: llm_client は使用しない
        self.pdf_processor = PDFProcessor(llm_client=None)  # AI不使用
        self.office_processor = OfficeProcessor()

    def extract_text(
        self,
        file_path: Path,
        mime_type: str,
        pre_extracted_text: Optional[str] = None,
        workspace: Optional[str] = None,
        progress_callback=None
    ) -> Dict[str, Any]:
        """
        ファイルからテキストを物理的に抽出（AI不使用）

        Args:
            file_path: ファイルパス
            mime_type: MIMEタイプ
            pre_extracted_text: 既に抽出済みのテキスト（HTML→PNG等の場合）
            workspace: ワークスペース（未使用、後方互換性のため残す）
            progress_callback: 進捗コールバック

        Returns:
            {
                'success': bool,
                'content': str,  # 物理抽出テキスト（画像/音声/動画は空）
                'char_count': int,
                'method': str,
                'metadata': dict,
                'requires_vision': bool,  # Stage F で Vision 処理が必要か
                'requires_transcription': bool  # Stage F で音声書き起こしが必要か
            }
        """
        logger.info("=" * 60)
        logger.info("[Stage E] 物理抽出開始（AI排除版）")

        # ============================================
        # [E-1] ファイル検証
        # ============================================
        if progress_callback:
            progress_callback("E-1")

        # ファイルなしの場合
        if file_path is None:
            logger.info("  └─ ファイルなし → Stage E スキップ推奨")
            content = pre_extracted_text or ""
            return {
                'success': True,
                'content': content,
                'char_count': len(content),
                'method': 'no_file',
                'metadata': {'skip_stage_e': True},
                'requires_vision': False,
                'requires_transcription': False
            }

        file_path = Path(file_path)
        logger.info(f"  ├─ ファイル: {file_path.name}")
        logger.info(f"  └─ MIMEタイプ: {mime_type}")

        if not file_path.exists():
            logger.error(f"[Stage E] ファイルが存在しません: {file_path}")
            return {
                'success': False,
                'content': '',
                'char_count': 0,
                'method': 'error',
                'error': f'File not found: {file_path}',
                'requires_vision': False,
                'requires_transcription': False
            }

        # ============================================
        # [E-2] MIMEタイプルーティング
        # ============================================
        if progress_callback:
            progress_callback("E-2")

        try:
            # ============================================
            # [E-3] 物理テキスト抽出
            # ============================================
            if progress_callback:
                progress_callback("E-3")

            # --- ドキュメント処理（物理抽出） ---
            if mime_type in self.DOCUMENT_MIME_TYPES or mime_type == 'application/pdf':
                content, method, metadata = self._extract_document(
                    file_path, mime_type, pre_extracted_text
                )
                return {
                    'success': True,
                    'content': content,
                    'char_count': len(content),
                    'method': method,
                    'metadata': metadata,
                    'requires_vision': True,  # PDF/Officeも画像ページがある可能性
                    'requires_transcription': False
                }

            # --- 画像処理（物理抽出不可 → Stage F-7へ） ---
            elif mime_type in self.IMAGE_MIME_TYPES or mime_type.startswith('image/'):
                logger.info("[E-3] 画像ファイル → 物理抽出不可、Stage F-7 で処理")
                content = pre_extracted_text or ""
                return {
                    'success': True,
                    'content': content,
                    'char_count': len(content),
                    'method': 'image_passthrough',
                    'metadata': {'mime_type': mime_type, 'file_path': str(file_path)},
                    'requires_vision': True,
                    'requires_transcription': False
                }

            # --- 音声処理（物理抽出不可 → Stage F-7へ） ---
            elif mime_type in self.AUDIO_MIME_TYPES or mime_type.startswith('audio/'):
                logger.info("[E-3] 音声ファイル → 物理抽出不可、Stage F-7 で処理")
                return {
                    'success': True,
                    'content': '',
                    'char_count': 0,
                    'method': 'audio_passthrough',
                    'metadata': {'mime_type': mime_type, 'file_path': str(file_path)},
                    'requires_vision': False,
                    'requires_transcription': True
                }

            # --- 動画処理（物理抽出不可 → Stage F-7へ） ---
            elif mime_type in self.VIDEO_MIME_TYPES or mime_type.startswith('video/'):
                logger.info("[E-3] 動画ファイル → 物理抽出不可、Stage F-7 で処理")
                return {
                    'success': True,
                    'content': '',
                    'char_count': 0,
                    'method': 'video_passthrough',
                    'metadata': {'mime_type': mime_type, 'file_path': str(file_path)},
                    'requires_vision': True,  # 動画のフレーム解析
                    'requires_transcription': True  # 音声書き起こし
                }

            # --- 未対応MIMEタイプ ---
            else:
                logger.warning(f"[E-3] 未対応のMIMEタイプ: {mime_type}")
                return {
                    'success': True,
                    'content': pre_extracted_text or '',
                    'char_count': len(pre_extracted_text or ''),
                    'method': 'unsupported',
                    'metadata': {'mime_type': mime_type},
                    'requires_vision': False,
                    'requires_transcription': False
                }

        except Exception as e:
            logger.error(f"[Stage E エラー] 物理抽出失敗: {e}", exc_info=True)
            return {
                'success': False,
                'content': '',
                'char_count': 0,
                'method': 'error',
                'error': str(e),
                'requires_vision': False,
                'requires_transcription': False
            }

    def _extract_document(
        self,
        file_path: Path,
        mime_type: str,
        pre_extracted_text: Optional[str]
    ) -> tuple:
        """
        ドキュメントから物理的にテキストを抽出（AI不使用）

        Returns:
            (content, method, metadata)
        """
        logger.info("[E-3] ドキュメント物理抽出開始")

        content = ""
        method = "document"
        metadata = {}

        # PDF処理
        if mime_type == 'application/pdf':
            logger.info("  ├─ PDF処理（pdfplumber/PyMuPDF）")
            result = self.pdf_processor.extract_text(str(file_path), progress_callback=None)
            if result.get('success'):
                content = result.get('content', '')
                method = 'pdf_physical'
                metadata = result.get('metadata', {})
            logger.info(f"  └─ PDF抽出完了: {len(content)} 文字")

        # Office文書処理
        elif 'openxmlformats' in mime_type or 'msword' in mime_type or 'ms-excel' in mime_type or 'ms-powerpoint' in mime_type:
            # ファイルタイプ判定
            if 'word' in mime_type:
                file_type = 'docx'
            elif 'sheet' in mime_type or 'excel' in mime_type:
                file_type = 'xlsx'
            elif 'presentation' in mime_type or 'powerpoint' in mime_type:
                file_type = 'pptx'
            else:
                file_type = 'office'

            logger.info(f"  ├─ Office処理 (type: {file_type})")
            result = self.office_processor.extract_text(str(file_path))
            if result.get('success'):
                content = result.get('content', '')
            method = f'{file_type}_physical'
            logger.info(f"  └─ Office抽出完了: {len(content)} 文字")

        # テキスト系ファイル処理
        elif mime_type.startswith('text/'):
            logger.info("  ├─ テキストファイル処理")
            try:
                # 複数のエンコーディングを試行
                encodings = ['utf-8', 'shift_jis', 'cp932', 'euc-jp', 'iso-2022-jp']
                for encoding in encodings:
                    try:
                        content = file_path.read_text(encoding=encoding)
                        logger.info(f"  ├─ エンコーディング: {encoding}")
                        break
                    except UnicodeDecodeError:
                        continue
                method = 'text_physical'
            except Exception as e:
                logger.error(f"  └─ テキスト読み込みエラー: {e}")
            logger.info(f"  └─ テキスト抽出完了: {len(content)} 文字")

        # pre_extracted_textがある場合は追加
        if pre_extracted_text:
            if content:
                content = pre_extracted_text + "\n\n---\n\n" + content
            else:
                content = pre_extracted_text

        logger.info("=" * 60)
        logger.info(f"[Stage E完了] 物理抽出結果:")
        logger.info(f"  ├─ 処理方式: {method}")
        logger.info(f"  └─ テキスト: {len(content)} 文字")
        logger.info("=" * 60)

        return content, method, metadata

    def process(self, file_path: Path, mime_type: str) -> str:
        """
        ファイルからテキストを抽出（process() エイリアス）

        Args:
            file_path: ファイルパス
            mime_type: MIMEタイプ

        Returns:
            extracted_text: 抽出されたテキスト
        """
        result = self.extract_text(file_path, mime_type)
        return result.get('content', '') if result.get('success') else ''
```

## shared/pipeline/stage_f_visual.py

```python
"""
Stage F: Dual-Vision Analysis (独立読解 10段階)

【設計 2026-01-26】コスト最適化 + 高精度維持のための新設計

核心: 「FはEの答えを知らない状態で、ゼロから視覚情報を暴き出す」

============================================
F-1〜F-5: 構造の下地作り（AIなし、Surya + スクリプト）
  - F-1: Image Normalization (正規化)
  - F-2: Surya Block Detection (領域検出)
  - F-3: Coordinate Quantization (座標量子化) ← トークン削減の肝
  - F-4: Logical Reading Order (読む順序の確定)
  - F-5: Block Classification (構造ラベル付与)

F-6〜F-8: 独立・二重読解（AIの本番）
  - F-6: Blind Prompting (プロンプト注入) ← Stage E結果を遮断
  - F-7: Dual Read - Path A (テキストの鬼 / 2.0 Flash or 2.5 Flash-Lite)
  - F-8: Dual Read - Path B (視覚の鬼 / 2.5 Flash)

F-9〜F-10: 検証・パッキング
  - F-9: Result Convergence (抽出結果の集約)
  - F-10: Payload Validation (契約保証)
============================================
"""
import json
import time
from pathlib import Path
from typing import Dict, Any, Optional, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from loguru import logger
from PIL import Image

from shared.ai.llm_client.llm_client import LLMClient
from .constants import (
    STAGE_F_OUTPUT_SCHEMA_VERSION,
    F1_TARGET_DPI,
    SURYA_MAX_DIM,
    QUANTIZE_GRID_SIZE,
    F7_MODEL_IMAGE,
    F7_MODEL_AV,
    F8_MODEL,
    F7_F8_MAX_TOKENS,
    F7_F8_TEMPERATURE,
    CHUNK_SIZE_PAGES,
)

# Surya のインポート（オプショナル）
try:
    from surya.detection import DetectionPredictor
    SURYA_AVAILABLE = True
except ImportError:
    SURYA_AVAILABLE = False
    logger.warning("[Stage F] Surya not installed - F-2 will be skipped")


class StageFVisualAnalyzer:
    """Stage F: 独立読解 10段階（Dual-Vision Analysis）"""

    def __init__(self, llm_client: LLMClient, enable_surya: bool = True):
        """
        Args:
            llm_client: LLMクライアント
            enable_surya: Suryaを有効化（デフォルト: True）
        """
        self.llm_client = llm_client
        self.enable_surya = enable_surya and SURYA_AVAILABLE

        # Surya detector (lazy loading)
        self._surya_detector = None

        # トークン使用量の収集用
        self._f7_usage: List[Dict[str, Any]] = []
        self._f8_usage: List[Dict[str, Any]] = []

        # 列境界情報の保存用（F-3.5で検出、F-7で使用）
        # {page_idx: [0, 250, 500, 750, 1000]} の形式
        self._page_column_boundaries: Dict[int, List[int]] = {}

    @property
    def surya_detector(self):
        """Surya detector の遅延初期化"""
        if self._surya_detector is None and self.enable_surya:
            try:
                self._surya_detector = DetectionPredictor()
                logger.info("[Stage F] Surya detector initialized")
            except Exception as e:
                logger.warning(f"[Stage F] Surya initialization failed: {e}")
                self.enable_surya = False
        return self._surya_detector

    def process(
        self,
        file_path: Optional[Path],
        mime_type: str,
        requires_vision: bool = False,
        requires_transcription: bool = False,
        post_body: Optional[Dict[str, Any]] = None,
        progress_callback=None,
        # 以下: pipeline.py から渡される追加引数（2026-01-28 統合）
        prompt: str = None,
        model: str = None,
        extracted_text: str = None,
        workspace: str = None,
        e2_table_bboxes: List[Dict] = None
    ) -> Dict[str, Any]:
        """
        Stage F メイン処理（10ステップ）

        Args:
            file_path: ファイルパス（添付なしの場合はNone）
            mime_type: MIMEタイプ
            requires_vision: Vision処理が必要か（Stage Eから）
            requires_transcription: 音声書き起こしが必要か（Stage Eから）
            post_body: 投稿本文
            progress_callback: 進捗コールバック
            prompt: YAMLから読み込んだプロンプト（F7/F8で使用）
            model: YAMLから読み込んだモデル（F7/F8で使用）
            extracted_text: Stage Eで抽出済みのテキスト
            workspace: ワークスペース
            e2_table_bboxes: Stage Eで検出した表のbbox座標

        Returns:
            Stage F 出力（Stage G への入力）
        """
        total_start = time.time()

        # トークン使用量をリセット
        self._f7_usage = []
        self._f8_usage = []

        # 予備知識を保存（プロンプトで使用、Stage E内容は排除）
        self._file_name = file_path.name if file_path else None
        self._doc_type = self._infer_doc_type(mime_type, file_path)
        self._workspace = workspace

        logger.info("=" * 60)
        logger.info("[Stage F] 独立読解 10段階 開始")
        logger.info(f"  ├─ ファイル: {file_path.name if file_path else 'なし'}")
        logger.info(f"  ├─ MIMEタイプ: {mime_type}")
        logger.info(f"  ├─ requires_vision: {requires_vision}")
        logger.info(f"  └─ requires_transcription: {requires_transcription}")
        logger.info("=" * 60)

        # 添付なし or 処理不要 → 空のpayloadを返す
        if file_path is None or (not requires_vision and not requires_transcription):
            logger.info("[Stage F] スキップ（添付なし or 処理不要）")
            return self._create_empty_payload(post_body)

        # ファイル存在確認
        if not file_path.exists():
            logger.error(f"[Stage F] ファイルが存在しません: {file_path}")
            return self._create_empty_payload(post_body, error=f"File not found: {file_path}")

        # メディアタイプ判定
        is_image = mime_type.startswith('image/') if mime_type else False
        is_audio = mime_type.startswith('audio/') if mime_type else False
        is_video = mime_type.startswith('video/') if mime_type else False
        is_document = mime_type in {'application/pdf'} or mime_type.startswith('text/')

        try:
            # ============================================
            # 音声/動画: F-7 のみ実行（Transcription特化）
            # ============================================
            if is_audio or is_video:
                return self._process_audio_video(
                    file_path, mime_type, is_video, post_body, progress_callback
                )

            # ============================================
            # 画像/PDF: F-1〜F-10 フル実行
            # ============================================
            return self._process_image_document(
                file_path, mime_type, is_document, post_body, progress_callback
            )

        except Exception as e:
            logger.error(f"[Stage F] 処理エラー: {e}", exc_info=True)
            return self._create_empty_payload(post_body, error=str(e))

        finally:
            total_elapsed = time.time() - total_start
            logger.info(f"[Stage F] 総処理時間: {total_elapsed:.2f}秒")

    def _process_audio_video(
        self,
        file_path: Path,
        mime_type: str,
        is_video: bool,
        post_body: Optional[Dict],
        progress_callback
    ) -> Dict[str, Any]:
        """音声/動画処理（F-7のみ）"""
        logger.info("[Stage F] 音声/動画モード → F-7 Transcription のみ実行")

        if progress_callback:
            progress_callback("F-7")

        # F-7: Transcription（gemini-2.5-flash-lite）
        f7_result = self._f7_transcription(file_path, mime_type, is_video)

        # F-9: 結果集約（音声/動画は F-7 のみ）
        if progress_callback:
            progress_callback("F-9")

        return {
            "schema_version": STAGE_F_OUTPUT_SCHEMA_VERSION,
            "post_body": post_body or {},
            "path_a_result": f7_result,
            "path_b_result": {},  # 音声/動画は Path B なし
            "media_type": "video" if is_video else "audio",
            "processing_mode": "transcription_only",
            "warnings": []
        }

    def _process_image_document(
        self,
        file_path: Path,
        mime_type: str,
        is_document: bool,
        post_body: Optional[Dict],
        progress_callback
    ) -> Dict[str, Any]:
        """
        画像/ドキュメント処理（F-1〜F-10）

        【チャンク処理】
        MAX_TOKENSエラー回避のため、5ページごとに分割処理を行う。
        これにより100ページ超のPDFでも安定して処理可能。
        """

        # ============================================
        # F-1: Image Normalization（全ページ）
        # ============================================
        if progress_callback:
            progress_callback("F-1")
        all_page_images = self._f1_normalize(file_path, is_document)
        total_pages = len(all_page_images)

        logger.info(f"[Stage F] 総ページ数: {total_pages}, チャンクサイズ: {CHUNK_SIZE_PAGES}")

        # ============================================
        # 5ページごとのチャンクに分割
        # ============================================
        page_chunks = [
            all_page_images[i:i + CHUNK_SIZE_PAGES]
            for i in range(0, total_pages, CHUNK_SIZE_PAGES)
        ]
        total_chunks = len(page_chunks)

        logger.info(f"[Stage F] チャンク数: {total_chunks}")

        # チャンク処理結果の蓄積用
        aggregated_full_texts = []
        aggregated_blocks = []
        aggregated_tables = []
        aggregated_diagrams = []
        aggregated_charts = []
        aggregated_structured_candidates = []
        chunk_warnings = []

        # ============================================
        # チャンクごとに F-2〜F-8 を実行
        # ============================================
        for chunk_idx, chunk_pages in enumerate(page_chunks):
            chunk_start_page = chunk_idx * CHUNK_SIZE_PAGES
            chunk_end_page = chunk_start_page + len(chunk_pages) - 1

            logger.info("=" * 50)
            logger.info(f"[Stage F] チャンク {chunk_idx + 1}/{total_chunks} 処理中")
            logger.info(f"  ├─ ページ範囲: {chunk_start_page + 1}〜{chunk_end_page + 1}")
            logger.info(f"  └─ ページ数: {len(chunk_pages)}")

            # F-2: Surya Block Detection（このチャンクのみ）
            if progress_callback:
                progress_callback(f"F-2 ({chunk_idx + 1}/{total_chunks})")
            surya_blocks = self._f2_detect_blocks(chunk_pages)

            # F-3: Coordinate Quantization
            if progress_callback:
                progress_callback(f"F-3 ({chunk_idx + 1}/{total_chunks})")
            quantized_blocks = self._f3_quantize(surya_blocks, chunk_pages)

            # F-3.5: Intelligent Filtering & Column Detection（トークン削減の肝）
            if progress_callback:
                progress_callback(f"F-3.5 ({chunk_idx + 1}/{total_chunks})")
            filtered_blocks = self._f35_filter_and_columnize(quantized_blocks)

            # F-4: Logical Reading Order
            if progress_callback:
                progress_callback(f"F-4 ({chunk_idx + 1}/{total_chunks})")
            ordered_blocks = self._f4_reading_order(filtered_blocks)

            # F-5: Block Classification
            if progress_callback:
                progress_callback(f"F-5 ({chunk_idx + 1}/{total_chunks})")
            classified_blocks = self._f5_classify(ordered_blocks)

            # F-6: ID焼き込み画像生成（Ver 4.0 - 座標はAIに渡さない）
            if progress_callback:
                progress_callback(f"F-6 ({chunk_idx + 1}/{total_chunks})")

            # IDマッピングをシステム内部に保持（AIには渡さない）
            id_mapping = self._f6_store_id_mapping(classified_blocks)
            self._id_mapping = id_mapping  # インスタンス変数に保存

            # F-7 & F-8: Dual Read（このチャンクのみ、並列実行）
            if progress_callback:
                progress_callback(f"F-7/F-8 ({chunk_idx + 1}/{total_chunks})")

            path_a_result = {}
            path_b_result = {}

            # このチャンク（ページ）の列境界情報を取得
            page_column_boundaries = self._page_column_boundaries.get(chunk_start_page, [0, QUANTIZE_GRID_SIZE])

            with ThreadPoolExecutor(max_workers=2) as executor:
                future_a = executor.submit(
                    self._f7_path_a_chunk_extraction,
                    chunk_pages, classified_blocks, chunk_idx, chunk_start_page, page_column_boundaries
                )
                future_b = executor.submit(
                    self._f8_path_b_chunk_analysis,
                    chunk_pages, classified_blocks, chunk_idx, chunk_start_page
                )

                for future in as_completed([future_a, future_b]):
                    try:
                        if future == future_a:
                            path_a_result = future.result()
                            logger.info(f"[F-7] チャンク{chunk_idx + 1} Path A 完了")
                        else:
                            path_b_result = future.result()
                            logger.info(f"[F-8] チャンク{chunk_idx + 1} Path B 完了")
                    except Exception as e:
                        logger.error(f"[F-7/F-8] チャンク{chunk_idx + 1} エラー: {e}")
                        chunk_warnings.append(f"CHUNK_{chunk_idx}_ERROR: {str(e)}")

            # チャンク結果を蓄積
            chunk_text = path_a_result.get("full_text_ordered", "")
            aggregated_full_texts.append(chunk_text)
            logger.info(f"[F-9蓄積] チャンク{chunk_idx + 1}: {len(chunk_text)}文字を蓄積 (累計{sum(len(t) for t in aggregated_full_texts)}文字)")

            # ブロックIDにチャンク情報を付加して蓄積
            for block in path_a_result.get("extracted_texts", []):
                block["chunk_idx"] = chunk_idx
                block["original_page"] = chunk_start_page + block.get("page", 0)
                aggregated_blocks.append(block)

            # 表データをマージして蓄積
            merged_tables = self._merge_chunk_tables(path_a_result, path_b_result, chunk_idx, chunk_start_page)
            aggregated_tables.extend(merged_tables)

            # 構造化データ候補を蓄積
            for candidate in path_b_result.get("structured_data_candidates", []):
                candidate["chunk_idx"] = chunk_idx
                aggregated_structured_candidates.append(candidate)

            # ダイアグラム・チャートを蓄積
            aggregated_diagrams.extend(path_b_result.get("diagrams", []))
            aggregated_charts.extend(path_b_result.get("charts", []))

            logger.info(f"[Stage F] チャンク{chunk_idx + 1} 完了: テキスト{len(path_a_result.get('full_text_ordered', ''))}文字, 表{len(merged_tables)}件")

        # ============================================
        # F-9: Result Convergence（全チャンク結果をマージ）
        # ============================================
        if progress_callback:
            progress_callback("F-9")

        logger.info("=" * 50)
        logger.info("[F-9] 全チャンク結果のマージ開始")

        # -----------------------------------------
        # アンカーベースのパケット配列を生成
        # -----------------------------------------
        anchors = self._build_anchor_packets(
            aggregated_blocks,
            aggregated_tables,
            aggregated_structured_candidates
        )

        logger.info(f"[F-9] アンカー生成: {len(anchors)}個")

        # 【Ver 3.8診断】蓄積テキストの確認
        logger.info(f"[F-9診断] aggregated_full_texts: {len(aggregated_full_texts)}チャンク")
        for i, txt in enumerate(aggregated_full_texts):
            logger.info(f"  ├─ チャンク{i+1}: {len(txt)}文字")
        final_full_text = "\n\n".join(aggregated_full_texts)
        logger.info(f"[F-9診断] 結合後: {len(final_full_text)}文字")

        # 後方互換のため従来形式も保持
        merged_result = {
            # 新形式: アンカーベース
            "anchors": anchors,
            # 旧形式: 互換性のため残す
            "text_source": {
                "full_text": final_full_text,
                "blocks": aggregated_blocks,
                "missed_texts": []
            },
            "tables": aggregated_tables,
            "structured_data_candidates": aggregated_structured_candidates,
            "visual_source": {
                "diagrams": aggregated_diagrams,
                "charts": aggregated_charts,
                "layout": {}
            },
            "metadata": {
                "path_a_model": F7_MODEL_IMAGE,
                "path_b_model": F8_MODEL,
                "table_count": len(aggregated_tables),
                "total_table_rows": sum(t.get("row_count", 0) for t in aggregated_tables),
                "total_pages": total_pages,
                "chunk_count": total_chunks,
                "chunk_size": CHUNK_SIZE_PAGES,
                "anchor_count": len(anchors)
            }
        }

        total_text_len = len(merged_result["text_source"]["full_text"])
        logger.info(f"[F-9] マージ完了: 総テキスト{total_text_len}文字, 総表{len(aggregated_tables)}件, アンカー{len(anchors)}個")

        # ============================================
        # F-10: Payload Validation
        # ============================================
        if progress_callback:
            progress_callback("F-10")

        validated_payload = self._f10_validate(merged_result, post_body)

        # チャンク処理情報を追加
        validated_payload["processing_mode"] = "dual_vision_chunked"
        validated_payload["chunk_info"] = {
            "total_pages": total_pages,
            "chunk_size": CHUNK_SIZE_PAGES,
            "chunk_count": total_chunks
        }
        validated_payload["warnings"].extend(chunk_warnings)

        return validated_payload

    # ============================================
    # F-1: Image Normalization
    # ============================================
    def _f1_normalize(self, file_path: Path, is_document: bool) -> List[Dict]:
        """
        F-1: ページ画像正規化
        PDF → 各ページを300dpiで画像化
        画像 → そのまま読み込み
        """
        f1_start = time.time()
        logger.info("[F-1] Image Normalization 開始")

        page_images = []
        DPI = F1_TARGET_DPI

        file_ext = file_path.suffix.lower()

        if file_ext == '.pdf':
            import fitz  # PyMuPDF
            doc = fitz.open(file_path)
            total_pages = len(doc)
            logger.info(f"  ├─ PDF: {total_pages}ページ")

            for page_num in range(total_pages):
                page = doc[page_num]
                mat = fitz.Matrix(DPI / 72, DPI / 72)
                pix = page.get_pixmap(matrix=mat)
                img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                page_images.append({
                    'page_index': page_num,
                    'image': img,
                    'width': pix.width,
                    'height': pix.height,
                    'dpi': DPI
                })
            doc.close()
        else:
            # 画像ファイル
            img = Image.open(file_path).convert("RGB")
            page_images.append({
                'page_index': 0,
                'image': img,
                'width': img.size[0],
                'height': img.size[1],
                'dpi': DPI
            })

        f1_elapsed = time.time() - f1_start
        logger.info(f"[F-1完了] {len(page_images)}ページ, {f1_elapsed:.2f}秒")

        return page_images

    # ============================================
    # F-2: Surya Block Detection
    # ============================================
    def _f2_detect_blocks(self, page_images: List[Dict]) -> List[Dict]:
        """
        F-2: Suryaブロック検出
        """
        f2_start = time.time()
        surya_blocks = []

        if not self.enable_surya or not page_images:
            logger.info("[F-2] Surya スキップ（無効 or ページなし）")
            return surya_blocks

        logger.info("[F-2] Surya Block Detection 開始")

        for page_data in page_images:
            page_idx = page_data['page_index']
            img = page_data['image']

            # リサイズ（Suryaメモリ対策）
            w, h = img.size
            scale = 1.0
            if max(w, h) > SURYA_MAX_DIM:
                scale = SURYA_MAX_DIM / max(w, h)
                img = img.resize((int(w * scale), int(h * scale)), Image.Resampling.LANCZOS)

            # Surya検出
            try:
                detection_results = self.surya_detector([img])
                if detection_results and detection_results[0].bboxes:
                    for block_id, box in enumerate(detection_results[0].bboxes):
                        raw_bbox = box.bbox
                        # 元の座標系に復元
                        restored_bbox = [
                            raw_bbox[0] / scale,
                            raw_bbox[1] / scale,
                            raw_bbox[2] / scale,
                            raw_bbox[3] / scale
                        ]
                        surya_blocks.append({
                            'page': page_idx,
                            'bbox': restored_bbox,
                            'block_id': f"p{page_idx}_b{block_id}",
                            'confidence': getattr(box, 'confidence', 1.0)
                        })
            except Exception as e:
                logger.warning(f"[F-2] Surya検出エラー (page {page_idx}): {e}")

        f2_elapsed = time.time() - f2_start
        logger.info(f"[F-2完了] {len(surya_blocks)}ブロック検出, {f2_elapsed:.2f}秒")

        return surya_blocks

    # ============================================
    # F-3: Coordinate Quantization
    # ============================================
    def _f3_quantize(self, surya_blocks: List[Dict], page_images: List[Dict]) -> List[Dict]:
        """
        F-3: 座標量子化（1000×1000グリッド）
        トークン削減の肝
        """
        f3_start = time.time()
        logger.info("[F-3] Coordinate Quantization 開始")

        page_dims = {p['page_index']: (p['width'], p['height']) for p in page_images}
        quantized = []

        for block in surya_blocks:
            page_idx = block['page']
            bbox = block['bbox']
            w, h = page_dims.get(page_idx, (1000, 1000))

            # 1000×1000 グリッドに量子化
            q_bbox = [
                int(bbox[0] * QUANTIZE_GRID_SIZE / w),
                int(bbox[1] * QUANTIZE_GRID_SIZE / h),
                int(bbox[2] * QUANTIZE_GRID_SIZE / w),
                int(bbox[3] * QUANTIZE_GRID_SIZE / h)
            ]

            quantized.append({
                **block,
                'bbox_original': bbox,
                'bbox': q_bbox,  # 量子化後の座標
                'page_width': w,
                'page_height': h
            })

        f3_elapsed = time.time() - f3_start
        logger.info(f"[F-3完了] {len(quantized)}ブロック量子化, {f3_elapsed:.2f}秒")

        return quantized

    # ============================================
    # F-3.5: 三つの物理的根拠（空白・罫線・密度）による動的分離
    # ============================================

    def _f35_filter_and_columnize(self, blocks: List[Dict]) -> List[Dict]:
        """
        F-3.5: 物理的根拠に基づく動的分離（Ver 3.2）

        【設計原則】
        1. ノイズ除去: 極小ブロックを物理削除（190→90程度）
        2. 分割軸の決定: 空白→罫線→密度の順に、縦OR横の一方を選択
        3. 排他的分割: 網の目状禁止、一方向のスライスのみ
        4. 横方向パッキング: 分割パッチ内で横のみ結合（縦結合禁止）
        5. ヘッダー/インデックス複製: 全パッチに基準列/行を随伴
        """
        import numpy as np

        f35_start = time.time()
        original_count = len(blocks)
        logger.info(f"[F-3.5] 動的分離開始: {original_count}ブロック")

        if not blocks:
            return []

        # ============================================
        # 【Ver 3.5】ページ別に全ブロックを保持（密度分析用）
        # ============================================
        all_pages = {}  # フィルタ前の全ブロック（密度分析用）
        for block in blocks:
            page = block.get('page', 0)
            if page not in all_pages:
                all_pages[page] = []
            all_pages[page].append(block)

        # ============================================
        # Step 1: ノイズブロックの物理削除（出力用）
        # 【Ver 3.5】閾値を大幅緩和 - 小さい文字ブロックも保持
        # ============================================
        MIN_BLOCK_SIZE = 5   # 最小サイズ（量子化後）← 15から緩和
        MIN_BLOCK_AREA = 50  # 最小面積 ← 300から緩和（偏差値「65」等を保持）

        filtered = []
        garbage_count = 0

        for block in blocks:
            bbox = block['bbox']
            w = bbox[2] - bbox[0]
            h = bbox[3] - bbox[1]
            area = w * h

            # 本当のゴミ（点やドット）だけ除外
            if w < MIN_BLOCK_SIZE and h < MIN_BLOCK_SIZE:
                garbage_count += 1
                continue
            if area < MIN_BLOCK_AREA:
                garbage_count += 1
                continue

            filtered.append(block)

        logger.info(f"  ├─ ノイズ除去: {garbage_count}ブロック削除 → 残り{len(filtered)}ブロック")

        # フィルタ後をページ別に整理（後処理用）
        pages = {}
        for block in filtered:
            page = block.get('page', 0)
            if page not in pages:
                pages[page] = []
            pages[page].append(block)

        all_processed = []

        for page_idx in all_pages.keys():
            # フィルタ後のブロック（後処理用）
            page_blocks = pages.get(page_idx, [])
            # フィルタ前の全ブロック（密度分析用）
            all_page_blocks = all_pages[page_idx]

            # ============================================
            # Step 2: 分割軸の決定（全ブロックで密度分析）
            # 【Ver 4.0】F-3.5の決定は「神の宣告」- F-7で上書き不可
            # ============================================
            split_axis, split_positions, split_reason = self._determine_split_axis(all_page_blocks)
            logger.info(f"  ├─ 密度分析: {len(all_page_blocks)}ブロック使用（フィルタ前）")
            logger.info(f"  ├─ ページ{page_idx}: 分割軸={split_axis}, 理由={split_reason}")

            # 【Ver 4.0】神の宣告をインスタンス変数に保存（F-7で参照、上書き禁止）
            self._current_split_axis = split_axis
            self._current_split_positions = split_positions
            self._current_split_reason = split_reason

            # 列境界情報を保存（互換性のため維持）
            if split_axis == 'vertical':
                self._page_column_boundaries[page_idx] = [0] + split_positions + [QUANTIZE_GRID_SIZE]
            else:
                self._page_column_boundaries[page_idx] = [0, QUANTIZE_GRID_SIZE]

            # ============================================
            # Step 3: 排他的分割（縦か横の一方のみ）
            # ============================================
            if split_axis == 'none':
                # 分割なし: 横方向パッキングのみ
                packed = self._horizontal_packing(page_blocks, page_idx)
                all_processed.extend(packed)

            elif split_axis == 'vertical':
                # 縦分割（左右に分ける）: インデックス列を複製
                split_result = self._vertical_split_with_index(
                    page_blocks, split_positions, page_idx
                )
                all_processed.extend(split_result)

            else:  # horizontal
                # 横分割（上下に分ける）: ヘッダー行を複製
                split_result = self._horizontal_split_with_header(
                    page_blocks, split_positions, page_idx
                )
                all_processed.extend(split_result)

        f35_elapsed = time.time() - f35_start
        reduction_rate = (1 - len(all_processed) / original_count) * 100 if original_count > 0 else 0
        logger.info(f"[F-3.5完了] {original_count} → {len(all_processed)}ブロック "
                    f"({reduction_rate:.1f}%削減), {f35_elapsed:.2f}秒")

        return all_processed

    def _determine_split_axis(self, blocks: List[Dict]) -> tuple:
        """
        【Ver 4.0】汎用グリッド制圧エンジン

        設計原則:
        1. 80%ルール: 表専有面積が80%以下なら分割しない
        2. ヘッダー最小化: ヘッダー数が少ない軸で分割
        3. 6本制限: 境界線は両端含め最大6本（5領域）

        Returns:
            (axis, positions, reason)
            axis: 'vertical' | 'horizontal' | 'none'
            positions: 分割位置のリスト（内部境界のみ、両端除く）
            reason: 決定理由
        """
        import numpy as np

        if not blocks:
            return ('none', [], 'ブロックなし')

        # ============================================
        # STEP 0: 表の専有面積を計算（80%ルール）
        # ============================================
        all_x = [b['bbox'][0] for b in blocks] + [b['bbox'][2] for b in blocks]
        all_y = [b['bbox'][1] for b in blocks] + [b['bbox'][3] for b in blocks]

        table_x_min, table_x_max = min(all_x), max(all_x)
        table_y_min, table_y_max = min(all_y), max(all_y)

        table_width = table_x_max - table_x_min
        table_height = table_y_max - table_y_min
        table_area = table_width * table_height
        page_area = QUANTIZE_GRID_SIZE * QUANTIZE_GRID_SIZE

        occupancy = table_area / page_area if page_area > 0 else 0

        logger.debug(f"    表専有率: {occupancy*100:.1f}% (W:{table_width}, H:{table_height})")

        # 80%以下なら分割不要
        if occupancy <= 0.80:
            return ('none', [], f'専有率{occupancy*100:.0f}%≤80%、分割不要')

        # ============================================
        # STEP 1: ヒストグラム構築（密度分析）
        # ============================================
        MIN_GAP = 20  # 最小ガター幅（緩和）
        MAX_INTERNAL_BOUNDARIES = 4  # 内部境界は最大4本（両端含め6本）

        x_histogram = np.zeros(QUANTIZE_GRID_SIZE)
        y_histogram = np.zeros(QUANTIZE_GRID_SIZE)
        x_line_height = np.zeros(QUANTIZE_GRID_SIZE)
        y_line_width = np.zeros(QUANTIZE_GRID_SIZE)

        for block in blocks:
            bbox = block['bbox']
            x_start, y_start = max(0, int(bbox[0])), max(0, int(bbox[1]))
            x_end, y_end = min(QUANTIZE_GRID_SIZE, int(bbox[2])), min(QUANTIZE_GRID_SIZE, int(bbox[3]))
            w, h = x_end - x_start, y_end - y_start

            x_histogram[x_start:x_end] += 1
            y_histogram[y_start:y_end] += 1

            if w <= 10:
                x_line_height[x_start:x_end] = np.maximum(x_line_height[x_start:x_end], h)
            if h <= 10:
                y_line_width[y_start:y_end] = np.maximum(y_line_width[y_start:y_end], w)

        # 【診断】X軸ヒストグラム全出力
        hist_str = "".join([f"[{i}]{int(x_histogram[i])}" for i in range(QUANTIZE_GRID_SIZE)])
        logger.info(f"  ├─ [X軸ヒストグラム] {hist_str}")

        # ============================================
        # STEP 2: 各軸の境界候補を検出
        # ============================================
        v_gaps = self._find_gaps(x_histogram, MIN_GAP, MAX_INTERNAL_BOUNDARIES)
        h_gaps = self._find_gaps(y_histogram, MIN_GAP, MAX_INTERNAL_BOUNDARIES)
        v_lines = self._find_lines(x_line_height, 300)[:MAX_INTERNAL_BOUNDARIES]
        h_lines = self._find_lines(y_line_width, 300)[:MAX_INTERNAL_BOUNDARIES]
        v_valleys = self._find_density_valleys(x_histogram, MAX_INTERNAL_BOUNDARIES)
        h_valleys = self._find_density_valleys(y_histogram, MAX_INTERNAL_BOUNDARIES)

        # 【Ver 3.7診断】各検出方法の結果を表示
        logger.info(f"  ├─ [診断] v_gaps: {v_gaps}")
        logger.info(f"  ├─ [診断] v_lines: {v_lines}")
        logger.info(f"  ├─ [診断] v_valleys: {v_valleys}")
        logger.info(f"  ├─ [診断] h_gaps: {h_gaps}")
        logger.info(f"  ├─ [診断] h_lines: {h_lines}")
        logger.info(f"  ├─ [診断] h_valleys: {h_valleys}")

        # 各軸の最良候補を統合（空白→罫線→密度の優先順）
        v_candidates = self._merge_boundary_candidates(v_gaps, v_lines, v_valleys, MAX_INTERNAL_BOUNDARIES)
        h_candidates = self._merge_boundary_candidates(h_gaps, h_lines, h_valleys, MAX_INTERNAL_BOUNDARIES)

        logger.info(f"  ├─ [診断] v_candidates（統合後）: {v_candidates}")
        logger.info(f"  ├─ [診断] h_candidates（統合後）: {h_candidates}")

        # ============================================
        # STEP 3: ヘッダー最小化ルールで軸を決定
        # ============================================
        # 垂直分割 = 列ヘッダーを複製（上端の行）
        # 水平分割 = 行ヘッダーを複製（左端の列）
        # → ヘッダー数が少ない方で分割

        # ユニークな行数・列数を概算（ヒストグラムのピーク数）
        v_peaks = self._count_density_peaks(x_histogram)  # 縦方向のピーク = 列数
        h_peaks = self._count_density_peaks(y_histogram)  # 横方向のピーク = 行数

        logger.debug(f"    列数(v_peaks):{v_peaks}, 行数(h_peaks):{h_peaks}")

        # 候補があるかチェック
        has_v = len(v_candidates) > 0
        has_h = len(h_candidates) > 0

        if not has_v and not has_h:
            return ('none', [], '境界候補なし')

        # ヘッダー最小化: ヘッダー数が少ない軸で分割
        # 垂直分割 → 行ヘッダー（h_peaks）を複製
        # 水平分割 → 列ヘッダー（v_peaks）を複製
        if has_v and has_h:
            # 両方候補がある場合、複製するヘッダー数が少ない方を選択
            if h_peaks <= v_peaks:
                # 行ヘッダーの方が少ない → 垂直分割
                axis = 'vertical'
                positions = v_candidates
                reason = f'ヘッダー最小化(行{h_peaks}≤列{v_peaks})→垂直分割'
            else:
                # 列ヘッダーの方が少ない → 水平分割
                axis = 'horizontal'
                positions = h_candidates
                reason = f'ヘッダー最小化(列{v_peaks}<行{h_peaks})→水平分割'
        elif has_v:
            axis = 'vertical'
            positions = v_candidates
            reason = '垂直境界のみ検出'
        else:
            axis = 'horizontal'
            positions = h_candidates
            reason = '水平境界のみ検出'

        return (axis, positions, reason)

    def _merge_boundary_candidates(
        self,
        gaps: List[tuple],
        lines: List[int],
        valleys: List[tuple],
        max_count: int
    ) -> List[int]:
        """空白・罫線・密度谷の候補を統合（重複除去・優先順位付き）"""
        candidates = []
        used_positions = set()
        MERGE_THRESHOLD = 30  # この距離以内は同一境界とみなす

        def add_if_new(pos):
            for used in used_positions:
                if abs(pos - used) < MERGE_THRESHOLD:
                    return False
            used_positions.add(pos)
            candidates.append(pos)
            return True

        # 優先1: 空白ガター（最も信頼性が高い）
        for pos, width in gaps:
            if len(candidates) >= max_count:
                break
            add_if_new(pos)

        # 優先2: 罫線
        for pos in lines:
            if len(candidates) >= max_count:
                break
            add_if_new(pos)

        # 優先3: 密度谷
        for pos, depth in valleys:
            if len(candidates) >= max_count:
                break
            add_if_new(pos)

        return sorted(candidates)

    def _count_density_peaks(self, histogram) -> int:
        """ヒストグラムからピーク（情報の柱）の数を数える"""
        import numpy as np

        if len(histogram) == 0 or np.max(histogram) == 0:
            return 0

        # スムージング
        kernel_size = 20
        kernel = np.ones(kernel_size) / kernel_size
        smoothed = np.convolve(histogram, kernel, mode='same')

        # 閾値以上の連続区間をカウント
        threshold = np.max(smoothed) * 0.2  # 最大値の20%以上
        in_peak = False
        peak_count = 0

        for val in smoothed:
            if val >= threshold and not in_peak:
                in_peak = True
                peak_count += 1
            elif val < threshold:
                in_peak = False

        return peak_count

    def _find_gaps(self, histogram, min_gap: int, max_count: int) -> List[tuple]:
        """ヒストグラムから空白ギャップを検出"""
        import numpy as np
        gaps = []
        in_gap = False
        gap_start = 0

        for i, val in enumerate(histogram):
            if val == 0 and not in_gap:
                in_gap = True
                gap_start = i
            elif val > 0 and in_gap:
                gap_width = i - gap_start
                if gap_width >= min_gap:
                    center = gap_start + gap_width // 2
                    if 50 < center < QUANTIZE_GRID_SIZE - 50:
                        gaps.append((center, gap_width))
                in_gap = False

        # 幅が広い順にソート
        gaps.sort(key=lambda x: x[1], reverse=True)
        return gaps[:max_count]

    def _find_lines(self, line_sizes, min_size: int) -> List[int]:
        """罫線位置を検出"""
        lines = []
        in_line = False
        line_start = 0

        for i, size in enumerate(line_sizes):
            if size >= min_size and not in_line:
                in_line = True
                line_start = i
            elif size < min_size and in_line:
                center = (line_start + i) // 2
                if 50 < center < QUANTIZE_GRID_SIZE - 50:
                    lines.append(center)
                in_line = False

        return lines

    def _find_density_valleys(self, histogram, max_count: int) -> List[tuple]:
        """
        【Ver 3.7修正】密度の谷を検出（隣接マージ版）

        問題: 同じ谷の隣接ピクセル(85,86,87,88)を別々に検出してしまう
        解決: 検出後に隣接する谷をマージし、真に独立した谷のみ返す
        """
        import numpy as np

        if len(histogram) < 100:
            return []

        # 移動平均でスムージング
        kernel_size = 30  # 大きめのカーネルで安定化
        smoothed = np.convolve(histogram, np.ones(kernel_size)/kernel_size, mode='same')

        # 全体の統計を取得
        mean_val = np.mean(smoothed)
        max_val = np.max(smoothed)

        # 局所最小値を検出（閾値を緩和）
        raw_valleys = []
        MIN_VALLEY_SEPARATION = 80  # 谷と谷の最小間隔（量子化座標）

        for i in range(60, len(smoothed) - 60):
            # 前後40ピクセルより低い = より広い範囲で谷を検出
            if smoothed[i] < smoothed[i-40] and smoothed[i] < smoothed[i+40]:
                # 深さ = 周囲との差
                depth = min(smoothed[i-40], smoothed[i+40]) - smoothed[i]
                # 相対的な深さも考慮（平均値の10%以上の落ち込み）
                relative_depth = (mean_val - smoothed[i]) / mean_val if mean_val > 0 else 0

                if depth > 0.3 or relative_depth > 0.1:
                    raw_valleys.append((i, depth, relative_depth))

        # 【重要】隣接する谷をマージ
        merged_valleys = []
        raw_valleys.sort(key=lambda x: x[0])  # 位置順でソート

        for pos, depth, rel_depth in raw_valleys:
            # 既存の谷と近すぎないかチェック
            is_new = True
            for j, (existing_pos, existing_depth, _) in enumerate(merged_valleys):
                if abs(pos - existing_pos) < MIN_VALLEY_SEPARATION:
                    # 近い谷がある場合、深い方を採用
                    if depth > existing_depth:
                        merged_valleys[j] = (pos, depth, rel_depth)
                    is_new = False
                    break

            if is_new:
                merged_valleys.append((pos, depth, rel_depth))

        # 深さ順でソートして返す
        merged_valleys.sort(key=lambda x: x[1], reverse=True)

        logger.debug(f"    [密度谷] raw={len(raw_valleys)}個 → merged={len(merged_valleys)}個")

        return [(pos, depth) for pos, depth, _ in merged_valleys[:max_count]]

    def _horizontal_packing(self, blocks: List[Dict], page_idx: int) -> List[Dict]:
        """横方向限定パッキング: 縦結合禁止、横のみ結合"""
        Y_TOLERANCE = 30  # 同じ行とみなすY座標の許容差

        # Y座標でグループ化
        blocks_sorted = sorted(blocks, key=lambda b: b['bbox'][1])
        rows = []
        current_row = []
        current_y = None

        for block in blocks_sorted:
            y_center = (block['bbox'][1] + block['bbox'][3]) / 2

            if current_y is None or abs(y_center - current_y) <= Y_TOLERANCE:
                current_row.append(block)
                if current_y is None:
                    current_y = y_center
                else:
                    current_y = (current_y + y_center) / 2  # 平均を更新
            else:
                if current_row:
                    rows.append(current_row)
                current_row = [block]
                current_y = y_center

        if current_row:
            rows.append(current_row)

        # 各行を1つのブロックに統合
        result = []
        for row_idx, row_blocks in enumerate(rows):
            if not row_blocks:
                continue

            # X座標でソート
            row_blocks.sort(key=lambda b: b['bbox'][0])

            # バウンディングボックスを統合
            x_min = min(b['bbox'][0] for b in row_blocks)
            y_min = min(b['bbox'][1] for b in row_blocks)
            x_max = max(b['bbox'][2] for b in row_blocks)
            y_max = max(b['bbox'][3] for b in row_blocks)

            # 【Ver 3.4修正】block_id を必ず付与（F-6が要求）
            # 元のブロックからblock_idを継承、なければ新規生成
            original_ids = [b.get('block_id') for b in row_blocks if b.get('block_id')]
            if original_ids:
                # 統合元の最初のblock_idを継承
                block_id = original_ids[0]
            else:
                # 新規生成: p{page}_r{row} 形式
                block_id = f"p{page_idx}_r{row_idx}"

            # ページサイズ情報を継承（座標変換で必要）
            ref_block = row_blocks[0]
            page_width = ref_block.get('page_width', 1000)
            page_height = ref_block.get('page_height', 1000)

            merged = {
                'block_id': block_id,  # F-6が要求するキー
                'bbox': [x_min, y_min, x_max, y_max],
                'page': page_idx,
                'page_width': page_width,
                'page_height': page_height,
                'row_id': row_idx,
                'block_count': len(row_blocks),
                'is_row_packed': True,
                'merged_from': original_ids if len(original_ids) > 1 else None
            }
            result.append(merged)

        logger.info(f"  ├─ 横パッキング: {len(blocks)}ブロック → {len(result)}行")
        return result

    def _vertical_split_with_index(self, blocks: List[Dict], positions: List[int], page_idx: int) -> List[Dict]:
        """縦分割（インデックス列複製付き）"""
        boundaries = [0] + positions + [QUANTIZE_GRID_SIZE]
        num_columns = len(boundaries) - 1

        # 列ごとにブロックを分類
        columns = {i: [] for i in range(num_columns)}

        for block in blocks:
            x_center = (block['bbox'][0] + block['bbox'][2]) / 2
            for i in range(num_columns):
                if boundaries[i] <= x_center < boundaries[i + 1]:
                    columns[i].append(block)
                    break

        # インデックス列（左端）を特定
        index_column = columns.get(0, [])

        result = []
        for col_id in range(num_columns):
            col_blocks = columns[col_id]
            if not col_blocks:
                continue

            # 横方向パッキング
            packed = self._horizontal_packing(col_blocks, page_idx)

            for block in packed:
                block['column_id'] = col_id
                block['num_columns'] = num_columns
                block['has_index_column'] = col_id > 0  # 2列目以降はインデックス列を随伴
                block['col_boundaries'] = [boundaries[col_id], boundaries[col_id + 1]]

            result.extend(packed)

        # 列境界を保存
        self._page_column_boundaries[page_idx] = boundaries

        logger.info(f"  ├─ 縦分割: {num_columns}列, インデックス列={len(index_column)}ブロック")
        return result

    def _horizontal_split_with_header(self, blocks: List[Dict], positions: List[int], page_idx: int) -> List[Dict]:
        """横分割（ヘッダー行複製付き）"""
        boundaries = [0] + positions + [QUANTIZE_GRID_SIZE]
        num_sections = len(boundaries) - 1

        # セクションごとにブロックを分類
        sections = {i: [] for i in range(num_sections)}

        for block in blocks:
            y_center = (block['bbox'][1] + block['bbox'][3]) / 2
            for i in range(num_sections):
                if boundaries[i] <= y_center < boundaries[i + 1]:
                    sections[i].append(block)
                    break

        # ヘッダー行（最上段）を特定
        header_section = sections.get(0, [])

        result = []
        for sec_id in range(num_sections):
            sec_blocks = sections[sec_id]
            if not sec_blocks:
                continue

            # 横方向パッキング
            packed = self._horizontal_packing(sec_blocks, page_idx)

            for block in packed:
                block['section_id'] = sec_id
                block['num_sections'] = num_sections
                block['has_header_row'] = sec_id > 0  # 2セクション目以降はヘッダー行を随伴
                block['section_boundaries'] = [boundaries[sec_id], boundaries[sec_id + 1]]

            result.extend(packed)

        # 水平分割の場合、列境界は1列扱い
        self._page_column_boundaries[page_idx] = [0, QUANTIZE_GRID_SIZE]

        logger.info(f"  ├─ 横分割: {num_sections}セクション, ヘッダー行={len(header_section)}ブロック")
        return result

    def _merge_column_blocks(
        self,
        col_blocks: List[Dict],
        page_idx: int,
        column_id: int,
        num_columns: int,
        col_x_start: int,
        col_x_end: int
    ) -> Dict:
        """
        【ページ完結型】列内の全ブロックを1つのテーブルブロックに統合

        Args:
            col_blocks: この列に属する全ブロック（Y座標でソート済み）
            page_idx: ページ番号
            column_id: 列番号
            num_columns: このページの総列数
            col_x_start: 列の左端X座標
            col_x_end: 列の右端X座標

        Returns:
            統合されたブロック（1ページ1列1ブロック）
        """
        if len(col_blocks) == 1:
            block = col_blocks[0]
            block['table_id'] = f"TBL_P{page_idx}_C{column_id}"
            block['is_page_complete'] = True
            block['row_count'] = 1
            return block

        # 全ブロックのバウンディングボックスを統合
        x_min = min(b['bbox'][0] for b in col_blocks)
        y_min = min(b['bbox'][1] for b in col_blocks)
        x_max = max(b['bbox'][2] for b in col_blocks)
        y_max = max(b['bbox'][3] for b in col_blocks)

        # 行データを構築（Y座標順）
        rows = []
        for block in col_blocks:
            rows.append({
                'block_id': block['block_id'],
                'y': block['y_center'],
                'bbox': block['bbox']
            })

        # ページ跨ぎの判定（下端に近いか）
        is_at_page_bottom = y_max > 900  # 1000グリッド中900以上
        is_at_page_top = y_min < 100     # 1000グリッド中100以下

        return {
            'page': page_idx,
            'block_id': f"col_P{page_idx}_C{column_id}",
            'table_id': f"TBL_P{page_idx}_C{column_id}",
            'bbox': [x_min, y_min, x_max, y_max],
            'bbox_original': col_blocks[0].get('bbox_original'),
            'page_width': col_blocks[0].get('page_width'),
            'page_height': col_blocks[0].get('page_height'),
            'column_id': column_id,
            'num_columns': num_columns,
            'col_x_range': [col_x_start, col_x_end],
            'x_center': (x_min + x_max) / 2,
            'y_center': (y_min + y_max) / 2,
            # ページ完結型メタデータ
            'is_page_complete': True,
            'row_count': len(col_blocks),
            'rows': rows,
            'original_block_ids': [b['block_id'] for b in col_blocks],
            # ページ跨ぎ用フラグ（Stage Hで使用）
            'is_continued_from_prev': is_at_page_top,  # 前ページから続く可能性
            'is_continued_to_next': is_at_page_bottom,  # 次ページに続く可能性
            'stitch_hint': {
                'prev_page': page_idx - 1 if is_at_page_top else None,
                'next_page': page_idx + 1 if is_at_page_bottom else None,
                'column_id': column_id
            }
        }

    def _merge_row_blocks(self, row_blocks: List[Dict], page_idx: int, column_id: int) -> Dict:
        """
        同じ行のブロックを1つにマージ（レガシー互換用）
        """
        if len(row_blocks) == 1:
            return row_blocks[0]

        # X座標でソート
        row_blocks.sort(key=lambda b: b['bbox'][0])

        # バウンディングボックスを統合
        x_min = min(b['bbox'][0] for b in row_blocks)
        y_min = min(b['bbox'][1] for b in row_blocks)
        x_max = max(b['bbox'][2] for b in row_blocks)
        y_max = max(b['bbox'][3] for b in row_blocks)

        # 元のblock_idを結合
        merged_id = "+".join(b['block_id'] for b in row_blocks)

        return {
            'page': page_idx,
            'block_id': f"merged_{merged_id}",
            'bbox': [x_min, y_min, x_max, y_max],
            'bbox_original': row_blocks[0].get('bbox_original'),
            'page_width': row_blocks[0].get('page_width'),
            'page_height': row_blocks[0].get('page_height'),
            'confidence': min(b.get('confidence', 1.0) for b in row_blocks),
            'column_id': column_id,
            'x_center': (x_min + x_max) / 2,
            'y_center': (y_min + y_max) / 2,
            'merged_count': len(row_blocks),
            'original_blocks': [b['block_id'] for b in row_blocks]
        }

    # ============================================
    # F-4: Logical Reading Order
    # ============================================
    def _f4_reading_order(self, blocks: List[Dict]) -> List[Dict]:
        """
        F-4: 読む順序の確定
        F-3.5で検出された列を使用して正確にソート
        """
        f4_start = time.time()
        logger.info("[F-4] Logical Reading Order 開始")

        if not blocks:
            return []

        # F-3.5で既にcolumn_id, x_center, y_centerが設定されている場合はそのまま使用
        for block in blocks:
            if 'x_center' not in block:
                bbox = block['bbox']
                block['x_center'] = (bbox[0] + bbox[2]) / 2
            if 'y_center' not in block:
                bbox = block['bbox']
                block['y_center'] = (bbox[1] + bbox[3]) / 2
            if 'column_id' not in block:
                # フォールバック: 左右2分割
                block['column_id'] = 0 if block['x_center'] < QUANTIZE_GRID_SIZE / 2 else 1

        # ソート: page → column → y → x
        sorted_blocks = sorted(
            blocks,
            key=lambda b: (
                b.get('page', 0),
                b.get('column_id', 0),
                b.get('y_center', 0),
                b.get('x_center', 0)
            )
        )

        # reading_order を付与
        for order, block in enumerate(sorted_blocks):
            block['reading_order'] = order

        f4_elapsed = time.time() - f4_start
        logger.info(f"[F-4完了] {len(sorted_blocks)}ブロック順序確定, {f4_elapsed:.2f}秒")

        return sorted_blocks

    # ============================================
    # F-5: Block Classification
    # ============================================
    def _f5_classify(self, blocks: List[Dict]) -> List[Dict]:
        """
        F-5: 構造ラベル付与
        ルールベースでブロックタイプを推定
        """
        f5_start = time.time()
        logger.info("[F-5] Block Classification 開始")

        for block in blocks:
            bbox = block['bbox']
            w = bbox[2] - bbox[0]
            h = bbox[3] - bbox[1]
            y = block.get('y_center', 500)
            area = w * h
            aspect_ratio = w / h if h > 0 else 1.0

            # ブロックタイプ推定
            block_type = 'body_hint'

            # 表の検出
            if aspect_ratio > 3.0 and 20 < h < 150:
                block_type = 'table_hint'
            elif 10000 < area < 500000 and 0.5 < aspect_ratio < 3.0:
                block_type = 'table_hint'
            # 見出し
            elif h < 80 and w > 200 and y < 200:
                block_type = 'heading_hint'
            # ヘッダー
            elif y < 80:
                block_type = 'header_hint'
            # フッター
            elif y > 920:
                block_type = 'footer_hint'
            # 注記
            elif area < 5000:
                block_type = 'note_hint'

            block['block_type_hint'] = block_type

        f5_elapsed = time.time() - f5_start
        type_counts = {}
        for b in blocks:
            t = b.get('block_type_hint', 'unknown')
            type_counts[t] = type_counts.get(t, 0) + 1
        logger.info(f"[F-5完了] ラベル分布: {type_counts}, {f5_elapsed:.2f}秒")

        return blocks

    # ============================================
    # F-6: ID焼き込み画像生成（Ver 4.0 - 座標排除）
    # ============================================
    def _f6_burn_ids_to_image(
        self,
        image: Image.Image,
        blocks: List[Dict],
        page_idx: int = 0
    ) -> Image.Image:
        """
        F-6: ブロックIDを画像に直接焼き込む

        【Ver 4.0】座標データはAIに渡さない。
        代わりに、IDを視覚的に画像に描画し、AIは「見たまま」判断する。

        Args:
            image: 元画像（PIL Image）
            blocks: ブロックリスト（block_id, bbox含む）
            page_idx: ページ番号

        Returns:
            ID焼き込み済み画像
        """
        from PIL import ImageDraw, ImageFont
        import io

        f6_start = time.time()
        logger.info(f"[F-6] ID焼き込み開始: {len(blocks)}ブロック")

        # 画像をコピー（元画像を変更しない）
        img_with_ids = image.copy()
        draw = ImageDraw.Draw(img_with_ids)

        # フォント設定（システムフォントを使用）
        try:
            font = ImageFont.truetype("arial.ttf", 14)
        except:
            try:
                font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 14)
            except:
                font = ImageFont.load_default()

        img_width, img_height = image.size

        # 各ブロックにIDを描画
        for block in blocks:
            block_id = block.get('block_id', '')
            bbox = block.get('bbox', [0, 0, 0, 0])

            # 量子化座標を実座標に変換
            page_width = block.get('page_width', QUANTIZE_GRID_SIZE)
            page_height = block.get('page_height', QUANTIZE_GRID_SIZE)

            x1 = int(bbox[0] * img_width / QUANTIZE_GRID_SIZE)
            y1 = int(bbox[1] * img_height / QUANTIZE_GRID_SIZE)
            x2 = int(bbox[2] * img_width / QUANTIZE_GRID_SIZE)
            y2 = int(bbox[3] * img_height / QUANTIZE_GRID_SIZE)

            # IDラベル（短縮形: p0_r5 → #5）
            short_id = block_id.split('_')[-1] if '_' in block_id else block_id
            label = f"#{short_id}"

            # 背景付きでIDを描画（視認性確保）
            text_bbox = draw.textbbox((x1, y1), label, font=font)
            padding = 2
            draw.rectangle(
                [text_bbox[0] - padding, text_bbox[1] - padding,
                 text_bbox[2] + padding, text_bbox[3] + padding],
                fill='yellow'
            )
            draw.text((x1, y1), label, fill='red', font=font)

            # ブロック境界を薄く描画（デバッグ用、オプション）
            # draw.rectangle([x1, y1, x2, y2], outline='blue', width=1)

        f6_elapsed = time.time() - f6_start
        logger.info(f"[F-6完了] {len(blocks)}個のID焼き込み, {f6_elapsed:.2f}秒")

        return img_with_ids

    def _f6_store_id_mapping(self, blocks: List[Dict]) -> Dict[str, Dict]:
        """
        ID→座標マッピングをシステム内部に保持（AIには渡さない）

        Returns:
            {block_id: {bbox, page, column_id, ...}}
        """
        mapping = {}
        for block in blocks:
            block_id = block.get('block_id', '')
            mapping[block_id] = {
                'bbox': block.get('bbox'),
                'page': block.get('page', 0),
                'column_id': block.get('column_id', 0),
                'row_id': block.get('row_id', 0),
            }
        return mapping

    # ============================================
    # F-7: Dual Read - Path A (Text Extraction)
    # 【Ver 4.0】座標排除 - ID焼き込み画像のみ使用
    # ============================================
    def _f7_path_a_text_extraction(
        self,
        file_path: Path,
        page_images: List[Dict] = None
    ) -> Dict[str, Any]:
        """
        F-7 Path A: 構造マッピング（gemini-2.0-flash）

        【Ver 4.0】座標データは渡さない。ID焼き込み画像を見て判断。
        """
        f7_start = time.time()
        logger.info("[F-7] Path A - 構造マッピング開始（座標排除版）")

        prompt = self._build_f7_prompt()

        try:
            response = self.llm_client.generate_with_vision(
                prompt=prompt,
                image_path=str(file_path),
                model=F7_MODEL_IMAGE,
                max_tokens=F7_F8_MAX_TOKENS,
                temperature=F7_F8_TEMPERATURE,
                response_format="json"
            )

            # JSON パース
            try:
                result = json.loads(response)
            except json.JSONDecodeError:
                import json_repair
                result = json_repair.repair_json(response, return_objects=True)

            f7_elapsed = time.time() - f7_start
            logger.info(f"[F-7完了] Path A: {len(response)}文字, {f7_elapsed:.2f}秒")

            return result

        except Exception as e:
            logger.error(f"[F-7] Path A エラー: {e}")
            return {"error": str(e), "extracted_texts": [], "tables": []}

    def _infer_doc_type(self, mime_type: str, file_path: Path) -> str:
        """MIMEタイプとファイル名からドキュメントタイプを推測"""
        if not mime_type:
            return "unknown"

        if mime_type == 'application/pdf':
            # ファイル名からヒントを得る
            name = file_path.name.lower() if file_path else ""
            if '成績' in name or 'score' in name or '偏差値' in name:
                return "成績表・偏差値表"
            elif '時間割' in name or 'schedule' in name or 'timetable' in name:
                return "時間割・スケジュール"
            elif '通信' in name or 'news' in name or 'letter' in name:
                return "お知らせ・通信"
            return "PDF文書"
        elif mime_type.startswith('image/'):
            return "画像"
        elif mime_type.startswith('text/'):
            return "テキスト"
        return "その他"

    def _build_f7_prompt(self) -> str:
        """
        【Ver 4.0】F-7 純粋構造マッピング・プロンプト

        座標データは一切含まない。AIは画像を「見たまま」判断する。
        """
        return """# 構造マッピング指示

## あなたのタスク
この画像には、各情報ユニットに **#ID番号** が視覚的に振られています。

1. 画像の **最上部（または最左部）** にある項目を「基準軸（Header）」として認識してください
2. 各 #ID が、どの基準軸項目の下に位置しているかを判定してください
3. 表形式のデータは、行と列の交差点として各セルの内容を読み取ってください

## 出力形式

```json
{
  "headers": {
    "horizontal": ["列見出し1", "列見出し2", "..."],
    "vertical": ["行見出し1", "行見出し2", "..."]
  },
  "cells": [
    {"id": "#0", "h_header": "列見出し", "v_header": "行見出し", "value": "セルの値"},
    {"id": "#1", "h_header": "列見出し", "v_header": "行見出し", "value": "セルの値"}
  ],
  "texts": [
    {"id": "#2", "type": "title", "value": "タイトルテキスト"},
    {"id": "#3", "type": "note", "value": "注釈テキスト"}
  ]
}
```

## ルール
- **#ID番号を正確に読み取る**：画像内の黄色背景・赤文字のラベルがIDです
- **全セルを抽出**：省略禁止、要約禁止
- **見たままを記録**：読めない文字は「[判読不可]」と記載
- **交差点で判断**：各セルがどの行見出し×列見出しの位置にあるかを判定

## 禁止事項
- 座標の推測（座標情報は与えられていません）
- データの要約や省略
- 存在しないIDの捏造
"""

    # ============================================
    # F-7: Transcription (音声/動画用)
    # ============================================
    def _f7_transcription(
        self,
        file_path: Path,
        mime_type: str,
        is_video: bool
    ) -> Dict[str, Any]:
        """
        F-7: 音声/動画の書き起こし（gemini-2.5-flash-lite）
        """
        f7_start = time.time()
        media_type = "動画" if is_video else "音声"
        logger.info(f"[F-7] Transcription ({media_type}) 開始")

        prompt = self._build_transcription_prompt(is_video)

        try:
            import google.generativeai as genai

            # ファイルアップロード
            logger.info(f"  ├─ ファイルアップロード中: {file_path.name}")
            uploaded_file = genai.upload_file(path=str(file_path), mime_type=mime_type)

            # 処理完了待機
            while uploaded_file.state.name == "PROCESSING":
                time.sleep(2)
                uploaded_file = genai.get_file(uploaded_file.name)

            if uploaded_file.state.name == "FAILED":
                raise ValueError(f"ファイル処理失敗: {uploaded_file.state.name}")

            # モデル初期化
            model = genai.GenerativeModel(F7_MODEL_AV)

            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]

            generation_config = genai.GenerationConfig(
                max_output_tokens=F7_F8_MAX_TOKENS,
                temperature=F7_F8_TEMPERATURE
            )

            # 生成
            response = model.generate_content(
                [prompt, uploaded_file],
                generation_config=generation_config,
                safety_settings=safety_settings,
                request_options={"timeout": 600}
            )

            transcript = ""
            visual_log = ""

            if response.candidates and response.candidates[0].content.parts:
                raw_text = response.candidates[0].content.parts[0].text

                # JSON パース試行
                try:
                    result = json.loads(raw_text)
                    transcript = result.get('transcript', raw_text)
                    visual_log = result.get('visual_log', '')
                except:
                    transcript = raw_text

            # ファイル削除
            try:
                genai.delete_file(name=uploaded_file.name)
            except:
                pass

            f7_elapsed = time.time() - f7_start
            logger.info(f"[F-7完了] Transcription: {len(transcript)}文字, {f7_elapsed:.2f}秒")

            return {
                "transcript": transcript,
                "visual_log": visual_log if is_video else "",
                "media_type": media_type,
                "model": F7_MODEL_AV
            }

        except Exception as e:
            logger.error(f"[F-7] Transcription エラー: {e}")
            return {"error": str(e), "transcript": "", "visual_log": ""}

    def _build_transcription_prompt(self, is_video: bool) -> str:
        """Transcription用プロンプト"""
        base = """# 音声/映像 完全書き起こし

## Mission
一言一句完全な書き起こしを行ってください。

## 重要な指示
- 「あー」「えー」「うーん」などのフィラーも全て書き起こす
- 言い淀み、言い直しもそのまま記録
- 笑い声、咳払いは [笑い]、[咳払い] のように記録
- 沈黙が長い場合は [沈黙 約5秒] のように記録
- 複数人の場合は話者を識別（話者A、話者B）
- 聞き取れない部分は [聞き取り不明] と記載

## 禁止事項
- 要約は絶対に禁止
- 文章の整理や言い換えは禁止
- 内容の省略は禁止

## 出力形式
```json
{
  "transcript": "[00:00] 話者A: えー、本日は...",
  "visual_log": ""
}
```
"""
        if is_video:
            base += """
## 動画の場合: visual_log も記録
```json
{
  "transcript": "...",
  "visual_log": "[00:00] 黒背景、中央にロゴ\\n[00:03] オフィス会議室が映る..."
}
```
"""
        return base

    # ============================================
    # F-8: Dual Read - Path B (Visual Analysis)
    # ============================================
    def _f8_path_b_visual_analysis(
        self,
        file_path: Path,
        page_images: List[Dict] = None
    ) -> Dict[str, Any]:
        """
        【Ver 4.0】F-8 Path B: 構造解析（座標排除版）
        """
        f8_start = time.time()
        logger.info("[F-8] Path B - Visual Analysis 開始（座標排除版）")

        prompt = self._build_f8_prompt()

        try:
            response = self.llm_client.generate_with_vision(
                prompt=prompt,
                image_path=str(file_path),
                model=F8_MODEL,
                max_tokens=F7_F8_MAX_TOKENS,
                temperature=F7_F8_TEMPERATURE,
                response_format="json"
            )

            # JSON パース
            try:
                result = json.loads(response)
            except json.JSONDecodeError:
                import json_repair
                result = json_repair.repair_json(response, return_objects=True)

            f8_elapsed = time.time() - f8_start
            logger.info(f"[F-8完了] Path B: {len(response)}文字, {f8_elapsed:.2f}秒")

            return result

        except Exception as e:
            logger.error(f"[F-8] Path B エラー: {e}")
            return {"error": str(e), "tables": [], "diagrams": [], "layout_analysis": {}}

    def _build_f8_prompt(self) -> str:
        """【Ver 4.0】F-8用プロンプト構築（座標排除版）"""
        return """# F-8: 構造解析

## Mission
画像内の表・図解の構造を解析せよ。
文字の書き起こしはPath Aが担当。あなたは**構造・関係性**に集中。

## 解析対象

### 1. 表の構造解析（最重要）

#### 検出すべき情報
- **セル結合**: colspan（横結合）、rowspan（縦結合）の正確な位置と範囲
- **ヘッダー構造**: 何行目までがヘッダーか、多段ヘッダーの場合その構造
- **データ型推定**: 各列が以下のいずれか
  - `text`: テキスト（氏名、項目名など）
  - `number`: 数値（個数、順位など）
  - `currency`: 金額（円、ドルなど）
  - `date`: 日付
  - `time`: 時刻
  - `percentage`: パーセンテージ
- **小計・合計行**: 太字や背景色で強調された集計行の位置（row index）
- **空白セルの意味**: 「データなし」か「上と同じ（ditto）」か

#### 表の種類（table_type）を判定
- `list_type`: リスト型（縦方向にデータが並ぶ）
  - ランキング表、名簿、成績一覧、商品リスト
  - 各行が1つのエントリ（人、商品、項目）を表す
- `matrix_type`: マトリクス型（横軸に日付や項目が並ぶ）
  - 時間割、月間予定表、週間スケジュール
  - 行と列の交差点にデータがある

### 2. 構造化可能なデータの検出
視覚的に表として表示されていなくても、以下のパターンを検出：
- **Key-Valueペア**: 「項目名: 値」の繰り返し
- **ランキング・順位表**: 「1位: ○○」「2位: △△」
- **カンマ区切りデータ**: 「A, B, C」のような並列データ
- **罫線なしの表**: 空白/タブで区切られた列データ

### 3. 図解・フローチャート
- 要素間の接続、矢印の向き
- 階層構造、グループ化
- 条件分岐（Yes/No）

### 4. グラフ・チャート
- グラフ種類（棒、折れ線、円、散布図）
- 軸ラベル、凡例
- データ傾向（増加、減少、ピーク位置）

### 5. レイアウト
- 段組構造、強調パターン
- セクション区切り

## 出力形式
```json
{{
  "tables": [
    {{
      "block_id": "p0_b5",
      "table_type": "list_type",
      "structure": {{
        "header_rows": 1,
        "total_rows": 10,
        "total_cols": 5,
        "merged_cells": [
          {{"row": 0, "col": 0, "rowspan": 2, "colspan": 1, "content_hint": "項目名"}}
        ],
        "column_types": ["text", "text", "currency", "currency", "percentage"],
        "summary_rows": [9],
        "has_footer": true
      }},
      "semantic_role": "四半期売上比較表",
      "data_quality": {{
        "empty_cells": 2,
        "ditto_cells": 0,
        "needs_verification": false
      }}
    }}
  ],
  "structured_data_candidates": [
    {{
      "location": "本文中段",
      "pattern": "key_value_pairs",
      "suggested_headers": ["項目", "内容"],
      "estimated_rows": 5,
      "source_text_hint": "提出期限: 2025-01-15..."
    }}
  ],
  "diagrams": [
    {{
      "block_id": "p0_b8",
      "type": "flowchart",
      "elements_count": 5,
      "connections_count": 4,
      "has_conditions": true,
      "semantic_role": "申請承認フロー"
    }}
  ],
  "charts": [
    {{
      "block_id": "p0_b12",
      "type": "bar_chart",
      "x_axis": "月",
      "y_axis": "売上（万円）",
      "data_points_approx": 12,
      "trend": "Q3で急増、Q4で減少"
    }}
  ],
  "layout_analysis": {{
    "column_structure": "2-column",
    "sections": [
      {{"name": "ヘッダー", "blocks": ["p0_b0"], "purpose": "タイトル"}},
      {{"name": "本文", "blocks": ["p0_b1", "p0_b2"], "purpose": "説明文"}},
      {{"name": "表エリア", "blocks": ["p0_b5"], "purpose": "データ表示"}}
    ],
    "emphasis_patterns": ["見出しは青色太字", "重要数値は赤色"]
  }}
}}
```

## 禁止事項
- 文字の書き起こし禁止（それはPath Aの仕事）
- 推測による補完禁止
- **表の行数・列数を間違えることは許されない（正確にカウント）**
"""

    # ============================================
    # チャンク処理用メソッド（MAX_TOKENSエラー回避）
    # ============================================

    def _save_chunk_as_temp_image(self, chunk_pages: List[Dict], chunk_idx: int) -> Path:
        """
        チャンク内の画像を一時ファイルとして保存
        複数ページの場合は縦に結合した1枚の画像として保存
        """
        import tempfile

        if len(chunk_pages) == 1:
            # 1ページのみ: そのまま保存
            img = chunk_pages[0]['image']
        else:
            # 複数ページ: 縦に結合
            images = [p['image'] for p in chunk_pages]
            total_height = sum(img.size[1] for img in images)
            max_width = max(img.size[0] for img in images)

            combined = Image.new('RGB', (max_width, total_height), (255, 255, 255))
            y_offset = 0
            for img in images:
                combined.paste(img, (0, y_offset))
                y_offset += img.size[1]

            img = combined

        # 一時ファイルに保存
        temp_file = tempfile.NamedTemporaryFile(
            suffix=f'_chunk{chunk_idx}.png',
            delete=False
        )
        img.save(temp_file.name, 'PNG')
        temp_file.close()

        return Path(temp_file.name)

    # ============================================
    # ============================================
    # 【Ver 4.0】泥棒ロジック（_detect_gutters）完全削除済み
    # F-7は自律判断を行わない。F-3.5の決定を無条件実行するのみ。
    # ============================================

    def _smart_crop_patches(
        self,
        image: Image.Image,
        column_boundaries: List[int],
        overlap: int = 50
    ) -> List[Dict]:
        """
        【Ver 4.0】F-3.5の決定を「神の宣告」として無条件実行

        ============================================
        泥棒ロジック完全排除版
        ============================================

        このメソッドは「自律判断」を一切行わない。
        F-3.5が決定した split_axis と boundaries だけを信じ、
        ただ切る。それ以外のことは1行もしない。
        """
        img_width, img_height = image.size
        patches = []

        # ============================================
        # F-3.5の決定を取得（上書き不可の神託）
        # ============================================
        split_axis = getattr(self, '_current_split_axis', 'none')
        split_positions = getattr(self, '_current_split_positions', [])

        # 【Ver 3.7】column_boundariesからも境界を復元（フォールバック）
        if not split_positions and column_boundaries and len(column_boundaries) > 2:
            # column_boundaries は [0, pos1, pos2, ..., 1000] 形式
            # 内部境界（0と1000以外）を抽出
            split_positions = [b for b in column_boundaries if 0 < b < QUANTIZE_GRID_SIZE]
            split_axis = 'vertical' if split_positions else 'none'
            logger.warning(f"[SmartCrop] _current_split_positionsが空 → column_boundariesから復元: {split_positions}")

        logger.info(f"[SmartCrop] F-3.5神託: split_axis={split_axis}")
        logger.info(f"[SmartCrop] split_positions: {split_positions}")
        logger.info(f"[SmartCrop] column_boundaries（引数）: {column_boundaries}")

        # ============================================
        # 分割なしの場合: 画像全体を1枚で返す
        # ============================================
        if split_axis == 'none' or not split_positions:
            logger.info("[SmartCrop] 分割なし → 全体を1枚で返却")
            return [{
                'image': image,
                'type': 'full',
                'info': '全体（F-3.5: 分割不要）',
                'patch_index': 0,
                'total_patches': 1,
                'is_continuation': False
            }]

        # ============================================
        # 垂直分割（縦に切る = 左右に分ける）
        # 【Ver 3.7】全境界線での強制スライス
        # ============================================
        if split_axis == 'vertical':
            # 量子化座標を実座標に変換
            boundaries_px = [0]
            for pos in split_positions:
                px = int(pos * img_width / QUANTIZE_GRID_SIZE)
                # 重複排除・有効範囲チェック
                if px > boundaries_px[-1] + 50 and px < img_width - 50:
                    boundaries_px.append(px)
            boundaries_px.append(img_width)

            num_columns = len(boundaries_px) - 1

            # 【診断ログ】全境界線を表示
            logger.info(f"[SmartCrop] ======== 垂直分割診断 ========")
            logger.info(f"[SmartCrop] split_positions（量子化）: {split_positions}")
            logger.info(f"[SmartCrop] boundaries_px（実座標）: {boundaries_px}")
            logger.info(f"[SmartCrop] 画像サイズ: {img_width}x{img_height}")
            logger.info(f"[SmartCrop] 生成予定列数: {num_columns}")

            # 最低2列は必要（1列なら分割なしと同じ）
            if num_columns < 2:
                logger.warning(f"[SmartCrop] 列数不足({num_columns}) → 全体を1枚で返却")
                return [{
                    'image': image,
                    'type': 'full',
                    'info': '全体（境界線不足）',
                    'patch_index': 0,
                    'total_patches': 1,
                    'is_continuation': False
                }]

            # インデックス列（左端12%）を事前に切り出し
            INDEX_WIDTH_RATIO = 0.12
            index_width = int(img_width * INDEX_WIDTH_RATIO)
            index_col = image.crop((0, 0, index_width + overlap, img_height))
            logger.info(f"[SmartCrop] インデックス列: 0〜{index_width}px")

            # 【Ver 3.7】全列を強制的にループ（途中離脱禁止）
            for col_idx in range(num_columns):
                x_start = boundaries_px[col_idx]
                x_end = boundaries_px[col_idx + 1]

                # のりしろ付きで切り出し
                crop_start = max(0, x_start - overlap) if col_idx > 0 else 0
                crop_end = min(img_width, x_end + overlap) if col_idx < num_columns - 1 else img_width

                col_img = image.crop((crop_start, 0, crop_end, img_height))
                col_width = crop_end - crop_start

                logger.info(f"[SmartCrop] 列{col_idx + 1}/{num_columns}: x={x_start}〜{x_end} (crop:{crop_start}〜{crop_end}, width={col_width})")

                if col_idx == 0:
                    # 最初の列はそのまま
                    patches.append({
                        'image': col_img,
                        'type': 'v_column',
                        'info': f'垂直列{col_idx + 1}/{num_columns} (x:{x_start}-{x_end})',
                        'patch_index': col_idx,
                        'total_patches': num_columns,
                        'is_continuation': False
                    })
                else:
                    # 2列目以降: インデックス列を左に結合
                    combined_width = index_col.width + col_img.width
                    combined = Image.new('RGB', (combined_width, img_height), (255, 255, 255))
                    combined.paste(index_col, (0, 0))
                    combined.paste(col_img, (index_col.width, 0))

                    patches.append({
                        'image': combined,
                        'type': 'v_column_with_index',
                        'info': f'垂直列{col_idx + 1}/{num_columns} (x:{x_start}-{x_end}, +index)',
                        'patch_index': col_idx,
                        'total_patches': num_columns,
                        'is_continuation': True
                    })

            logger.info(f"[SmartCrop] ======== 垂直分割完了: {len(patches)}パッチ ========")
            return patches

        # ============================================
        # 水平分割（横に切る = 上下に分ける）
        # 縦方向の検討は1行も許さない
        # ============================================
        if split_axis == 'horizontal':
            # 量子化座標を実座標に変換
            boundaries_px = [0]
            for pos in split_positions:
                px = int(pos * img_height / QUANTIZE_GRID_SIZE)
                boundaries_px.append(px)
            boundaries_px.append(img_height)

            num_sections = len(boundaries_px) - 1
            logger.info(f"[SmartCrop] 水平分割: {num_sections}セクション at {boundaries_px}")

            # ヘッダー行（最上部）
            header_height = boundaries_px[1] if len(boundaries_px) > 1 else img_height
            header_row = image.crop((0, 0, img_width, min(header_height + overlap, img_height)))

            for sec_idx in range(num_sections):
                y_start = max(0, boundaries_px[sec_idx] - overlap)
                y_end = min(img_height, boundaries_px[sec_idx + 1] + overlap)

                sec_img = image.crop((0, y_start, img_width, y_end))
                sec_height = y_end - y_start

                if sec_idx == 0:
                    # 最初のセクションはそのまま
                    patches.append({
                        'image': sec_img,
                        'type': 'h_section',
                        'info': f'水平セクション{sec_idx + 1}/{num_sections}',
                        'patch_index': sec_idx,
                        'total_patches': num_sections,
                        'is_continuation': False
                    })
                else:
                    # 2セクション目以降: ヘッダー行を上に結合
                    # ヘッダーの高さをセクションに合わせてスケール（最大20%まで）
                    scaled_header_height = min(int(sec_height * 0.2), header_row.height)
                    scaled_header = header_row.resize(
                        (img_width, scaled_header_height),
                        Image.Resampling.LANCZOS
                    )

                    combined_height = scaled_header.height + sec_img.height
                    combined = Image.new('RGB', (img_width, combined_height), (255, 255, 255))
                    combined.paste(scaled_header, (0, 0))
                    combined.paste(sec_img, (0, scaled_header.height))

                    patches.append({
                        'image': combined,
                        'type': 'h_section_with_header',
                        'info': f'水平セクション{sec_idx + 1}/{num_sections}（ヘッダー付き）',
                        'patch_index': sec_idx,
                        'total_patches': num_sections,
                        'is_continuation': True
                    })

            logger.info(f"[SmartCrop] 水平分割完了: {len(patches)}パッチ")
            return patches

        # ここに到達することはない（安全弁）
        logger.warning(f"[SmartCrop] 不明なsplit_axis: {split_axis}")
        return [{
            'image': image,
            'type': 'full',
            'info': '全体（フォールバック）',
            'patch_index': 0,
            'total_patches': 1,
            'is_continuation': False
        }]

    def _f7_path_a_chunk_extraction(
        self,
        chunk_pages: List[Dict],
        blocks: List[Dict],
        chunk_idx: int,
        chunk_start_page: int,
        column_boundaries: List[int] = None
    ) -> Dict[str, Any]:
        """
        【Ver 4.0】F-7 Path A: 座標排除・ID焼き込み版

        【設計原則】
        1. 座標データはAIに渡さない
        2. ID焼き込み画像を使用
        3. AIは「見たまま」構造をマッピング
        """
        import tempfile
        f7_start = time.time()
        logger.info(f"[F-7] Path A - チャンク{chunk_idx + 1} ID焼き込み版開始")

        # 元画像を取得
        if not chunk_pages or 'image' not in chunk_pages[0]:
            logger.error(f"[F-7] チャンク{chunk_idx + 1} 画像データなし")
            return {"error": "No image data", "extracted_texts": [], "tables": [], "full_text_ordered": ""}

        original_image: Image.Image = chunk_pages[0]['image']

        # 【Ver 4.0】ID焼き込み画像を生成
        id_burned_image = self._f6_burn_ids_to_image(original_image, blocks, chunk_start_page)

        # スマートクロッピングでパッチを生成（ID焼き込み済み画像から）
        patches = self._smart_crop_patches(id_burned_image, column_boundaries, overlap=50)

        # 各パッチを処理
        all_extracted_texts = []
        all_tables = []
        all_full_texts = []
        patch_errors = []

        for patch_idx, patch in enumerate(patches):
            patch_img = patch['image']
            patch_type = patch['type']
            patch_info = patch['info']
            is_continuation = patch.get('is_continuation', False)

            logger.info(f"[F-7] パッチ{patch_idx + 1}/{len(patches)}: {patch_info} ({patch_img.size[0]}x{patch_img.size[1]}) 継続={is_continuation}")

            # 一時ファイルに保存
            with tempfile.NamedTemporaryFile(suffix=f'_chunk{chunk_idx}_patch{patch_idx}.png', delete=False) as f:
                patch_img.save(f, format='PNG')
                temp_path = Path(f.name)

            try:
                # 【Ver 4.0】座標なしプロンプト
                prompt = self._build_f7_smart_prompt(
                    chunk_idx, chunk_start_page, patch_info, patch_type, len(patches),
                    patch_index=patch_idx, is_continuation=is_continuation
                )

                response = self.llm_client.generate_with_vision(
                    prompt=prompt,
                    image_path=str(temp_path),
                    model=F7_MODEL_IMAGE,
                    max_tokens=F7_F8_MAX_TOKENS,
                    temperature=F7_F8_TEMPERATURE,
                    response_format="json"
                )

                # JSON パース
                try:
                    result = json.loads(response)
                except json.JSONDecodeError:
                    import json_repair
                    result = json_repair.repair_json(response, return_objects=True)

                logger.info(f"[F-7完了] パッチ{patch_idx + 1}/{len(patches)}: {len(response)}文字")

                # トークン使用量を収集
                if hasattr(self.llm_client, 'last_usage') and self.llm_client.last_usage:
                    usage = self.llm_client.last_usage.copy()
                    usage['chunk_idx'] = chunk_idx
                    usage['patch_idx'] = patch_idx
                    self._f7_usage.append(usage)

                # 結果を蓄積
                for text_block in result.get("extracted_texts", []):
                    text_block["patch_idx"] = patch_idx
                    text_block["patch_info"] = patch_info
                    all_extracted_texts.append(text_block)

                for table in result.get("tables", []):
                    table["patch_idx"] = patch_idx
                    table["patch_info"] = patch_info
                    all_tables.append(table)

                # 【Ver 3.8】full_text_orderedをGemini応答から構築
                # Geminiは cells/texts を返すが full_text_ordered は返さない
                patch_text_parts = []

                # textsから抽出
                for text_item in result.get("texts", []):
                    if isinstance(text_item, dict) and text_item.get("value"):
                        patch_text_parts.append(str(text_item["value"]))

                # cellsから抽出（表データ）
                for cell in result.get("cells", []):
                    if isinstance(cell, dict) and cell.get("value"):
                        patch_text_parts.append(str(cell["value"]))

                # extracted_textsからも抽出（フォールバック）
                for block in result.get("extracted_texts", []):
                    if isinstance(block, dict) and block.get("text"):
                        patch_text_parts.append(str(block["text"]))

                # 直接full_text_orderedがあればそれを優先
                patch_full_text = result.get("full_text_ordered", "")
                if not patch_full_text and patch_text_parts:
                    patch_full_text = "\n".join(patch_text_parts)

                all_full_texts.append(patch_full_text)
                logger.debug(f"[F-7] パッチ{patch_idx + 1} テキスト収集: {len(patch_full_text)}文字")

            except Exception as e:
                logger.error(f"[F-7] パッチ{patch_idx + 1}/{len(patches)} エラー: {e}")
                patch_errors.append(f"patch{patch_idx}: {str(e)}")

            finally:
                try:
                    temp_path.unlink()
                except:
                    pass

        f7_elapsed = time.time() - f7_start
        logger.info(f"[F-7完了] チャンク{chunk_idx + 1} 全{len(patches)}パッチ完了: {f7_elapsed:.2f}秒")

        return {
            "extracted_texts": all_extracted_texts,
            "tables": all_tables,
            "full_text_ordered": "\n\n".join(all_full_texts),
            "patch_count": len(patches),
            "errors": patch_errors if patch_errors else None
        }

    def _build_f7_smart_prompt(
        self,
        chunk_idx: int,
        start_page: int,
        patch_info: str,
        patch_type: str,
        total_patches: int,
        patch_index: int = 0,
        is_continuation: bool = False
    ) -> str:
        """
        【Ver 4.0】F-7 純粋構造マッピング・プロンプト（座標排除版）
        """
        base_prompt = self._build_f7_prompt()

        # 継続性の指示（2パッチ目以降）
        continuation_instruction = ""
        if is_continuation:
            continuation_instruction = f"""
## ⚠️ 継続パッチ
- これはパッチ {patch_index + 1}/{total_patches} です
- 前のパッチと同じ表の続きです
- 左端の基準列を参照して、データを正確にマッピングしてください

"""

        context = f"""
## 画像情報
- ページ: {start_page + 1}
- パッチ: {patch_index + 1}/{total_patches}
- タイプ: {patch_type}
- 詳細: {patch_info}
"""
        return base_prompt + continuation_instruction + context

    def _f7_process_single_image(
        self,
        image: Image.Image,
        chunk_idx: int,
        chunk_start_page: int,
        f7_start: float
    ) -> Dict[str, Any]:
        """【Ver 4.0】F-7: 単一画像処理（座標排除版）"""
        import tempfile
        with tempfile.NamedTemporaryFile(suffix=f'_chunk{chunk_idx}.png', delete=False) as f:
            image.save(f, format='PNG')
            temp_image_path = Path(f.name)

        prompt = self._build_f7_chunk_prompt(chunk_idx, chunk_start_page, 1)

        try:
            response = self.llm_client.generate_with_vision(
                prompt=prompt,
                image_path=str(temp_image_path),
                model=F7_MODEL_IMAGE,
                max_tokens=F7_F8_MAX_TOKENS,
                temperature=F7_F8_TEMPERATURE,
                response_format="json"
            )

            try:
                result = json.loads(response)
            except json.JSONDecodeError:
                import json_repair
                result = json_repair.repair_json(response, return_objects=True)

            f7_elapsed = time.time() - f7_start
            logger.info(f"[F-7完了] チャンク{chunk_idx + 1} Path A: {len(response)}文字, {f7_elapsed:.2f}秒")

            if hasattr(self.llm_client, 'last_usage') and self.llm_client.last_usage:
                usage = self.llm_client.last_usage.copy()
                usage['chunk_idx'] = chunk_idx
                self._f7_usage.append(usage)

            return result

        except Exception as e:
            logger.error(f"[F-7] チャンク{chunk_idx + 1} Path A エラー: {e}")
            return {"error": str(e), "extracted_texts": [], "tables": [], "full_text_ordered": ""}

        finally:
            try:
                temp_image_path.unlink()
            except:
                pass

    def _build_f7_column_prompt(self, chunk_idx: int, start_page: int, col_idx: int, total_cols: int) -> str:
        """【Ver 4.0】F-7列用プロンプト（座標排除版）"""
        base_prompt = self._build_f7_prompt()

        column_info = f"""
## 列情報
- 列: {col_idx + 1}/{total_cols}
- ページ: {start_page + 1}
"""
        return base_prompt + column_info

    def _build_f7_chunk_prompt(self, chunk_idx: int, start_page: int, page_count: int) -> str:
        """【Ver 4.0】F-7チャンク用プロンプト（座標排除版）"""
        base_prompt = self._build_f7_prompt()

        chunk_info = f"""
## チャンク情報
- チャンク: {chunk_idx + 1}
- ページ: {start_page + 1}〜{start_page + page_count}
"""
        return base_prompt + chunk_info

    def _f8_path_b_chunk_analysis(
        self,
        chunk_pages: List[Dict],
        blocks: List[Dict],
        chunk_idx: int,
        chunk_start_page: int
    ) -> Dict[str, Any]:
        """
        【Ver 4.0】F-8 Path B: 座標排除版・構造解析

        Args:
            chunk_pages: このチャンクのページ画像リスト
            blocks: ブロックリスト（座標はAIに渡さない）
            chunk_idx: チャンクインデックス
            chunk_start_page: このチャンクの開始ページ番号
        """
        f8_start = time.time()
        logger.info(f"[F-8] Path B - チャンク{chunk_idx + 1} Visual Analysis 開始（座標排除版）")

        # チャンク画像を一時ファイルに保存
        temp_image_path = self._save_chunk_as_temp_image(chunk_pages, chunk_idx)

        prompt = self._build_f8_chunk_prompt(chunk_idx, chunk_start_page, len(chunk_pages))

        try:
            response = self.llm_client.generate_with_vision(
                prompt=prompt,
                image_path=str(temp_image_path),
                model=F8_MODEL,
                max_tokens=F7_F8_MAX_TOKENS,
                temperature=F7_F8_TEMPERATURE,
                response_format="json"
            )

            # JSON パース
            try:
                result = json.loads(response)
            except json.JSONDecodeError:
                import json_repair
                result = json_repair.repair_json(response, return_objects=True)

            f8_elapsed = time.time() - f8_start
            logger.info(f"[F-8完了] チャンク{chunk_idx + 1} Path B: {len(response)}文字, {f8_elapsed:.2f}秒")

            # トークン使用量を収集
            if hasattr(self.llm_client, 'last_usage') and self.llm_client.last_usage:
                usage = self.llm_client.last_usage.copy()
                usage['chunk_idx'] = chunk_idx
                self._f8_usage.append(usage)
                logger.info(f"[F-8] トークン使用量: prompt={usage.get('prompt_tokens', 0)}, completion={usage.get('completion_tokens', 0)}")

            return result

        except Exception as e:
            logger.error(f"[F-8] チャンク{chunk_idx + 1} Path B エラー: {e}")
            return {"error": str(e), "tables": [], "diagrams": [], "charts": [], "structured_data_candidates": []}

        finally:
            # 一時ファイル削除
            try:
                temp_image_path.unlink()
            except:
                pass

    def _build_f8_chunk_prompt(self, chunk_idx: int, start_page: int, page_count: int) -> str:
        """【Ver 4.0】F-8チャンク用プロンプト構築（座標排除版）"""
        base_prompt = self._build_f8_prompt()

        chunk_info = f"""
## チャンク情報
- チャンク番号: {chunk_idx + 1}
- ページ範囲: {start_page + 1}〜{start_page + page_count}ページ目
"""
        return base_prompt + chunk_info

    def _merge_chunk_tables(
        self,
        path_a_result: Dict[str, Any],
        path_b_result: Dict[str, Any],
        chunk_idx: int,
        chunk_start_page: int
    ) -> List[Dict[str, Any]]:
        """
        チャンク内の表データをマージ

        Path A（テキスト内容）と Path B（構造情報）を統合し、
        チャンク情報を付加して返す
        """
        path_a_tables = path_a_result.get("tables", [])
        path_b_tables = path_b_result.get("tables", [])

        merged_tables = []
        for a_table in path_a_tables:
            block_id = a_table.get("block_id", "")

            # Path B から対応する構造情報を探す
            b_structure = {}
            for b_table in path_b_tables:
                if b_table.get("block_id") == block_id:
                    b_structure = b_table
                    break

            # columns/headers どちらも受け付ける（カラムナ形式優先）
            columns = a_table.get("columns") or a_table.get("headers", [])
            rows = a_table.get("rows", [])

            merged_table = {
                "block_id": f"chunk{chunk_idx}_{block_id}",  # チャンク情報を付加
                "chunk_idx": chunk_idx,
                "chunk_start_page": chunk_start_page,
                "table_title": a_table.get("table_title", ""),
                "table_type": a_table.get("table_type", b_structure.get("table_type", "visual_table")),
                "columns": columns,
                "rows": rows,
                "row_count": a_table.get("row_count", len(rows)),
                "col_count": a_table.get("col_count", len(columns)),
                "caption": a_table.get("caption", ""),
                # Path B からの構造情報
                "structure": b_structure.get("structure", {}),
                "semantic_role": b_structure.get("semantic_role", ""),
                "data_quality": b_structure.get("data_quality", {})
            }
            merged_tables.append(merged_table)

        return merged_tables

    # ============================================
    # F-9: Result Convergence
    # ============================================
    def _f9_merge_results(
        self,
        path_a: Dict[str, Any],
        path_b: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        F-9: 抽出結果の集約
        Path AとPath Bの結果をマージ
        特に表データは Path A (テキスト) + Path B (構造) を統合
        """
        f9_start = time.time()
        logger.info("[F-9] Result Convergence 開始")

        # Path A の表データ（テキスト内容）
        path_a_tables = path_a.get("tables", [])
        # Path B の表データ（構造情報）
        path_b_tables = path_b.get("tables", [])
        # Path B の構造化データ候補
        structured_candidates = path_b.get("structured_data_candidates", [])

        # 表データの統合: Path A のテキスト + Path B の構造
        merged_tables = []
        for a_table in path_a_tables:
            block_id = a_table.get("block_id", "")

            # Path B から対応する構造情報を探す
            b_structure = {}
            for b_table in path_b_tables:
                if b_table.get("block_id") == block_id:
                    b_structure = b_table
                    break

            # columns/headers どちらも受け付ける（カラムナ形式優先）
            columns = a_table.get("columns") or a_table.get("headers", [])
            rows = a_table.get("rows", [])

            merged_table = {
                "block_id": block_id,
                "table_title": a_table.get("table_title", ""),
                "table_type": a_table.get("table_type", b_structure.get("table_type", "visual_table")),
                "columns": columns,  # カラムナ形式で統一
                "rows": rows,
                "row_count": a_table.get("row_count", len(rows)),
                "col_count": a_table.get("col_count", len(columns)),
                "caption": a_table.get("caption", ""),
                # Path B からの構造情報
                "structure": b_structure.get("structure", {}),
                "semantic_role": b_structure.get("semantic_role", ""),
                "data_quality": b_structure.get("data_quality", {})
            }
            merged_tables.append(merged_table)

        # 表の統計
        total_rows = sum(t.get("row_count", 0) for t in merged_tables)
        total_tables = len(merged_tables)

        logger.info(f"[F-9] 表統合: {total_tables}テーブル, 合計{total_rows}行")

        merged = {
            "text_source": {
                "full_text": path_a.get("full_text_ordered", ""),
                "blocks": path_a.get("extracted_texts", []),
                "missed_texts": path_a.get("missed_texts", [])
            },
            "tables": merged_tables,  # 統合済み表データ
            "structured_data_candidates": structured_candidates,
            "visual_source": {
                "diagrams": path_b.get("diagrams", []),
                "charts": path_b.get("charts", []),
                "layout": path_b.get("layout_analysis", {})
            },
            "metadata": {
                "path_a_model": F7_MODEL_IMAGE,
                "path_b_model": F8_MODEL,
                "table_count": total_tables,
                "total_table_rows": total_rows
            }
        }

        f9_elapsed = time.time() - f9_start
        logger.info(f"[F-9完了] マージ完了, {f9_elapsed:.2f}秒")

        return merged

    # ============================================
    # F-9 Helper: アンカーパケット生成
    # ============================================
    def _build_anchor_packets(
        self,
        text_blocks: List[Dict],
        tables: List[Dict],
        structured_candidates: List[Dict]
    ) -> List[Dict[str, Any]]:
        """
        アンカーベースのパケット配列を生成

        テキストブロックと表を統一的なアンカー形式に変換し、
        Stage Gでの振り分け（H1/H2）を容易にする

        Args:
            text_blocks: F-7から抽出されたテキストブロック
            tables: F-7/F-8からマージされた表データ
            structured_candidates: F-8で検出された構造化候補

        Returns:
            アンカーパケット配列:
            [
                {"anchor_id": "B-001", "type": "text", "content": "...", "page": 1},
                {"anchor_id": "B-002", "type": "table", "title": "...", "columns": [...], "rows": [...], "is_heavy": true}
            ]
        """
        anchors = []
        anchor_index = 1

        # 表のblock_idを収集（テキストから除外するため）
        table_block_ids = set(t.get("block_id", "") for t in tables)

        # テキストブロックをアンカー化
        for block in text_blocks:
            block_id = block.get("block_id", "")

            # 表として既に処理されているブロックはスキップ
            if block_id in table_block_ids:
                continue

            text = block.get("text", "")
            if not text or len(text.strip()) < 3:
                continue

            anchors.append({
                "anchor_id": f"B-{anchor_index:03d}",
                "original_block_id": block_id,
                "type": "text",
                "block_type": block.get("block_type", "paragraph"),
                "content": text,
                "page": block.get("original_page", block.get("page", 0)),
                "reading_order": block.get("reading_order", 0),
                "confidence": block.get("confidence", "medium"),
                "is_heavy": False  # テキストは常に軽量
            })
            anchor_index += 1

        # 表をアンカー化
        for table in tables:
            block_id = table.get("block_id", "")
            rows = table.get("rows", [])
            columns = table.get("columns", []) or table.get("headers", [])

            # 重い表の判定（20行以上 or 5列以上）
            is_heavy = len(rows) >= 20 or len(columns) >= 5

            anchors.append({
                "anchor_id": f"B-{anchor_index:03d}",
                "original_block_id": block_id,
                "type": "table",
                "table_type": table.get("table_type", "visual_table"),
                "title": table.get("table_title", ""),
                "columns": columns,
                "rows": rows,
                "row_count": len(rows),
                "col_count": len(columns),
                "page": table.get("chunk_start_page", 0),
                "is_heavy": is_heavy,
                "structure": table.get("structure", {}),
                "semantic_role": table.get("semantic_role", "")
            })
            anchor_index += 1

        # 構造化候補をアンカー化（表として検出されなかったが構造化可能なデータ）
        for candidate in structured_candidates:
            anchors.append({
                "anchor_id": f"B-{anchor_index:03d}",
                "type": "structured_candidate",
                "candidate_type": candidate.get("type", "key_value"),
                "content": candidate.get("content", {}),
                "page": candidate.get("page", 0),
                "is_heavy": False
            })
            anchor_index += 1

        # reading_order でソート（テキストの場合）
        anchors.sort(key=lambda x: (x.get("page", 0), x.get("reading_order", 0)))

        logger.info(f"[F-9] アンカー生成: text={sum(1 for a in anchors if a['type'] == 'text')}, "
                   f"table={sum(1 for a in anchors if a['type'] == 'table')}, "
                   f"heavy={sum(1 for a in anchors if a.get('is_heavy', False))}")

        return anchors

    # ============================================
    # F-10: Payload Validation
    # ============================================
    def _f10_validate(
        self,
        merged_result: Dict[str, Any],
        post_body: Optional[Dict]
    ) -> Dict[str, Any]:
        """
        F-10: 契約保証（Payload Validation）
        - 必須項目チェック
        - 表データの完全性検証
        - 文字数検証
        """
        f10_start = time.time()
        logger.info("[F-10] Payload Validation 開始")

        warnings = []

        # 1. full_text の存在確認
        full_text = merged_result.get("text_source", {}).get("full_text", "")
        full_text_len = len(full_text)
        if not full_text:
            warnings.append("F10_WARN: full_text is empty")
        logger.info(f"[F-10] full_text: {full_text_len}文字")

        # 2. 表データの完全性検証
        tables = merged_result.get("tables", [])
        table_warnings = self._validate_tables(tables)
        warnings.extend(table_warnings)

        # 3. ブロックの検証
        blocks = merged_result.get("text_source", {}).get("blocks", [])
        blocks_text_len = sum(len(b.get("text", "")) for b in blocks)
        logger.info(f"[F-10] blocks: {len(blocks)}個, 合計{blocks_text_len}文字")

        # 4. 文字数の整合性チェック（警告のみ、エラーにはしない）
        if blocks_text_len > 0 and full_text_len > 0:
            # full_text は blocks の統合なので、概ね同じ長さになるはず
            diff_ratio = abs(full_text_len - blocks_text_len) / max(full_text_len, blocks_text_len)
            if diff_ratio > 0.5:  # 50%以上の差異は警告
                warnings.append(f"F10_WARN: full_text({full_text_len}) と blocks合計({blocks_text_len}) の差異が大きい")

        # 5. 表データの統計
        table_count = len(tables)
        total_rows = sum(t.get("row_count", 0) for t in tables)
        tables_with_columns = sum(1 for t in tables if t.get("columns") or t.get("headers"))

        logger.info(f"[F-10] 表統計: {table_count}テーブル, {total_rows}行, columns付き={tables_with_columns}")

        # 6. トークン使用量の集計
        f7_total = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0, "model": F7_MODEL_IMAGE}
        for u in self._f7_usage:
            f7_total["prompt_tokens"] += u.get("prompt_tokens", 0)
            f7_total["completion_tokens"] += u.get("completion_tokens", 0)
            f7_total["total_tokens"] += u.get("total_tokens", 0)

        f8_total = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0, "model": F8_MODEL}
        for u in self._f8_usage:
            f8_total["prompt_tokens"] += u.get("prompt_tokens", 0)
            f8_total["completion_tokens"] += u.get("completion_tokens", 0)
            f8_total["total_tokens"] += u.get("total_tokens", 0)

        logger.info(f"[F-10] F7トークン合計: prompt={f7_total['prompt_tokens']}, completion={f7_total['completion_tokens']}")
        logger.info(f"[F-10] F8トークン合計: prompt={f8_total['prompt_tokens']}, completion={f8_total['completion_tokens']}")

        # 7. 最終payload構築
        payload = {
            "schema_version": STAGE_F_OUTPUT_SCHEMA_VERSION,
            "post_body": post_body or {},
            "full_text": full_text,
            "text_blocks": blocks,
            "tables": tables,
            "structured_data_candidates": merged_result.get("structured_data_candidates", []),
            "visual_elements": {
                "diagrams": merged_result.get("visual_source", {}).get("diagrams", []),
                "charts": merged_result.get("visual_source", {}).get("charts", []),
            },
            "layout_analysis": merged_result.get("visual_source", {}).get("layout", {}),
            "metadata": {
                **merged_result.get("metadata", {}),
                "full_text_char_count": full_text_len,
                "blocks_count": len(blocks),
                "table_count": table_count,
                "total_table_rows": total_rows,
            },
            "media_type": "image",
            "processing_mode": "dual_vision",
            "warnings": warnings,
            "llm_usage": {
                "F7": f7_total,
                "F8": f8_total
            }
        }

        f10_elapsed = time.time() - f10_start
        logger.info(f"[F-10完了] Validation完了, warnings={len(warnings)}, {f10_elapsed:.2f}秒")

        return payload

    def _validate_tables(self, tables: List[Dict]) -> List[str]:
        """表データの完全性を検証（カラムナ形式対応）"""
        warnings = []

        for i, table in enumerate(tables):
            table_id = table.get("block_id", f"table_{i}")

            # columns/headers どちらも受け付ける（カラムナ形式優先）
            columns = table.get("columns") or table.get("headers", [])
            rows = table.get("rows", [])

            if not columns and not rows:
                warnings.append(f"F10_TABLE_WARN: {table_id} has no columns and no rows")
                continue

            # 列数の整合性チェック
            if columns:
                col_count = len(columns)
                for row_idx, row in enumerate(rows):
                    if isinstance(row, list) and len(row) != col_count:
                        warnings.append(f"F10_TABLE_WARN: {table_id} row {row_idx} has {len(row)} cols, expected {col_count}")

            # data_summary の検出（禁止パターン）
            if "data_summary" in table:
                warnings.append(f"F10_TABLE_ERROR: {table_id} uses data_summary (PROHIBITED)")

            # 辞書リスト形式の検出（禁止パターン）
            if rows and isinstance(rows[0], dict):
                warnings.append(f"F10_TABLE_ERROR: {table_id} uses dict rows (PROHIBITED - use columnar format)")

            # 空の rows チェック
            if not rows:
                warnings.append(f"F10_TABLE_WARN: {table_id} has columns but no rows")

            # セル内のカンマ検出（構造化不十分の可能性）
            for row_idx, row in enumerate(rows):
                if isinstance(row, list):
                    for col_idx, cell in enumerate(row):
                        if isinstance(cell, str) and ", " in cell and len(cell.split(", ")) > 2:
                            warnings.append(f"F10_TABLE_HINT: {table_id} row {row_idx} col {col_idx} may need further expansion (contains comma-separated data)")

        return warnings

    # ============================================
    # ユーティリティ
    # ============================================
    def _create_empty_payload(
        self,
        post_body: Optional[Dict],
        error: str = None
    ) -> Dict[str, Any]:
        """空のpayloadを生成"""
        payload = {
            "schema_version": STAGE_F_OUTPUT_SCHEMA_VERSION,
            "post_body": post_body or {},
            "path_a_result": {},
            "path_b_result": {},
            "media_type": "none",
            "processing_mode": "skipped",
            "warnings": []
        }
        if error:
            payload["warnings"].append(f"F_ERROR: {error}")
        return payload
```

## shared/pipeline/stage_g1_table_refiner.py

```python
"""
Stage G1: Table Refiner（表専用整理）- AI研磨対応版

【設計 2026-01-28】表データの統合・検算・整形 + AI研磨

役割: G-Gate から受け取った表データを検証・整形し、H1 用 JSON を出力
      必要に応じて AI（Flash-Lite）で「研磨」を行い、物理的な不整合を解消

============================================
入力（G-Gate から）:
  - tables: 表データリスト
  - table_page_context: 表ページのテキストコンテキスト

出力（H1 へ）:
  - tables: 検証済み表データ（anchor_id, page, headers, rows）
  - validation_results: 各表の検算結果
  - table_page_context: 表周辺のテキスト
  - token_usage: トークン使用量

処理フロー:
  1. ルールベース検算
  2. 自動修正（セル数調整等）
  3. AI研磨（必要な表のみ）← 投資ポイント
  4. 最終検証・整形
============================================
"""
import json
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass, field
from loguru import logger
import re

# G1 で使用するモデル
G1_MODEL = "gemini-2.5-flash-lite"


@dataclass
class TableValidationResult:
    """表の検算結果"""
    anchor_id: str
    is_valid: bool
    warnings: List[str] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    auto_fixed: bool = False
    ai_polished: bool = False
    fix_description: str = ""


class StageG1TableRefiner:
    """G1: 表データの統合・検算・整形（AI研磨対応）"""

    # 検算の閾値
    MAX_EMPTY_CELL_RATIO = 0.5  # 空セル率がこれ以上なら警告
    MAX_DUPLICATE_ROW_RATIO = 0.3  # 重複行率がこれ以上なら警告

    # AI研磨の投入条件
    POLISH_THRESHOLD_CELL_MISMATCH = 2  # セル数不一致がこれ以上なら研磨
    POLISH_THRESHOLD_EMPTY_RATIO = 0.3  # 空セル率がこれ以上なら研磨

    def __init__(self, llm_client=None):
        """
        Args:
            llm_client: LLMクライアント（AI研磨に使用）
        """
        self.llm = llm_client
        self._token_usage: Dict[str, Any] = {
            'prompt_tokens': 0,
            'completion_tokens': 0,
            'total_tokens': 0,
            'model': G1_MODEL,
            'polished_tables': 0
        }

    def process(self, g1_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        表データを検証・整形（AI研磨対応）

        Args:
            g1_input: G-Gate からの入力
                - tables: 表データリスト
                - table_page_context: 表ページのテキストコンテキスト

        Returns:
            H1 用の整形済み JSON
        """
        logger.info("[G1] 表整理開始（AI研磨対応）...")

        tables = g1_input.get('tables', [])
        table_page_context = g1_input.get('table_page_context', {})

        if not tables:
            logger.info("[G1] 表なし → スキップ")
            logger.info(f"[G1] トークン使用量: 0 (表なし)")
            return {
                'tables': [],
                'validation_results': [],
                'table_page_context': {},
                'statistics': {
                    'total_tables': 0,
                    'valid_tables': 0,
                    'fixed_tables': 0,
                    'polished_tables': 0,
                    'total_rows': 0
                },
                'token_usage': self._token_usage.copy()
            }

        # 各表を検証・整形
        validated_tables = []
        validation_results = []
        total_rows = 0
        fixed_count = 0
        polished_count = 0

        for table in tables:
            anchor_id = table.get('anchor_id', '')
            logger.debug(f"[G1] 検証: {anchor_id}")

            # Step 1: ルールベース検算
            validation = self._validate_table(table)

            # Step 2: 自動修正（セル数調整等）
            if validation.errors and not validation.is_valid:
                fixed_table, fix_result = self._auto_fix_table(table, validation)
                if fix_result.is_valid:
                    table = fixed_table
                    validation = fix_result
                    fixed_count += 1

            # Step 3: AI研磨（全件実行 - 常駐化）
            # 2026-01-28: 条件判定を撤廃し、全ての表をAIに通す
            # 理由: 1文字のズレ、金額の泣き別れ（￥と1,000が分かれる等）を
            #       H1に渡る前に100%仕留める
            if self.llm:  # LLMクライアントがあれば必ず実行
                logger.info(f"[G1] AI研磨実行（常駐）: {anchor_id}")
                polished_table, polish_result = self._polish_table_with_ai(
                    table,
                    validation,
                    table_page_context.get(f"page_{table.get('page', 0)}", "")
                )
                if polish_result.is_valid:
                    table = polished_table
                    validation = polish_result
                    polished_count += 1
                    self._token_usage['polished_tables'] += 1
                else:
                    # 研磨後も無効な場合、元のvalidationにai_polished=Trueを記録
                    validation.ai_polished = True
                    logger.warning(f"[G1] AI研磨後も検証失敗: {anchor_id}")

            validation_results.append(validation)

            # Step 4: 最終整形
            formatted_table = self._format_table_for_h1(table, validation)
            validated_tables.append(formatted_table)
            total_rows += len(formatted_table.get('rows', []))

            # 警告・エラーをログ
            for warn in validation.warnings:
                logger.warning(f"[G1] {anchor_id}: {warn}")
            for err in validation.errors:
                logger.error(f"[G1] {anchor_id}: {err}")

        # 統計
        valid_count = sum(1 for v in validation_results if v.is_valid)
        statistics = {
            'total_tables': len(tables),
            'valid_tables': valid_count,
            'fixed_tables': fixed_count,
            'polished_tables': polished_count,
            'total_rows': total_rows,
            'heavy_tables': sum(1 for t in validated_tables if t.get('is_heavy', False))
        }

        # トークン使用量をログ出力
        logger.info(f"[G1] トークン使用量: prompt={self._token_usage['prompt_tokens']}, "
                   f"completion={self._token_usage['completion_tokens']}, "
                   f"total={self._token_usage['total_tokens']} (model={self._token_usage['model']})")
        logger.info(f"[G1] AI研磨: {polished_count}表に適用")

        logger.info(f"[G1] 完了: {valid_count}/{len(tables)}表が有効, "
                   f"{fixed_count}表を自動修正, {polished_count}表をAI研磨, 計{total_rows}行")

        return {
            'tables': validated_tables,
            'validation_results': [self._validation_to_dict(v) for v in validation_results],
            'table_page_context': table_page_context,
            'statistics': statistics,
            'token_usage': self._token_usage.copy()
        }

    # ============================================
    # AI研磨機能
    # ============================================
    def _needs_ai_polish(self, table: Dict[str, Any], validation: TableValidationResult) -> bool:
        """
        AI研磨が必要かどうかを判定

        投入条件:
        1. セル数不一致が閾値以上
        2. 空セル率が閾値以上
        3. E から来た生テキストがある（構造化が不完全な可能性）
        4. 警告が複数ある
        """
        # LLMクライアントがなければ研磨不可
        if not self.llm:
            return False

        headers = table.get('headers', [])
        rows = table.get('rows', [])

        # 表が小さすぎる場合はスキップ（コスト対効果が低い）
        if len(rows) < 2 or len(headers) < 2:
            return False

        # 条件1: セル数不一致
        mismatch_count = 0
        for row in rows:
            if isinstance(row, list) and len(row) != len(headers):
                mismatch_count += 1
        if mismatch_count >= self.POLISH_THRESHOLD_CELL_MISMATCH:
            logger.debug(f"[G1] 研磨理由: セル数不一致 {mismatch_count}行")
            return True

        # 条件2: 空セル率
        if rows and headers:
            total_cells = len(rows) * len(headers)
            empty_cells = sum(
                1 for row in rows if isinstance(row, list)
                for cell in row if not str(cell).strip()
            )
            if total_cells > 0:
                empty_ratio = empty_cells / total_cells
                if empty_ratio >= self.POLISH_THRESHOLD_EMPTY_RATIO:
                    logger.debug(f"[G1] 研磨理由: 空セル率 {empty_ratio:.1%}")
                    return True

        # 条件3: E の生テキストがある
        if table.get('e_raw_text'):
            logger.debug(f"[G1] 研磨理由: E生テキストあり")
            return True

        # 条件4: 警告が多い
        if len(validation.warnings) >= 3:
            logger.debug(f"[G1] 研磨理由: 警告{len(validation.warnings)}件")
            return True

        return False

    def _polish_table_with_ai(
        self,
        table: Dict[str, Any],
        validation: TableValidationResult,
        context: str = ""
    ) -> Tuple[Dict[str, Any], TableValidationResult]:
        """
        AIで表を研磨

        Args:
            table: 研磨対象の表
            validation: 現在の検算結果
            context: 表周辺のテキスト（文脈判断用）

        Returns:
            (研磨後の表, 新しい検算結果)
        """
        anchor_id = table.get('anchor_id', 'unknown')

        try:
            # プロンプト構築
            prompt = self._build_polish_prompt(table, validation, context)

            # AI呼び出し
            logger.info(f"[G1] AI研磨実行中: {anchor_id}")
            response = self.llm.call_model(
                tier="default",
                prompt=prompt,
                model_name=G1_MODEL,
                temperature=0.0,
                response_format='json'
            )

            # トークン使用量を記録
            if hasattr(self.llm, 'last_usage') and self.llm.last_usage:
                usage = self.llm.last_usage
                self._token_usage['prompt_tokens'] += usage.get('prompt_tokens', 0)
                self._token_usage['completion_tokens'] += usage.get('completion_tokens', 0)
                self._token_usage['total_tokens'] += usage.get('total_tokens', 0)
                logger.info(f"[G1] AI研磨トークン: prompt={usage.get('prompt_tokens', 0)}, "
                           f"completion={usage.get('completion_tokens', 0)}")

            # レスポンス処理
            if not response.get('success'):
                logger.warning(f"[G1] AI研磨失敗: {response.get('error')}")
                return table, validation

            content = response.get('content', '')
            polished_data = self._parse_polish_response(content)

            if polished_data:
                # 修復不能フラグの処理（敗戦処理）
                if polished_data.get('is_unrepairable'):
                    reason = polished_data.get('reason', '理由不明')
                    logger.warning(f"[G1] 修復不能（白旗）: {anchor_id} - {reason}")

                    # 元のテーブルに修復不能フラグを付与
                    unrepairable_table = table.copy()
                    unrepairable_table['status'] = 'unrepairable'
                    unrepairable_table['unrepairable_reason'] = reason

                    # バリデーション結果も更新
                    new_validation = TableValidationResult(
                        anchor_id=anchor_id,
                        is_valid=False,
                        warnings=validation.warnings,
                        errors=validation.errors + [f"AI判定: 修復不能 - {reason}"],
                        auto_fixed=False,
                        ai_polished=True,
                        fix_description=f"UNREPAIRABLE: {reason}"
                    )
                    return unrepairable_table, new_validation

                # 修復成功時の処理
                polished_table = table.copy()
                polished_table['headers'] = polished_data.get('headers', table.get('headers', []))
                polished_table['rows'] = polished_data.get('rows', table.get('rows', []))
                polished_table['row_count'] = len(polished_table['rows'])
                polished_table['col_count'] = len(polished_table['headers'])
                polished_table['status'] = 'polished'  # 正常修復

                # 再検証
                new_validation = self._validate_table(polished_table)
                new_validation.ai_polished = True
                new_validation.fix_description = f"AI polished: {polished_data.get('changes_made', 'unknown')}"

                logger.info(f"[G1] AI研磨成功: {anchor_id} - {polished_data.get('changes_made', '')}")
                return polished_table, new_validation

        except Exception as e:
            logger.error(f"[G1] AI研磨エラー: {anchor_id} - {e}")

        return table, validation

    def _build_polish_prompt(
        self,
        table: Dict[str, Any],
        validation: TableValidationResult,
        context: str
    ) -> str:
        """
        AI研磨用プロンプトを構築

        最小限のデータのみをAIに渡し、浪費を防ぐ
        """
        headers = table.get('headers', [])
        rows = table.get('rows', [])
        e_raw_text = table.get('e_raw_text', '')

        # 問題点のサマリー
        issues = []
        for warn in validation.warnings[:5]:  # 最大5件
            issues.append(f"- {warn}")
        for err in validation.errors[:3]:  # 最大3件
            issues.append(f"- ERROR: {err}")

        prompt = f"""あなたは精密な表修復の職人です。
以下の【視覚枠組み（headers/rows）】と【生テキスト（OCR結果）】を照合し、正しい表に修復してください。

【絶対ルール】
1. 座標が微妙にズレている文字を、文脈判断で正しいセル(Row/Col)に割り振れ
2. OCRで分割された一つの単語（例: "￥" と "1,000"）は結合せよ
3. 空セルは空文字列 "" のままでよい（無理に埋めない）
4. 行の追加・削除は禁止（行数は維持）
5. 列の追加・削除は禁止（列数は維持）

【現在の枠組み】
- ヘッダー: {json.dumps(headers, ensure_ascii=False)}
- 行数: {len(rows)}行
- 列数: {len(headers)}列

【現在のデータ（修正前）】
```json
{json.dumps(rows[:20], ensure_ascii=False, indent=2)}
```
{f"（※全{len(rows)}行中、最初の20行のみ表示）" if len(rows) > 20 else ""}

【検出された問題】
{chr(10).join(issues) if issues else "軽微なズレの可能性"}

{f'''【OCR生テキスト（参考）】
```
{e_raw_text[:1500]}
```
''' if e_raw_text else ''}

{f'''【周辺コンテキスト】
{context[:500]}
''' if context else ''}

【出力形式】
以下のJSON形式のみを出力せよ。解説は一切不要。

■ 修復可能な場合：
```json
{{
  "headers": ["列1", "列2", ...],
  "rows": [
    ["値1", "値2", ...],
    ...
  ],
  "changes_made": "修正内容の簡潔な説明"
}}
```

■ 修復不能な場合（行列の整合が物理的に不可能）：
```json
{{
  "is_unrepairable": true,
  "reason": "修復不能の理由（例: 列数が行ごとにバラバラで統一不可能）"
}}
```
修復を諦める勇気も職人の技。無理に嘘のデータを作るな。
"""
        return prompt

    def _parse_polish_response(self, content: str) -> Optional[Dict[str, Any]]:
        """
        AI研磨レスポンスをパース

        Returns:
            - 修復成功時: {'headers': [...], 'rows': [...], 'changes_made': ...}
            - 修復不能時: {'is_unrepairable': True, 'reason': ...}
            - パース失敗時: None
        """
        try:
            # JSONブロックを抽出
            json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # 直接JSONの場合
                json_str = content.strip()

            data = json.loads(json_str)

            # 修復不能フラグの確認（敗戦処理）
            if data.get('is_unrepairable'):
                logger.warning(f"[G1] AI判定: 修復不能 - {data.get('reason', '理由不明')}")
                return data  # そのまま返す

            # 修復成功時の必須フィールド確認
            if 'headers' in data and 'rows' in data:
                return data

            logger.warning(f"[G1] AI研磨レスポンスに必須フィールドなし")
            return None

        except json.JSONDecodeError as e:
            logger.warning(f"[G1] AI研磨レスポンスのJSONパース失敗: {e}")
            try:
                import json_repair
                return json_repair.loads(content)
            except:
                return None

    # ============================================
    # 検算・修正機能（既存）
    # ============================================
    def _validate_table(self, table: Dict[str, Any]) -> TableValidationResult:
        """
        表の検算を実行

        チェック項目:
        1. headers と rows のセル数一致
        2. 空行の検出
        3. 空列の検出
        4. 重複行の検出
        5. 行数・列数の整合性
        """
        anchor_id = table.get('anchor_id', 'unknown')
        headers = table.get('headers', [])
        rows = table.get('rows', [])
        declared_row_count = table.get('row_count', len(rows))
        declared_col_count = table.get('col_count', len(headers))

        warnings = []
        errors = []

        # 1. ヘッダーと行のセル数一致チェック
        if headers:
            header_count = len(headers)
            mismatch_rows = []
            for i, row in enumerate(rows):
                if isinstance(row, list) and len(row) != header_count:
                    mismatch_rows.append(i)
            if mismatch_rows:
                if len(mismatch_rows) <= 3:
                    warnings.append(f"Cell count mismatch in rows: {mismatch_rows}")
                else:
                    warnings.append(f"Cell count mismatch in {len(mismatch_rows)} rows")

        # 2. 空行の検出
        empty_rows = []
        for i, row in enumerate(rows):
            if isinstance(row, list) and all(not str(cell).strip() for cell in row):
                empty_rows.append(i)
        if empty_rows:
            warnings.append(f"Empty rows: {empty_rows[:5]}{'...' if len(empty_rows) > 5 else ''}")

        # 3. 空列の検出（全ての行でその列が空）
        if headers and rows:
            empty_cols = []
            for col_idx in range(len(headers)):
                all_empty = True
                for row in rows:
                    if isinstance(row, list) and col_idx < len(row):
                        if str(row[col_idx]).strip():
                            all_empty = False
                            break
                if all_empty:
                    empty_cols.append(headers[col_idx] if col_idx < len(headers) else col_idx)
            if empty_cols:
                warnings.append(f"Empty columns: {empty_cols}")

        # 4. 重複行の検出
        if rows:
            row_strings = [str(row) for row in rows if isinstance(row, list)]
            seen = {}
            duplicates = []
            for i, rs in enumerate(row_strings):
                if rs in seen:
                    duplicates.append((seen[rs], i))
                else:
                    seen[rs] = i
            if duplicates:
                dup_ratio = len(duplicates) / len(rows)
                if dup_ratio > self.MAX_DUPLICATE_ROW_RATIO:
                    warnings.append(f"High duplicate row ratio: {dup_ratio:.1%}")

        # 5. 行数・列数の整合性
        actual_row_count = len(rows)
        actual_col_count = len(headers) if headers else (max(len(r) for r in rows if isinstance(r, list)) if rows else 0)

        if declared_row_count != actual_row_count:
            errors.append(f"Row count: declared={declared_row_count}, actual={actual_row_count}")

        if declared_col_count != actual_col_count and declared_col_count > 0:
            errors.append(f"Column count: declared={declared_col_count}, actual={actual_col_count}")

        # 6. 空セル率チェック
        if rows and headers:
            total_cells = len(rows) * len(headers)
            empty_cells = 0
            for row in rows:
                if isinstance(row, list):
                    empty_cells += sum(1 for cell in row if not str(cell).strip())
            if total_cells > 0:
                empty_ratio = empty_cells / total_cells
                if empty_ratio > self.MAX_EMPTY_CELL_RATIO:
                    warnings.append(f"High empty cell ratio: {empty_ratio:.1%}")

        # 判定
        is_valid = len(errors) == 0

        return TableValidationResult(
            anchor_id=anchor_id,
            is_valid=is_valid,
            warnings=warnings,
            errors=errors
        )

    def _auto_fix_table(
        self,
        table: Dict[str, Any],
        validation: TableValidationResult
    ) -> Tuple[Dict[str, Any], TableValidationResult]:
        """
        表の自動修正を試みる（ルールベース）

        修正内容:
        1. 行のセル数を揃える（不足は空文字で埋める、過剰は切り詰める）
        2. row_count / col_count を実際の値に更新
        """
        fixed_table = table.copy()
        fix_descriptions = []

        headers = fixed_table.get('headers', [])
        rows = fixed_table.get('rows', [])
        target_col_count = len(headers) if headers else 0

        # 1. 行のセル数を揃える
        if target_col_count > 0:
            fixed_rows = []
            for row in rows:
                if isinstance(row, list):
                    if len(row) < target_col_count:
                        row = row + [''] * (target_col_count - len(row))
                    elif len(row) > target_col_count:
                        row = row[:target_col_count]
                    fixed_rows.append(row)
                elif isinstance(row, dict):
                    fixed_rows.append([str(row.get(h, '')) for h in headers])

            fixed_table['rows'] = fixed_rows
            if fixed_rows != rows:
                fix_descriptions.append("Normalized cell counts")

        # 2. row_count / col_count を更新
        fixed_table['row_count'] = len(fixed_table.get('rows', []))
        fixed_table['col_count'] = len(fixed_table.get('headers', []))

        # 再検証
        new_validation = self._validate_table(fixed_table)
        new_validation.auto_fixed = True
        new_validation.fix_description = '; '.join(fix_descriptions) if fix_descriptions else 'No changes'

        return fixed_table, new_validation

    def _format_table_for_h1(
        self,
        table: Dict[str, Any],
        validation: TableValidationResult
    ) -> Dict[str, Any]:
        """H1 用に表を整形"""
        rows = table.get('rows', [])
        headers = table.get('headers', [])

        # is_heavy の再判定（20行以上 or 5列以上）
        is_heavy = len(rows) >= 20 or len(headers) >= 5 or table.get('is_heavy', False)

        return {
            'anchor_id': table.get('anchor_id', ''),
            'page': table.get('page', 0),
            'title': table.get('title', ''),
            'table_type': table.get('table_type', 'visual_table'),
            'headers': headers,
            'rows': rows,
            'row_count': len(rows),
            'col_count': len(headers),
            'source': table.get('source', 'unknown'),
            'is_heavy': is_heavy,
            'is_valid': validation.is_valid,
            'ai_polished': validation.ai_polished,
            'validation_warnings': validation.warnings
        }

    def _validation_to_dict(self, validation: TableValidationResult) -> Dict[str, Any]:
        """検算結果を辞書に変換"""
        return {
            'anchor_id': validation.anchor_id,
            'is_valid': validation.is_valid,
            'warnings': validation.warnings,
            'errors': validation.errors,
            'auto_fixed': validation.auto_fixed,
            'ai_polished': validation.ai_polished,
            'fix_description': validation.fix_description
        }

    # ============================================
    # ヘルパー関数
    # ============================================
    def structure_from_e_text(
        self,
        raw_text: str,
        context: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """E の生テキストから表構造を推測"""
        if not raw_text:
            return None

        lines = [l for l in raw_text.split('\n') if l.strip()]
        if len(lines) < 2:
            return None

        delimiter = self._detect_delimiter(raw_text)
        headers = [c.strip() for c in lines[0].split(delimiter) if c.strip()]
        rows = []

        for line in lines[1:]:
            cells = [c.strip() for c in line.split(delimiter)]
            if any(c for c in cells):
                rows.append(cells)

        if not headers or not rows:
            return None

        title = self._guess_table_title(raw_text, context)
        table_type = self._guess_table_type(headers, rows)

        return {
            'headers': headers,
            'rows': rows,
            'title': title,
            'table_type': table_type,
            'row_count': len(rows),
            'col_count': len(headers)
        }

    def _detect_delimiter(self, text: str) -> str:
        """区切り文字を検出"""
        tab_count = text.count('\t')
        pipe_count = text.count('|')
        comma_count = text.count(',')

        if tab_count > pipe_count and tab_count > comma_count:
            return '\t'
        elif pipe_count > comma_count:
            return '|'
        else:
            return '\t'

    def _guess_table_title(self, raw_text: str, context: Optional[str]) -> str:
        """表のタイトルを推測"""
        if context:
            patterns = [
                r'【(.+?)】',
                r'■\s*(.+)',
                r'◆\s*(.+)',
                r'^(.+?[表一覧リスト])\s*$',
            ]
            for pattern in patterns:
                match = re.search(pattern, context, re.MULTILINE)
                if match:
                    return match.group(1).strip()
        return ""

    def _guess_table_type(self, headers: List[str], rows: List[List[str]]) -> str:
        """表のタイプを推測"""
        header_text = ' '.join(headers).lower()
        all_text = header_text + ' ' + ' '.join(str(cell) for row in rows for cell in row).lower()

        if any(kw in header_text for kw in ['順位', '位', 'rank', '№']):
            return 'ranking'
        if any(kw in header_text for kw in ['日付', '日時', '時間', '曜日']):
            return 'schedule'
        if any(kw in all_text for kw in ['円', '¥', '料金', '価格', '金額']):
            return 'pricing'
        if any(kw in header_text for kw in ['氏名', '名前', '生徒', '参加者']):
            return 'roster'
        if any(kw in header_text for kw in ['点数', '得点', '成績', 'スコア']):
            return 'score'

        return 'visual_table'
```

## shared/pipeline/stage_g2_text_refiner.py

```python
"""
Stage G2: Text Refiner（テキスト専用整理）- AI研磨対応版

【設計 2026-01-28】テキストの統合・ソート・整形 + AI校閲

役割: G-Gate から受け取ったテキストセグメントを整理し、H2 用 JSON を出力
      AI（Flash-Lite）で断片的なテキストを「一本の完璧な原稿」に繋ぎ直す

============================================
入力（G-Gate から）:
  - segments: テキストセグメントリスト
  - post_body: 投稿本文
  - placeholder_count: 表プレースホルダー数

出力（H2 へ）:
  - segments: 整理済みセグメント（ref_id, page, text, type）
  - unified_text: AI研磨済み統合テキスト
  - dedup_stats: 重複排除統計
  - token_usage: トークン使用量

処理フロー:
  1. ページ順 → 読み順でソート
  2. 重複テキストの排除（信頼度高い方を採用）
  3. REF_ID を再付与
  4. unified_text を構築
  5. AI研磨（常駐）← 投資ポイント
     - OCRのゴミを浄化
     - 文脈を復元
     - アンカー（表参照）を保持
============================================
"""
from typing import Dict, Any, List, Optional, Set
from dataclasses import dataclass
from loguru import logger
import re
from difflib import SequenceMatcher

# G2 で使用するモデル
G2_MODEL = "gemini-2.5-flash-lite"


@dataclass
class DedupStats:
    """重複排除統計"""
    total_input: int = 0
    total_output: int = 0
    duplicates_removed: int = 0
    merged_segments: int = 0


class StageG2TextRefiner:
    """G2: テキストの統合・ソート・整形"""

    # 類似度の閾値（これ以上なら重複とみなす）
    SIMILARITY_THRESHOLD = 0.85

    # ソースの優先順位（数字が小さいほど優先）
    SOURCE_PRIORITY = {
        'post_body': 1,
        'stage_f': 2,
        'stage_e': 3,
        'g_gate': 4,
    }

    def __init__(self, llm_client=None):
        """
        Args:
            llm_client: LLMクライアント（オプション、複雑なテキスト整形時に使用）
        """
        self.llm = llm_client
        self._token_usage: Dict[str, Any] = {
            'prompt_tokens': 0,
            'completion_tokens': 0,
            'total_tokens': 0,
            'model': G2_MODEL
        }

    def process(self, g2_input: Dict[str, Any]) -> Dict[str, Any]:
        """
        テキストセグメントを整理・統合

        Args:
            g2_input: G-Gate からの入力
                - segments: テキストセグメントリスト
                - post_body: 投稿本文
                - placeholder_count: 表プレースホルダー数

        Returns:
            H2 用の整形済み JSON
            {
                'segments': [...],
                'unified_text': str,
                'dedup_stats': {...},
                'post_body': {...}
            }
        """
        logger.info("[G2] テキスト整理開始...")

        segments = g2_input.get('segments', [])
        post_body = g2_input.get('post_body', {})
        placeholder_count = g2_input.get('placeholder_count', 0)

        stats = DedupStats(total_input=len(segments))

        if not segments:
            logger.info("[G2] セグメントなし → post_body のみ")
            logger.info(f"[G2] トークン使用量: 0 (セグメントなし)")
            unified_text = post_body.get('text', '') if post_body else ''
            return {
                'segments': [],
                'unified_text': unified_text,
                'dedup_stats': self._stats_to_dict(stats),
                'post_body': post_body or {},
                'token_usage': self._token_usage.copy()
            }

        # ============================================
        # Step 1: ページ順・読み順でソート
        # ============================================
        sorted_segments = self._sort_segments(segments)
        logger.debug(f"[G2] ソート完了: {len(sorted_segments)}セグメント")

        # ============================================
        # Step 2: 重複排除
        # ============================================
        deduped_segments = self._deduplicate_segments(sorted_segments)
        stats.duplicates_removed = len(sorted_segments) - len(deduped_segments)
        logger.info(f"[G2] 重複排除: {stats.duplicates_removed}件削除")

        # ============================================
        # Step 3: REF_ID を再付与
        # ============================================
        renumbered_segments = self._renumber_ref_ids(deduped_segments)
        stats.total_output = len(renumbered_segments)

        # ============================================
        # Step 4: unified_text を構築
        # ============================================
        unified_text = self._build_unified_text(renumbered_segments)

        # ============================================
        # Step 5: AI研磨（全件実行 - 常駐化）
        # ============================================
        # 2026-01-28: 条件判定なし、全てのテキストをAIに通す
        # 理由: OCRのゴミ、文脈の断絶を、H2に渡る前に100%浄化する
        if self.llm and unified_text:  # LLMクライアントがあれば必ず実行
            logger.info(f"[G2] AI研磨実行（常駐）: {len(unified_text)}文字")
            post_body_text = post_body.get('text', '') if post_body else ''
            polished_text = self._polish_text_with_ai(unified_text, post_body_text)
            if polished_text:
                unified_text = polished_text
                logger.info(f"[G2] AI研磨完了: {len(unified_text)}文字")
            else:
                logger.warning("[G2] AI研磨失敗 → 元テキストを維持")

        # トークン使用量をログ出力
        logger.info(f"[G2] トークン使用量: prompt={self._token_usage['prompt_tokens']}, "
                   f"completion={self._token_usage['completion_tokens']}, "
                   f"total={self._token_usage['total_tokens']} (model={self._token_usage['model']})")

        logger.info(f"[G2] 完了: {stats.total_input}→{stats.total_output}セグメント, unified_text={len(unified_text)}文字")

        return {
            'segments': renumbered_segments,
            'unified_text': unified_text,
            'dedup_stats': self._stats_to_dict(stats),
            'post_body': post_body or {},
            'placeholder_count': placeholder_count,
            'token_usage': self._token_usage.copy()
        }

    def _sort_segments(self, segments: List[Dict]) -> List[Dict]:
        """
        セグメントをページ順 → 読み順でソート

        ソート順:
        1. page (昇順)
        2. segment_type (post_body が最初)
        3. reading_order または ref_id (昇順)
        """
        def sort_key(seg):
            page = seg.get('page', 0)

            # segment_type の優先度
            seg_type = seg.get('segment_type', 'paragraph')
            type_order = {
                'post_body': 0,
                'heading': 1,
                'table_marker': 2,
                'paragraph': 3,
                'list_item': 4,
            }.get(seg_type, 5)

            # reading_order または ref_id から順序を取得
            reading_order = seg.get('reading_order', 0)
            if reading_order == 0:
                # ref_id から番号を抽出
                ref_id = seg.get('ref_id', '')
                match = re.search(r'(\d+)', ref_id)
                if match:
                    reading_order = int(match.group(1))

            return (page, type_order, reading_order)

        return sorted(segments, key=sort_key)

    def _deduplicate_segments(self, segments: List[Dict]) -> List[Dict]:
        """
        重複セグメントを排除

        ルール:
        1. 完全一致 → 優先度高いソースを採用
        2. 高類似度 → 長い方を採用
        3. 包含関係 → 長い方を採用
        """
        if not segments:
            return []

        result = []
        seen_texts: Set[str] = set()

        for seg in segments:
            text = seg.get('text', '').strip()

            # 空テキストは table_marker 以外スキップ
            if not text and seg.get('segment_type') != 'table_marker':
                continue

            # table_marker は常に採用
            if seg.get('segment_type') == 'table_marker':
                result.append(seg)
                continue

            # 正規化したテキスト
            normalized = self._normalize_text(text)

            # 完全一致チェック
            if normalized in seen_texts:
                logger.debug(f"[G2] 重複スキップ (完全一致): {text[:50]}...")
                continue

            # 類似度チェック（既存のセグメントと比較）
            is_duplicate = False
            for existing in result:
                existing_text = existing.get('text', '').strip()
                if not existing_text:
                    continue

                similarity = self._calculate_similarity(text, existing_text)
                if similarity >= self.SIMILARITY_THRESHOLD:
                    # 長い方を採用
                    if len(text) > len(existing_text):
                        result.remove(existing)
                        result.append(seg)
                        seen_texts.add(self._normalize_text(existing_text))
                        logger.debug(f"[G2] 類似テキスト置換: {existing_text[:30]}... → {text[:30]}...")
                    else:
                        logger.debug(f"[G2] 重複スキップ (類似): {text[:50]}...")
                    is_duplicate = True
                    break

                # 包含関係チェック
                if text in existing_text or existing_text in text:
                    # 長い方を採用
                    if len(text) > len(existing_text):
                        result.remove(existing)
                        result.append(seg)
                        seen_texts.add(self._normalize_text(existing_text))
                    is_duplicate = True
                    break

            if not is_duplicate:
                result.append(seg)
                seen_texts.add(normalized)

        return result

    def _normalize_text(self, text: str) -> str:
        """テキストを正規化（比較用）"""
        # 空白を統一
        normalized = re.sub(r'\s+', ' ', text.strip().lower())
        # 句読点を除去
        normalized = re.sub(r'[、。，．,\.]+', '', normalized)
        return normalized

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """2つのテキストの類似度を計算（0.0〜1.0）"""
        if not text1 or not text2:
            return 0.0

        # 短いテキストは厳しく判定
        if len(text1) < 20 or len(text2) < 20:
            return 1.0 if text1 == text2 else 0.0

        return SequenceMatcher(None, text1, text2).ratio()

    def _renumber_ref_ids(self, segments: List[Dict]) -> List[Dict]:
        """REF_ID を連番で再付与"""
        result = []
        ref_counter = 0

        for seg in segments:
            ref_counter += 1
            new_seg = seg.copy()
            new_seg['ref_id'] = f'REF_{ref_counter:03d}'
            result.append(new_seg)

        return result

    def _build_unified_text(self, segments: List[Dict]) -> str:
        """
        統合テキストを構築

        フォーマット:
        - post_body: そのまま
        - heading: 【見出し】形式
        - table_marker: プレースホルダーをそのまま挿入
        - paragraph/list_item: そのまま
        """
        parts = []
        current_page = -1

        for seg in segments:
            page = seg.get('page', 0)
            seg_type = seg.get('segment_type', 'paragraph')
            text = seg.get('text', '').strip()
            placeholder = seg.get('table_placeholder')

            # ページ区切り（オプション）
            if page != current_page and current_page >= 0:
                # parts.append(f"\n--- Page {page + 1} ---\n")
                pass
            current_page = page

            # セグメントタイプに応じたフォーマット
            if seg_type == 'table_marker' and placeholder:
                parts.append(f"\n{placeholder}\n")
            elif seg_type == 'heading':
                parts.append(f"\n【{text}】\n")
            elif seg_type == 'post_body':
                parts.append(text)
            elif seg_type == 'list_item':
                parts.append(f"・{text}")
            else:
                parts.append(text)

        # 結合して整形
        unified = '\n\n'.join(p for p in parts if p.strip())

        # 連続する改行を整理
        unified = re.sub(r'\n{3,}', '\n\n', unified)

        return unified.strip()

    def _stats_to_dict(self, stats: DedupStats) -> Dict[str, Any]:
        """統計を辞書に変換"""
        return {
            'total_input': stats.total_input,
            'total_output': stats.total_output,
            'duplicates_removed': stats.duplicates_removed,
            'merged_segments': stats.merged_segments,
            'dedup_rate': f"{(stats.duplicates_removed / max(stats.total_input, 1)) * 100:.1f}%"
        }

    # ============================================
    # 追加ヘルパー: E と F のテキストマージ
    # ============================================
    def merge_e_f_texts(
        self,
        e_text: str,
        f_text: str,
        post_body_text: str = ""
    ) -> str:
        """
        E と F のテキストをマージ（重複排除付き）

        Args:
            e_text: Stage E の抽出テキスト
            f_text: Stage F の抽出テキスト
            post_body_text: 投稿本文

        Returns:
            マージ済みテキスト
        """
        segments = []
        ref_counter = 0

        # post_body
        if post_body_text:
            ref_counter += 1
            segments.append({
                'ref_id': f'REF_{ref_counter:03d}',
                'page': 0,
                'text': post_body_text,
                'segment_type': 'post_body',
                'source': 'post_body'
            })

        # E テキストを段落分割
        e_paragraphs = self._split_paragraphs(e_text)
        for para in e_paragraphs:
            ref_counter += 1
            segments.append({
                'ref_id': f'REF_{ref_counter:03d}',
                'page': 0,
                'text': para,
                'segment_type': 'paragraph',
                'source': 'stage_e'
            })

        # F テキストを段落分割
        f_paragraphs = self._split_paragraphs(f_text)
        for para in f_paragraphs:
            ref_counter += 1
            segments.append({
                'ref_id': f'REF_{ref_counter:03d}',
                'page': 0,
                'text': para,
                'segment_type': 'paragraph',
                'source': 'stage_f'
            })

        # 重複排除
        deduped = self._deduplicate_segments(segments)

        # 統合テキスト構築
        return self._build_unified_text(deduped)

    def _split_paragraphs(self, text: str) -> List[str]:
        """テキストを段落に分割"""
        if not text:
            return []

        # 空行で分割
        paragraphs = re.split(r'\n\s*\n', text)
        return [p.strip() for p in paragraphs if p.strip()]

    # ============================================
    # クロスバリデーション: E vs F の一致度計算
    # ============================================
    def cross_validate(
        self,
        e_segments: List[Dict],
        f_segments: List[Dict]
    ) -> Dict[str, Any]:
        """
        E と F のセグメントをクロスバリデーション

        Returns:
            {
                'matched_count': int,  # 一致したセグメント数
                'e_only_count': int,   # E にのみあるセグメント数
                'f_only_count': int,   # F にのみあるセグメント数
                'match_rate': float,   # 一致率
                'confidence': str      # 'high', 'medium', 'low'
            }
        """
        e_texts = {self._normalize_text(s.get('text', '')) for s in e_segments if s.get('text')}
        f_texts = {self._normalize_text(s.get('text', '')) for s in f_segments if s.get('text')}

        matched = e_texts & f_texts
        e_only = e_texts - f_texts
        f_only = f_texts - e_texts

        total = len(e_texts | f_texts)
        match_rate = len(matched) / max(total, 1)

        # 信頼度判定
        if match_rate >= 0.7:
            confidence = 'high'
        elif match_rate >= 0.4:
            confidence = 'medium'
        else:
            confidence = 'low'

        return {
            'matched_count': len(matched),
            'e_only_count': len(e_only),
            'f_only_count': len(f_only),
            'match_rate': match_rate,
            'confidence': confidence
        }

    # ============================================
    # AI研磨機能（2026-01-28 追加）
    # 連鎖研磨（スライディング・ウィンドウ）対応
    # ============================================

    # 連鎖研磨のパラメータ
    CHUNK_SIZE = 6000      # 1チャンクあたりの文字数
    OVERLAP_SIZE = 500     # チャンク間の重複（文脈バッファ）

    def _polish_text_with_ai(
        self,
        unified_text: str,
        post_body_text: str = ""
    ) -> Optional[str]:
        """
        AIでテキストを研磨（知能化）- 連鎖研磨対応

        断片的なテキストを「一本の完璧な原稿」に繋ぎ直す。
        長文の場合はスライディング・ウィンドウ方式で分割処理。

        Args:
            unified_text: ルールベースで統合されたテキスト
            post_body_text: 投稿本文（背景情報）

        Returns:
            研磨後のテキスト（失敗時はNone）
        """
        if not self.llm:
            return None

        try:
            # 短いテキストは単発処理
            if len(unified_text) <= self.CHUNK_SIZE:
                return self._polish_single_chunk(unified_text, post_body_text)

            # 長文は連鎖研磨
            logger.info(f"[G2] 連鎖研磨開始: {len(unified_text)}文字 → チャンク分割")
            return self._polish_chained(unified_text, post_body_text)

        except Exception as e:
            logger.error(f"[G2] AI研磨エラー: {e}")
            return None

    def _polish_single_chunk(
        self,
        text: str,
        post_body_text: str = "",
        context_prefix: str = ""
    ) -> Optional[str]:
        """
        単一チャンクを研磨

        Args:
            text: 研磨対象テキスト
            post_body_text: 投稿本文
            context_prefix: 前チャンクからの文脈（連鎖研磨時）

        Returns:
            研磨後テキスト
        """
        prompt = self._build_text_polish_prompt(text, post_body_text, context_prefix)

        logger.debug(f"[G2] チャンク研磨: {len(text)}文字")
        response = self.llm.call_model(
            tier="default",
            prompt=prompt,
            model_name=G2_MODEL,
            temperature=0.0
        )

        # トークン使用量を記録
        if hasattr(self.llm, 'last_usage') and self.llm.last_usage:
            usage = self.llm.last_usage
            self._token_usage['prompt_tokens'] += usage.get('prompt_tokens', 0)
            self._token_usage['completion_tokens'] += usage.get('completion_tokens', 0)
            self._token_usage['total_tokens'] += usage.get('total_tokens', 0)

        if not response.get('success'):
            logger.warning(f"[G2] チャンク研磨失敗: {response.get('error')}")
            return None

        polished = response.get('content', '').strip()

        # アンカー保持チェック
        original_anchors = set(re.findall(r'\[→\s*TBL_\d+\s*参照\]', text))
        polished_anchors = set(re.findall(r'\[→\s*TBL_\d+\s*参照\]', polished))

        if original_anchors and not original_anchors.issubset(polished_anchors):
            missing = original_anchors - polished_anchors
            logger.warning(f"[G2] アンカー欠落検出: {missing}")
            # 欠落したアンカーを末尾に追加して救済
            for anchor in missing:
                polished += f"\n{anchor}"

        return polished

    def _polish_chained(
        self,
        unified_text: str,
        post_body_text: str
    ) -> Optional[str]:
        """
        連鎖研磨（スライディング・ウィンドウ方式）

        長文を分割し、前チャンクの末尾を次チャンクの文脈として渡す。
        情報を1ミリも捨てずに全文を研磨する。

        Args:
            unified_text: 長文テキスト
            post_body_text: 投稿本文

        Returns:
            連結された研磨済みテキスト
        """
        # チャンク分割
        chunks = self._split_into_chunks(unified_text)
        logger.info(f"[G2] 連鎖研磨: {len(chunks)}チャンクに分割")

        polished_chunks = []
        context_prefix = ""

        for i, chunk in enumerate(chunks):
            logger.info(f"[G2] チャンク {i+1}/{len(chunks)} 研磨中...")

            # 最初のチャンクのみpost_bodyを渡す
            pb = post_body_text if i == 0 else ""

            polished = self._polish_single_chunk(chunk, pb, context_prefix)

            if polished:
                polished_chunks.append(polished)
                # 次のチャンクへの文脈バッファ
                context_prefix = polished[-self.OVERLAP_SIZE:] if len(polished) > self.OVERLAP_SIZE else polished
            else:
                # 研磨失敗時は元のチャンクを使用
                logger.warning(f"[G2] チャンク {i+1} 研磨失敗 → 元テキスト維持")
                polished_chunks.append(chunk)
                context_prefix = chunk[-self.OVERLAP_SIZE:] if len(chunk) > self.OVERLAP_SIZE else chunk

        # チャンクを結合（重複部分を除去）
        result = self._merge_polished_chunks(polished_chunks)

        logger.info(f"[G2] 連鎖研磨完了: {len(unified_text)}→{len(result)}文字, "
                   f"トークン計={self._token_usage['total_tokens']}")

        return result

    def _split_into_chunks(self, text: str) -> List[str]:
        """
        テキストをチャンクに分割

        段落境界で分割し、アンカーを壊さないように配慮。
        """
        chunks = []
        current_pos = 0
        text_len = len(text)

        while current_pos < text_len:
            # チャンク終了位置
            end_pos = min(current_pos + self.CHUNK_SIZE, text_len)

            if end_pos < text_len:
                # 段落境界を探す（チャンクサイズの80%〜100%の範囲で）
                search_start = current_pos + int(self.CHUNK_SIZE * 0.8)
                search_text = text[search_start:end_pos]

                # 段落区切り（空行）を探す
                para_break = search_text.rfind('\n\n')
                if para_break != -1:
                    end_pos = search_start + para_break + 2
                else:
                    # 単一改行を探す
                    line_break = search_text.rfind('\n')
                    if line_break != -1:
                        end_pos = search_start + line_break + 1

            chunk = text[current_pos:end_pos].strip()
            if chunk:
                chunks.append(chunk)

            current_pos = end_pos

        return chunks

    def _merge_polished_chunks(self, chunks: List[str]) -> str:
        """
        研磨済みチャンクを結合

        重複部分を検出して除去し、滑らかに接続。
        """
        if not chunks:
            return ""
        if len(chunks) == 1:
            return chunks[0]

        result = chunks[0]

        for i in range(1, len(chunks)):
            next_chunk = chunks[i]

            # 重複部分を検出（result末尾とnext_chunk冒頭の一致を探す）
            overlap_found = False
            for overlap_len in range(min(self.OVERLAP_SIZE, len(result), len(next_chunk)), 50, -10):
                if result[-overlap_len:] == next_chunk[:overlap_len]:
                    # 重複部分を除去して結合
                    result = result + next_chunk[overlap_len:]
                    overlap_found = True
                    break

            if not overlap_found:
                # 重複が見つからない場合は段落区切りで結合
                result = result + "\n\n" + next_chunk

        return result

    def _build_text_polish_prompt(
        self,
        unified_text: str,
        post_body_text: str = "",
        context_prefix: str = ""
    ) -> str:
        """
        AI研磨用プロンプトを構築

        キラープロンプト: 校閲記者としての絶対命令
        連鎖研磨時は前チャンクの文脈を受け取る

        Args:
            unified_text: 研磨対象テキスト
            post_body_text: 投稿本文（最初のチャンクのみ）
            context_prefix: 前チャンクからの文脈（連鎖研磨時）
        """
        # 連鎖研磨の文脈セクション
        context_section = ""
        if context_prefix:
            context_section = f"""【前セクションからの文脈】
以下は直前のセクションの末尾です。この続きとして自然に繋がるように校閲してください。
```
...{context_prefix}
```

"""

        prompt = f"""あなたは超一流の校閲記者です。
以下の断片的なテキストを、論理的に繋がる一本の文章に校閲・修復してください。

【絶対の掟】
1. **アンカーは聖域**
   `[→ TBL_001 参照]` などの表参照マーカーは情報の座標です。
   **1文字も変えず**、文脈上適切な位置に必ず残してください。

2. **OCRのゴミを浄化**
   読み間違い（「は」が「1」になる等）や不自然な改行を、
   文脈から推測して自然な日本語に修正してください。

3. **情報の完全維持**
   余計な要約・圧縮はしないでください。
   原文の情報を**最大限活かして**浄化するだけです。
   削除していいのはOCRのゴミのみ。

4. **post_body との接続**
   投稿本文（背景情報）と読み取り内容を、
   矛盾なく滑らかに接続してください。

{f'''【投稿本文（背景情報）】
{post_body_text[:500]}
''' if post_body_text else ''}{context_section}【校閲対象テキスト】
```
{unified_text}
```

【出力】
校閲・修復後のテキストのみを出力してください。
説明や注釈は一切不要です。
"""
        return prompt
```

## shared/pipeline/stage_g_gate.py

```python
"""
Stage G Gate: 仕分けゲート（表とテキストの物理的分離）

【設計 2026-01-28】G の入り口で「表」と「テキスト」を物理的に分離

役割: Stage E + Stage F の出力を受け取り、
      G1（表専用）と G2（テキスト専用）に振り分ける

============================================
入力:
  - stage_e_result: 物理抽出テキスト（ページ付き）
  - stage_f_payload: 独立読解結果（アンカー付き）
  - post_body: 投稿本文

出力:
  - g1_input: 表データ + 表ページのコンテキスト
  - g2_input: 純粋テキスト + post_body + 表の跡地マーカー

仕分けルール:
  1. 座標重なりチェック: E のテキストが F の表エリア内 → G1
  2. 構造自己主張チェック: E がタブ区切り等を持つ → G1 候補
  3. アンカー自動発行: 抜き取った跡地に目印を埋め込む
============================================
"""
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from loguru import logger
import re


@dataclass
class TableCandidate:
    """表候補データ"""
    anchor_id: str
    page: int
    source: str  # 'stage_e', 'stage_f.f7', 'stage_f.f8', 'merged'
    headers: List[str] = field(default_factory=list)
    rows: List[List[str]] = field(default_factory=list)
    title: str = ""
    table_type: str = "unknown"
    bbox: Optional[Dict[str, float]] = None  # 座標情報
    confidence: str = "medium"
    raw_text: str = ""  # E から来た生テキスト（構造化前）


@dataclass
class TextSegment:
    """テキストセグメント"""
    ref_id: str
    page: int
    text: str
    segment_type: str  # 'post_body', 'heading', 'paragraph', 'list_item'
    source: str  # 'post_body', 'stage_e', 'stage_f'
    table_placeholder: Optional[str] = None  # 表があった場所のマーカー


class StageGGate:
    """Gの入り口で表とテキストを物理的に分離する仕分けゲート"""

    # タブ区切りやカンマ区切りを検出するパターン
    TABULAR_PATTERNS = [
        r'\t.*\t',  # タブ区切り
        r'│.*│',    # 罫線（パイプ）
        r'\|.*\|',  # Markdown表
        r'^\s*\d+\.\s+\S+\s+\d+',  # 順位表パターン（例: "1. 山田 100"）
    ]

    def __init__(self):
        self._table_counter = 0
        self._text_counter = 0

    def route(
        self,
        stage_e_result: Dict[str, Any],
        stage_f_payload: Dict[str, Any],
        post_body: Optional[Dict[str, Any]] = None
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        E + F の出力を G1（表用）と G2（テキスト用）に仕分け

        Args:
            stage_e_result: Stage E の出力
            stage_f_payload: Stage F の出力
            post_body: 投稿本文

        Returns:
            (g1_input, g2_input)
        """
        logger.info("[G-Gate] 仕分け開始...")

        # カウンターリセット
        self._table_counter = 0
        self._text_counter = 0

        # ============================================
        # Step 1: Stage F から表とテキストブロックを抽出
        # ============================================
        f_tables = self._extract_f_tables(stage_f_payload)
        f_text_blocks = self._extract_f_text_blocks(stage_f_payload)
        f_anchors = stage_f_payload.get('anchors', [])

        logger.info(f"[G-Gate] Stage F: tables={len(f_tables)}, text_blocks={len(f_text_blocks)}, anchors={len(f_anchors)}")

        # ============================================
        # Step 2: Stage E からテキストを抽出し、表候補を検出
        # ============================================
        e_content = stage_e_result.get('content', '')
        e_metadata = stage_e_result.get('metadata', {})
        e_table_bboxes = e_metadata.get('table_bboxes', [])

        e_table_candidates, e_text_segments = self._analyze_e_content(
            e_content, e_table_bboxes, f_tables
        )

        logger.info(f"[G-Gate] Stage E: table_candidates={len(e_table_candidates)}, text_segments={len(e_text_segments)}")

        # ============================================
        # Step 3: 座標重なりチェック → 表データをマージ
        # ============================================
        merged_tables = self._merge_tables_by_coordinate(
            e_table_candidates, f_tables, f_anchors
        )

        logger.info(f"[G-Gate] マージ後: tables={len(merged_tables)}")

        # ============================================
        # Step 3.5: 表数の安全制限（最大10表/ページ、合計最大20表）
        # ============================================
        MAX_TABLES_PER_PAGE = 10
        MAX_TABLES_TOTAL = 20

        if len(merged_tables) > MAX_TABLES_TOTAL:
            original_count = len(merged_tables)
            # 重要度スコアでソート（行数 × 列数 = 面積）
            merged_tables.sort(
                key=lambda t: t.get('row_count', 0) * max(t.get('col_count', 1), 1),
                reverse=True
            )
            # 上位MAX_TABLES_TOTAL個を採用
            merged_tables = merged_tables[:MAX_TABLES_TOTAL]
            logger.warning(f"[G-Gate Safety] 表数制限: {original_count} → {MAX_TABLES_TOTAL} (サイズ順で採用)")

        # ページごとの制限もチェック
        from collections import defaultdict
        page_tables = defaultdict(list)
        for tbl in merged_tables:
            page_tables[tbl.get('page', 0)].append(tbl)

        limited_tables = []
        for page, tables in page_tables.items():
            if len(tables) > MAX_TABLES_PER_PAGE:
                # サイズ順でソートして上位を採用
                tables.sort(
                    key=lambda t: t.get('row_count', 0) * max(t.get('col_count', 1), 1),
                    reverse=True
                )
                tables = tables[:MAX_TABLES_PER_PAGE]
                logger.warning(f"[G-Gate Safety] ページ{page}の表数制限: → {MAX_TABLES_PER_PAGE}")
            limited_tables.extend(tables)

        merged_tables = limited_tables
        # ページ順で再ソート
        merged_tables.sort(key=lambda x: (x.get('page', 0), x.get('anchor_id', '')))

        # ============================================
        # Step 4: G1 入力を構築（表 + 表ページのコンテキスト）
        # ============================================
        g1_input = self._build_g1_input(merged_tables, e_text_segments, f_text_blocks)

        # ============================================
        # Step 5: G2 入力を構築（テキスト + 表の跡地マーカー）
        # ============================================
        g2_input = self._build_g2_input(
            e_text_segments, f_text_blocks, merged_tables, post_body
        )

        # 統計ログ
        logger.info(f"[G-Gate] 仕分け完了:")
        logger.info(f"  ├─ G1: {len(g1_input.get('tables', []))}表, {len(g1_input.get('table_page_context', {}))}ページコンテキスト")
        logger.info(f"  └─ G2: {len(g2_input.get('segments', []))}セグメント, {g2_input.get('placeholder_count', 0)}プレースホルダー")

        return g1_input, g2_input

    def _extract_f_tables(self, stage_f_payload: Dict[str, Any]) -> List[Dict]:
        """Stage F から表データを抽出"""
        tables = []

        # トップレベルの tables
        for tbl in stage_f_payload.get('tables', []):
            tables.append({
                'source': 'stage_f',
                'block_id': tbl.get('block_id', ''),
                'page': tbl.get('page', tbl.get('chunk_start_page', 0)),
                'title': tbl.get('table_title', ''),
                'table_type': tbl.get('table_type', 'visual_table'),
                'headers': tbl.get('headers', tbl.get('columns', [])),
                'rows': tbl.get('rows', []),
                'bbox': tbl.get('bbox'),
                'row_count': tbl.get('row_count', len(tbl.get('rows', []))),
                'col_count': tbl.get('col_count', len(tbl.get('headers', tbl.get('columns', [])))),
            })

        # アンカーから表を抽出
        for anchor in stage_f_payload.get('anchors', []):
            if anchor.get('type') == 'table':
                # 既に tables に含まれていないかチェック
                anchor_id = anchor.get('anchor_id', '')
                if not any(t.get('block_id') == anchor_id for t in tables):
                    tables.append({
                        'source': 'stage_f.anchor',
                        'block_id': anchor_id,
                        'page': anchor.get('page', 0),
                        'title': anchor.get('title', anchor.get('content', '')[:50]),
                        'table_type': anchor.get('table_type', 'visual_table'),
                        'headers': anchor.get('columns', []),
                        'rows': anchor.get('rows', []),
                        'is_heavy': anchor.get('is_heavy', False),
                        'row_count': anchor.get('row_count', len(anchor.get('rows', []))),
                        'col_count': anchor.get('col_count', len(anchor.get('columns', []))),
                    })

        return tables

    def _extract_f_text_blocks(self, stage_f_payload: Dict[str, Any]) -> List[Dict]:
        """Stage F からテキストブロックを抽出"""
        blocks = []

        for block in stage_f_payload.get('text_blocks', []):
            blocks.append({
                'block_id': block.get('block_id', ''),
                'page': block.get('page', block.get('original_page', 0)),
                'text': block.get('text', ''),
                'block_type': block.get('block_type', 'paragraph'),
                'reading_order': block.get('reading_order', 0),
                'confidence': block.get('confidence', 'medium'),
            })

        # アンカーからテキストを抽出
        for anchor in stage_f_payload.get('anchors', []):
            if anchor.get('type') == 'text':
                anchor_id = anchor.get('anchor_id', '')
                if not any(b.get('block_id') == anchor_id for b in blocks):
                    blocks.append({
                        'block_id': anchor_id,
                        'page': anchor.get('page', 0),
                        'text': anchor.get('content', ''),
                        'block_type': 'paragraph',
                        'reading_order': anchor.get('reading_order', 0),
                    })

        return blocks

    def _analyze_e_content(
        self,
        e_content: str,
        e_table_bboxes: List[Dict],
        f_tables: List[Dict]
    ) -> Tuple[List[TableCandidate], List[TextSegment]]:
        """
        Stage E のコンテンツを分析し、表候補とテキストセグメントに分離

        ルール:
        1. e_table_bboxes に含まれる領域 → 表候補
        2. タブ区切り等の構造を持つテキスト → 表候補
        3. それ以外 → テキストセグメント
        """
        table_candidates = []
        text_segments = []

        if not e_content:
            return table_candidates, text_segments

        # ページ区切りで分割（\f または [Page X] マーカー）
        page_pattern = r'(?:\f|\[Page\s*(\d+)\])'
        pages = re.split(page_pattern, e_content)

        current_page = 0
        for i, part in enumerate(pages):
            if not part:
                continue

            # ページ番号の更新
            if part.isdigit():
                current_page = int(part) - 1  # 0-indexed
                continue

            # 段落に分割
            paragraphs = self._split_paragraphs(part)

            for para in paragraphs:
                para_clean = para.strip()
                if not para_clean:
                    continue

                # 構造自己主張チェック: タブ区切り等を検出
                if self._is_tabular_text(para_clean):
                    self._table_counter += 1
                    anchor_id = f"E_TBL_{self._table_counter:03d}"

                    table_candidates.append(TableCandidate(
                        anchor_id=anchor_id,
                        page=current_page,
                        source='stage_e',
                        raw_text=para_clean,
                        confidence='medium'
                    ))
                    logger.debug(f"[G-Gate] E表候補検出: {anchor_id} (page={current_page})")
                else:
                    self._text_counter += 1
                    ref_id = f"E_TXT_{self._text_counter:03d}"

                    text_segments.append(TextSegment(
                        ref_id=ref_id,
                        page=current_page,
                        text=para_clean,
                        segment_type=self._detect_segment_type(para_clean),
                        source='stage_e'
                    ))

        return table_candidates, text_segments

    def _is_tabular_text(self, text: str) -> bool:
        """テキストが表形式かどうかを判定"""
        # 短すぎるテキストは表ではない
        if len(text) < 20:
            return False

        # 複数行あるかチェック
        lines = text.strip().split('\n')
        if len(lines) < 2:
            return False

        # タブ区切り・罫線パターンをチェック
        for pattern in self.TABULAR_PATTERNS:
            if re.search(pattern, text, re.MULTILINE):
                # 同じパターンが複数行にあれば表の可能性が高い
                matches = re.findall(pattern, text, re.MULTILINE)
                if len(matches) >= 2:
                    return True

        # 列の揃いをチェック（各行の区切り位置が似ている）
        tab_counts = [line.count('\t') for line in lines if line.strip()]
        if tab_counts and len(set(tab_counts)) == 1 and tab_counts[0] >= 2:
            return True

        return False

    def _detect_segment_type(self, text: str) -> str:
        """テキストセグメントの種類を検出"""
        text_stripped = text.strip()

        # 見出しパターン
        if len(text_stripped) < 50 and not text_stripped.endswith('。'):
            if re.match(r'^[■□●○◆◇▶▷★☆【】]', text_stripped):
                return 'heading'
            if re.match(r'^\d+\.\s', text_stripped):
                return 'heading'

        # リストアイテム
        if re.match(r'^[\-\*・]\s', text_stripped):
            return 'list_item'
        if re.match(r'^\(\d+\)\s', text_stripped):
            return 'list_item'

        return 'paragraph'

    def _split_paragraphs(self, text: str) -> List[str]:
        """テキストを段落に分割"""
        # 空行で分割
        paragraphs = re.split(r'\n\s*\n', text)
        return [p.strip() for p in paragraphs if p.strip()]

    def _merge_tables_by_coordinate(
        self,
        e_table_candidates: List[TableCandidate],
        f_tables: List[Dict],
        f_anchors: List[Dict]
    ) -> List[Dict]:
        """
        座標重なりチェックにより E と F の表をマージ

        ルール:
        1. 同じページで座標が重なる → マージ（F を優先、E で補完）
        2. F のみ → そのまま採用
        3. E のみ → 構造化を試みて採用
        """
        merged = []
        used_e_indices = set()

        # F の表を基準にマージ
        for f_tbl in f_tables:
            f_page = f_tbl.get('page', 0)
            f_bbox = f_tbl.get('bbox')

            # 同じページの E 表候補を探す
            matching_e = None
            for i, e_cand in enumerate(e_table_candidates):
                if i in used_e_indices:
                    continue
                if e_cand.page == f_page:
                    # 座標チェック（bbox があれば）またはページ一致で採用
                    matching_e = e_cand
                    used_e_indices.add(i)
                    break

            # マージした表を作成
            self._table_counter += 1
            anchor_id = f"TBL_{self._table_counter:03d}"

            merged_table = {
                'anchor_id': anchor_id,
                'page': f_page,
                'title': f_tbl.get('title', ''),
                'table_type': f_tbl.get('table_type', 'visual_table'),
                'headers': f_tbl.get('headers', []),
                'rows': f_tbl.get('rows', []),
                'row_count': f_tbl.get('row_count', 0),
                'col_count': f_tbl.get('col_count', 0),
                'source': 'stage_f' if not matching_e else 'merged',
                'is_heavy': f_tbl.get('is_heavy', False) or f_tbl.get('row_count', 0) >= 20,
            }

            # E のデータで補完
            if matching_e and matching_e.raw_text:
                merged_table['e_raw_text'] = matching_e.raw_text
                merged_table['source'] = 'merged'

            merged.append(merged_table)

        # E のみの表候補を追加
        for i, e_cand in enumerate(e_table_candidates):
            if i in used_e_indices:
                continue

            self._table_counter += 1
            anchor_id = f"TBL_{self._table_counter:03d}"

            # E のテキストから構造を推測
            headers, rows = self._parse_tabular_text(e_cand.raw_text)

            merged.append({
                'anchor_id': anchor_id,
                'page': e_cand.page,
                'title': '',
                'table_type': 'e_detected',
                'headers': headers,
                'rows': rows,
                'row_count': len(rows),
                'col_count': len(headers) if headers else 0,
                'source': 'stage_e',
                'e_raw_text': e_cand.raw_text,
                'is_heavy': len(rows) >= 20,
            })

        # ページ順でソート
        merged.sort(key=lambda x: (x.get('page', 0), x.get('anchor_id', '')))

        return merged

    def _parse_tabular_text(self, text: str) -> Tuple[List[str], List[List[str]]]:
        """タブ区切りテキストから headers と rows を抽出"""
        lines = [l.strip() for l in text.split('\n') if l.strip()]
        if not lines:
            return [], []

        # 区切り文字を検出
        delimiter = '\t'
        if '\t' not in text and '|' in text:
            delimiter = '|'

        # 最初の行をヘッダーとして扱う
        headers = [c.strip() for c in lines[0].split(delimiter) if c.strip()]
        rows = []

        for line in lines[1:]:
            cells = [c.strip() for c in line.split(delimiter)]
            if cells and any(c for c in cells):
                rows.append(cells)

        return headers, rows

    def _build_g1_input(
        self,
        merged_tables: List[Dict],
        e_text_segments: List[TextSegment],
        f_text_blocks: List[Dict]
    ) -> Dict[str, Any]:
        """
        G1 入力を構築: 表 + 表ページのコンテキスト

        G1 JSON 構造:
        {
            "tables": [
                {
                    "anchor_id": "TBL_001",
                    "page": 1,
                    "title": "成績一覧",
                    "table_type": "ranking",
                    "headers": ["順位", "氏名", "点数"],
                    "rows": [["1", "山田", "100"], ...],
                    "source": "merged",
                    "is_heavy": true
                }
            ],
            "table_page_context": {
                "page_1": "表の前後のテキスト..."
            }
        }
        """
        # 表があるページを特定
        table_pages = set(t.get('page', 0) for t in merged_tables)

        # 表ページのテキストコンテキストを収集
        table_page_context = {}
        for page in table_pages:
            context_texts = []

            # E のテキストから収集
            for seg in e_text_segments:
                if seg.page == page:
                    context_texts.append(seg.text)

            # F のテキストから収集
            for block in f_text_blocks:
                if block.get('page', 0) == page:
                    context_texts.append(block.get('text', ''))

            if context_texts:
                table_page_context[f"page_{page}"] = '\n'.join(context_texts)

        return {
            'tables': merged_tables,
            'table_page_context': table_page_context,
            'table_count': len(merged_tables),
            'heavy_table_count': sum(1 for t in merged_tables if t.get('is_heavy', False))
        }

    def _build_g2_input(
        self,
        e_text_segments: List[TextSegment],
        f_text_blocks: List[Dict],
        merged_tables: List[Dict],
        post_body: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        G2 入力を構築: テキスト + 表の跡地マーカー

        G2 JSON 構造:
        {
            "segments": [
                {
                    "ref_id": "REF_001",
                    "page": 0,
                    "text": "投稿本文...",
                    "segment_type": "post_body",
                    "source": "post_body",
                    "table_placeholder": null
                },
                {
                    "ref_id": "REF_002",
                    "page": 1,
                    "text": "",
                    "segment_type": "table_marker",
                    "source": "g_gate",
                    "table_placeholder": "[→ TBL_001 参照]"
                }
            ]
        }
        """
        segments = []
        ref_counter = 0
        placeholder_count = 0

        # 表があるページとアンカーのマップ
        table_page_anchors = {}
        for tbl in merged_tables:
            page = tbl.get('page', 0)
            anchor_id = tbl.get('anchor_id', '')
            if page not in table_page_anchors:
                table_page_anchors[page] = []
            table_page_anchors[page].append(anchor_id)

        # 1. post_body を先頭に
        if post_body and post_body.get('text'):
            ref_counter += 1
            segments.append({
                'ref_id': f'REF_{ref_counter:03d}',
                'page': 0,
                'text': post_body['text'],
                'segment_type': 'post_body',
                'source': 'post_body',
                'table_placeholder': None
            })

        # 2. E のテキストセグメントを追加（表ページ以外 or 表の跡地マーカー付き）
        # ページ順でソート
        all_segments = []

        for seg in e_text_segments:
            all_segments.append({
                'page': seg.page,
                'text': seg.text,
                'segment_type': seg.segment_type,
                'source': seg.source,
                'reading_order': 0
            })

        for block in f_text_blocks:
            # 重複チェック（E に同じテキストがあればスキップ）
            block_text = block.get('text', '')
            if any(s['text'] == block_text for s in all_segments):
                continue

            all_segments.append({
                'page': block.get('page', 0),
                'text': block_text,
                'segment_type': block.get('block_type', 'paragraph'),
                'source': 'stage_f',
                'reading_order': block.get('reading_order', 0)
            })

        # ページと読み順でソート
        all_segments.sort(key=lambda x: (x['page'], x['reading_order']))

        # セグメントを追加（表の跡地にはプレースホルダーを挿入）
        inserted_placeholders = set()
        for seg in all_segments:
            page = seg['page']

            # このページに表があり、まだプレースホルダーを挿入していない場合
            if page in table_page_anchors and page not in inserted_placeholders:
                for anchor_id in table_page_anchors[page]:
                    ref_counter += 1
                    placeholder_count += 1
                    segments.append({
                        'ref_id': f'REF_{ref_counter:03d}',
                        'page': page,
                        'text': '',
                        'segment_type': 'table_marker',
                        'source': 'g_gate',
                        'table_placeholder': f'[→ {anchor_id} 参照]'
                    })
                inserted_placeholders.add(page)

            # テキストセグメントを追加
            ref_counter += 1
            segments.append({
                'ref_id': f'REF_{ref_counter:03d}',
                'page': seg['page'],
                'text': seg['text'],
                'segment_type': seg['segment_type'],
                'source': seg['source'],
                'table_placeholder': None
            })

        return {
            'segments': segments,
            'segment_count': len(segments),
            'placeholder_count': placeholder_count,
            'post_body': post_body or {}
        }
```

## shared/pipeline/stage_g_refiner.py

```python
"""
Stage G: Integration Refiner (統合精錬) - v2.0

【設計 2026-01-28】G-Gate + G1 + G2 による物理的分離

役割: Stage E（物理抽出）と Stage F（独立読解）の結果を統合し、
      G1（表専用）と G2（テキスト専用）で整理してから H1/H2 へ渡す

============================================
新アーキテクチャ:

[Stage E] + [Stage F]
         ↓
    [G-Gate] ←─── 仕分けゲート（表とテキストを物理的に分離）
         ↓
   ┌─────┴─────┐
   ↓           ↓
 [G1]        [G2]
 表整理      テキスト整理
   ↓           ↓
 [H1]        [H2]

入力:
  - stage_e_result: 物理抽出テキスト
  - stage_f_payload: 独立読解結果（アンカー付き）
  - post_body: 投稿本文

出力:
  - g1_result: 表データ（検証済み）→ H1 へ
  - g2_result: テキストセグメント（重複排除済み）→ H2 へ
  - unified_text: 統合テキスト（後方互換）
  - source_inventory: REF_ID付きセグメント（後方互換）
  - table_inventory: TBL_ID付き表（後方互換）
============================================
"""
import json
from typing import Dict, Any, List, Optional, Tuple
from loguru import logger

from shared.ai.llm_client.llm_client import LLMClient
from .constants import STAGE_H_INPUT_SCHEMA_VERSION, G_MODEL
from .stage_g_gate import StageGGate
from .stage_g1_table_refiner import StageG1TableRefiner
from .stage_g2_text_refiner import StageG2TextRefiner


class StageGRefiner:
    """Stage G: 統合精錬（G-Gate + G1 + G2）"""

    def __init__(self, llm_client: LLMClient):
        """
        Args:
            llm_client: LLMクライアント
        """
        self.llm = llm_client
        self._g_usage: Dict[str, Any] = {}

        # 新しいサブモジュールを初期化（LLMクライアントを渡す）
        self.gate = StageGGate()
        self.g1 = StageG1TableRefiner(llm_client=llm_client)
        self.g2 = StageG2TextRefiner(llm_client=llm_client)

    def process(
        self,
        stage_e_result: Optional[Dict[str, Any]] = None,
        stage_f_payload: Optional[Dict[str, Any]] = None,
        post_body: Optional[Dict[str, Any]] = None,
        model: str = "gemini-2.0-flash-lite",
        workspace: str = "default"
    ) -> Dict[str, Any]:
        """
        Stage E と Stage F の結果を統合（v2.0: G-Gate + G1 + G2）

        Args:
            stage_e_result: Stage E の出力
            stage_f_payload: Stage F の出力
            post_body: 投稿本文
            model: 使用するモデル（未使用、後方互換のため残す）
            workspace: ワークスペース

        Returns:
            {
                'unified_text': str,
                'source_inventory': List[Dict],
                'table_inventory': List[Dict],
                'cross_validation': Dict,
                'ref_count': int,
                'warnings': List[str],
                'g1_result': Dict,  # H1 用
                'g2_result': Dict,  # H2 用
                'processing_mode': str
            }
        """
        logger.info(f"[Stage G] 統合精錬開始（v2.0: G-Gate + G1 + G2）")

        # 入力のデフォルト値
        stage_e_result = stage_e_result or {}
        stage_f_payload = stage_f_payload or {}

        # 入力データ取得
        e_content = stage_e_result.get('content', '')
        e_method = stage_e_result.get('method', 'unknown')
        f_processing_mode = stage_f_payload.get('processing_mode', 'unknown')

        logger.info(f"[Stage G] 入力:")
        logger.info(f"  ├─ Stage E: {len(e_content)}文字 (method={e_method})")
        logger.info(f"  ├─ Stage F: tables={len(stage_f_payload.get('tables', []))}, anchors={len(stage_f_payload.get('anchors', []))}")
        logger.info(f"  └─ F processing_mode: {f_processing_mode}")

        # ============================================
        # 特殊ケースの処理（従来ロジックを維持）
        # ============================================

        # 添付なし（E, F 両方スキップ）の場合
        if not e_content and f_processing_mode == 'skipped':
            logger.info("[Stage G] 添付なし → post_body のみで処理")
            return self._process_post_body_only(post_body)

        # Stage F がスキップされた場合（ドキュメントのみ）
        if f_processing_mode == 'skipped':
            logger.info("[Stage G] Stage F スキップ → Stage E のみで処理")
            return self._process_e_only(stage_e_result, post_body)

        # 音声/動画の場合（Transcription のみ）
        if f_processing_mode == 'transcription_only':
            logger.info("[Stage G] Transcription モード → F-7 結果を使用")
            return self._process_transcription(stage_f_payload, post_body)

        # ============================================
        # 通常処理: G-Gate → G1 → G2
        # ============================================
        logger.info("[Stage G] 通常モード → G-Gate + G1 + G2")

        try:
            # Step 1: G-Gate で仕分け
            g1_input, g2_input = self.gate.route(
                stage_e_result=stage_e_result,
                stage_f_payload=stage_f_payload,
                post_body=post_body
            )

            # Step 2: G1 で表を整理
            g1_result = self.g1.process(g1_input)

            # Step 3: G2 でテキストを整理
            g2_result = self.g2.process(g2_input)

            # Step 4: 後方互換のための出力を構築
            return self._build_unified_result(g1_result, g2_result, post_body)

        except Exception as e:
            logger.warning(f"[Stage G] G-Gate/G1/G2 処理失敗、フォールバック: {e}")
            # フォールバック: 従来のルールベース処理
            return self._legacy_rule_based_merge(stage_e_result, stage_f_payload, post_body)

    def _build_unified_result(
        self,
        g1_result: Dict[str, Any],
        g2_result: Dict[str, Any],
        post_body: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        G1 と G2 の結果を統合し、後方互換の出力形式を構築

        Args:
            g1_result: G1（表整理）の出力
            g2_result: G2（テキスト整理）の出力
            post_body: 投稿本文

        Returns:
            後方互換の Stage G 出力
        """
        # source_inventory: G2 のセグメントから構築
        source_inventory = []
        for seg in g2_result.get('segments', []):
            # table_marker はスキップ（table_inventory に含まれる）
            if seg.get('segment_type') == 'table_marker':
                continue

            source_inventory.append({
                'ref_id': seg.get('ref_id', ''),
                'text': seg.get('text', ''),
                'type': seg.get('segment_type', 'paragraph'),
                'source': seg.get('source', 'unknown'),
                'page': seg.get('page', 0),
                'confidence': 'high' if seg.get('source') == 'post_body' else 'medium'
            })

        # table_inventory: G1 の表から構築
        table_inventory = []
        for tbl in g1_result.get('tables', []):
            table_inventory.append({
                'ref_id': tbl.get('anchor_id', ''),
                'table_title': tbl.get('title', ''),
                'table_type': tbl.get('table_type', 'visual_table'),
                'headers': tbl.get('headers', []),
                'rows': tbl.get('rows', []),
                'row_count': tbl.get('row_count', 0),
                'col_count': tbl.get('col_count', 0),
                'page': tbl.get('page', 0),
                'source': tbl.get('source', 'unknown'),
                'is_heavy': tbl.get('is_heavy', False),
                'is_valid': tbl.get('is_valid', True)
            })

        # unified_text: G2 の出力を使用
        unified_text = g2_result.get('unified_text', '')

        # cross_validation: 統計情報を構築
        g1_stats = g1_result.get('statistics', {})
        g2_stats = g2_result.get('dedup_stats', {})

        cross_validation = {
            'mode': 'g_gate_v2',
            'table_count': g1_stats.get('total_tables', 0),
            'valid_tables': g1_stats.get('valid_tables', 0),
            'total_table_rows': g1_stats.get('total_rows', 0),
            'text_segments': g2_stats.get('total_output', 0),
            'duplicates_removed': g2_stats.get('duplicates_removed', 0),
            'dedup_rate': g2_stats.get('dedup_rate', '0%')
        }

        # 警告を収集
        warnings = []
        for val in g1_result.get('validation_results', []):
            for warn in val.get('warnings', []):
                warnings.append(f"G1_{val.get('anchor_id', '')}: {warn}")
            for err in val.get('errors', []):
                warnings.append(f"G1_ERROR_{val.get('anchor_id', '')}: {err}")

        # トークン使用量を集計
        g1_tokens = g1_result.get('token_usage', {})
        g2_tokens = g2_result.get('token_usage', {})
        total_tokens = {
            'G1': g1_tokens,
            'G2': g2_tokens,
            'total_prompt': g1_tokens.get('prompt_tokens', 0) + g2_tokens.get('prompt_tokens', 0),
            'total_completion': g1_tokens.get('completion_tokens', 0) + g2_tokens.get('completion_tokens', 0),
            'total': g1_tokens.get('total_tokens', 0) + g2_tokens.get('total_tokens', 0)
        }

        logger.info(f"[Stage G] 統合完了:")
        logger.info(f"  ├─ source_inventory: {len(source_inventory)}件")
        logger.info(f"  ├─ table_inventory: {len(table_inventory)}件")
        logger.info(f"  ├─ unified_text: {len(unified_text)}文字")
        logger.info(f"  ├─ warnings: {len(warnings)}件")
        logger.info(f"  └─ トークン合計: {total_tokens['total']} (G1={g1_tokens.get('total_tokens', 0)}, G2={g2_tokens.get('total_tokens', 0)})")

        return {
            'unified_text': unified_text,
            'source_inventory': source_inventory,
            'table_inventory': table_inventory,
            'cross_validation': cross_validation,
            'ref_count': len(source_inventory) + len(table_inventory),
            'warnings': warnings,
            'processing_mode': 'g_gate_v2',
            'post_body': post_body or {},
            # 新しい出力（H1/H2 で直接使用）
            'g1_result': g1_result,
            'g2_result': g2_result,
            'token_usage': total_tokens
        }

    # ============================================
    # H1/H2 ルーティング（新設計対応版）
    # ============================================
    def route_anchors_to_stages(
        self,
        stage_g_result: Dict[str, Any],
        anchors: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        アンカー単位でH1/H2への振り分けを行う（v2.0対応）

        新設計では G1/G2 の結果を直接使用可能

        Args:
            stage_g_result: Stage G の処理結果
            anchors: Stage F からのアンカー配列（レガシー用）

        Returns:
            {
                'h1_payload': {...},
                'h2_payload': {...},
                'anchor_map': {...}
            }
        """
        logger.info("[Stage G] H1/H2 ルーティング開始")

        # 新設計: g1_result と g2_result が存在する場合
        g1_result = stage_g_result.get('g1_result')
        g2_result = stage_g_result.get('g2_result')

        if g1_result and g2_result:
            return self._route_from_g1_g2(g1_result, g2_result, stage_g_result)

        # レガシー: 従来のルーティングロジック
        return self._legacy_route_anchors(stage_g_result, anchors)

    def _route_from_g1_g2(
        self,
        g1_result: Dict[str, Any],
        g2_result: Dict[str, Any],
        stage_g_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        G1/G2 の結果から H1/H2 ペイロードを構築

        Args:
            g1_result: G1（表整理）の出力
            g2_result: G2（テキスト整理）の出力
            stage_g_result: Stage G 全体の出力

        Returns:
            H1/H2 ルーティング結果
        """
        tables = g1_result.get('tables', [])
        segments = g2_result.get('segments', [])

        # H1 ペイロード: 全ての表（重い表を優先処理）
        heavy_tables = [t for t in tables if t.get('is_heavy', False)]
        light_tables = [t for t in tables if not t.get('is_heavy', False)]

        h1_payload = {
            'heavy_tables': heavy_tables,
            'light_tables': light_tables,
            'table_anchors': [t.get('anchor_id', '') for t in heavy_tables],
            'table_page_context': g1_result.get('table_page_context', {}),
            'validation_results': g1_result.get('validation_results', [])
        }

        # H2 ペイロード: テキストセグメント + 軽い表
        text_anchors = [s for s in segments if s.get('segment_type') != 'table_marker']

        h2_payload = {
            'text_anchors': text_anchors,
            'light_tables': light_tables,
            'reduced_text': g2_result.get('unified_text', ''),
            'dedup_stats': g2_result.get('dedup_stats', {}),
            'post_body': g2_result.get('post_body', {})
        }

        # アンカーマップ
        anchor_map = {}
        for t in heavy_tables:
            anchor_map[t.get('anchor_id', '')] = 'h1'
        for t in light_tables:
            anchor_map[t.get('anchor_id', '')] = 'h2'
        for s in text_anchors:
            anchor_map[s.get('ref_id', '')] = 'h2'

        logger.info(f"[Stage G] ルーティング完了（v2.0）:")
        logger.info(f"  ├─ H1: {len(heavy_tables)}重い表 + {len(light_tables)}軽い表")
        logger.info(f"  ├─ H2: {len(text_anchors)}テキスト")
        logger.info(f"  └─ reduced_text: {len(h2_payload['reduced_text'])}文字")

        return {
            'h1_payload': h1_payload,
            'h2_payload': h2_payload,
            'anchor_map': anchor_map
        }

    # ============================================
    # 特殊ケース処理（従来ロジック）
    # ============================================
    def _process_post_body_only(self, post_body: Optional[Dict]) -> Dict[str, Any]:
        """投稿本文のみの処理（添付なし）"""
        text = post_body.get('text', '') if post_body else ''

        source_inventory = []
        if text:
            source_inventory.append({
                'ref_id': 'REF_001',
                'text': text,
                'type': 'post_body',
                'source': 'post_body',
                'confidence': 'high'
            })

        # 空の G1/G2 結果を作成
        g1_result = {
            'tables': [],
            'validation_results': [],
            'table_page_context': {},
            'statistics': {'total_tables': 0, 'valid_tables': 0, 'total_rows': 0}
        }
        g2_result = {
            'segments': source_inventory,
            'unified_text': text,
            'dedup_stats': {'total_input': 1, 'total_output': 1, 'duplicates_removed': 0},
            'post_body': post_body or {}
        }

        return {
            'unified_text': text,
            'source_inventory': source_inventory,
            'table_inventory': [],
            'cross_validation': {'mode': 'post_body_only'},
            'ref_count': len(source_inventory),
            'warnings': [],
            'processing_mode': 'post_body_only',
            'post_body': post_body or {},
            'g1_result': g1_result,
            'g2_result': g2_result
        }

    def _process_e_only(
        self,
        stage_e_result: Dict[str, Any],
        post_body: Optional[Dict]
    ) -> Dict[str, Any]:
        """Stage E のみの処理（ドキュメント、Stage F スキップ）"""
        e_content = stage_e_result.get('content', '')

        source_inventory = []
        ref_index = 1

        # post_body を先頭に
        if post_body and post_body.get('text'):
            source_inventory.append({
                'ref_id': f'REF_{ref_index:03d}',
                'text': post_body['text'],
                'type': 'post_body',
                'source': 'post_body',
                'confidence': 'high'
            })
            ref_index += 1

        # Stage E テキストを追加
        if e_content:
            paragraphs = self._split_paragraphs(e_content)
            for para in paragraphs:
                if para.strip():
                    source_inventory.append({
                        'ref_id': f'REF_{ref_index:03d}',
                        'text': para.strip(),
                        'type': 'paragraph',
                        'source': 'stage_e',
                        'confidence': 'high'
                    })
                    ref_index += 1

        # unified_text 構築
        unified_parts = []
        if post_body and post_body.get('text'):
            unified_parts.append(post_body['text'])
        if e_content:
            unified_parts.append(e_content)
        unified_text = '\n\n'.join(unified_parts)

        # G1/G2 結果
        g1_result = {
            'tables': [],
            'validation_results': [],
            'table_page_context': {},
            'statistics': {'total_tables': 0, 'valid_tables': 0, 'total_rows': 0}
        }
        g2_result = {
            'segments': source_inventory,
            'unified_text': unified_text,
            'dedup_stats': {'total_input': len(source_inventory), 'total_output': len(source_inventory), 'duplicates_removed': 0},
            'post_body': post_body or {}
        }

        return {
            'unified_text': unified_text,
            'source_inventory': source_inventory,
            'table_inventory': [],
            'cross_validation': {'mode': 'e_only'},
            'ref_count': len(source_inventory),
            'warnings': [],
            'processing_mode': 'e_only',
            'post_body': post_body or {},
            'g1_result': g1_result,
            'g2_result': g2_result
        }

    def _process_transcription(
        self,
        stage_f_payload: Dict[str, Any],
        post_body: Optional[Dict]
    ) -> Dict[str, Any]:
        """音声/動画の Transcription 処理"""
        f_path_a = stage_f_payload.get('path_a_result', {})
        transcript = f_path_a.get('transcript', '')
        visual_log = f_path_a.get('visual_log', '')

        source_inventory = []
        ref_index = 1

        # post_body を先頭に
        if post_body and post_body.get('text'):
            source_inventory.append({
                'ref_id': f'REF_{ref_index:03d}',
                'text': post_body['text'],
                'type': 'post_body',
                'source': 'post_body',
                'confidence': 'high'
            })
            ref_index += 1

        # Transcript を追加
        if transcript:
            source_inventory.append({
                'ref_id': f'REF_{ref_index:03d}',
                'text': transcript,
                'type': 'transcript',
                'source': 'stage_f.path_a',
                'confidence': 'high'
            })
            ref_index += 1

        # Visual log を追加（動画の場合）
        if visual_log:
            source_inventory.append({
                'ref_id': f'REF_{ref_index:03d}',
                'text': visual_log,
                'type': 'visual_log',
                'source': 'stage_f.path_a',
                'confidence': 'high'
            })
            ref_index += 1

        # unified_text 構築
        unified_parts = []
        if post_body and post_body.get('text'):
            unified_parts.append(f"【投稿本文】\n{post_body['text']}")
        if transcript:
            unified_parts.append(f"【書き起こし】\n{transcript}")
        if visual_log:
            unified_parts.append(f"【映像ログ】\n{visual_log}")
        unified_text = '\n\n---\n\n'.join(unified_parts)

        # G1/G2 結果
        g1_result = {
            'tables': [],
            'validation_results': [],
            'table_page_context': {},
            'statistics': {'total_tables': 0, 'valid_tables': 0, 'total_rows': 0}
        }
        g2_result = {
            'segments': source_inventory,
            'unified_text': unified_text,
            'dedup_stats': {'total_input': len(source_inventory), 'total_output': len(source_inventory), 'duplicates_removed': 0},
            'post_body': post_body or {}
        }

        return {
            'unified_text': unified_text,
            'source_inventory': source_inventory,
            'table_inventory': [],
            'cross_validation': {'mode': 'transcription'},
            'ref_count': len(source_inventory),
            'warnings': [],
            'processing_mode': 'transcription',
            'post_body': post_body or {},
            'g1_result': g1_result,
            'g2_result': g2_result
        }

    # ============================================
    # レガシー処理（フォールバック用）
    # ============================================
    def _legacy_rule_based_merge(
        self,
        stage_e_result: Dict[str, Any],
        stage_f_payload: Dict[str, Any],
        post_body: Optional[Dict]
    ) -> Dict[str, Any]:
        """レガシーのルールベース統合（フォールバック）"""
        logger.warning("[Stage G] レガシーモードで処理")

        e_content = stage_e_result.get('content', '')
        f_full_text = stage_f_payload.get('full_text', '')
        f_tables = stage_f_payload.get('tables', [])

        source_inventory = []
        table_inventory = []
        ref_index = 1
        tbl_index = 1

        # post_body
        if post_body and post_body.get('text'):
            source_inventory.append({
                'ref_id': f'REF_{ref_index:03d}',
                'text': post_body['text'],
                'type': 'post_body',
                'source': 'post_body',
                'confidence': 'high'
            })
            ref_index += 1

        # E テキスト
        if e_content:
            paragraphs = self._split_paragraphs(e_content)
            for para in paragraphs:
                if para.strip():
                    source_inventory.append({
                        'ref_id': f'REF_{ref_index:03d}',
                        'text': para.strip(),
                        'type': 'paragraph',
                        'source': 'stage_e',
                        'confidence': 'medium'
                    })
                    ref_index += 1

        # F 表
        for tbl in f_tables:
            table_inventory.append({
                'ref_id': f'TBL_{tbl_index:03d}',
                'table_title': tbl.get('table_title', ''),
                'table_type': tbl.get('table_type', 'visual_table'),
                'headers': tbl.get('headers', tbl.get('columns', [])),
                'rows': tbl.get('rows', []),
                'row_count': len(tbl.get('rows', [])),
                'col_count': len(tbl.get('headers', tbl.get('columns', []))),
                'source': 'stage_f'
            })
            tbl_index += 1

        # unified_text
        unified_parts = [s['text'] for s in source_inventory]
        unified_text = '\n\n'.join(unified_parts)

        # 空の G1/G2 結果
        g1_result = {
            'tables': table_inventory,
            'validation_results': [],
            'table_page_context': {},
            'statistics': {'total_tables': len(table_inventory), 'valid_tables': len(table_inventory), 'total_rows': sum(t.get('row_count', 0) for t in table_inventory)}
        }
        g2_result = {
            'segments': source_inventory,
            'unified_text': unified_text,
            'dedup_stats': {'total_input': len(source_inventory), 'total_output': len(source_inventory), 'duplicates_removed': 0},
            'post_body': post_body or {}
        }

        return {
            'unified_text': unified_text,
            'source_inventory': source_inventory,
            'table_inventory': table_inventory,
            'cross_validation': {'mode': 'legacy_rule_based'},
            'ref_count': len(source_inventory) + len(table_inventory),
            'warnings': ['Used legacy rule-based merge as fallback'],
            'processing_mode': 'legacy_rule_based',
            'post_body': post_body or {},
            'g1_result': g1_result,
            'g2_result': g2_result
        }

    def _legacy_route_anchors(
        self,
        stage_g_result: Dict[str, Any],
        anchors: Optional[List[Dict]]
    ) -> Dict[str, Any]:
        """レガシーのアンカールーティング"""
        logger.info("[Stage G] レガシールーティング使用")

        h1_payload = {'heavy_tables': [], 'table_anchors': []}
        h2_payload = {'text_anchors': [], 'light_tables': [], 'reduced_text': ''}
        anchor_map = {}

        table_inventory = stage_g_result.get('table_inventory', [])
        source_inventory = stage_g_result.get('source_inventory', [])

        # 表をH1/H2に振り分け
        for tbl in table_inventory:
            ref_id = tbl.get('ref_id', '')
            rows = tbl.get('rows', [])
            headers = tbl.get('headers', [])
            is_heavy = len(rows) >= 20 or len(headers) >= 5

            if is_heavy:
                h1_payload['heavy_tables'].append(tbl)
                h1_payload['table_anchors'].append(ref_id)
                anchor_map[ref_id] = 'h1'
            else:
                h2_payload['light_tables'].append(tbl)
                anchor_map[ref_id] = 'h2'

        # テキストはH2
        for src in source_inventory:
            ref_id = src.get('ref_id', '')
            h2_payload['text_anchors'].append(src)
            anchor_map[ref_id] = 'h2'

        h2_payload['reduced_text'] = stage_g_result.get('unified_text', '')

        return {
            'h1_payload': h1_payload,
            'h2_payload': h2_payload,
            'anchor_map': anchor_map
        }

    def _split_paragraphs(self, text: str) -> List[str]:
        """テキストを段落に分割"""
        import re
        paragraphs = re.split(r'\n\s*\n|\r\n\s*\r\n', text)
        return [p.strip() for p in paragraphs if p.strip()]
```

## shared/pipeline/stage_h1_table.py

```python
"""
Stage H1: Table Specialist (表処理専門)

【設計 2026-01-27】Stage HI分割: H1 + H2

役割: Stage G の table_inventory から定型表・構造化表を処理
      スキーマやテンプレートに基づいて表データを構造化

============================================
入力:
  - table_inventory: REF_ID付き表リスト（Stage G出力）
  - doc_type: ドキュメントタイプ
  - workspace: ワークスペース

出力:
  - processed_tables: 処理済み表データ
  - extracted_metadata: 表から抽出したメタデータ
  - table_text_fragments: H2から削除すべきテキスト断片

特徴:
  - 軽量モデル使用（Flash-Lite）または LLMなしのルールベース処理
  - カラムナ形式を辞書リストに復元
  - H2への入力量削減のため、処理済み表のテキストを返す
============================================
"""
import json
from typing import Dict, Any, List, Optional, Set
from loguru import logger

from .utils.table_parser import recompose_columnar_data, is_columnar_format, extract_table_text_for_removal


class StageH1Table:
    """Stage H1: 表処理専門"""

    # 定型表のスキーマ定義（doc_type別）
    TABLE_SCHEMAS = {
        "school_letter": {
            "weekly_schedule": {
                "required_columns": ["曜日", "時間", "科目"],
                "alt_columns": [["日", "時限", "教科"], ["曜", "時間割", "授業"]],
                "table_type": "schedule"
            },
            "event_list": {
                "required_columns": ["日付", "行事"],
                "alt_columns": [["日", "イベント"], ["月日", "予定"]],
                "table_type": "event"
            },
            "持ち物リスト": {
                "required_columns": ["品目", "数量"],
                "alt_columns": [["持ち物", "個数"], ["もちもの", "かず"]],
                "table_type": "item_list"
            }
        },
        "flyer": {
            "price_list": {
                "required_columns": ["商品", "価格"],
                "alt_columns": [["品名", "金額"], ["メニュー", "値段"]],
                "table_type": "price"
            },
            "schedule": {
                "required_columns": ["日時", "内容"],
                "alt_columns": [["時間", "プログラム"]],
                "table_type": "schedule"
            }
        },
        "default": {
            "generic_table": {
                "required_columns": [],
                "table_type": "generic"
            }
        }
    }

    def __init__(self, llm_client=None):
        """
        Args:
            llm_client: LLMクライアント（オプション、複雑な表処理時に使用）
        """
        self.llm = llm_client

    def process(
        self,
        table_inventory: List[Dict[str, Any]],
        doc_type: str = "default",
        workspace: str = "default",
        unified_text: str = ""
    ) -> Dict[str, Any]:
        """
        表データを処理

        Args:
            table_inventory: Stage G の table_inventory
            doc_type: ドキュメントタイプ
            workspace: ワークスペース
            unified_text: Stage G の unified_text（テキスト断片抽出用）

        Returns:
            {
                'processed_tables': List[Dict],  # 処理済み表
                'extracted_metadata': Dict,       # 表から抽出したメタデータ
                'table_text_fragments': List[str], # H2で削除すべきテキスト
                'removed_table_ids': Set[str],    # H2から除外する表のID
                'statistics': Dict                 # 処理統計
            }
        """
        logger.info(f"[Stage H1] 表処理開始: {len(table_inventory)}表 (doc_type={doc_type})")

        if not table_inventory:
            logger.info("[Stage H1] 表なし、スキップ")
            return {
                'processed_tables': [],
                'extracted_metadata': {},
                'table_text_fragments': [],
                'removed_table_ids': set(),
                'statistics': {'total': 0, 'processed': 0, 'skipped': 0}
            }

        processed_tables = []
        extracted_metadata = {}
        table_text_fragments = []
        removed_table_ids = set()
        unrepairable_tables = []  # G1で修復不能と判定された表
        stats = {'total': len(table_inventory), 'processed': 0, 'skipped': 0, 'unrepairable': 0}

        # doc_type に対応するスキーマを取得
        schemas = self.TABLE_SCHEMAS.get(doc_type, self.TABLE_SCHEMAS['default'])

        for table in table_inventory:
            ref_id = table.get('ref_id', 'UNKNOWN')
            table_title = table.get('table_title', '')
            table_type = table.get('table_type', 'unknown')

            # ============================================
            # G1の「白旗」検知: unrepairable フラグ
            # ============================================
            if table.get('status') == 'unrepairable':
                reason = table.get('unrepairable_reason', '理由不明')
                logger.warning(f"[Stage H1] 修復不能表をスキップ: {ref_id} - {reason}")

                # 修復不能表の情報を記録（レポート用）
                unrepairable_tables.append({
                    'ref_id': ref_id,
                    'table_title': table_title,
                    'reason': reason,
                    'page': table.get('page', 0)
                })
                stats['unrepairable'] += 1
                stats['skipped'] += 1

                # H2への通知用：この表は処理できなかったことを伝える
                removed_table_ids.add(ref_id)
                continue  # 次の表へ（ハルシネーション防止）

            logger.debug(f"[Stage H1] 表処理: {ref_id} - {table_title} ({table_type})")

            # カラムナ形式を辞書リストに変換
            rows_data = self._normalize_table_rows(table)

            # スキーママッチング
            matched_schema = self._match_schema(table, schemas)

            if matched_schema:
                # 定型表として処理
                logger.info(f"[Stage H1] 定型表検出: {ref_id} → {matched_schema['table_type']}")

                processed_table = {
                    'ref_id': ref_id,
                    'table_title': table_title,
                    'table_type': matched_schema['table_type'],
                    'schema_matched': True,
                    'columns': table.get('columns', []) or table.get('headers', []),
                    'rows': rows_data,
                    'row_count': len(rows_data),
                    'source': table.get('source', 'stage_g')
                }
                processed_tables.append(processed_table)

                # メタデータ抽出（特定のtable_typeに対して）
                meta = self._extract_metadata_from_table(processed_table, matched_schema['table_type'])
                if meta:
                    extracted_metadata.update(meta)

                # H2から削除すべきテキスト断片を収集
                fragments = extract_table_text_for_removal(table)
                table_text_fragments.extend(fragments)

                # この表はH1で処理済みとしてマーク
                removed_table_ids.add(ref_id)
                stats['processed'] += 1

            else:
                # 定型外の表は軽量処理のみ
                logger.debug(f"[Stage H1] 汎用表: {ref_id}")

                processed_table = {
                    'ref_id': ref_id,
                    'table_title': table_title,
                    'table_type': table_type or 'generic',
                    'schema_matched': False,
                    'columns': table.get('columns', []) or table.get('headers', []),
                    'rows': rows_data,
                    'row_count': len(rows_data),
                    'source': table.get('source', 'stage_g')
                }
                processed_tables.append(processed_table)

                # 汎用表も大きければH2から削除対象に
                if len(rows_data) >= 3:
                    fragments = extract_table_text_for_removal(table)
                    table_text_fragments.extend(fragments)
                    removed_table_ids.add(ref_id)
                    stats['processed'] += 1
                else:
                    stats['skipped'] += 1

        logger.info(f"[Stage H1] 完了: processed={stats['processed']}, skipped={stats['skipped']}, unrepairable={stats['unrepairable']}")

        # 修復不能表があれば警告
        if unrepairable_tables:
            logger.warning(f"[Stage H1] 修復不能表: {len(unrepairable_tables)}件 → H2へ通知済み")

        return {
            'processed_tables': processed_tables,
            'extracted_metadata': extracted_metadata,
            'table_text_fragments': table_text_fragments,
            'removed_table_ids': removed_table_ids,
            'unrepairable_tables': unrepairable_tables,  # G1で修復不能と判定された表の一覧
            'statistics': stats
        }

    def _normalize_table_rows(self, table: Dict) -> List[Dict]:
        """
        表の行データを正規化（カラムナ形式→辞書リスト）
        【データ救済版】ヘッダーがなくても全データを保持

        Args:
            table: 表データ

        Returns:
            辞書リスト形式の行データ
        """
        # columns + rows 形式（カラムナ）
        if 'columns' in table and 'rows' in table:
            if is_columnar_format(table):
                return recompose_columnar_data(table)

        # headers + rows 形式
        if 'headers' in table and 'rows' in table:
            headers = table.get('headers', []) or []
            rows = table.get('rows', []) or []
            result = []

            for row_idx, row in enumerate(rows):
                if isinstance(row, list):
                    # 【データ救済】ヘッダーが不足している場合、仮ヘッダーを生成
                    effective_headers = list(headers)  # コピー
                    while len(effective_headers) < len(row):
                        effective_headers.append(f'column_{len(effective_headers) + 1}')

                    # 全データを辞書に格納（ヘッダーがなくても消さない）
                    row_dict = {}
                    for i, value in enumerate(row):
                        if i < len(effective_headers):
                            key = effective_headers[i] or f'column_{i + 1}'
                        else:
                            key = f'column_{i + 1}'
                        # 値が存在すれば必ず保持
                        if value is not None and str(value).strip():
                            row_dict[key] = value

                    # 空でない行のみ追加（ただし1つでも値があれば追加）
                    if row_dict:
                        result.append(row_dict)
                    elif any(v for v in row if v):  # row自体に値があるが辞書化で消えた場合
                        # フォールバック：インデックスベースで保持
                        fallback_dict = {f'value_{i}': v for i, v in enumerate(row) if v}
                        if fallback_dict:
                            result.append(fallback_dict)

                elif isinstance(row, dict):
                    if row:  # 空でない辞書のみ
                        result.append(row)

            return result

        # rows のみ（ヘッダーなし）→ 仮ヘッダーで辞書化
        if 'rows' in table:
            rows = table['rows']
            if not isinstance(rows, list):
                return []

            result = []
            for row in rows:
                if isinstance(row, list):
                    # 仮ヘッダーで辞書化
                    row_dict = {f'column_{i + 1}': v for i, v in enumerate(row) if v is not None and str(v).strip()}
                    if row_dict:
                        result.append(row_dict)
                elif isinstance(row, dict) and row:
                    result.append(row)
            return result

        return []

    def _match_schema(self, table: Dict, schemas: Dict) -> Optional[Dict]:
        """
        表がスキーマにマッチするかチェック

        Args:
            table: 表データ
            schemas: スキーマ定義

        Returns:
            マッチしたスキーマ、またはNone
        """
        columns = table.get('columns', []) or table.get('headers', [])
        if not columns:
            return None

        columns_lower = [str(c).lower() for c in columns]

        for schema_name, schema in schemas.items():
            required = schema.get('required_columns', [])
            alt_sets = schema.get('alt_columns', [])

            # 必須カラムのチェック
            if required:
                required_lower = [c.lower() for c in required]
                if all(any(req in col for col in columns_lower) for req in required_lower):
                    return schema

            # 代替カラムセットのチェック
            for alt_set in alt_sets:
                alt_lower = [c.lower() for c in alt_set]
                if all(any(alt in col for col in columns_lower) for alt in alt_lower):
                    return schema

        return None

    def _extract_metadata_from_table(self, table: Dict, table_type: str) -> Dict[str, Any]:
        """
        表からメタデータを抽出

        Args:
            table: 処理済み表データ
            table_type: 表タイプ

        Returns:
            抽出したメタデータ
        """
        metadata = {}
        rows = table.get('rows', [])

        if table_type == 'schedule':
            # 時間割/スケジュール表
            schedule_items = []
            for row in rows:
                if isinstance(row, dict):
                    schedule_items.append(row)
            if schedule_items:
                metadata['weekly_schedule'] = schedule_items

        elif table_type == 'event':
            # イベントリスト
            events = []
            for row in rows:
                if isinstance(row, dict):
                    events.append(row)
            if events:
                metadata['event_list'] = events

        elif table_type == 'item_list':
            # 持ち物リスト
            items = []
            for row in rows:
                if isinstance(row, dict):
                    items.append(row)
            if items:
                metadata['required_items'] = items

        elif table_type == 'price':
            # 価格表
            prices = []
            for row in rows:
                if isinstance(row, dict):
                    prices.append(row)
            if prices:
                metadata['price_list'] = prices

        return metadata

    def remove_table_text_from_unified(
        self,
        unified_text: str,
        table_text_fragments: List[str]
    ) -> str:
        """
        unified_text から表関連テキストを削除

        H2への入力量を削減するため、H1で処理済みの表の
        テキスト表現を削除

        Args:
            unified_text: Stage G の unified_text
            table_text_fragments: 削除すべきテキスト断片

        Returns:
            軽量化された unified_text
        """
        if not table_text_fragments:
            return unified_text

        result = unified_text

        # 断片を長い順にソート（部分一致を避けるため）
        sorted_fragments = sorted(table_text_fragments, key=len, reverse=True)

        for fragment in sorted_fragments:
            if len(fragment) < 10:
                continue  # 短すぎる断片はスキップ

            # 断片を削除（複数回出現する可能性あり）
            if fragment in result:
                result = result.replace(fragment, '')

        # 連続する空行を整理
        import re
        result = re.sub(r'\n{3,}', '\n\n', result)

        original_len = len(unified_text)
        reduced_len = len(result)
        reduction = original_len - reduced_len

        logger.info(f"[Stage H1] テキスト軽量化: {original_len}→{reduced_len}文字 (-{reduction}文字, -{reduction*100//original_len if original_len > 0 else 0}%)")

        return result.strip()
```

## shared/pipeline/stage_h2_text.py

```python
"""
Stage H2: Text Specialist (テキスト処理専門)

【設計 2026-01-27】Stage HI分割: H1 + H2

役割: H1で軽量化されたテキストを処理し、構造化 + 統合・要約を実行
      従来のStage HI統合版と同等の機能（ただし入力量が削減済み）

============================================
入力:
  - reduced_text: H1で表テキストを削除した軽量テキスト
  - h1_result: Stage H1の処理結果（processed_tables等）
  - source_inventory: REF_ID付きセグメントリスト

出力:
  - document_date: 基準日付
  - tags: 検索用タグ
  - metadata: 構造化データ（H1の結果も含む）
  - title: ドキュメントタイトル
  - summary: 要約
  - calendar_events: カレンダーイベント
  - tasks: タスクリスト
  - audit_canonical_text: 監査用正本テキスト

特徴:
  - 従来のStage HIと同じ処理（互換性維持）
  - 入力テキスト量が削減されているため、トークン消費が減少
  - H1で抽出した表メタデータをマージ
============================================
"""
import re
import json
import json_repair
from typing import Dict, Any, Optional, List
from pathlib import Path
from string import Template
from loguru import logger
from datetime import datetime

from shared.ai.llm_client.llm_client import LLMClient
from .constants import STAGE_H_INPUT_SCHEMA_VERSION


class StageH2Text:
    """Stage H2: テキスト処理専門（従来のStage HI互換）"""

    def __init__(self, llm_client: LLMClient):
        """
        Args:
            llm_client: LLMクライアント
        """
        self.llm = llm_client

    def process(
        self,
        file_name: str,
        doc_type: str,
        workspace: str,
        reduced_text: str,
        prompt: str,
        model: str,
        h1_result: Optional[Dict[str, Any]] = None,
        stage_f_structure: Optional[Dict[str, Any]] = None,
        stage_g_result: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        テキスト処理 + 構造化 + 要約

        Args:
            file_name: ファイル名
            doc_type: ドキュメントタイプ
            workspace: ワークスペース
            reduced_text: H1で軽量化されたテキスト
            prompt: プロンプト
            model: モデル名
            h1_result: Stage H1の結果
            stage_f_structure: Stage F の構造化情報
            stage_g_result: Stage G の結果

        Returns:
            Stage HI 互換の出力形式
        """
        logger.info(f"[Stage H2] テキスト処理開始... (doc_type={doc_type}, model={model})")

        # 入力サイズをログ
        original_size = len(stage_g_result.get('unified_text', '')) if stage_g_result else 0
        reduced_size = len(reduced_text)
        reduction_pct = (original_size - reduced_size) * 100 // original_size if original_size > 0 else 0
        logger.info(f"[Stage H2] 入力テキスト: {reduced_size}文字 (元: {original_size}文字, -{reduction_pct}%削減)")

        # Stage G の結果から source_inventory を取得
        source_inventory = []
        table_inventory = []
        if stage_g_result:
            source_inventory = stage_g_result.get('source_inventory', [])
            # H1で処理済みの表を除外した table_inventory
            removed_ids = h1_result.get('removed_table_ids', set()) if h1_result else set()
            table_inventory = [
                t for t in stage_g_result.get('table_inventory', [])
                if t.get('ref_id') not in removed_ids
            ]
            logger.info(f"[Stage H2] source_inventory={len(source_inventory)}, 残存table_inventory={len(table_inventory)}")

        if not reduced_text or not reduced_text.strip():
            logger.warning("[Stage H2] 入力テキストが空です")
            return self._get_fallback_result(doc_type, h1_result)

        try:
            # プロンプト構築
            logger.info("[Stage H2] プロンプト構築中...")

            # H1の結果から情報を取得
            h1_tables = h1_result.get('processed_tables', []) if h1_result else []
            unrepairable_tables = h1_result.get('unrepairable_tables', []) if h1_result else []

            if unrepairable_tables:
                logger.info(f"[Stage H2] 修復不能表: {len(unrepairable_tables)}件 → プロンプトに通知")

            full_prompt = self._build_prompt(
                prompt_template=prompt,
                file_name=file_name,
                doc_type=doc_type,
                workspace=workspace,
                combined_text=reduced_text,
                source_inventory=source_inventory,
                table_inventory=table_inventory,
                h1_tables=h1_tables,
                unrepairable_tables=unrepairable_tables
            )
            logger.info(f"[Stage H2] プロンプト構築完了 ({len(full_prompt)}文字)")

            # LLM呼び出し
            logger.info(f"[Stage H2] LLM呼び出し中... (model={model})")
            response = self.llm.call_model(
                tier="default",
                prompt=full_prompt,
                model_name=model
            )
            logger.info(f"[Stage H2] LLM応答受信: success={response.get('success')}")

            if not response.get("success"):
                logger.error(f"[Stage H2 エラー] LLM呼び出し失敗: {response.get('error')}")
                return self._get_fallback_result(doc_type, h1_result)

            # JSON抽出
            content = response.get("content", "")
            logger.info(f"[Stage H2] ===== LLMレスポンス（最初の1000文字）=====\n{content[:1000]}")
            # リトライ禁止（2026-01-28）: エラー時は即座にフォールバック
            result = self._extract_json_with_retry(content, model=model, max_retries=0)

            # Stage F の構造化情報をマージ
            if stage_f_structure:
                result = self._merge_stage_f_structure(result, stage_f_structure)

            # H1 の結果をマージ
            if h1_result:
                result = self._merge_h1_result(result, h1_result)

            # audit_canonical_text の生成
            audit_canonical_text = self._generate_audit_canonical_text(
                result, reduced_text, source_inventory
            )

            # 結果の整形
            final_result = {
                'document_date': result.get('document_date'),
                'tags': result.get('tags', []),
                'metadata': result.get('metadata', {}),
                'title': result.get('title', ''),
                'summary': result.get('summary', ''),
                'calendar_events': result.get('calendar_events', []),
                'tasks': result.get('tasks', []),
                'audit_canonical_text': audit_canonical_text
            }

            logger.info(f"[Stage H2完了] title={final_result['title'][:50] if final_result['title'] else 'N/A'}...")
            return final_result

        except Exception as e:
            logger.error(f"[Stage H2 エラー] 処理失敗: {e}", exc_info=True)
            return self._get_fallback_result(doc_type, h1_result)

    def _build_prompt(
        self,
        prompt_template: str,
        file_name: str,
        doc_type: str,
        workspace: str,
        combined_text: str,
        source_inventory: List[Dict],
        table_inventory: List[Dict],
        h1_tables: List[Dict],
        unrepairable_tables: List[Dict] = None
    ) -> str:
        """プロンプトを構築（アンカー活用強化版）"""
        # source_inventory を簡略化
        inventory_summary = []
        for item in source_inventory[:30]:
            inventory_summary.append({
                'ref_id': item.get('ref_id'),
                'type': item.get('type'),
                'text': item.get('text', '')[:200]
            })

        # H1で処理済み表の概要（詳細データはメタデータとして既に含まれている）
        h1_tables_summary = []
        for tbl in h1_tables[:10]:
            h1_tables_summary.append({
                'ref_id': tbl.get('ref_id'),
                'table_title': tbl.get('table_title'),
                'table_type': tbl.get('table_type'),
                'row_count': tbl.get('row_count', 0)
            })

        # テンプレート変数を置換
        template = Template(prompt_template)
        prompt = template.substitute(
            file_name=file_name,
            doc_type=doc_type,
            workspace=workspace,
            combined_text=combined_text,
            current_date=datetime.now().strftime("%Y-%m-%d"),
            source_inventory_json=json.dumps(inventory_summary, ensure_ascii=False, indent=2),
            table_inventory_json=json.dumps(h1_tables_summary, ensure_ascii=False, indent=2),
            source_count=len(source_inventory),
            table_count=len(h1_tables)
        )

        # ============================================
        # アンカー活用強化: H1で処理済みの表を参照させる
        # ============================================
        anchor_instructions = self._build_anchor_instructions(h1_tables, unrepairable_tables or [])
        if anchor_instructions:
            prompt = prompt + "\n\n" + anchor_instructions

        return prompt

    def _build_anchor_instructions(
        self,
        h1_tables: List[Dict],
        unrepairable_tables: List[Dict]
    ) -> str:
        """
        アンカー活用のための追加指示を構築

        G2が埋め込んだアンカー（[→ TBL_xxx 参照]）を
        AIが最大限に活用するための指示
        """
        if not h1_tables and not unrepairable_tables:
            return ""

        instructions = []
        instructions.append("=" * 50)
        instructions.append("【重要: 表データの活用指示】")
        instructions.append("=" * 50)

        instructions.append("""
テキスト中に `[→ TBL_xxx 参照]` というアンカー（しおり）が出現します。
これは「ここに表データがある」という座標です。以下のルールで活用してください。

【ルール1: アンカーは聖域】
アンカーは情報の座標です。要約や構造化において、
このアンカーの存在を認識し、表の内容を考慮して文脈を補完してください。

【ルール2: 表の内容を参照】
以下に、各アンカーに対応する表の概要を示します。
要約やカレンダーイベント、タスク抽出の際は、これらの表データを活用してください。
""")

        # H1で処理済みの表
        if h1_tables:
            instructions.append("\n■ 処理済み表データ（H1で構造化済み）:")
            for tbl in h1_tables[:10]:
                ref_id = tbl.get('ref_id', '')
                title = tbl.get('table_title', '(無題)')
                ttype = tbl.get('table_type', 'generic')
                rows = tbl.get('rows', [])
                row_count = tbl.get('row_count', len(rows))

                instructions.append(f"  - {ref_id}: {title} ({ttype}, {row_count}行)")

                # 表の内容サンプル（最初の3行）
                if rows and len(rows) > 0:
                    sample_rows = rows[:3]
                    for i, row in enumerate(sample_rows):
                        if isinstance(row, dict):
                            row_text = ', '.join(f"{k}:{v}" for k, v in list(row.items())[:4])
                            instructions.append(f"      行{i+1}: {row_text[:80]}")

        # 修復不能だった表
        if unrepairable_tables:
            instructions.append("\n■ 修復不能だった表（参照のみ）:")
            for tbl in unrepairable_tables:
                ref_id = tbl.get('ref_id', '')
                title = tbl.get('table_title', '(無題)')
                reason = tbl.get('reason', '')
                instructions.append(f"  - {ref_id}: {title} (※構造化できず: {reason})")
            instructions.append("  ※これらの表は構造化に失敗しましたが、テキスト中の情報として参照可能です。")

        instructions.append("""
【ルール3: カレンダー・タスクへの反映】
表に日付や予定が含まれている場合は、calendar_events や tasks として抽出してください。
表のデータは信頼できる情報源です。積極的に活用してください。

【ルール4: 要約への反映】
表の存在と概要を要約に含めてください。
例: 「週間予定表によると、月曜は国語、火曜は算数...」
""")

        return "\n".join(instructions)

    def _extract_json_with_retry(
        self,
        content: str,
        model: str,
        max_retries: int = 2
    ) -> Dict[str, Any]:
        """JSON抽出（リトライ機能付き）"""
        for attempt in range(max_retries + 1):
            try:
                result = self._extract_json(content)
                logger.debug(f"[Stage H2] JSON抽出成功 (試行{attempt + 1}/{max_retries + 1})")
                return result

            except Exception as e:
                if attempt < max_retries:
                    logger.warning(f"[Stage H2] JSON抽出失敗 (試行{attempt + 1}/{max_retries + 1}): {e}")
                    content = self._retry_json_extraction(content, str(e), model)
                else:
                    logger.error(f"[Stage H2] JSON抽出失敗（最終試行）: {e}")
                    raise

        return {}

    def _extract_json(self, content: str) -> Dict[str, Any]:
        """コンテンツからJSONを抽出"""
        patterns = [
            r'```json\s*(.*?)```',
            r'```\s*(.*?)```',
            r'\{[\s\S]*?\}',
        ]

        json_str = None
        for pattern in patterns:
            match = re.search(pattern, content, re.DOTALL)
            if match:
                json_str = match.group(1) if match.lastindex else match.group(0)
                json_str = json_str.strip()
                if json_str.startswith('{'):
                    break
                else:
                    json_str = None

        if not json_str:
            match = re.search(r'\{[\s\S]*\}', content, re.DOTALL)
            if match:
                json_str = match.group(0).strip()
            else:
                json_str = content.strip()

        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.error(f"[Stage H2] JSON解析失敗: {e}")
            try:
                return json_repair.loads(json_str)
            except Exception as repair_error:
                logger.error(f"[Stage H2] JSON修復も失敗: {repair_error}")
                raise

    def _retry_json_extraction(
        self,
        failed_content: str,
        error_message: str,
        model: str
    ) -> str:
        """JSON抽出失敗時、LLMにJSON修正を依頼"""
        prompt = f"""以下のJSONにエラーがあります。修正してください。

エラー: {error_message}

元のJSON:
```
{failed_content[:3000]}
```

修正されたJSONを ```json ブロックで出力してください。
"""

        try:
            response = self.llm.call_model(
                tier="default",
                prompt=prompt,
                model_name=model
            )

            if response.get("success"):
                return response.get("content", "")
            else:
                return failed_content

        except Exception as e:
            logger.error(f"[Stage H2] JSON修正エラー: {e}")
            return failed_content

    def _merge_stage_f_structure(
        self,
        result: Dict[str, Any],
        stage_f_structure: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Stage F の構造化情報をマージ"""
        metadata = result.get('metadata', {})

        schema_ver = stage_f_structure.get('schema_version', '')
        is_v1_1 = (schema_ver == STAGE_H_INPUT_SCHEMA_VERSION)

        if is_v1_1:
            stage_f_text_blocks = stage_f_structure.get('text_blocks', [])
            if stage_f_text_blocks:
                metadata['_raw_text_blocks'] = stage_f_text_blocks

        visual_elements = stage_f_structure.get('visual_elements', {})
        if visual_elements:
            deadline_info = visual_elements.get('deadline_info')
            if deadline_info and not result.get('document_date'):
                result['document_date'] = deadline_info

        result['metadata'] = metadata
        return result

    def _merge_h1_result(
        self,
        result: Dict[str, Any],
        h1_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """H1の結果をマージ"""
        metadata = result.get('metadata', {})

        # H1で抽出したメタデータをマージ
        h1_metadata = h1_result.get('extracted_metadata', {})
        if h1_metadata:
            for key, value in h1_metadata.items():
                if key not in metadata:
                    metadata[key] = value
                    logger.debug(f"[Stage H2] H1メタデータをマージ: {key}")

        # H1で処理した表を extracted_tables に追加
        processed_tables = h1_result.get('processed_tables', [])
        if processed_tables:
            # 既存の extracted_tables と統合
            existing_tables = metadata.get('extracted_tables', [])
            metadata['extracted_tables'] = existing_tables + processed_tables
            logger.info(f"[Stage H2] H1処理済み表をマージ: {len(processed_tables)}表")

        # 処理統計
        stats = h1_result.get('statistics', {})
        if stats:
            metadata['_h1_statistics'] = stats

        result['metadata'] = metadata
        return result

    def _generate_audit_canonical_text(
        self,
        result: Dict[str, Any],
        combined_text: str,
        source_inventory: List[Dict]
    ) -> str:
        """監査用正本テキストを生成"""
        parts = []

        title = result.get('title', '')
        if title:
            parts.append(f"# {title}\n")

        metadata = result.get('metadata', {})
        basic_info = metadata.get('basic_info', {})
        if basic_info:
            parts.append("## 基本情報")
            for key, value in basic_info.items():
                if value:
                    parts.append(f"- {key}: {value}")
            parts.append("")

        summary = result.get('summary', '')
        if summary:
            parts.append("## 要約")
            parts.append(summary)
            parts.append("")

        articles = metadata.get('articles', [])
        if articles:
            parts.append("## 記事・お知らせ")
            for article in articles:
                title_a = article.get('title', '')
                body = article.get('body', '')
                if title_a:
                    parts.append(f"### {title_a}")
                if body:
                    parts.append(body)
                parts.append("")

        calendar_events = result.get('calendar_events', [])
        if calendar_events:
            parts.append("## カレンダーイベント")
            for event in calendar_events:
                date = event.get('event_date', '')
                name = event.get('event_name', '')
                parts.append(f"- {date}: {name}")
            parts.append("")

        tasks = result.get('tasks', [])
        if tasks:
            parts.append("## タスク")
            for task in tasks:
                name = task.get('task_name', '')
                deadline = task.get('deadline', '')
                parts.append(f"- {name} (期限: {deadline})")
            parts.append("")

        # 表データ（H1で処理したもの）
        extracted_tables = metadata.get('extracted_tables', [])
        if extracted_tables:
            parts.append("## 抽出された表")
            for tbl in extracted_tables[:5]:
                ref_id = tbl.get('ref_id', '')
                title = tbl.get('table_title', '表')
                row_count = tbl.get('row_count', 0)
                parts.append(f"- {ref_id}: {title} ({row_count}行)")
            if len(extracted_tables) > 5:
                parts.append(f"  ... 他 {len(extracted_tables) - 5} 表")

        if source_inventory:
            parts.append("\n## 参照元（REF_ID）")
            for item in source_inventory[:10]:
                ref_id = item.get('ref_id', '')
                text = item.get('text', '')[:100]
                parts.append(f"- {ref_id}: {text}...")
            if len(source_inventory) > 10:
                parts.append(f"  ... 他 {len(source_inventory) - 10} 件")

        return '\n'.join(parts)

    def _get_fallback_result(
        self,
        doc_type: str,
        h1_result: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """フォールバック結果を返す"""
        metadata = {
            'doc_type': doc_type,
            'extraction_failed': True
        }

        # H1の結果があればマージ
        if h1_result:
            h1_metadata = h1_result.get('extracted_metadata', {})
            metadata.update(h1_metadata)

            processed_tables = h1_result.get('processed_tables', [])
            if processed_tables:
                metadata['extracted_tables'] = processed_tables

        return {
            'document_date': None,
            'tags': [],
            'metadata': metadata,
            'title': '',
            'summary': '',
            'calendar_events': [],
            'tasks': [],
            'audit_canonical_text': ''
        }
```

## shared/pipeline/stage_h_kakeibo.py

```python
"""
Stage H: Kakeibo Structuring (家計簿構造化)

家計簿レシート専用のStage H処理
- 税額按分計算
- 商品分類
- マスタデータとの紐付け
"""

from typing import Dict, Any, List
from loguru import logger
from datetime import date

from shared.common.database.client import DatabaseClient


class StageHKakeibo:
    """家計簿専用のStage H（税額按分・分類）"""

    def __init__(self, db_client: DatabaseClient):
        """
        Args:
            db_client: データベースクライアント
        """
        self.db = db_client

        # マスタデータをロード
        self.aliases = self._load_aliases()
        self.product_dict = self._load_product_dictionary()
        self.situations = self._load_situations()
        self.categories = self._load_categories()

    def process(
        self,
        stage_g_output: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Stage Gの出力から最終データを生成

        Args:
            stage_g_output: Stage Gの構造化データ

        Returns:
            Dict: 最終データ（DB保存可能な形式）
        """
        try:
            # 【重要】レシート記載の金額は改ざんしない
            # 割引行は別行としてそのまま保持（マイナス金額）
            items = stage_g_output.get("items", [])

            # 割引を商品にリンク（税込価計算用）
            items = self._link_discounts_to_items(items)

            # 1. 商品を正規化（マスタとの紐付け）
            normalized_items = []
            for item in items:
                # 値引き行も含めて処理（金額はそのまま）
                if item.get("line_type") == "DISCOUNT":
                    normalized_items.append({
                        "raw_item": item,
                        "normalized": {
                            "product_name": item.get("product_name", "値引"),
                            "category_id": None,
                            "tax_rate": self._get_discount_tax_rate(item, items),
                            "tax_rate_source": "discount",
                            "is_discount": True
                        }
                    })
                    continue

                normalized = self._normalize_item(
                    item,
                    stage_g_output["shop_info"]["name"],
                    stage_g_output.get("amounts", {})
                )
                normalized_items.append({
                    "raw_item": item,
                    "normalized": normalized
                })

            # 2. シチュエーション判定
            trans_date = date.fromisoformat(stage_g_output["transaction_info"]["date"])
            situation_id = self._determine_situation(trans_date)

            # 3. 税額を按分計算
            items_with_tax, tax_subtotals = self._calculate_and_distribute_tax(
                normalized_items,
                stage_g_output.get("amounts", {})
            )

            # 4. 最終データを構築
            result = {
                "receipt": {
                    **stage_g_output["shop_info"],
                    **stage_g_output["transaction_info"],
                    **stage_g_output.get("amounts", {}),
                    **tax_subtotals,  # 税対象額を追加
                    "situation_id": situation_id
                },
                "items": items_with_tax,
                "payment": stage_g_output.get("payment", {}),
                "other_info": stage_g_output.get("other_info", {})
            }

            logger.info(f"Stage H completed: {len(items_with_tax)} items processed")
            return result

        except Exception as e:
            logger.error(f"Stage H failed: {e}")
            raise

    def _get_discount_tax_rate(self, discount_item: Dict, all_items: List[Dict]) -> int:
        """
        割引行の税率を判定（適用先商品から推定）

        Args:
            discount_item: 割引行データ
            all_items: 全明細行リスト

        Returns:
            int: 税率（8 or 10）
        """
        # 行番号でインデックスを作成
        items_by_line = {item.get("line_number"): item for item in all_items}

        # 明示的に値引き適用先が指定されている場合
        applied_to_line = discount_item.get("discount_applied_to")
        if applied_to_line and applied_to_line in items_by_line:
            target = items_by_line[applied_to_line]
            tax_mark = target.get("tax_mark")
            if tax_mark and (
                tax_mark in ["*", "※", "◆"] or
                "8%" in str(tax_mark) or
                "8" in str(tax_mark)
            ):
                return 8
            return 10

        # 直前の商品から推定
        discount_line_num = discount_item.get("line_number")
        if discount_line_num:
            for i in range(discount_line_num - 1, 0, -1):
                if i in items_by_line and items_by_line[i].get("line_type") != "DISCOUNT":
                    target = items_by_line[i]
                    tax_mark = target.get("tax_mark")
                    if tax_mark and (
                        tax_mark in ["*", "※", "◆"] or
                        "8%" in str(tax_mark) or
                        "8" in str(tax_mark)
                    ):
                        return 8
                    return 10

        # デフォルト10%
        return 10

    def _link_discounts_to_items(self, items: List[Dict]) -> List[Dict]:
        """
        割引行を商品にリンク（税込価計算用）

        各商品に linked_discount フィールドを追加
        割引の適用先が明示されていない場合は直前の商品に適用

        Args:
            items: Stage Gで抽出された全明細行

        Returns:
            List[Dict]: リンク情報が追加された明細行リスト
        """
        # 行番号でインデックスを作成
        items_by_line = {item.get("line_number"): item for item in items}

        # 各商品のlinked_discountを初期化
        for item in items:
            if item.get("line_type") != "DISCOUNT":
                item["linked_discount"] = 0

        # 割引を適用先にリンク
        for item in items:
            if item.get("line_type") != "DISCOUNT":
                continue

            discount_amount = item.get("amount", 0)  # 負の値
            applied_to_line = item.get("discount_applied_to")

            target = None
            if applied_to_line and applied_to_line in items_by_line:
                # 明示的に適用先が指定されている場合
                target = items_by_line[applied_to_line]
            else:
                # 直前の商品を探す
                discount_line_num = item.get("line_number")
                if discount_line_num:
                    for i in range(discount_line_num - 1, 0, -1):
                        if i in items_by_line and items_by_line[i].get("line_type") != "DISCOUNT":
                            target = items_by_line[i]
                            break

            if target and target.get("line_type") != "DISCOUNT":
                target["linked_discount"] = target.get("linked_discount", 0) + discount_amount
                logger.info(f"Linked discount {discount_amount}円 to {target.get('product_name')}")

        return items

    def _normalize_item(self, item: Dict, shop_name: str, amounts: Dict = None) -> Dict:
        """
        商品名を正規化し、カテゴリ・税率を判定

        Args:
            item: 商品データ（Stage Gの出力）
            shop_name: 店舗名
            amounts: レシート全体の金額情報（税率判定に使用）

        Returns:
            Dict: {"product_name": "正規化後", "category_id": "...", "tax_rate": 10}
        """
        # 商品名を取得（空文字列もNoneとして扱う）
        product_name = item.get("product_name") or item.get("line_text") or item.get("ocr_raw_text") or "不明"
        # 空文字列の場合は「不明」に
        if not product_name or not product_name.strip():
            product_name = "不明"

        receipt_tax_mark = item.get("tax_mark")  # レシートの税率マーク

        # レシート全体の税率情報を確認（最優先）
        receipt_level_tax_rate = None
        if amounts:
            tax_8_amount = amounts.get("tax_8_amount") or 0
            tax_10_amount = amounts.get("tax_10_amount") or 0

            # 8%のみの場合
            if tax_8_amount > 0 and tax_10_amount == 0:
                receipt_level_tax_rate = 8
                logger.debug(f"Receipt has only 8% tax, setting all items to 8%")
            # 10%のみの場合
            elif tax_10_amount > 0 and tax_8_amount == 0:
                receipt_level_tax_rate = 10
                logger.debug(f"Receipt has only 10% tax, setting all items to 10%")
            # 混在の場合は個別判定に進む

        # レシート全体の税率が判定できた場合はそれを使用（最優先）
        if receipt_level_tax_rate is not None:
            return {
                "product_name": product_name,
                "category_id": None,
                "tax_rate": receipt_level_tax_rate,
                "tax_rate_source": "receipt_level",
                "tax_amount": None
            }

        # 1. エイリアス変換
        product_name = self.aliases.get(product_name.lower(), product_name)

        # 2. 商品辞書マッチング
        for entry in self.product_dict:
            if entry["raw_keyword"].lower() in product_name.lower():
                return {
                    "product_name": entry["official_name"],
                    "category_id": entry["category_id"],
                    "tax_rate": entry["tax_rate"],
                    "tax_rate_source": "master",  # マスタから取得
                    "tax_amount": None  # 後で計算
                }

        # 3. 商品名から税率パターンを検出（「外8」「内8」などのレシート記載パターン）
        if "外8" in product_name or "内8" in product_name or "外 8" in product_name or "内 8" in product_name:
            tax_rate = 8
            tax_rate_source = "product_name_pattern"
            # 商品名から税率パターンを削除
            product_name = product_name.replace("外8", "").replace("内8", "").replace("外 8", "").replace("内 8", "").strip()
            # 空文字列になった場合は「不明」に
            if not product_name:
                product_name = "不明"
        elif "外10" in product_name or "内10" in product_name or "外 10" in product_name or "内 10" in product_name:
            tax_rate = 10
            tax_rate_source = "product_name_pattern"
            # 商品名から税率パターンを削除
            product_name = product_name.replace("外10", "").replace("内10", "").replace("外 10", "").replace("内 10", "").strip()
            # 空文字列になった場合は「不明」に
            if not product_name:
                product_name = "不明"
        # 4. レシートのマークから税率を判定
        # 8%マークの判定（複数パターン対応）
        elif receipt_tax_mark and (
            receipt_tax_mark in ["*", "※", "◆"] or  # よくある軽減税率マーク
            "8%" in str(receipt_tax_mark) or
            "8" in str(receipt_tax_mark) or
            "(軽)" in str(receipt_tax_mark) or
            "外8" in str(receipt_tax_mark) or  # 外税8%のパターン
            "内8" in str(receipt_tax_mark)  # 内税8%のパターン
        ):
            tax_rate = 8
            tax_rate_source = "receipt_mark"
        # 10%マークの判定
        elif receipt_tax_mark and (
            receipt_tax_mark in ["★", "☆"] or  # よくある標準税率マーク
            "10%" in str(receipt_tax_mark) or
            "10" in str(receipt_tax_mark) or
            "外10" in str(receipt_tax_mark) or  # 外税10%のパターン
            "内10" in str(receipt_tax_mark)  # 内税10%のパターン
        ):
            tax_rate = 10
            tax_rate_source = "receipt_mark"
        else:
            # デフォルト10%（あとで要レビュー）
            tax_rate = 10
            tax_rate_source = "default"

        return {
            "product_name": product_name,
            "category_id": None,
            "tax_rate": tax_rate,
            "tax_rate_source": tax_rate_source,
            "tax_amount": None
        }

    def _determine_situation(self, trans_date: date) -> str:
        """
        取引日からシチュエーションを判定

        Args:
            trans_date: 取引日

        Returns:
            str: シチュエーションID
        """
        weekday = trans_date.weekday()  # 0=月曜, 6=日曜

        # 土日
        if weekday >= 5:
            for s in self.situations:
                if s["name"] == "週末":
                    return s["id"]

        # 平日
        for s in self.situations:
            if s["name"] == "平日":
                return s["id"]

        # デフォルト（最初のシチュエーション）
        return self.situations[0]["id"] if self.situations else None

    def _calculate_and_distribute_tax(
        self,
        normalized_items: List[Dict],
        amounts: Dict
    ) -> List[Dict]:
        """
        税額を按分計算（内税・外税対応）

        Args:
            normalized_items: 正規化済み商品リスト
            amounts: Stage Gで抽出した金額情報

        Returns:
            List[Dict]: 税額が計算された商品リスト
        """
        # 【重要】内税・外税の判定
        # Stage Gで判定済みの場合はそれを優先
        tax_type = amounts.get("tax_display_type")

        if tax_type:
            logger.info(f"Stage Gで判定済み: tax_display_type={tax_type}")
        else:
            # フォールバック: 小計と合計の比較で判定
            subtotal = amounts.get("subtotal")
            total = amounts.get("total")

            if subtotal is not None and total is not None and subtotal < total:
                tax_type = "excluded"  # 外税
                logger.info(f"外税レシート検出: 小計={subtotal}円 < 合計={total}円")
            else:
                tax_type = "included"  # 内税
                logger.info(f"内税レシート検出: 小計={subtotal}円 = 合計={total}円")

        # 商品を8%と10%にグループ化
        items_8 = []
        items_10 = []

        for item_data in normalized_items:
            if item_data["normalized"]["tax_rate"] == 8:
                items_8.append(item_data)
            else:
                items_10.append(item_data)

        # レシート記載の税額を使用（優先）
        tax_8_amount = amounts.get("tax_8_amount") or 0
        tax_10_amount = amounts.get("tax_10_amount") or 0

        # レシート記載がない場合のみ逆算（通常は記載されている）
        if tax_8_amount == 0:
            total_8 = sum(item["raw_item"]["amount"] or 0 for item in items_8)
            if total_8 > 0:
                if tax_type == "excluded":
                    tax_8_amount = round(total_8 * 8 / 100)  # 外税：本体価格×税率
                else:
                    tax_8_amount = round(total_8 * 8 / 108)  # 内税：税込額から逆算
                logger.warning(f"8% tax not in receipt, calculated: {tax_8_amount}円 (type={tax_type})")

        if tax_10_amount == 0:
            total_10 = sum(item["raw_item"]["amount"] or 0 for item in items_10)
            if total_10 > 0:
                if tax_type == "excluded":
                    tax_10_amount = round(total_10 * 10 / 100)  # 外税：本体価格×税率
                else:
                    tax_10_amount = round(total_10 * 10 / 110)  # 内税：税込額から逆算
                logger.warning(f"10% tax not in receipt, calculated: {tax_10_amount}円 (type={tax_type})")

        # 各商品に税額を按分（内外タイプを渡す）
        self._distribute_tax_to_items(items_8, tax_8_amount, tax_type)
        self._distribute_tax_to_items(items_10, tax_10_amount, tax_type)

        # 税対象額を計算（税抜額）
        total_8 = sum(item["raw_item"]["amount"] or 0 for item in items_8)
        total_10 = sum(item["raw_item"]["amount"] or 0 for item in items_10)

        # 税対象額を返す（内税の場合は税抜額、外税の場合も税抜額）
        if tax_type == "included":
            # 内税：税込額から税額を引いて税抜額を計算
            tax_8_subtotal = total_8 - tax_8_amount if total_8 > 0 else 0
            tax_10_subtotal = total_10 - tax_10_amount if total_10 > 0 else 0
        else:
            # 外税：表示額がそのまま税抜額
            tax_8_subtotal = total_8
            tax_10_subtotal = total_10

        tax_subtotals = {
            "tax_8_subtotal": tax_8_subtotal,
            "tax_10_subtotal": tax_10_subtotal
        }

        return normalized_items, tax_subtotals

    def _distribute_tax_to_items(self, items: List[Dict], total_tax: int, tax_type: str):
        """
        商品データの7要素を設定
        1. 数量
        2. 表示額
        3. 外or内
        4. 税率
        5. 本体価
        6. 税額
        7. 税込価

        Args:
            items: 商品リスト
            total_tax: グループ全体の税額
            tax_type: "included"（内税）or "excluded"（外税）
        """
        if not items:
            return

        # 金額0円の商品（セット内訳行など）を除外
        items_with_amount = [item for item in items if (item["raw_item"].get("amount") or 0) != 0]
        items_zero_amount = [item for item in items if (item["raw_item"].get("amount") or 0) == 0]

        # 金額0円の商品には税額0を設定
        for item in items_zero_amount:
            quantity = item["raw_item"].get("quantity", 1)
            displayed_amount = 0
            item["normalized"]["quantity"] = quantity
            item["normalized"]["displayed_amount"] = displayed_amount
            item["normalized"]["tax_display_type"] = tax_type
            item["normalized"]["base_price"] = 0
            item["normalized"]["tax_amount"] = 0
            item["normalized"]["tax_included_amount"] = 0
            logger.debug(f"Zero-amount item excluded from tax distribution: {item['raw_item'].get('product_name')}")

        # 金額がある商品のみで税額按分を行う
        if not items_with_amount:
            return

        if total_tax == 0:
            # 税額が0の場合も7要素を設定（割引は考慮）
            for item in items_with_amount:
                quantity = item["raw_item"].get("quantity", 1)
                displayed_amount = item["raw_item"].get("amount") or 0
                linked_discount = item["raw_item"].get("linked_discount", 0)

                # 税込価を計算（表示額 + 割引）
                tax_included_amount = displayed_amount + linked_discount

                # 7要素を設定
                item["normalized"]["quantity"] = quantity  # 1. 数量
                item["normalized"]["displayed_amount"] = displayed_amount  # 2. 表示額
                item["normalized"]["tax_display_type"] = tax_type  # 3. 外or内
                # 4. 税率 は _normalize_item で既に設定済み
                item["normalized"]["tax_amount"] = 0  # 6. 税額

                if tax_type == "excluded":
                    # 外税：表示額 = 本体価
                    item["normalized"]["base_price"] = displayed_amount + linked_discount  # 5. 本体価
                    item["normalized"]["tax_included_amount"] = displayed_amount + linked_discount  # 7. 税込価
                else:
                    # 内税：表示額 = 税込額
                    item["normalized"]["tax_included_amount"] = tax_included_amount  # 7. 税込価
                    item["normalized"]["base_price"] = tax_included_amount  # 5. 本体価
            return

        # Step 1: 各商品の税込価を計算（金額がある商品のみ）
        tax_included_amounts = []
        for item in items_with_amount:
            displayed_amount = item["raw_item"].get("amount") or 0
            linked_discount = item["raw_item"].get("linked_discount", 0)
            tax_included_amount = displayed_amount + linked_discount
            tax_included_amounts.append(tax_included_amount)

        # Step 2: 各商品の理論税額を計算（小数のまま保持）
        theoretical_taxes_float = []
        for i, item in enumerate(items_with_amount):
            tax_included_amount = tax_included_amounts[i]
            tax_rate = item["normalized"].get("tax_rate", 10)
            line_type = item["raw_item"].get("line_type", "ITEM")

            # 割引行は税額0（商品行にすでに割引後の税額が含まれているため）
            if line_type == "DISCOUNT":
                theoretical_tax = 0.0
            elif tax_type == "excluded":
                # 外税：理論税額 = 税抜額 × 税率 / 100
                theoretical_tax = tax_included_amount * tax_rate / 100
            else:
                # 内税：理論税額 = 税込価 - (税込価 / (1 + 税率/100))
                theoretical_tax = tax_included_amount - (tax_included_amount / (1 + tax_rate / 100))

            theoretical_taxes_float.append(theoretical_tax)

        # Step 3: 理論税額の合計（小数）とレシート記載税額の差分
        total_theoretical_tax = sum(theoretical_taxes_float)
        remainder = total_tax - total_theoretical_tax

        # Step 4: 各商品の理論税額を四捨五入し、端数を按分
        theoretical_taxes_rounded = [round(tax) for tax in theoretical_taxes_float]
        total_rounded = sum(theoretical_taxes_rounded)
        final_remainder = total_tax - total_rounded

        # Step 5: 最終端数を税込価の大きい順に1円ずつ配分
        distributed_tax = theoretical_taxes_rounded.copy()

        if final_remainder != 0:
            # 税込価の絶対値でソート（インデックスを保持）
            indexed_amounts = [(i, abs(tax_included_amounts[i])) for i in range(len(items_with_amount))]
            indexed_amounts.sort(key=lambda x: x[1], reverse=True)

            # 端数を1円ずつ配分
            for j in range(abs(final_remainder)):
                idx = indexed_amounts[j % len(items_with_amount)][0]
                if final_remainder > 0:
                    distributed_tax[idx] += 1
                else:
                    distributed_tax[idx] -= 1

        # 各商品に7要素を設定（金額がある商品のみ）
        for i, item in enumerate(items_with_amount):
            quantity = item["raw_item"].get("quantity", 1)
            displayed_amount = item["raw_item"].get("amount") or 0
            linked_discount = item["raw_item"].get("linked_discount", 0)  # リンクされた割引（負の値）

            # 税込価を計算（表示額 + 割引）
            # 割引は負の値なので加算すると減算になる
            tax_included_amount = displayed_amount + linked_discount

            # 按分された税額を使用
            tax_amount = distributed_tax[i]

            # 税率から本体価を計算
            tax_rate = item["normalized"].get("tax_rate", 10)
            if tax_type == "excluded":
                # 外税：表示額 = 本体価、税込価 = 本体価 + 税額
                base_price = displayed_amount + linked_discount
                tax_included_amount = base_price + tax_amount
            else:
                # 内税：税込価 - 按分税額 = 本体価
                base_price = tax_included_amount - tax_amount

            # 7要素を設定
            item["normalized"]["quantity"] = quantity  # 1. 数量
            item["normalized"]["displayed_amount"] = displayed_amount  # 2. 表示額
            item["normalized"]["tax_display_type"] = tax_type  # 3. 外or内
            # 4. 税率 は _normalize_item で既に設定済み
            item["normalized"]["base_price"] = base_price  # 5. 本体価
            item["normalized"]["tax_amount"] = tax_amount  # 6. 税額
            item["normalized"]["tax_included_amount"] = tax_included_amount  # 7. 税込価

            if linked_discount != 0:
                logger.info(f"{item['raw_item'].get('product_name')}: 表示額={displayed_amount}, 割引={linked_discount}, 税込価={tax_included_amount}, 本体価={base_price}, 税額={tax_amount}")

        logger.debug(f"Distributed tax ({tax_type})")

    # ========================================
    # マスタデータ読み込み
    # ========================================

    def _load_aliases(self) -> Dict[str, str]:
        """エイリアステーブルを読み込み"""
        result = self.db.client.table("MASTER_Rules_transaction_dict").select("*").execute()
        # product_name → official_name のマッピング
        aliases = {}
        for row in result.data:
            if row.get("product_name") and row.get("official_name"):
                aliases[row["product_name"].lower()] = row["official_name"]
        return aliases

    def _load_product_dictionary(self) -> List[Dict]:
        """商品辞書を読み込み"""
        result = self.db.client.table("MASTER_Product_classify").select("*").execute()
        return result.data

    def _load_situations(self) -> List[Dict]:
        """シチュエーションマスタを読み込み（名目）"""
        result = self.db.client.table("MASTER_Categories_purpose").select("*").execute()
        return result.data

    def _load_categories(self) -> List[Dict]:
        """カテゴリマスタを読み込み（商品カテゴリ）"""
        result = self.db.client.table("MASTER_Categories_product").select("*").execute()
        return result.data
```

## shared/pipeline/stage_hi_combined.py

```python
"""
Stage H+I: Combined Structuring & Synthesis (構造化 + 統合・要約)

【設計 2026-01-24】Stage F → G → H+I の情報進化パイプライン
============================================
役割: Stage G の整理済み出力を受け取り、1回のLLM呼び出しで
      構造化（旧Stage H）と統合・要約（旧Stage I）を同時に実行

入力:
  - unified_text: Stage G で整理されたMarkdown全文
  - source_inventory: REF_ID付きセグメントリスト
  - table_inventory: REF_ID付き表リスト
  - stage_f_structure: Stage F の構造化情報（フォールバック用）

出力:
  - document_date: 基準日付
  - tags: 検索用タグ
  - metadata: 構造化データ（basic_info, articles, weekly_schedule, etc.）
  - title: ドキュメントタイトル
  - summary: 要約
  - calendar_events: カレンダーイベント
  - tasks: タスクリスト
  - audit_canonical_text: 監査用正本テキスト

特徴:
  - 1回のLLM呼び出しでH+Iを実行（コスト削減）
  - REF_IDによる参照追跡
  - 情報の完全維持（1文字も削除しない）
============================================
"""
import re
import json
import json_repair
from typing import Dict, Any, Optional, List
from pathlib import Path
from string import Template
from loguru import logger
from datetime import datetime

from shared.ai.llm_client.llm_client import LLMClient
from .constants import STAGE_H_INPUT_SCHEMA_VERSION


class StageHICombined:
    """Stage H+I: 構造化 + 統合・要約（統合版）"""

    def __init__(self, llm_client: LLMClient):
        """
        Args:
            llm_client: LLMクライアント
        """
        self.llm = llm_client

    def process(
        self,
        file_name: str,
        doc_type: str,
        workspace: str,
        combined_text: str,
        prompt: str,
        model: str,
        stage_f_structure: Optional[Dict[str, Any]] = None,
        stage_g_result: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        構造化 + 統合・要約（統合版）

        Args:
            file_name: ファイル名
            doc_type: ドキュメントタイプ
            workspace: ワークスペース
            combined_text: 統合テキスト（Stage G の unified_text または Stage F の full_text）
            prompt: プロンプト
            model: モデル名
            stage_f_structure: Stage F の構造化情報
            stage_g_result: Stage G の結果（REF_ID付き目録）

        Returns:
            {
                'document_date': str,
                'tags': List[str],
                'metadata': Dict[str, Any],
                'title': str,
                'summary': str,
                'calendar_events': List[Dict],
                'tasks': List[Dict],
                'audit_canonical_text': str
            }
        """
        logger.info(f"[Stage H+I] 構造化+統合開始... (doc_type={doc_type}, model={model})")

        # Stage G の結果があれば使用
        source_inventory = []
        table_inventory = []
        if stage_g_result:
            source_inventory = stage_g_result.get('source_inventory', [])
            table_inventory = stage_g_result.get('table_inventory', [])
            logger.info(f"[Stage H+I] Stage G 結果: source_inventory={len(source_inventory)}, table_inventory={len(table_inventory)}")

        if not combined_text or not combined_text.strip():
            logger.warning("[Stage H+I] 入力テキストが空です")
            return self._get_fallback_result(doc_type)

        try:
            # プロンプト構築
            logger.info("[Stage H+I] プロンプト構築中...")
            full_prompt = self._build_prompt(
                prompt_template=prompt,
                file_name=file_name,
                doc_type=doc_type,
                workspace=workspace,
                combined_text=combined_text,
                source_inventory=source_inventory,
                table_inventory=table_inventory
            )
            logger.info(f"[Stage H+I] プロンプト構築完了 ({len(full_prompt)}文字)")

            # LLM呼び出し
            logger.info(f"[Stage H+I] LLM呼び出し中... (model={model})")
            response = self.llm.call_model(
                tier="default",
                prompt=full_prompt,
                model_name=model
            )
            logger.info(f"[Stage H+I] LLM応答受信: success={response.get('success')}")

            if not response.get("success"):
                logger.error(f"[Stage H+I エラー] LLM呼び出し失敗: {response.get('error')}")
                return self._get_fallback_result(doc_type)

            # JSON抽出
            content = response.get("content", "")
            logger.info(f"[Stage H+I] ===== LLMレスポンス（最初の1000文字）=====\n{content[:1000]}")
            # リトライ禁止（2026-01-28）: エラー時は即座にフォールバック
            result = self._extract_json_with_retry(content, model=model, max_retries=0)

            # Stage F の構造化情報をマージ
            if stage_f_structure:
                result = self._merge_stage_f_structure(result, stage_f_structure)

            # audit_canonical_text の生成（監査用正本）
            audit_canonical_text = self._generate_audit_canonical_text(
                result, combined_text, source_inventory
            )

            # 結果の整形
            final_result = {
                'document_date': result.get('document_date'),
                'tags': result.get('tags', []),
                'metadata': result.get('metadata', {}),
                'title': result.get('title', ''),
                'summary': result.get('summary', ''),
                'calendar_events': result.get('calendar_events', []),
                'tasks': result.get('tasks', []),
                'audit_canonical_text': audit_canonical_text
            }

            logger.info(f"[Stage H+I完了] title={final_result['title'][:50] if final_result['title'] else 'N/A'}...")
            return final_result

        except Exception as e:
            logger.error(f"[Stage H+I エラー] 処理失敗: {e}", exc_info=True)
            return self._get_fallback_result(doc_type)

    def _build_prompt(
        self,
        prompt_template: str,
        file_name: str,
        doc_type: str,
        workspace: str,
        combined_text: str,
        source_inventory: List[Dict],
        table_inventory: List[Dict]
    ) -> str:
        """プロンプトを構築"""
        # source_inventory を簡略化
        inventory_summary = []
        for item in source_inventory[:30]:  # 最大30件
            inventory_summary.append({
                'ref_id': item.get('ref_id'),
                'type': item.get('type'),
                'text': item.get('text', '')[:200]  # 最大200文字
            })

        # table_inventory を簡略化
        tables_summary = []
        for item in table_inventory[:10]:  # 最大10件
            tables_summary.append({
                'ref_id': item.get('ref_id'),
                'table_title': item.get('table_title'),
                'table_type': item.get('table_type')
            })

        # テンプレート変数を置換
        template = Template(prompt_template)
        prompt = template.substitute(
            file_name=file_name,
            doc_type=doc_type,
            workspace=workspace,
            combined_text=combined_text,
            current_date=datetime.now().strftime("%Y-%m-%d"),
            source_inventory_json=json.dumps(inventory_summary, ensure_ascii=False, indent=2),
            table_inventory_json=json.dumps(tables_summary, ensure_ascii=False, indent=2),
            source_count=len(source_inventory),
            table_count=len(table_inventory)
        )

        return prompt

    def _extract_json_with_retry(
        self,
        content: str,
        model: str,
        max_retries: int = 2
    ) -> Dict[str, Any]:
        """JSON抽出（リトライ機能付き）"""
        for attempt in range(max_retries + 1):
            try:
                result = self._extract_json(content)
                logger.debug(f"[Stage H+I] JSON抽出成功 (試行{attempt + 1}/{max_retries + 1})")
                return result

            except Exception as e:
                if attempt < max_retries:
                    logger.warning(f"[Stage H+I] JSON抽出失敗 (試行{attempt + 1}/{max_retries + 1}): {e}")
                    content = self._retry_json_extraction(content, str(e), model)
                else:
                    logger.error(f"[Stage H+I] JSON抽出失敗（最終試行）: {e}")
                    raise

        return {}

    def _extract_json(self, content: str) -> Dict[str, Any]:
        """コンテンツからJSONを抽出"""
        # 複数のパターンでJSONブロックを探す
        patterns = [
            r'```json\s*(.*?)```',
            r'```\s*(.*?)```',
            r'\{[\s\S]*?\}',
        ]

        json_str = None
        for pattern in patterns:
            match = re.search(pattern, content, re.DOTALL)
            if match:
                json_str = match.group(1) if match.lastindex else match.group(0)
                json_str = json_str.strip()
                if json_str.startswith('{'):
                    break
                else:
                    json_str = None

        if not json_str:
            match = re.search(r'\{[\s\S]*\}', content, re.DOTALL)
            if match:
                json_str = match.group(0).strip()
            else:
                json_str = content.strip()

        # JSONパース
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.error(f"[Stage H+I] JSON解析失敗: {e}")
            try:
                return json_repair.loads(json_str)
            except Exception as repair_error:
                logger.error(f"[Stage H+I] JSON修復も失敗: {repair_error}")
                raise

    def _retry_json_extraction(
        self,
        failed_content: str,
        error_message: str,
        model: str
    ) -> str:
        """JSON抽出失敗時、LLMにJSON修正を依頼"""
        prompt = f"""以下のJSONにエラーがあります。修正してください。

エラー: {error_message}

元のJSON:
```
{failed_content[:3000]}
```

修正されたJSONを ```json ブロックで出力してください。
"""

        try:
            response = self.llm.call_model(
                tier="default",
                prompt=prompt,
                model_name=model
            )

            if response.get("success"):
                return response.get("content", "")
            else:
                return failed_content

        except Exception as e:
            logger.error(f"[Stage H+I] JSON修正エラー: {e}")
            return failed_content

    def _merge_stage_f_structure(
        self,
        result: Dict[str, Any],
        stage_f_structure: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Stage F の構造化情報をマージ"""
        metadata = result.get('metadata', {})

        # v1.1契約の判定
        schema_ver = stage_f_structure.get('schema_version', '')
        is_v1_1 = (schema_ver == STAGE_H_INPUT_SCHEMA_VERSION)

        if is_v1_1:
            stage_f_tables = stage_f_structure.get('tables', [])
            stage_f_text_blocks = stage_f_structure.get('text_blocks', [])

            if stage_f_text_blocks:
                metadata['_raw_text_blocks'] = stage_f_text_blocks
                logger.info(f"[Stage H+I] Stage F text_blocks を _raw_text_blocks に保存")

        # visual_elements からデッドライン情報を抽出
        visual_elements = stage_f_structure.get('visual_elements', {})
        if visual_elements:
            deadline_info = visual_elements.get('deadline_info')
            if deadline_info and not result.get('document_date'):
                result['document_date'] = deadline_info

        result['metadata'] = metadata
        return result

    def _generate_audit_canonical_text(
        self,
        result: Dict[str, Any],
        combined_text: str,
        source_inventory: List[Dict]
    ) -> str:
        """
        監査用正本テキストを生成

        Stage H+I の出力から、全情報を含む監査用正本を生成。
        後で人間が検証可能な形式。
        """
        parts = []

        # タイトル
        title = result.get('title', '')
        if title:
            parts.append(f"# {title}\n")

        # 基本情報
        metadata = result.get('metadata', {})
        basic_info = metadata.get('basic_info', {})
        if basic_info:
            parts.append("## 基本情報")
            for key, value in basic_info.items():
                if value:
                    parts.append(f"- {key}: {value}")
            parts.append("")

        # 要約
        summary = result.get('summary', '')
        if summary:
            parts.append("## 要約")
            parts.append(summary)
            parts.append("")

        # 記事・お知らせ
        articles = metadata.get('articles', [])
        if articles:
            parts.append("## 記事・お知らせ")
            for article in articles:
                title_a = article.get('title', '')
                body = article.get('body', '')
                if title_a:
                    parts.append(f"### {title_a}")
                if body:
                    parts.append(body)
                parts.append("")

        # カレンダーイベント
        calendar_events = result.get('calendar_events', [])
        if calendar_events:
            parts.append("## カレンダーイベント")
            for event in calendar_events:
                date = event.get('event_date', '')
                name = event.get('event_name', '')
                parts.append(f"- {date}: {name}")
            parts.append("")

        # タスク
        tasks = result.get('tasks', [])
        if tasks:
            parts.append("## タスク")
            for task in tasks:
                name = task.get('task_name', '')
                deadline = task.get('deadline', '')
                parts.append(f"- {name} (期限: {deadline})")
            parts.append("")

        # 元テキスト参照
        if source_inventory:
            parts.append("## 参照元（REF_ID）")
            for item in source_inventory[:10]:  # 最大10件表示
                ref_id = item.get('ref_id', '')
                text = item.get('text', '')[:100]
                parts.append(f"- {ref_id}: {text}...")
            if len(source_inventory) > 10:
                parts.append(f"  ... 他 {len(source_inventory) - 10} 件")

        return '\n'.join(parts)

    def _get_fallback_result(self, doc_type: str) -> Dict[str, Any]:
        """フォールバック結果を返す"""
        return {
            'document_date': None,
            'tags': [],
            'metadata': {
                'doc_type': doc_type,
                'extraction_failed': True
            },
            'title': '',
            'summary': '',
            'calendar_events': [],
            'tasks': [],
            'audit_canonical_text': ''
        }
```

## shared/pipeline/stage_j_chunking.py

```python
"""
Stage J: Chunking (チャンク化)

メタデータからチャンクを生成
- 役割: 検索用チャンクの作成
- 処理: MetadataChunker でメタデータチャンク生成
"""
from typing import Dict, Any, List
from loguru import logger

from shared.common.processing.metadata_chunker import MetadataChunker


class StageJChunking:
    """Stage J: チャンク化"""

    def __init__(self):
        """初期化"""
        self.chunker = MetadataChunker()

    def create_chunks(
        self,
        display_subject: str,
        summary: str,
        tags: List[str],
        document_date: str,
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        メタデータからチャンクを生成

        Args:
            display_subject: 件名/ファイル名
            summary: 要約
            tags: タグリスト
            document_date: ドキュメント日付
            metadata: 構造化メタデータ

        Returns:
            チャンクリスト [
                {
                    'chunk_text': str,
                    'chunk_type': str,
                    'search_weight': float
                },
                ...
            ]
        """
        logger.info("[Stage J] チャンク化開始...")

        try:
            # metadata から構造化データを展開
            document_data = {
                'file_name': display_subject,
                'summary': summary,
                'tags': tags,
                'document_date': document_date,
                # metadata の中身を直接展開
                'persons': metadata.get('persons', []) if isinstance(metadata, dict) else [],
                'organizations': metadata.get('organizations', []) if isinstance(metadata, dict) else [],
                'people': metadata.get('people', []) if isinstance(metadata, dict) else [],
                # Stage H の構造化データを追加
                'text_blocks': metadata.get('text_blocks', []) if isinstance(metadata, dict) else [],
                'structured_tables': metadata.get('structured_tables', []) if isinstance(metadata, dict) else [],
                'weekly_schedule': metadata.get('weekly_schedule', []) if isinstance(metadata, dict) else [],
                'other_text': metadata.get('other_text', []) if isinstance(metadata, dict) else [],
                # Stage I の抽出データを追加
                'calendar_events': metadata.get('calendar_events', []) if isinstance(metadata, dict) else [],
                'tasks': metadata.get('tasks', []) if isinstance(metadata, dict) else [],
                # basic_info も展開（あれば）
                'doc_type': metadata.get('basic_info', {}).get('related_class', '') if isinstance(metadata, dict) else ''
            }

            chunks = self.chunker.create_metadata_chunks(document_data)

            logger.info(f"[Stage J完了] チャンク数: {len(chunks)}")

            return chunks

        except Exception as e:
            logger.error(f"[Stage J エラー] チャンク化失敗: {e}", exc_info=True)
            return []

    def process(
        self,
        display_subject: str,
        summary: str,
        tags: List[str],
        document_date: Any,
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        チャンク生成（process() エイリアス）

        Args:
            display_subject: 表示件名
            summary: 要約
            tags: タグ
            document_date: ドキュメント日付
            metadata: メタデータ

        Returns:
            chunks: チャンクリスト
        """
        return self.create_chunks(
            display_subject=display_subject,
            summary=summary,
            tags=tags,
            document_date=document_date,
            metadata=metadata
        )
```

## shared/pipeline/stage_k_embedding.py

```python
"""
Stage K: Embedding (ベクトル化)

チャンクをベクトル化して search_index に保存
- 役割: チャンクをベクトル化
- モデル: OpenAI text-embedding-3-small (1536次元)
"""
from typing import Dict, Any, List
from loguru import logger

from shared.ai.llm_client.llm_client import LLMClient
from shared.common.database.client import DatabaseClient


class StageKEmbedding:
    """Stage K: ベクトル化"""

    def __init__(self, llm_client: LLMClient, db_client: DatabaseClient):
        """
        Args:
            llm_client: LLMクライアント
            db_client: データベースクライアント
        """
        self.llm_client = llm_client
        self.db = db_client

    def embed_and_save(
        self,
        document_id: str,
        chunks: List[Dict[str, Any]],
        delete_existing: bool = False,
        owner_id: str = None
    ) -> Dict[str, Any]:
        """
        チャンクをベクトル化して search_index に保存

        Args:
            document_id: ドキュメントID
            chunks: チャンクリスト
            delete_existing: 既存のチャンクを削除するか
            owner_id: オーナーID（省略時は親ドキュメントから継承）

        Returns:
            {
                'success': bool,
                'saved_count': int,
                'failed_count': int
            }
        """
        logger.info("[Stage K] ベクトル化 + search_index保存開始...")

        # Phase 3: owner_id を取得（指定がない場合は親ドキュメントから継承）
        if owner_id is None:
            try:
                parent_doc = self.db.client.table('Rawdata_FILE_AND_MAIL')\
                    .select('owner_id')\
                    .eq('id', document_id)\
                    .execute()
                if parent_doc.data:
                    owner_id = parent_doc.data[0].get('owner_id')
                    logger.debug(f"[Stage K] 親ドキュメントから owner_id 継承: {owner_id}")
            except Exception as e:
                logger.warning(f"[Stage K 警告] 親ドキュメントの owner_id 取得エラー: {e}")

        if owner_id is None:
            logger.error(f"[Stage K エラー] owner_id が取得できません: document_id={document_id}")
            return {
                'success': False,
                'saved_count': 0,
                'failed_count': len(chunks),
                'error': 'owner_id is required but not available'
            }

        # 既存ドキュメントの場合は、古いチャンクを削除
        if delete_existing:
            try:
                logger.info(f"[Stage K] 既存チャンク削除: document_id={document_id}")
                self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
            except Exception as e:
                logger.warning(f"[Stage K 警告] 既存チャンク削除エラー（継続）: {e}")

        saved_count = 0
        failed_count = 0

        for chunk in chunks:
            try:
                # null文字を除去
                chunk_text = chunk['chunk_text'].replace('\u0000', '') if chunk['chunk_text'] else ''

                # Embedding生成
                embedding = self.llm_client.generate_embedding(chunk_text)

                # search_indexに保存
                chunk_data = {
                    'document_id': document_id,
                    'owner_id': owner_id,  # Phase 3: 親ドキュメントから継承
                    'chunk_content': chunk_text,
                    'chunk_size': len(chunk_text),
                    'chunk_type': chunk['chunk_type'],
                    'embedding': embedding,
                    'search_weight': chunk.get('search_weight', 1.0),
                    'chunk_index': chunk.get('chunk_index', 0),
                    'chunk_metadata': chunk.get('metadata')  # 構造化データを保存
                }

                self.db.client.table('10_ix_search_index').insert(chunk_data).execute()
                saved_count += 1

            except Exception as e:
                logger.error(f"[Stage K エラー] チャンク保存失敗: {e}")
                failed_count += 1

        logger.info(f"[Stage K完了] {saved_count}/{len(chunks)}チャンクを保存 (失敗: {failed_count})")

        # chunk_countを更新
        if saved_count > 0:
            try:
                self.db.client.table('Rawdata_FILE_AND_MAIL')\
                    .update({'chunk_count': saved_count})\
                    .eq('id', document_id)\
                    .execute()
                logger.debug(f"[Stage K] chunk_count更新: {saved_count}個")
            except Exception as e:
                logger.warning(f"[Stage K 警告] chunk_count更新エラー（継続）: {e}")

        # 成功条件: 最低1チャンク以上保存 & 失敗なし
        is_success = saved_count > 0 and failed_count == 0

        return {
            'success': is_success,
            'saved_count': saved_count,
            'failed_count': failed_count
        }

    def process(self, chunks: List[Dict[str, Any]], document_id: str) -> None:
        """
        チャンクをベクトル化して保存（process() エイリアス）

        Args:
            chunks: チャンクリスト
            document_id: ドキュメントID
        """
        self.embed_and_save(document_id, chunks)
```

## shared/pipeline/utils/__init__.py

```python
"""
Pipeline Utilities
"""
from .table_parser import recompose_columnar_data

__all__ = ['recompose_columnar_data']
```

## shared/pipeline/utils/table_parser.py

```python
"""
Table Parser Utilities

カラムナ形式（columns + rows）を辞書リスト形式に復元するユーティリティ
Stage F → Stage H1 間のデータ変換に使用
"""
from typing import Any, Dict, List, Union


def recompose_columnar_data(columnar_json: Union[Dict, List, Any]) -> List[Dict]:
    """
    軽量なカラムナ形式を通常の辞書リストに復元する

    Args:
        columnar_json: カラムナ形式のJSON
            {"columns": ["A", "B"], "rows": [["v1", "v2"], ["v3", "v4"]]}

    Returns:
        辞書リスト形式
            [{"A": "v1", "B": "v2"}, {"A": "v3", "B": "v4"}]

    Examples:
        >>> recompose_columnar_data({"columns": ["順位", "氏名"], "rows": [[1, "山田"], [2, "田中"]]})
        [{"順位": 1, "氏名": "山田"}, {"順位": 2, "氏名": "田中"}]

        >>> recompose_columnar_data([{"A": 1}])  # 既に辞書リスト形式
        [{"A": 1}]
    """
    # None や空の場合
    if not columnar_json:
        return []

    # 既に辞書リスト形式の場合はそのまま返す
    if isinstance(columnar_json, list):
        return columnar_json

    # 辞書でない場合
    if not isinstance(columnar_json, dict):
        return []

    # columns と rows を取得
    cols = columnar_json.get("columns", [])
    rows = columnar_json.get("rows", [])

    # columns がない場合（カラムナ形式ではない）
    if not cols:
        # headers + rows 形式の可能性をチェック
        headers = columnar_json.get("headers", [])
        if headers and rows:
            cols = headers
        else:
            return []

    # rows がない場合
    if not rows:
        return []

    # 辞書リストに変換
    result = []
    for row in rows:
        if isinstance(row, list):
            # 行の要素数が columns より少ない場合は空文字で補完
            row_dict = {}
            for i, col in enumerate(cols):
                row_dict[col] = row[i] if i < len(row) else ""
            result.append(row_dict)
        elif isinstance(row, dict):
            # 既に辞書形式の行はそのまま
            result.append(row)

    return result


def is_columnar_format(data: Any) -> bool:
    """
    データがカラムナ形式かどうかを判定

    Args:
        data: 判定対象のデータ

    Returns:
        カラムナ形式なら True
    """
    if not isinstance(data, dict):
        return False

    has_columns = "columns" in data and isinstance(data["columns"], list)
    has_rows = "rows" in data and isinstance(data["rows"], list)

    # rows の最初の要素が list であることを確認（辞書リストではない）
    if has_columns and has_rows and data["rows"]:
        first_row = data["rows"][0]
        return isinstance(first_row, list)

    return has_columns and has_rows


def extract_table_text_for_removal(table: Dict) -> List[str]:
    """
    表データからH2で削除すべきテキスト断片を抽出

    H1で処理した表の内容がH2のテキストに重複して含まれている場合、
    そのテキストを削除するための断片リストを生成

    Args:
        table: 表データ（columns/rows または headers/rows 形式）

    Returns:
        削除対象のテキスト断片リスト
    """
    fragments = []

    # テーブルタイトル
    title = table.get("table_title", "")
    if title:
        fragments.append(title)

    # columns/headers
    cols = table.get("columns", []) or table.get("headers", [])
    if cols:
        # ヘッダー行全体
        fragments.append(" ".join(str(c) for c in cols))

    # rows
    rows = table.get("rows", [])
    for row in rows:
        if isinstance(row, list):
            # 行全体のテキスト
            row_text = " ".join(str(cell) for cell in row)
            if len(row_text) > 5:  # 短すぎる断片は除外
                fragments.append(row_text)
            # 各セルの値（長いもののみ）
            for cell in row:
                cell_str = str(cell)
                if len(cell_str) > 10:
                    fragments.append(cell_str)
        elif isinstance(row, dict):
            # 辞書形式の行
            row_text = " ".join(str(v) for v in row.values())
            if len(row_text) > 5:
                fragments.append(row_text)

    return fragments
```

## shared/ai/__init__.py

```python

```

## shared/ai/embeddings/__init__.py

```python

```

## shared/ai/embeddings/embeddings.py

```python
"""
Embedding Client (DEPRECATED - OpenAI text-embedding-3-small を使用してください)
このクラスは後方互換性のために残されていますが、使用は推奨されません。
代わりに LLMClient.generate_embedding() を使用してください。
"""
from typing import List, Optional
from openai import OpenAI
from shared.common.config.settings import settings


class EmbeddingClient:
    """
    OpenAI text-embedding-3-small を使用したEmbedding生成クライアント (1536次元)

    注意: このクラスは非推奨です。LLMClient.generate_embedding() を使用してください。
    """

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = ***REDACTED*** or settings.OPENAI_API_KEY
        if not self.api_key:
            raise ValueError("OpenAI API Key が設定されていません")

        self.client = OpenAI(api_key=***REDACTED***
        self.model_name = "text-embedding-3-small"
        self.dimensions = 1536

    def generate_embedding(self, text: str, task_type: str = "RETRIEVAL_DOCUMENT") -> List[float]:
        """
        Embeddingを生成 (1536次元)

        注意: task_type パラメータは互換性のために残されていますが、使用されません
        """
        if not text or not text.strip():
            raise ValueError("空のテキストはembedding化できません")

        response = self.client.embeddings.create(
            model=self.model_name,
            input=text,
            dimensions=self.dimensions
        )

        return response.data[0].embedding

    def generate_embeddings_batch(self, texts: List[str], task_type: str = "RETRIEVAL_DOCUMENT") -> List[List[float]]:
        """バッチでEmbeddingを生成 (1536次元)"""
        if not texts:
            return []

        embeddings = []
        for text in texts:
            if text and text.strip():
                embedding = self.generate_embedding(text, task_type)
                embeddings.append(embedding)
            else:
                # OpenAI text-embedding-3-smallは1536次元
                embeddings.append([0.0] * 1536)

        return embeddings

    def generate_query_embedding(self, query: str) -> List[float]:
        """クエリ用のEmbeddingを生成 (1536次元)"""
        return self.generate_embedding(query, task_type="RETRIEVAL_QUERY")
```

## shared/ai/llm_client/__init__.py

```python

```

## shared/ai/llm_client/exceptions.py

```python
"""
LLMクライアントのカスタム例外
"""


class MaxTokensExceededError(Exception):
    """max_tokens上限に達して出力が途中で切れた場合のエラー"""

    def __init__(self, message: str, partial_output: str, finish_reason_name: str):
        """
        Args:
            message: エラーメッセージ
            partial_output: 途中で切れた出力テキスト
            finish_reason_name: finish_reasonの名前
        """
        super().__init__(message)
        self.partial_output = partial_output
        self.finish_reason_name = finish_reason_name
```

## shared/ai/llm_client/llm_client.py

```python
"""
LLMクライアント（v3.0: マルチプロバイダ対応）
Gemini / Anthropic / OpenAI を統一インターフェースで利用
"""

import os
import base64
from typing import Dict, List, Any, Optional, Union
from pathlib import Path
import mimetypes

import google.generativeai as genai
from anthropic import Anthropic, RateLimitError
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, RetryError
from loguru import logger

from shared.common.config.model_tiers import AIProvider, get_model_config
from shared.common.config.settings import settings
from .exceptions import MaxTokensExceededError

class LLMClient:
    """統合LLMクライアント"""
    
    def __init__(self):
        """設定からAPIキーを取得し、各プロバイダーを初期化"""

        # Settings経由でAPIキーを取得（環境変数管理の統一）
        self.gemini_api_key = ***REDACTED*** or os.getenv("GOOGLE_API_KEY")  # 後方互換性のためGOOGLE_API_KEYもサポート
        self.anthropic_api_key = ***REDACTED***
        self.openai_api_key = ***REDACTED***

        # Gemini設定 (トップレベル関数のみ使用)
        if self.gemini_api_key:
            genai.configure(api_key=***REDACTED***
        else:
            pass

        # Anthropic設定
        if self.anthropic_api_key:
            self.anthropic_client = Anthropic(api_key=***REDACTED***
        else:
            self.anthropic_client = None

        # OpenAI設定
        if self.openai_api_key:
            self.openai_client = OpenAI(api_key=***REDACTED***
        else:
            self.openai_client = None
    
    def generate_with_images(
        self,
        prompt: str,
        image_data: Union[str, List[str]],
        model: str = "gemini-2.5-flash-lite",
        temperature: float = 0.0,
        max_tokens: int = 8192
    ) -> str:
        """
        画像データを使ってGemini Vision APIを呼び出し

        Args:
            prompt: プロンプト
            image_data: Base64エンコードされた画像データ（単一または複数）
            model: モデル名
            temperature: 温度パラメータ
            max_tokens: 最大トークン数

        Returns:
            生成されたテキスト
        """
        if not self.gemini_api_key:
            raise ValueError("Gemini API key is missing")

        try:
            model_obj = genai.GenerativeModel(model)

            # 画像データをリスト化
            if isinstance(image_data, str):
                image_data_list = [image_data]
            else:
                image_data_list = image_data

            # コンテンツパーツを構築
            content_parts = [prompt]

            # 画像を追加
            for img_base64 in image_data_list:
                # Base64をバイトにデコード
                img_bytes = base64.b64decode(img_base64)

                # Geminiの画像形式に変換
                image_part = {
                    'mime_type': 'image/png',
                    'data': img_bytes
                }
                content_parts.append(image_part)

            # 安全フィルター設定
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]

            # APIを呼び出し
            response = model_obj.generate_content(
                content_parts,
                generation_config=genai.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=temperature
                ),
                safety_settings=safety_settings
            )

            # レスポンスの検証
            if not response.candidates:
                raise ValueError("Gemini returned no candidates")

            candidate = response.candidates[0]

            # finish_reason をチェック
            if candidate.finish_reason != 1:  # 1 = STOP (正常終了)
                finish_reason_name = candidate.finish_reason.name if hasattr(candidate.finish_reason, 'name') else str(candidate.finish_reason)
                error_details = {
                    "finish_reason": candidate.finish_reason,
                    "finish_reason_name": finish_reason_name
                }
                if hasattr(candidate, 'safety_ratings') and candidate.safety_ratings:
                    error_details["safety_ratings"] = [
                        {
                            "category": rating.category.name if hasattr(rating.category, 'name') else str(rating.category),
                            "probability": rating.probability.name if hasattr(rating.probability, 'name') else str(rating.probability)
                        }
                        for rating in candidate.safety_ratings
                    ]
                logger.error(f"Gemini Vision失敗: {error_details}")
                raise ValueError(f"Gemini finish_reason: {finish_reason_name} ({candidate.finish_reason})")

            # テキストを取得
            text_content = candidate.content.parts[0].text if candidate.content.parts else ""
            return text_content

        except Exception as e:
            logger.error(f"Gemini Vision API エラー: {e}")
            raise

    def call_model(
        self,
        tier: str,
        prompt: str,
        file_path: Optional[Path] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        指定されたタスクに最適なモデルを呼び出し

        Args:
            tier: モデル階層("stagea_classification", "stageh_extraction", "ui_response")
            prompt: プロンプト
            file_path: ファイルパス (GeminiのStage A分類用)
            **kwargs: 追加パラメータ

        Returns:
            モデルレスポンス
        """
        config = get_model_config(tier)
        provider = config["provider"]
        # kwargsからmodel_nameが渡されていればそれを優先、なければconfigから取得
        model_name = kwargs.pop('model_name', None) or config["model"]

        # モデル名からプロバイダーを自動判定（明示的なmodel_name指定時）
        if model_name:
            if 'claude' in model_name.lower():
                provider = AIProvider.CLAUDE
            elif 'gemini' in model_name.lower():
                provider = AIProvider.GEMINI
            elif 'gpt' in model_name.lower() or 'text-embedding' in model_name.lower():
                provider = AIProvider.OPENAI

        if provider == AIProvider.GEMINI:
            if not self.gemini_api_key:
                return {"success": False, "error": "Gemini API key is missing", "model": model_name}
            return self._call_gemini(model_name, prompt, file_path, config, **kwargs)

        elif provider == AIProvider.CLAUDE:
            if not self.anthropic_client:
                return {"success": False, "error": "Anthropic API key is missing", "model": model_name}
            try:
                return self._call_claude(model_name, prompt, config, **kwargs)
            except RetryError as e:
                # リトライが全て失敗した場合
                original_error = e.last_attempt.exception()
                return {"success": False, "error": str(original_error), "model": model_name, "provider": "claude"}

        elif provider == AIProvider.OPENAI:
            if not self.openai_client:
                return {"success": False, "error": "OpenAI API key is missing", "model": model_name}
            return self._call_openai(model_name, prompt, config, **kwargs)

        else:
            raise ValueError(f"未対応のプロバイダー: {provider}")

    def _call_gemini(
        self,
        model_name: str,
        prompt: str,
        file_path: Optional[Path],
        config: Dict,
        **kwargs
    ) -> Dict[str, Any]:
        """Gemini API呼び出し（トップレベル関数のみ使用）"""
        uploaded_file = None
        try:
            model = genai.GenerativeModel(model_name)

            content_parts = [prompt]

            if file_path and file_path.exists():
                # MIMEタイプを自動判定
                mime_type, _ = mimetypes.guess_type(str(file_path))
                if not mime_type:
                    mime_type = "application/pdf"  # デフォルト

                # ファイルをアップロード（トップレベル関数のみ使用）
                uploaded_file = genai.upload_file(path=str(file_path), mime_type=mime_type)
                content_parts.append(uploaded_file)

            # 安全フィルター設定（finish_reason: 2 対策）
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]

            # 生成設定
            generation_config = genai.GenerationConfig(
                max_output_tokens=config.get("max_tokens", 65536),
                temperature=config.get("temperature", 0.1)
            )

            # response_format が kwargs に含まれている場合
            response_format = kwargs.get('response_format')
            if response_format in ["json", "json_object"]:
                generation_config.response_mime_type = "application/json"

            response = model.generate_content(
                content_parts,
                generation_config=generation_config,
                safety_settings=safety_settings
            )

            # レスポンスの検証
            if not response.candidates:
                self._cleanup_uploaded_file(uploaded_file)
                return {"success": False, "error": "Gemini returned no candidates", "model": model_name, "provider": "gemini"}

            candidate = response.candidates[0]

            # finish_reason をチェック
            if candidate.finish_reason != 1:  # 1 = STOP (正常終了)
                self._cleanup_uploaded_file(uploaded_file)
                # 詳細なエラー情報を取得
                error_details = {
                    "finish_reason": candidate.finish_reason,
                    "finish_reason_name": candidate.finish_reason.name if hasattr(candidate.finish_reason, 'name') else str(candidate.finish_reason)
                }
                # safety_ratingsがあれば追加
                if hasattr(candidate, 'safety_ratings') and candidate.safety_ratings:
                    error_details["safety_ratings"] = [
                        {
                            "category": rating.category.name if hasattr(rating.category, 'name') else str(rating.category),
                            "probability": rating.probability.name if hasattr(rating.probability, 'name') else str(rating.probability)
                        }
                        for rating in candidate.safety_ratings
                    ]
                logger.error(f"Gemini Vision失敗: {error_details}")
                return {
                    "success": False,
                    "error": f"Gemini finish_reason: {candidate.finish_reason}",
                    "error_details": error_details,
                    "model": model_name,
                    "provider": "gemini"
                }

            # テキストを取得
            text_content = candidate.content.parts[0].text if candidate.content.parts else ""

            # トークン使用量を取得
            usage = {}
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                usage = {
                    "prompt_tokens": getattr(response.usage_metadata, 'prompt_token_count', 0),
                    "completion_tokens": getattr(response.usage_metadata, 'candidates_token_count', 0),
                    "total_tokens": getattr(response.usage_metadata, 'total_token_count', 0)
                }
                logger.info(f"[Gemini] トークン使用量: prompt={usage['prompt_tokens']}, completion={usage['completion_tokens']}, total={usage['total_tokens']}")

            # ファイルを削除
            self._cleanup_uploaded_file(uploaded_file)

            return {
                "success": True,
                "content": text_content,
                "model": model_name,
                "provider": "gemini",
                "usage": usage
            }

        except Exception as e:
            self._cleanup_uploaded_file(uploaded_file)
            return {"success": False, "error": str(e), "model": model_name, "provider": "gemini"}

    def _cleanup_uploaded_file(self, uploaded_file) -> None:
        """
        アップロードされたファイルを削除（トップレベル関数のみ使用）

        Args:
            uploaded_file: アップロードされたファイルオブジェクト
        """
        if not uploaded_file:
            return

        try:
            genai.delete_file(name=uploaded_file.name)
        except Exception:
            # 削除に失敗しても処理は継続
            pass

    @retry(
        retry=retry_if_exception_type(RateLimitError),
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=2, max=60)
    )
    def _call_claude(
        self,
        model_name: str,
        prompt: str,
        config: Dict,
        **kwargs
    ) -> Dict[str, Any]:
        """Anthropic API呼び出し"""
        try:
            # ✅ DEBUG: 送信するプロンプトの先頭部分をログに出力
            from loguru import logger
            logger.debug(f"[Anthropic CALL] Model: {model_name}, Prompt start: {prompt[:300]}...")

            # Anthropicの最大トークン数制限を適用
            max_tokens = config.get("max_tokens", 8192)
            # Anthropic models: max 64000 tokens
            if max_tokens > 64000:
                max_tokens = 64000

            response = self.anthropic_client.messages.create(
                model=model_name,
                max_tokens=max_tokens,
                temperature=config.get("temperature", 0.0),
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )

            # ✅ DEBUG: Anthropic からの生の応答コンテンツ全体をログに出力
            raw_content = response.content[0].text
            logger.debug(f"[Anthropic RAW RESP] Content length: {len(raw_content)} chars")
            # 応答が長すぎる場合があるため、先頭2000文字のみをログに記録
            logger.debug(f"[Anthropic RAW RESP] Content preview: {raw_content[:2000]}")

            # トークン使用量を取得
            usage = {}
            if hasattr(response, 'usage') and response.usage:
                usage = {
                    "prompt_tokens": getattr(response.usage, 'input_tokens', 0),
                    "completion_tokens": getattr(response.usage, 'output_tokens', 0),
                    "total_tokens": getattr(response.usage, 'input_tokens', 0) + getattr(response.usage, 'output_tokens', 0)
                }
                logger.info(f"[Anthropic] トークン使用量: prompt={usage['prompt_tokens']}, completion={usage['completion_tokens']}, total={usage['total_tokens']}")

            return {
                "success": True,
                "content": raw_content,
                "model": model_name,
                "provider": "claude",
                "usage": usage
            }

        except RateLimitError:
            # RateLimitErrorは再スローしてtenacityにリトライさせる
            raise
        except Exception as e:
            return {"success": False, "error": str(e), "model": model_name, "provider": "claude"}

    def _call_openai(
        self,
        model_name: str,
        prompt: str,
        config: Dict,
        **kwargs
    ) -> Dict[str, Any]:
        """OpenAI API呼び出し"""
        try:
            # ✅ GPT-5.1では max_completion_tokens を使用、旧モデルでは max_tokens（後方互換性）
            max_completion_tokens = config.get("max_completion_tokens")
            max_tokens = config.get("max_tokens", 16384)

            # パラメータを動的に構築
            api_params = {
                "model": model_name,
                "messages": [
                    {"role": "user", "content": prompt}
                ]
            }

            # temperatureはGPT-5.1などの一部モデルでサポートされていないため、configに含まれている場合のみ設定
            if "temperature" in config:
                api_params["temperature"] = config["temperature"]

            # max_completion_tokens が設定されていればそれを使用、なければ max_tokens
            if max_completion_tokens:
                api_params["max_completion_tokens"] = max_completion_tokens
            else:
                api_params["max_tokens"] = max_tokens

            response = self.openai_client.chat.completions.create(**api_params)

            # トークン使用量を取得
            usage = {}
            if hasattr(response, 'usage') and response.usage:
                usage = {
                    "prompt_tokens": getattr(response.usage, 'prompt_tokens', 0),
                    "completion_tokens": getattr(response.usage, 'completion_tokens', 0),
                    "total_tokens": getattr(response.usage, 'total_tokens', 0)
                }
                logger.info(f"[OpenAI] トークン使用量: prompt={usage['prompt_tokens']}, completion={usage['completion_tokens']}, total={usage['total_tokens']}")

            return {
                "success": True,
                "content": response.choices[0].message.content,
                "model": model_name,
                "provider": "openai",
                "usage": usage
            }

        except Exception as e:
            return {"success": False, "error": str(e), "model": model_name, "provider": "openai"}

    def generate_embedding(self, text: str) -> List[float]:
        """
        Embedding生成

        Args:
            text: Embeddingを生成するテキスト

        Returns:
            1536次元のembeddingベクトル
        """
        config = get_model_config("embeddings")

        if not self.openai_client:
            raise ConnectionError("OpenAI client not initialized for embedding generation.")

        # text-embedding-3-smallモデルで1536次元を明示的に指定
        response = self.openai_client.embeddings.create(
            model=config["model"],
            input=text,
            dimensions=config.get("dimensions", 1536)  # デフォルト1536次元
        )

        return response.data[0].embedding

    def generate_with_vision(
        self,
        prompt: str,
        image_path: str,
        model: str = "gemini-2.0-flash-exp",
        temperature: float = 0.0,
        max_tokens: int = 65536,
        response_format: Optional[str] = None
    ) -> str:
        """
        画像ファイルを使ってGemini Vision APIを呼び出し

        Args:
            prompt: プロンプト
            image_path: 画像ファイルのパス（PNG, JPEG等）
            model: モデル名
            temperature: 温度パラメータ
            max_tokens: 最大トークン数
            response_format: レスポンスフォーマット（"json", "json_object" など）

        Returns:
            生成されたテキスト

        Raises:
            ValueError: APIキーがない、またはレスポンスが不正な場合
            Exception: その他のエラー
        """
        if not self.gemini_api_key:
            raise ValueError("Gemini API key is missing")

        try:
            model_obj = genai.GenerativeModel(model)

            # ファイルをアップロード
            mime_type, _ = mimetypes.guess_type(image_path)
            if not mime_type:
                mime_type = "image/jpeg"  # デフォルト

            uploaded_file = genai.upload_file(path=image_path, mime_type=mime_type)

            # コンテンツパーツを構築
            content_parts = [prompt, uploaded_file]

            # 安全フィルター設定
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]

            # 生成設定
            generation_config = genai.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=temperature
            )

            # response_format が指定されている場合
            if response_format in ["json", "json_object"]:
                generation_config.response_mime_type = "application/json"

            # APIを呼び出し（タイムアウト5分、1回リトライ）
            max_retries = 1
            last_error = None

            for attempt in range(max_retries + 1):  # 0回目（初回）+ 1回（リトライ）
                try:
                    logger.info(f"[Gemini Vision] API呼び出し試行 {attempt + 1}/{max_retries + 1}")
                    response = model_obj.generate_content(
                        content_parts,
                        generation_config=generation_config,
                        safety_settings=safety_settings,
                        request_options={"timeout": 300}  # 5分タイムアウト
                    )
                    break  # 成功したらループを抜ける
                except Exception as e:
                    last_error = e
                    error_str = str(e).lower()
                    # タイムアウトまたはネットワークエラーの場合
                    if any(keyword in error_str for keyword in ['timeout', 'deadline', 'network', 'connection']):
                        if attempt < max_retries:
                            logger.warning(f"[Gemini Vision] タイムアウト/ネットワークエラー。リトライします（試行 {attempt + 1}/{max_retries + 1}）: {e}")
                            continue
                        else:
                            logger.error(f"[Gemini Vision] タイムアウト/ネットワークエラー。リトライ上限に達しました: {e}")
                            raise
                    else:
                        # タイムアウト以外のエラーは即座に失敗
                        logger.error(f"[Gemini Vision] API エラー（リトライ不可）: {e}")
                        raise
            else:
                # ループが最後まで実行された（全てのリトライが失敗）
                if last_error:
                    raise last_error

            # アップロードファイルを削除
            try:
                genai.delete_file(name=uploaded_file.name)
            except Exception:
                pass

            # レスポンスの検証
            if not response.candidates:
                raise ValueError("Gemini returned no candidates")

            candidate = response.candidates[0]

            # finish_reason をチェック
            finish_reason_name = candidate.finish_reason.name if hasattr(candidate.finish_reason, 'name') else str(candidate.finish_reason)
            logger.info(f"[Gemini Vision] finish_reason: {finish_reason_name} ({candidate.finish_reason})")

            # テキストを取得（finish_reasonに関わらず取得）
            text_content = candidate.content.parts[0].text if candidate.content.parts else ""

            # finish_reason == 3 (MAX_TOKENS): トークン上限に達した場合
            if candidate.finish_reason == 3:
                error_msg = f"MAX_TOKENS上限に達しました。出力が途中で切れています。({len(text_content)}文字)"
                logger.error(f"[Gemini Vision] {error_msg}")
                logger.error(f"[Gemini Vision] 途中で切れた出力（最後の500文字）: {text_content[-500:]}")
                raise MaxTokensExceededError(
                    message=error_msg,
                    partial_output=text_content,
                    finish_reason_name=finish_reason_name
                )

            # finish_reason != 1 (STOP以外のその他のエラー)
            if candidate.finish_reason != 1:  # 1 = STOP (正常終了)
                error_details = {
                    "finish_reason": candidate.finish_reason,
                    "finish_reason_name": finish_reason_name
                }
                if hasattr(candidate, 'safety_ratings') and candidate.safety_ratings:
                    error_details["safety_ratings"] = [
                        {
                            "category": rating.category.name if hasattr(rating.category, 'name') else str(rating.category),
                            "probability": rating.probability.name if hasattr(rating.probability, 'name') else str(rating.probability)
                        }
                        for rating in candidate.safety_ratings
                    ]
                logger.error(f"Gemini Vision失敗: {error_details}")
                raise ValueError(f"Gemini finish_reason: {finish_reason_name} ({candidate.finish_reason})")

            # テキスト長とトークン使用量をログ出力
            logger.info(f"[Gemini Vision] 応答テキスト長: {len(text_content)}文字")

            # トークン使用量を取得・保存
            self.last_usage = {}
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                self.last_usage = {
                    "prompt_tokens": getattr(response.usage_metadata, 'prompt_token_count', 0),
                    "completion_tokens": getattr(response.usage_metadata, 'candidates_token_count', 0),
                    "total_tokens": getattr(response.usage_metadata, 'total_token_count', 0),
                    "model": model
                }
                logger.info(f"[Gemini Vision] トークン使用量: prompt={self.last_usage['prompt_tokens']}, completion={self.last_usage['completion_tokens']}, total={self.last_usage['total_tokens']}")

            return text_content

        except Exception as e:
            logger.error(f"Gemini Vision API エラー: {e}")
            raise

    def transcribe_image(
        self,
        image_path: Path,
        prompt: str = "この画像内の表組みやリストを、Markdown形式で正確に書き起こしてください。",
        model: str = "gemini-2.5-pro"
    ) -> Dict[str, Any]:
        """
        画像ファイルをGemini Visionで文字起こし

        Args:
            image_path: 画像ファイルのパス（PNG, JPEG等）
            prompt: Geminiに送るプロンプト
            model: 使用するGeminiモデル（デフォルト: gemini-2.5-pro）

        Returns:
            {"success": bool, "content": str, "model": str, "provider": str}
        """
        if not self.gemini_api_key:
            return {"success": False, "error": "Gemini API key is missing", "model": "gemini-2.5-flash"}

        # 指定されたGeminiモデルを使用
        return self._call_gemini(
            model_name=model,
            prompt=prompt,
            file_path=image_path,
            config={
                "max_tokens": 65536,  # Gemini 2.5の最大出力トークン数（65,536）
                "temperature": 0.0
            }
        )
```

## shared/__init__.py

```python
# shared package
```

## pyproject.toml

```python
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "document-management-system"
version = "1.0.0"
description = "Document Management System"
requires-python = ">=3.10"

[tool.setuptools.packages.find]
where = ["."]
include = ["shared*", "scripts*", "services*", "frontend*", "database*"]

[tool.setuptools.package-data]
"*" = ["*.json", "*.yaml", "*.yml", "*.html", "*.css", "*.js"]
```

