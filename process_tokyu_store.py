"""
æ±æ€¥ã‚¹ãƒˆã‚¢ ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼ å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ã„æ–¹:
    # å…¨ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’å‡¦ç†ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    python process_tokyu_store.py

    # ç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ã¿å‡¦ç†
    python process_tokyu_store.py --category "é‡èœ" --category "æœç‰©"

    # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚ªãƒ•ã«ã—ã¦å‹•ä½œç¢ºèª
    python process_tokyu_store.py --no-headless
"""

import os
import sys
import json
import asyncio
import logging
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
root_dir = Path(__file__).parent
sys.path.insert(0, str(root_dir))

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
from dotenv import load_dotenv
load_dotenv(root_dir / ".env")

from B_ingestion.tokyu_store.product_ingestion import TokyuStoreProductIngestionPipeline

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tokyu_store_output.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


async def process_all_categories(
    pipeline: TokyuStoreProductIngestionPipeline,
    target_categories: List[str] = None,
    max_pages_per_category: int = 100
) -> Dict[str, Any]:
    """
    ã™ã¹ã¦ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’å‡¦ç†

    Args:
        pipeline: å•†å“å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        target_categories: å‡¦ç†å¯¾è±¡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼åãƒªã‚¹ãƒˆï¼ˆNoneãªã‚‰å…¨ã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼‰
        max_pages_per_category: ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚ãŸã‚Šã®æœ€å¤§ãƒšãƒ¼ã‚¸æ•°

    Returns:
        å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼
    """
    logger.info("=" * 80)
    logger.info("æ±æ€¥ã‚¹ãƒˆã‚¢ ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼ å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
    logger.info("=" * 80)

    start_time = datetime.now()

    # ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’å‹•çš„ã«å–å¾—
    all_categories = await pipeline.discover_categories()

    if not all_categories:
        logger.error("âŒ ã‚«ãƒ†ã‚´ãƒªãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        logger.info("ğŸ’¡ æ‰‹å‹•ã§ã‚«ãƒ†ã‚´ãƒªãƒ¼URLã‚’æŒ‡å®šã—ã¦ãã ã•ã„")

        # æ‰‹å‹•ã‚«ãƒ†ã‚´ãƒªãƒ¼å®šç¾©ï¼ˆå®Ÿéš›ã®ã‚µã‚¤ãƒˆã‹ã‚‰å–å¾—ã—ãŸURLï¼‰
        manual_categories = [
            {"name": "é‡èœ", "url": f"{pipeline.scraper.base_url}/shop/c/cC10"},
            {"name": "æœç‰©", "url": f"{pipeline.scraper.base_url}/shop/c/cC11"},
            {"name": "ãŠé­š", "url": f"{pipeline.scraper.base_url}/shop/c/cC20"},
            {"name": "ãŠè‚‰", "url": f"{pipeline.scraper.base_url}/shop/c/cC30"},
            {"name": "æƒ£èœ", "url": f"{pipeline.scraper.base_url}/shop/c/cC40"},
            {"name": "ç‰›ä¹³ãƒ»ä¹³è£½å“ãƒ»åµ", "url": f"{pipeline.scraper.base_url}/shop/c/cC50"},
            {"name": "ãƒ‘ãƒ³ãƒ»ç”Ÿè“å­ãƒ»ã‚·ãƒªã‚¢ãƒ«", "url": f"{pipeline.scraper.base_url}/shop/c/cC51"},
            {"name": "ãƒãƒ«ãƒ‰ç·èœãƒ»è±†è…ãƒ»ç´è±†ãƒ»æ¼¬ç‰©", "url": f"{pipeline.scraper.base_url}/shop/c/cC52"},
            {"name": "å†·å‡é£Ÿå“ãƒ»ã‚¢ã‚¤ã‚¹", "url": f"{pipeline.scraper.base_url}/shop/c/cC53"},
            {"name": "ç±³ãƒ»é¤…", "url": f"{pipeline.scraper.base_url}/shop/c/cC54"},
            {"name": "éººé¡", "url": f"{pipeline.scraper.base_url}/shop/c/cC55"},
            {"name": "ä¹¾ç‰©ãƒ»ç“¶ç¼¶è©°ãƒ»ç²‰é¡", "url": f"{pipeline.scraper.base_url}/shop/c/cC56"},
            {"name": "èª¿å‘³æ–™ãƒ»ä¸­è¯ææ–™", "url": f"{pipeline.scraper.base_url}/shop/c/cC57"},
            {"name": "ãŠè“å­", "url": f"{pipeline.scraper.base_url}/shop/c/cC58"},
            {"name": "æ°´ãƒ»é£²æ–™", "url": f"{pipeline.scraper.base_url}/shop/c/cC59"},
            {"name": "é…’é¡", "url": f"{pipeline.scraper.base_url}/shop/c/cC60"},
        ]
        all_categories = manual_categories
        logger.info(f"ğŸ“ æ‰‹å‹•ã‚«ãƒ†ã‚´ãƒªãƒ¼å®šç¾©ã‚’ä½¿ç”¨: {len(all_categories)}ä»¶")

    # å¯¾è±¡ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    if target_categories:
        categories = [
            cat for cat in all_categories
            if cat["name"] in target_categories
        ]
        logger.info(f"å‡¦ç†å¯¾è±¡ã‚«ãƒ†ã‚´ãƒªãƒ¼: {', '.join(target_categories)}")
    else:
        categories = all_categories
        logger.info(f"å…¨{len(categories)}ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’å‡¦ç†")

    # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’å‡¦ç†
    results = []
    total_products = 0
    total_new = 0
    total_updated = 0

    for i, category in enumerate(categories, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"[{i}/{len(categories)}] ã‚«ãƒ†ã‚´ãƒªãƒ¼: {category['name']}")
        logger.info(f"{'='*80}")

        try:
            result = await pipeline.process_category_all_pages(
                category_url=category["url"],
                category_name=category["name"],
                max_pages=max_pages_per_category
            )

            results.append(result)
            total_products += result["total_products"]
            total_new += result["new_products"]
            total_updated += result["updated_products"]

        except Exception as e:
            logger.error(f"ã‚«ãƒ†ã‚´ãƒªãƒ¼ '{category['name']}' ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            continue

    # ã‚µãƒãƒªãƒ¼å‡ºåŠ›
    end_time = datetime.now()
    duration = end_time - start_time

    logger.info("\n" + "=" * 80)
    logger.info("å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼")
    logger.info("=" * 80)
    logger.info(f"å‡¦ç†æ™‚é–“: {duration}")
    logger.info(f"ã‚«ãƒ†ã‚´ãƒªãƒ¼æ•°: {len(results)}/{len(categories)}")
    logger.info(f"ç·å•†å“æ•°: {total_products}ä»¶")
    logger.info(f"  æ–°è¦: {total_new}ä»¶")
    logger.info(f"  æ›´æ–°: {total_updated}ä»¶")

    # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ã®è©³ç´°
    logger.info("\nã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥è©³ç´°:")
    for result in results:
        logger.info(
            f"  {result['category_name']}: "
            f"{result['total_products']}ä»¶ "
            f"(æ–°è¦{result['new_products']}ä»¶ã€æ›´æ–°{result['updated_products']}ä»¶ã€"
            f"{result['pages_processed']}ãƒšãƒ¼ã‚¸)"
        )

    summary = {
        "start_time": start_time.isoformat(),
        "end_time": end_time.isoformat(),
        "duration_seconds": duration.total_seconds(),
        "categories_processed": len(results),
        "total_categories": len(categories),
        "total_products": total_products,
        "new_products": total_new,
        "updated_products": total_updated,
        "category_results": results
    }

    return summary


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description='æ±æ€¥ã‚¹ãƒˆã‚¢ ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼ å•†å“ãƒ‡ãƒ¼ã‚¿å–å¾—'
    )
    parser.add_argument(
        '--category',
        action='append',
        help='å‡¦ç†å¯¾è±¡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼åï¼ˆè¤‡æ•°æŒ‡å®šå¯ï¼‰'
    )
    parser.add_argument(
        '--max-pages',
        type=int,
        default=100,
        help='ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚ãŸã‚Šã®æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰'
    )
    parser.add_argument(
        '--no-headless',
        action='store_true',
        help='ãƒ–ãƒ©ã‚¦ã‚¶ã‚’è¡¨ç¤ºã™ã‚‹ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰'
    )
    parser.add_argument(
        '--zip-code',
        type=str,
        default=None,
        help='é…é”ã‚¨ãƒªã‚¢ã®éƒµä¾¿ç•ªå·ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ã‚³ãƒ¼ãƒ‰å†…ã®è¨­å®šï¼‰'
    )

    args = parser.parse_args()

    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    login_id = os.getenv("TOKYU_STORE_LOGIN_ID")
    password = os.getenv("TOKYU_STORE_PASSWORD")
    zip_code = args.zip_code or os.getenv("DELIVERY_ZIP_CODE", "158-0094")

    if not login_id or not password:
        logger.error("âŒ ç’°å¢ƒå¤‰æ•° TOKYU_STORE_LOGIN_ID ã¨ TOKYU_STORE_PASSWORD ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        logger.error("   .env ãƒ•ã‚¡ã‚¤ãƒ«ã«ä»¥ä¸‹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„:")
        logger.error("   TOKYU_STORE_LOGIN_ID=your_email@example.com")
        logger.error("   TOKYU_STORE_PASSWORD=your_password")
        logger.error("   DELIVERY_ZIP_CODE=158-0094")
        sys.exit(1)

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
    pipeline = TokyuStoreProductIngestionPipeline(
        login_id=login_id,
        password=password,
        zip_code=zip_code,
        headless=not args.no_headless
    )

    try:
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•
        success = await pipeline.start()
        if not success:
            logger.error("âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•å¤±æ•—")
            sys.exit(1)

        # ã‚«ãƒ†ã‚´ãƒªãƒ¼å‡¦ç†
        summary = await process_all_categories(
            pipeline=pipeline,
            target_categories=args.category,
            max_pages_per_category=args.max_pages
        )

        # ã‚µãƒãƒªãƒ¼ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_file = f"_runtime/data/tokyu_store/tokyu_store_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        logger.info(f"\nâœ… å‡¦ç†çµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")

    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")

    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
        sys.exit(1)

    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        await pipeline.close()
        logger.info("âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼çµ‚äº†")


if __name__ == "__main__":
    asyncio.run(main())
