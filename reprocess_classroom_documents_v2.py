"""
Google Classroom ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ v2

å‡¦ç†çŠ¶æ…‹ç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ« (document_reprocessing_queue) ã‚’ä½¿ç”¨ã—ãŸæ”¹è‰¯ç‰ˆã€‚
é‡è¤‡å‡¦ç†ã‚’é˜²ãã€å‡¦ç†é€²æ—ã‚’è¿½è·¡ã—ã€ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒªãƒˆãƒ©ã‚¤ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚

å‡¦ç†å†…å®¹:
1. ã™ã¹ã¦ã®ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã¾ãŸã¯æŒ‡å®šã•ã‚ŒãŸãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚­ãƒ¥ãƒ¼ã«ç™»éŒ²
2. ã‚­ãƒ¥ãƒ¼ã‹ã‚‰é †æ¬¡ã‚¿ã‚¹ã‚¯ã‚’å–å¾—ã—ã¦å‡¦ç†
3. æ—¢å­˜ã®2æ®µéšãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆGeminiåˆ†é¡ + ClaudeæŠ½å‡ºï¼‰ã§å‡¦ç†
4. full_textã€æ§‹é€ åŒ–metadataã‚’ç”Ÿæˆ
5. å‡¦ç†çŠ¶æ…‹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ç®¡ç†ï¼ˆpending â†’ processing â†’ completed/failedï¼‰

ä½¿ã„æ–¹:
    # å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‡¦ç†ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    python reprocess_classroom_documents_v2.py --limit=100

    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆç¢ºèªã®ã¿ï¼‰
    python reprocess_classroom_documents_v2.py --dry-run

    # ç‰¹å®šã®ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®ã¿å‡¦ç†
    python reprocess_classroom_documents_v2.py --workspace=ema_classroom --limit=20
    python reprocess_classroom_documents_v2.py --workspace=ikuya_classroom --limit=20

    # ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã®ã¿ï¼ˆå‡¦ç†ã¯å®Ÿè¡Œã—ãªã„ï¼‰
    python reprocess_classroom_documents_v2.py --populate-only --limit=50

    # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å‡¦ç†å®Ÿè¡Œ
    python reprocess_classroom_documents_v2.py --process-queue --limit=10

    # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’ä¿æŒã—ãªã„ï¼ˆStage1 AIã«åˆ¤å®šã•ã›ã‚‹ï¼‰
    python reprocess_classroom_documents_v2.py --no-preserve-workspace
"""

import asyncio
from typing import List, Dict, Any, Optional
from loguru import logger
import json
import sys
from datetime import datetime

from core.database.client import DatabaseClient
from pipelines.two_stage_ingestion import TwoStageIngestionPipeline


class ClassroomReprocessorV2:
    """Google Classroomãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†å‡¦ç†ï¼ˆå‡¦ç†çŠ¶æ…‹ç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œç‰ˆï¼‰"""

    def __init__(self, worker_id: str = "reprocessor_v2"):
        self.db = DatabaseClient()
        self.pipeline = TwoStageIngestionPipeline()
        self.worker_id = worker_id

    def populate_queue_from_workspace(
        self,
        workspace: str = 'all',
        limit: int = 100,
        reason: str = 'classroom_reprocessing',
        preserve_workspace: bool = True
    ) -> int:
        """
        æŒ‡å®šã•ã‚ŒãŸworkspaceã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 

        Args:
            workspace: å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ ('all' ã§å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹)
            limit: è¿½åŠ ã™ã‚‹æœ€å¤§ä»¶æ•°
            reason: å†å‡¦ç†ã®ç†ç”±
            preserve_workspace: workspaceã‚’ä¿æŒã™ã‚‹ã‹

        Returns:
            è¿½åŠ ã—ãŸä»¶æ•°
        """
        logger.info(f"ã‚­ãƒ¥ãƒ¼ã¸ã®è¿½åŠ ã‚’é–‹å§‹: workspace={workspace}")

        # å¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        if workspace == 'all':
            # å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å¯¾è±¡
            result = self.db.client.table('documents').select('*').limit(limit).execute()
        else:
            # ç‰¹å®šã®ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®ã¿
            result = self.db.client.table('documents').select('*').eq(
                'workspace', workspace
            ).limit(limit).execute()

        documents = result.data if result.data else []
        logger.info(f"å¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {len(documents)}ä»¶")

        if not documents:
            logger.info("è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            return 0

        added_count = 0
        skipped_count = 0

        for doc in documents:
            doc_id = doc['id']
            file_name = doc.get('file_name', 'unknown')

            # æ—¢ã«ã‚­ãƒ¥ãƒ¼ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            existing = self.db.client.table('document_reprocessing_queue').select('id, status').eq(
                'document_id', doc_id
            ).eq('status', 'pending').execute()

            if existing.data:
                logger.debug(f"ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢ã«ã‚­ãƒ¥ãƒ¼ã«ç™»éŒ²æ¸ˆã¿ï¼‰: {file_name}")
                skipped_count += 1
                continue

            # ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
            try:
                queue_data = {
                    'document_id': doc_id,
                    'reprocess_reason': reason,
                    'reprocess_type': 'full',
                    'priority': 0,
                    'preserve_workspace': preserve_workspace,
                    'original_file_name': file_name,
                    'original_workspace': doc.get('workspace'),
                    'original_doc_type': doc.get('doc_type'),
                    'original_source_id': doc.get('source_id'),
                    'created_by': self.worker_id
                }

                self.db.client.table('document_reprocessing_queue').insert(queue_data).execute()
                added_count += 1
                logger.debug(f"ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ : {file_name}")

            except Exception as e:
                logger.error(f"ã‚­ãƒ¥ãƒ¼è¿½åŠ ã‚¨ãƒ©ãƒ¼: {file_name} - {e}")

        logger.info(f"ã‚­ãƒ¥ãƒ¼è¿½åŠ å®Œäº†: {added_count}ä»¶è¿½åŠ , {skipped_count}ä»¶ã‚¹ã‚­ãƒƒãƒ—")
        return added_count

    async def process_queue(self, limit: int = 100) -> Dict[str, int]:
        """
        ã‚­ãƒ¥ãƒ¼ã‹ã‚‰é †æ¬¡ã‚¿ã‚¹ã‚¯ã‚’å–å¾—ã—ã¦å‡¦ç†

        Args:
            limit: å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•°

        Returns:
            å‡¦ç†çµæœã®çµ±è¨ˆï¼ˆæˆåŠŸæ•°ã€å¤±æ•—æ•°ãªã©ï¼‰
        """
        logger.info(f"ã‚­ãƒ¥ãƒ¼å‡¦ç†é–‹å§‹: æœ€å¤§{limit}ä»¶")

        stats = {
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'total': 0
        }

        for i in range(limit):
            # æ¬¡ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
            task = self._get_next_task()

            if not task:
                logger.info("å‡¦ç†ã™ã‚‹ã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“")
                break

            stats['total'] += 1
            queue_id = task['queue_id']
            document_id = task['document_id']
            file_name = task['file_name']
            preserve_workspace = task.get('preserve_workspace', True)

            logger.info(f"\n{'='*80}")
            logger.info(f"[{i+1}/{limit}] å‡¦ç†é–‹å§‹: {file_name}")
            logger.info(f"Queue ID: {queue_id}")
            logger.info(f"Document ID: {document_id}")

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å†å‡¦ç†
            success = await self._reprocess_document(
                queue_id=queue_id,
                document_id=document_id,
                file_name=file_name,
                preserve_workspace=preserve_workspace
            )

            if success:
                stats['success'] += 1
            else:
                stats['failed'] += 1

            # é€²æ—è¡¨ç¤º
            logger.info(f"é€²æ—: æˆåŠŸ={stats['success']}, å¤±æ•—={stats['failed']}, åˆè¨ˆ={stats['total']}")

        return stats

    def _get_next_task(self) -> Optional[Dict[str, Any]]:
        """
        æ¬¡ã®å‡¦ç†å¯¾è±¡ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å–å¾—
        ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢æ•° get_next_reprocessing_task ã‚’ä½¿ç”¨

        Returns:
            ã‚¿ã‚¹ã‚¯æƒ…å ±ã€ã¾ãŸã¯None
        """
        try:
            response = self.db.client.rpc(
                'get_next_reprocessing_task',
                {'p_worker_id': self.worker_id}
            ).execute()

            if response.data and len(response.data) > 0:
                return response.data[0]
            return None

        except Exception as e:
            logger.error(f"æ¬¡ã‚¿ã‚¹ã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    async def _reprocess_document(
        self,
        queue_id: str,
        document_id: str,
        file_name: str,
        preserve_workspace: bool = True
    ) -> bool:
        """
        å˜ä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å†å‡¦ç†ã—ã€çµæœã‚’ã‚­ãƒ¥ãƒ¼ã«è¨˜éŒ²

        Args:
            queue_id: ã‚­ãƒ¥ãƒ¼ID
            document_id: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«å
            preserve_workspace: workspaceã‚’ä¿æŒã™ã‚‹ã‹

        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—
            doc = self.db.get_document_by_id(document_id)
            if not doc:
                error_msg = "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                logger.error(error_msg)
                self._mark_task_failed(queue_id, error_msg)
                return False

            source_type = doc.get('source_type', '')

            # ============================================
            # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆclassroom_text, text_onlyï¼‰ã®å‡¦ç†
            # ============================================
            if source_type in ['classroom_text', 'text_only']:
                logger.info(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œå‡ºï¼ˆ{source_type}ï¼‰")
                return await self._reprocess_text_only_document(
                    queue_id=queue_id,
                    document_id=document_id,
                    doc=doc,
                    preserve_workspace=preserve_workspace
                )

            # ============================================
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆdrive, email_attachmentç­‰ï¼‰ã®å‡¦ç†
            # ============================================
            # ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’å–å¾—
            file_id = self._extract_file_id(doc)
            if not file_id:
                error_msg = "ãƒ•ã‚¡ã‚¤ãƒ«IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                logger.error(f"{error_msg}: {file_name}")
                logger.error(f"  source_id: {doc.get('source_id')}")
                self._mark_task_failed(queue_id, error_msg)
                return False

            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ID: {file_id}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
            file_meta = {
                'id': file_id,
                'name': file_name,
                'mimeType': self._guess_mime_type(file_name),
                'doc_type': doc.get('doc_type', 'other')  # doc_typeã‚’è¿½åŠ ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: otherï¼‰
            }

            # workspaceã‚’æ±ºå®š
            workspace_to_use = doc.get('workspace', 'unknown') if preserve_workspace else 'unknown'
            logger.info(f"Workspace: {workspace_to_use} (preserve={preserve_workspace})")

            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å‡¦ç†
            result = await self.pipeline.process_file(
                file_meta=file_meta,
                workspace=workspace_to_use,
                force_reprocess=True
            )

            if result and result.get('success'):
                logger.success(f"âœ… å†å‡¦ç†æˆåŠŸ: {file_name}")
                self._mark_task_completed(queue_id, success=True)
                return True

            elif result and result.get('error') and 'duplicate key' in str(result.get('error')):
                # é‡è¤‡ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€å¤ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã—ã¦å†è©¦è¡Œ
                logger.warning(f"é‡è¤‡æ¤œå‡ºã€å¤ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã—ã¦å†è©¦è¡Œ")
                self.db.client.table('documents').delete().eq('id', document_id).execute()

                # å†è©¦è¡Œ
                result = await self.pipeline.process_file(
                    file_meta=file_meta,
                    workspace=workspace_to_use,
                    force_reprocess=True
                )

                if result and result.get('success'):
                    logger.success(f"âœ… å†å‡¦ç†æˆåŠŸï¼ˆå†è©¦è¡Œï¼‰: {file_name}")
                    self._mark_task_completed(queue_id, success=True)
                    return True
                else:
                    error_msg = f"å†è©¦è¡Œã‚‚å¤±æ•—: {result.get('error', 'unknown')}"
                    logger.error(f"âŒ {error_msg}")
                    self._mark_task_failed(queue_id, error_msg)
                    return False
            else:
                error_msg = result.get('error', 'unknown error') if result else 'no result'
                logger.error(f"âŒ å†å‡¦ç†å¤±æ•—: {error_msg}")
                self._mark_task_failed(queue_id, error_msg)
                return False

        except Exception as e:
            error_msg = f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.exception(e)
            self._mark_task_failed(queue_id, error_msg, error_details={'exception': str(e)})
            return False

    def _mark_task_completed(self, queue_id: str, success: bool):
        """ã‚¿ã‚¹ã‚¯ã‚’å®Œäº†ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
        try:
            self.db.client.rpc(
                'mark_reprocessing_task_completed',
                {
                    'p_queue_id': queue_id,
                    'p_success': success
                }
            ).execute()
        except Exception as e:
            logger.error(f"ã‚¿ã‚¹ã‚¯å®Œäº†ãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

    def _mark_task_failed(
        self,
        queue_id: str,
        error_message: str,
        error_details: Optional[Dict] = None
    ):
        """ã‚¿ã‚¹ã‚¯ã‚’å¤±æ•—ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
        try:
            self.db.client.rpc(
                'mark_reprocessing_task_completed',
                {
                    'p_queue_id': queue_id,
                    'p_success': False,
                    'p_error_message': error_message,
                    'p_error_details': json.dumps(error_details) if error_details else None
                }
            ).execute()
        except Exception as e:
            logger.error(f"ã‚¿ã‚¹ã‚¯å¤±æ•—ãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

    async def _reprocess_text_only_document(
        self,
        queue_id: str,
        document_id: str,
        doc: Dict[str, Any],
        preserve_workspace: bool = True
    ) -> bool:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆclassroom_textï¼‰ã‚’å†å‡¦ç†

        Args:
            queue_id: ã‚­ãƒ¥ãƒ¼ID
            document_id: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
            doc: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
            preserve_workspace: workspaceã‚’ä¿æŒã™ã‚‹ã‹

        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        from core.ai.stage1_classifier import Stage1Classifier
        from core.ai.stage2_extractor import Stage2Extractor
        from config.yaml_loader import get_classification_yaml_string

        file_name = doc.get('file_name', 'text_only')
        full_text = doc.get('full_text', '')

        if not full_text:
            error_msg = "full_textãŒç©ºã§ã™"
            logger.error(f"{error_msg}: {file_name}")
            self._mark_task_failed(queue_id, error_msg)
            return False

        logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(full_text)}æ–‡å­—")

        try:
            # Stage 1ã¨Stage 2ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
            stage1_classifier = Stage1Classifier(llm_client=self.pipeline.llm_client)
            stage2_extractor = Stage2Extractor(llm_client=self.pipeline.llm_client)
            yaml_string = get_classification_yaml_string()

            # workspaceã‚’æ±ºå®š
            workspace_to_use = doc.get('workspace', 'unknown') if preserve_workspace else 'unknown'

            # ============================================
            # Stage 1: Geminiåˆ†é¡
            # ============================================
            logger.info("[Stage 1] Geminiåˆ†é¡é–‹å§‹...")
            # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆã€file_pathã¯ä¸è¦
            # mime_typeã‚’text/plainã«è¨­å®šã—ã€text_contentã‚’æ¸¡ã™
            from pathlib import Path as PathLib
            stage1_result = await stage1_classifier.classify(
                file_path=PathLib("dummy"),  # ãƒ€ãƒŸãƒ¼ãƒ‘ã‚¹ï¼ˆä½¿ç”¨ã•ã‚Œãªã„ï¼‰
                doc_types_yaml=yaml_string,
                mime_type="text/plain",  # PDFã§ã¯ãªã„ã“ã¨ã‚’ç¤ºã™
                text_content=full_text
            )

            # Stage1ã¯doc_typeã¨workspaceã‚’è¿”ã•ãªã„ï¼ˆå…¥åŠ›å…ƒã§æ±ºå®šã•ã‚Œã‚‹ãŸã‚ï¼‰
            stage1_doc_type = doc.get('doc_type', 'unknown')  # å…ƒã®doc_typeã‚’ä¿æŒ
            summary = stage1_result.get('summary', '')
            relevant_date = stage1_result.get('relevant_date')

            logger.info(f"[Stage 1] å®Œäº†: summary={summary[:50]}...")

            # ============================================
            # Stage 2: Claudeè©³ç´°æŠ½å‡º
            # ============================================
            logger.info("[Stage 2] Claudeè©³ç´°æŠ½å‡ºé–‹å§‹...")
            stage2_result = stage2_extractor.extract_metadata(
                full_text=full_text,
                file_name=file_name,
                stage1_result=stage1_result,
                workspace=doc.get('workspace', 'unknown')  # å…ƒã®workspaceã‚’ä½¿ç”¨
            )

            # Stage 2ã®çµæœã‚’åæ˜ 
            doc_type = stage2_result.get('doc_type', stage1_doc_type)
            summary = stage2_result.get('summary', summary)
            document_date = stage2_result.get('document_date')
            tags = stage2_result.get('tags', [])
            metadata = stage2_result.get('metadata', {})

            logger.info(f"[Stage 2] å®Œäº†: doc_type={doc_type}")

            # ============================================
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
            # ============================================
            update_data = {
                # doc_type ã¨ workspace ã¯å…¥åŠ›å…ƒã®åæ˜ ãªã®ã§æ›´æ–°ã—ãªã„ï¼ˆç ´å£Šè¡Œç‚ºã«ãªã‚‹ï¼‰
                # 'doc_type': doc_type,
                # 'workspace': workspace_to_use,
                'summary': summary,
                'metadata': metadata,
                'processing_status': 'completed',
                'processing_stage': 'stage1_and_stage2',
                'stage1_model': 'gemini-2.5-flash',
                'stage2_model': 'claude-haiku-4-5-20251001',
                'relevant_date': relevant_date
            }

            response = self.db.client.table('documents').update(update_data).eq('id', document_id).execute()

            if response.data:
                logger.success(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†å‡¦ç†æˆåŠŸ: {file_name}")
                logger.info(f"  Stage1: {stage1_doc_type}")
                logger.info(f"  Stage2: {doc_type}")
                self._mark_task_completed(queue_id, success=True)
                return True
            else:
                error_msg = "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°å¤±æ•—"
                logger.error(error_msg)
                self._mark_task_failed(queue_id, error_msg)
                return False

        except Exception as e:
            error_msg = f"ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.exception(e)
            self._mark_task_failed(queue_id, error_msg, error_details={'exception': str(e)})
            return False

    def _extract_file_id(self, doc: Dict[str, Any]) -> str:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰Google Drive ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’æŠ½å‡º"""
        # 1. metadata->original_file_id ã‚’ç¢ºèª
        metadata = doc.get('metadata')
        if metadata is None:
            metadata = {}
        elif isinstance(metadata, str):
            try:
                metadata = json.loads(metadata)
            except:
                metadata = {}

        if metadata.get('original_file_id'):
            return metadata['original_file_id']

        # 3. source_id ã‚’ç¢ºèªï¼ˆæ•°å­—ã ã‘ã®å ´åˆã¯Classroom IDãªã®ã§ä½¿ã‚ãªã„ï¼‰
        source_id = doc.get('source_id', '')
        if source_id and not source_id.isdigit():
            return source_id

        return ''

    def _guess_mime_type(self, file_name: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ MIME ã‚¿ã‚¤ãƒ—ã‚’æ¨æ¸¬"""
        ext = file_name.lower().split('.')[-1] if '.' in file_name else ''

        mime_map = {
            'pdf': 'application/pdf',
            'jpg': 'image/jpeg',
            'jpeg': 'image/jpeg',
            'png': 'image/png',
            'gif': 'image/gif',
            'doc': 'application/msword',
            'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            'xls': 'application/vnd.ms-excel',
            'xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        }

        return mime_map.get(ext, 'application/octet-stream')

    def get_queue_stats(self) -> Dict[str, int]:
        """ã‚­ãƒ¥ãƒ¼ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        try:
            # å„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ä»¶æ•°ã‚’é›†è¨ˆ
            stats = {}
            statuses = ['pending', 'processing', 'completed', 'failed', 'skipped']

            for status in statuses:
                result = self.db.client.table('document_reprocessing_queue').select(
                    '*', count='exact'
                ).eq('status', status).execute()
                stats[status] = result.count if result.count else 0

            stats['total'] = sum(stats.values())

            return stats

        except Exception as e:
            logger.error(f"çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def print_queue_stats(self):
        """ã‚­ãƒ¥ãƒ¼ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        stats = self.get_queue_stats()

        logger.info("\n" + "="*80)
        logger.info("ã‚­ãƒ¥ãƒ¼çµ±è¨ˆ")
        logger.info("="*80)
        logger.info(f"å¾…æ©Ÿä¸­ (pending):   {stats.get('pending', 0):>5}ä»¶")
        logger.info(f"å‡¦ç†ä¸­ (processing): {stats.get('processing', 0):>5}ä»¶")
        logger.info(f"å®Œäº†   (completed):  {stats.get('completed', 0):>5}ä»¶")
        logger.info(f"å¤±æ•—   (failed):     {stats.get('failed', 0):>5}ä»¶")
        logger.info(f"ã‚¹ã‚­ãƒƒãƒ— (skipped):  {stats.get('skipped', 0):>5}ä»¶")
        logger.info("-" * 80)
        logger.info(f"åˆè¨ˆ:                {stats.get('total', 0):>5}ä»¶")
        logger.info("="*80 + "\n")

    async def run(
        self,
        limit: int = 100,
        dry_run: bool = False,
        populate_only: bool = False,
        process_queue_only: bool = False,
        preserve_workspace: bool = True,
        workspace: str = 'all'
    ):
        """
        å†å‡¦ç†ã‚’å®Ÿè¡Œ

        Args:
            limit: å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•°
            dry_run: Trueã®å ´åˆã€å®Ÿéš›ã®å‡¦ç†ã¯è¡Œã‚ãšç¢ºèªã®ã¿
            populate_only: Trueã®å ´åˆã€ã‚­ãƒ¥ãƒ¼è¿½åŠ ã®ã¿ï¼ˆå‡¦ç†ã¯å®Ÿè¡Œã—ãªã„ï¼‰
            process_queue_only: Trueã®å ´åˆã€ã‚­ãƒ¥ãƒ¼å‡¦ç†ã®ã¿ï¼ˆæ–°è¦è¿½åŠ ã—ãªã„ï¼‰
            preserve_workspace: Trueã®å ´åˆã€æ—¢å­˜ã®workspaceã‚’ä¿æŒ
        """
        logger.info("\n" + "="*80)
        logger.info("Google Classroom ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ v2")
        logger.info("="*80)

        if dry_run:
            logger.warning("ğŸ” DRY RUN ãƒ¢ãƒ¼ãƒ‰: å®Ÿéš›ã®å‡¦ç†ã¯è¡Œã„ã¾ã›ã‚“")

        # ã‚­ãƒ¥ãƒ¼ã®ç¾åœ¨ã®çŠ¶æ…‹ã‚’è¡¨ç¤º
        self.print_queue_stats()

        # ã‚­ãƒ¥ãƒ¼ã¸ã®è¿½åŠ 
        if not process_queue_only:
            logger.info(f"\nğŸ“¥ ã‚­ãƒ¥ãƒ¼ã¸ã®è¿½åŠ ã‚’é–‹å§‹...")
            logger.info(f"  å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹: {workspace}")
            logger.info(f"  æœ€å¤§ä»¶æ•°: {limit}")
            logger.info(f"  Workspaceä¿æŒ: {preserve_workspace}")

            if not dry_run:
                added = self.populate_queue_from_workspace(
                    workspace=workspace,
                    limit=limit,
                    preserve_workspace=preserve_workspace
                )
                logger.info(f"âœ… {added}ä»¶ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã—ã¾ã—ãŸ")

                # æ›´æ–°å¾Œã®çµ±è¨ˆã‚’è¡¨ç¤º
                self.print_queue_stats()

        # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å‡¦ç†
        if not populate_only:
            if dry_run:
                logger.info(f"\nğŸ” DRY RUN: {limit}ä»¶ã®å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ")
            else:
                logger.info(f"\nâš™ï¸  ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ã®å‡¦ç†ã‚’é–‹å§‹...")

                # ç¢ºèª
                print("\nå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/N): ", end='')
                response = input().strip().lower()
                if response != 'y':
                    logger.info("å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
                    return

                # å‡¦ç†å®Ÿè¡Œ
                stats = await self.process_queue(limit=limit)

                # æœ€çµ‚çµæœ
                logger.info("\n" + "="*80)
                logger.info("å†å‡¦ç†å®Œäº†")
                logger.info("="*80)
                logger.info(f"æˆåŠŸ: {stats['success']}ä»¶")
                logger.info(f"å¤±æ•—: {stats['failed']}ä»¶")
                logger.info(f"åˆè¨ˆ: {stats['total']}ä»¶")
                logger.info("="*80)

                # æœ€çµ‚çš„ãªã‚­ãƒ¥ãƒ¼ã®çŠ¶æ…‹ã‚’è¡¨ç¤º
                self.print_queue_stats()


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‘ãƒ¼ã‚¹
    dry_run = '--dry-run' in sys.argv or '-n' in sys.argv
    populate_only = '--populate-only' in sys.argv
    process_queue_only = '--process-queue' in sys.argv
    preserve_workspace = '--no-preserve-workspace' not in sys.argv
    limit = 100
    workspace = 'all'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹

    # --limit ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®å‡¦ç†
    for arg in sys.argv:
        if arg.startswith('--limit='):
            try:
                limit = int(arg.split('=')[1])
            except:
                pass
        elif arg.startswith('--workspace='):
            workspace = arg.split('=')[1]

    reprocessor = ClassroomReprocessorV2()
    await reprocessor.run(
        limit=limit,
        dry_run=dry_run,
        populate_only=populate_only,
        process_queue_only=process_queue_only,
        preserve_workspace=preserve_workspace,
        workspace=workspace
    )


if __name__ == "__main__":
    asyncio.run(main())
