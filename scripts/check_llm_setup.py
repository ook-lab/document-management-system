"""
åˆæœŸå‹•ä½œç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ (LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ)
- å„AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¸ã®æ¥ç¶šã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç¢ºèªã™ã‚‹
"""
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.ai.llm_client import LLMClient

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

def check_ai_connection():
    """å…¨AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¸ã®æ¥ç¶šã‚’ãƒã‚§ãƒƒã‚¯"""
    print("=" * 50)
    print("ğŸš€ LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)

    try:
        # LLMClientã‚’åˆæœŸåŒ–ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€ï¼‰
        client = LLMClient()
    except Exception as e:
        print(f"âŒ LLMClientåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # --- 1. Gemini (Stage 1) ãƒ†ã‚¹ãƒˆ ---
    print("\n--- 1. Gemini (Stage 1 Classifier) ---")
    try:
        # NOTE: ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ã§ã®ç°¡å˜ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§æ¥ç¶šç¢ºèª
        prompt = "ã“ã®æ–‡æ›¸ã®doc_typeã‚’'timetable'ã¾ãŸã¯'other'ã®ã©ã¡ã‚‰ã‹ã«åˆ†é¡ã—ã€JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„: \"æ˜æ—¥ã®æ•°å­¦ã®ãƒ†ã‚¹ãƒˆç¯„å›²\""
        
        response = client.call_model(
            tier="stage1_classification", 
            prompt=prompt
        )
        
        if response.get("success"):
            print(f"âœ… Geminiæ¥ç¶šæˆåŠŸ (ãƒ¢ãƒ‡ãƒ«: {response['model']})")
            print(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹æŠœç²‹: {response['content'][:50]}...")
        else:
            print(f"âŒ Geminiæ¥ç¶šå¤±æ•—: {response['error']}")
    except Exception as e:
        print(f"âŒ Geminiãƒ†ã‚¹ãƒˆä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}")

    # --- 2. Claude (Stage 2) ãƒ†ã‚¹ãƒˆ ---
    print("\n--- 2. Claude (Stage 2 Extractor) ---")
    try:
        prompt = "ä»¥ä¸‹ã®å†…å®¹ã‹ã‚‰æ—¥ä»˜ã¨å ´æ‰€ã‚’JSONã§æŠ½å‡ºã—ã¦ãã ã•ã„: ä¼šè­°ã¯2025å¹´1æœˆ10æ—¥ã«æ±äº¬æœ¬ç¤¾ã§è¡Œã‚ã‚Œã¾ã™ã€‚"
        
        response = client.call_model(
            tier="stage2_extraction", 
            prompt=prompt
        )
        
        if response.get("success"):
            print(f"âœ… Claudeæ¥ç¶šæˆåŠŸ (ãƒ¢ãƒ‡ãƒ«: {response['model']})")
            print(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹æŠœç²‹: {response['content'][:50]}...")
        else:
            print(f"âŒ Claudeæ¥ç¶šå¤±æ•—: {response['error']}")
    except Exception as e:
        print(f"âŒ Claudeãƒ†ã‚¹ãƒˆä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}")
        
    # --- 3. OpenAI (UI/Embedding) ãƒ†ã‚¹ãƒˆ ---
    print("\n--- 3. OpenAI (UI Responder/Embedding) ---")
    
    # 3a. UI Responderãƒ†ã‚¹ãƒˆ
    try:
        prompt = "ä»¥ä¸‹ã®æƒ…å ±ã‚’è¦ªåˆ‡ãªæ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„: å±±ç”°å¤ªéƒã¯æ˜æ—¥ã€ç†ç§‘ã®ãƒ†ã‚¹ãƒˆã‚’åˆå¾Œ1æ™‚ã«å—ã‘ã¾ã™ã€‚"
        
        response = client.call_model(
            tier="ui_response", 
            prompt=prompt
        )
        
        if response.get("success"):
            print(f"âœ… GPT (UI) æ¥ç¶šæˆåŠŸ (ãƒ¢ãƒ‡ãƒ«: {response['model']})")
            print(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹æŠœç²‹: {response['content'][:50]}...")
        else:
            print(f"âŒ GPT (UI) æ¥ç¶šå¤±æ•—: {response['error']}")
    except Exception as e:
        print(f"âŒ GPT (UI) ãƒ†ã‚¹ãƒˆä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}")

    # 3b. Embeddingãƒ†ã‚¹ãƒˆ
    try:
        embedding = client.generate_embedding("ãƒ†ã‚¹ãƒˆç”¨ã®æ–‡æ›¸ã‚’åŸ‹ã‚è¾¼ã¿åŒ–ã—ã¾ã™ã€‚")
        if embedding and len(embedding) == 1536: # text-embedding-3-smallã®æ¬¡å…ƒæ•°
            print(f"âœ… Embeddingç”ŸæˆæˆåŠŸ (æ¬¡å…ƒæ•°: {len(embedding)})")
        else:
            print(f"âŒ Embeddingç”Ÿæˆå¤±æ•— (æ¬¡å…ƒæ•°: {len(embedding)})")
    except Exception as e:
        print(f"âŒ Embeddingãƒ†ã‚¹ãƒˆä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}")
        
    print("\n=" * 50)
    print("ãƒ†ã‚¹ãƒˆå®Œäº†ã€‚âŒ ãŒã‚ã‚‹å ´åˆã¯.envã¨Secretsã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    print("=" * 50)

if __name__ == "__main__":
    check_ai_connection()