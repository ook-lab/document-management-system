"""
æ—¥æ¬¡åŒæœŸã‚¹ã‚¯ãƒªãƒ—ãƒˆ (v4.0: GitHub Actionså¯¾å¿œç‰ˆ)

è¨­è¨ˆæ›¸: FINAL_UNIFIED_COMPLETE_v4.md ã® 7.2ç¯€ ãŠã‚ˆã³ AUTO_INBOX_COMPLETE_v3.0.md ã® 6.5ç¯€ã«æº–æ‹ 
ç›®çš„: Google Driveã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œçŸ¥ã—ã€TwoStageIngestionPipelineã‚’å®Ÿè¡Œã™ã‚‹ã€‚
"""

import os
import sys
import asyncio
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
from loguru import logger
import argparse
import traceback

# ãƒ‘ã‚¹è¨­å®š
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.connectors.google_drive import GoogleDriveConnector
from pipelines.two_stage_ingestion import TwoStageIngestionPipeline

# ãƒ­ã‚°è¨­å®š (loguruã‚’ä½¿ç”¨)
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)

logger.add(log_dir / f'daily_sync_{datetime.now():%Y%m%d}.log', rotation="10 MB", level="INFO")
logger.add(sys.stdout, level="INFO")


class DailySyncProcessor:
    """æ—¥æ¬¡åŒæœŸå‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, workspace_folders: Dict[str, str]):
        
        self.workspace_folders = workspace_folders
        self.drive = GoogleDriveConnector()
        self.pipeline = TwoStageIngestionPipeline()
        
    def _scan_folder(self, folder_id: str) -> List[Dict[str, Any]]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€å†…ã®æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã™ã‚‹
        
        Phase 1Aã§ã¯ã€InBoxæ–¹å¼ã§ã¯ãªãç‰¹å®šã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã™ã‚‹ã“ã¨ã‚’æƒ³å®šã™ã‚‹ã€‚
        """
        logger.info(f"ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€ID [{folder_id}] ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­...")
        
        # Google Drive Connectorã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
        # ç¾çŠ¶ã¯ã€å˜ç´”ã«ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã™ã‚‹
        files = self.drive.list_files_in_folder(folder_id)
        
        # å®Ÿéš›ã«ã¯ã“ã“ã§ DB ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€æ—¢ã«å‡¦ç†æ¸ˆã¿ã® source_id ã‚’æŒã¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ã™ã‚‹
        
        logger.info(f"âœ… {len(files)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")
        return files

    async def run_sync(self):
        """åŒæœŸå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        logger.info("=" * 60)
        logger.info("è‡ªå‹•æ—¥æ¬¡åŒæœŸå‡¦ç† é–‹å§‹ (v4.0 Hybrid AI)")
        logger.info("=" * 60)
        
        stats = {
            'total_files': 0,
            'processed_success': 0,
            'processed_failed': 0
        }
        
        for workspace, folder_id in self.workspace_folders.items():
            if not folder_id:
                logger.warning(f"ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ [{workspace}] ã®ãƒ•ã‚©ãƒ«ãƒ€IDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue
                
            files_to_process = self._scan_folder(folder_id)
            
            for file_meta in files_to_process:
                stats['total_files'] += 1
                try:
                    # ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
                    result = await self.pipeline.process_file(file_meta, workspace=workspace)
                    
                    if result:
                        stats['processed_success'] += 1
                    else:
                        stats['processed_failed'] += 1
                        
                except Exception as e:
                    logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {file_meta['name']} - {e}")
                    logger.error(traceback.format_exc())
                    stats['processed_failed'] += 1
        
        logger.info("=" * 60)
        logger.info("è‡ªå‹•æ—¥æ¬¡åŒæœŸå‡¦ç† å®Œäº†ã‚µãƒãƒªãƒ¼")
        logger.info(f"ç·æ¤œå‡ºãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['total_files']}")
        logger.info(f"å‡¦ç†æˆåŠŸæ•°: {stats['processed_success']}")
        logger.info(f"å‡¦ç†å¤±æ•—æ•°: {stats['processed_failed']}")
        logger.info("=" * 60)
        
        return stats


async def main():
    parser = argparse.ArgumentParser(description='è‡ªå‹•æ—¥æ¬¡åŒæœŸã‚¹ã‚¯ãƒªãƒ—ãƒˆ v4.0')
    parser.add_argument('--business-id', type=str, default=os.getenv('BUSINESS_FOLDER_ID'), help='ãƒ“ã‚¸ãƒã‚¹ç”¨ãƒ•ã‚©ãƒ«ãƒ€ID')
    parser.add_argument('--personal-id', type=str, default=os.getenv('PERSONAL_FOLDER_ID'), help='å€‹äººç”¨ãƒ•ã‚©ãƒ«ãƒ€ID')
    
    args = parser.parse_args()

    # å‡¦ç†å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€ã®å®šç¾© (Phase 1Aã§ã¯ã€PROGRESS_TRACKER.mdã«åŸºã¥ãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¨­å®šã—ãŸç‰¹å®šã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½¿ç”¨)
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ Phase 1A ã§ä½¿ç”¨ã™ã‚‹ç‰¹å®šã®ãƒ•ã‚©ãƒ«ãƒ€IDã‚’ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯å¼•æ•°ã§æ¸¡ã™ã“ã¨ã‚’æƒ³å®š
    workspace_folders = {
        'personal': args.personal_id,
        'business': args.business_id 
    }
    
    # ãƒ•ã‚©ãƒ«ãƒ€IDãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼çµ‚äº†
    if not args.personal_id and not args.business_id:
        logger.error("å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚©ãƒ«ãƒ€ID (PERSONAL_FOLDER_ID ã¾ãŸã¯ BUSINESS_FOLDER_ID) ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)

    processor = DailySyncProcessor(workspace_folders)
    stats = await processor.run_sync()

    # çµ‚äº†ã‚³ãƒ¼ãƒ‰æ±ºå®š
    if stats['total_files'] == 0:
        logger.info("å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒ0ä»¶ã§ã—ãŸã€‚")
        sys.exit(3)

    failure_rate = stats['processed_failed'] / stats['total_files']

    if failure_rate >= 0.5:
        logger.error(f"å¤±æ•—ç‡ãŒé«˜ã™ãã¾ã™ ({failure_rate:.1%})ã€‚ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã®å•é¡Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        sys.exit(1)
    elif stats['processed_failed'] > 0:
        logger.warning(f"{stats['processed_failed']}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(2)
    else:
        logger.info("å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
        sys.exit(0)

if __name__ == "__main__":
    asyncio.run(main())