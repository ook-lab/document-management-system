#!/usr/bin/env python3
"""10ä»¶ãšã¤å†å‡¦ç†ï¼ˆpendingã‹ã‚‰å–å¾—ï¼‰"""
import sys
import asyncio
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from loguru import logger
from A_common.database.client import DatabaseClient
from C_ai_common.llm_client.llm_client import LLMClient
from G_unified_pipeline.pipeline import UnifiedDocumentPipeline
from A_common.connectors.google_drive import GoogleDriveConnector

async def main():
    logger.info("=== 10ä»¶ãƒãƒƒãƒå†å‡¦ç† ===")
    
    db = DatabaseClient(use_service_role=True)
    llm_client = LLMClient()
    pipeline = UnifiedDocumentPipeline(llm_client=llm_client)
    drive = GoogleDriveConnector()
    
    # pending ã®10ä»¶ã‚’å–å¾—
    query = db.client.table('Rawdata_FILE_AND_MAIL').select(
        'id, file_name, source_id, mime_type, workspace, doc_type'
    ).eq('processing_status', 'pending').limit(10)
    
    result = query.execute()
    docs = result.data
    
    logger.info(f"å‡¦ç†å¯¾è±¡: {len(docs)}ä»¶")
    
    if not docs:
        logger.info("âœ… pending ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    success = 0
    failed = 0
    
    for i, doc in enumerate(docs, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"[{i}/{len(docs)}] {doc['file_name']}")
        logger.info(f"{'='*80}")
        logger.info(f"  source_id: {doc['source_id']}")
        logger.info(f"  workspace: {doc.get('workspace')}, doc_type: {doc.get('doc_type')}")
        
        try:
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            temp_dir = Path("temp/reimport")
            temp_dir.mkdir(parents=True, exist_ok=True)
            temp_file = temp_dir / doc['file_name']
            
            logger.info("  ğŸ“¥ Drive ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            drive.download_file(doc['source_id'], str(temp_file))
            logger.info(f"  âœ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {temp_file.stat().st_size:,} bytes")
            
            # å†å‡¦ç†
            logger.info("  ğŸ”„ Eâ†’K ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œä¸­...")
            process_result = await pipeline.process_document(
                file_path=temp_file,
                file_name=doc['file_name'],
                doc_type=doc.get('doc_type', 'classroom'),
                workspace=doc.get('workspace', 'default'),
                mime_type=doc.get('mime_type', 'application/pdf'),
                source_id=doc['source_id'],
                existing_document_id=doc['id']
            )
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            if temp_file.exists():
                temp_file.unlink()
            
            if process_result.get('success'):
                logger.info(f"  âœ… æˆåŠŸ")
                success += 1
            else:
                logger.error(f"  âŒ å¤±æ•—: {process_result.get('error')}")
                failed += 1
                
        except Exception as e:
            logger.error(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            failed += 1
    
    logger.info(f"\n{'='*80}")
    logger.info(f"âœ… ãƒãƒƒãƒå®Œäº†: æˆåŠŸ{success}ä»¶ / å¤±æ•—{failed}ä»¶")
    logger.info(f"{'='*80}")

if __name__ == "__main__":
    asyncio.run(main())
