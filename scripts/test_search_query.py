#!/usr/bin/env python3
"""
æ¤œç´¢ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§è³ªå•ã‚’å—ã‘å–ã‚Šã€æ¤œç´¢â†’å›ç­”ç”Ÿæˆã‚’å®Ÿè¡Œã—ã¦çµæœã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
"""
import sys
import os
import asyncio
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from core.database.client import DatabaseClient
from core.ai.llm_client import LLMClient


def format_metadata(metadata: dict, indent: int = 0) -> str:
    """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã‚„ã™ãæ•´å½¢"""
    if not metadata:
        return ""

    lines = []
    prefix = "  " * indent

    for key, value in metadata.items():
        if isinstance(value, dict):
            lines.append(f"{prefix}{key}:")
            lines.append(format_metadata(value, indent + 1))
        elif isinstance(value, list):
            if not value:
                continue
            lines.append(f"{prefix}{key}:")
            for item in value:
                if isinstance(item, dict):
                    for sub_key, sub_value in item.items():
                        lines.append(f"{prefix}  - {sub_key}: {sub_value}")
                else:
                    lines.append(f"{prefix}  - {item}")
        else:
            lines.append(f"{prefix}{key}: {value}")

    return "\n".join(lines)


def build_context(documents: list) -> str:
    """æ¤œç´¢çµæœã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
    if not documents:
        return "é–¢é€£ã™ã‚‹æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

    context_parts = []
    for idx, doc in enumerate(documents, 1):
        file_name = doc.get('file_name', 'ç„¡é¡Œ')
        doc_type = doc.get('doc_type', 'ä¸æ˜')
        summary = doc.get('summary', '')
        similarity = doc.get('similarity', 0)
        metadata = doc.get('metadata', {})

        # åŸºæœ¬æƒ…å ±
        context_part = f"""
ã€æ–‡æ›¸{idx}ã€‘
ãƒ•ã‚¡ã‚¤ãƒ«å: {file_name}
æ–‡æ›¸ã‚¿ã‚¤ãƒ—: {doc_type}
é¡ä¼¼åº¦: {similarity:.2f}"""

        # ã‚µãƒãƒªãƒ¼è¿½åŠ 
        if summary:
            context_part += f"\nè¦ç´„: {summary}"

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢ã—ã¦è¿½åŠ 
        if metadata:
            formatted_metadata = format_metadata(metadata)
            if formatted_metadata:
                context_part += f"\n\nè©³ç´°æƒ…å ±:\n{formatted_metadata}"

        context_parts.append(context_part)

    return "\n".join(context_parts)


async def search_and_answer(query: str, workspace: str = None, limit: int = 50):
    """æ¤œç´¢â†’å›ç­”ç”Ÿæˆã‚’å®Ÿè¡Œ"""

    print(f"ğŸ” è³ªå•: {query}")
    print("=" * 80)

    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    db_client = DatabaseClient()
    llm_client = LLMClient()

    try:
        # 1. Embeddingã‚’ç”Ÿæˆ
        print("\nğŸ“Š Embeddingç”Ÿæˆä¸­...")
        embedding = llm_client.generate_embedding(query)
        print(f"âœ… Embeddingç”Ÿæˆå®Œäº† (æ¬¡å…ƒæ•°: {len(embedding)})")

        # 2. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ
        print(f"\nğŸ” ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ä¸­... (limit={limit})")
        results = await db_client.search_documents(query, embedding, limit, workspace)
        print(f"âœ… {len(results)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

        if not results:
            print("\nâš ï¸  é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        # 3. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        print("\nğŸ“ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ä¸­...")
        context = build_context(results)

        print("\n" + "=" * 80)
        print("ğŸ“„ æ¤œç´¢çµæœã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:")
        print("=" * 80)
        print(context)

        # 4. å›ç­”ç”Ÿæˆ
        print("\n" + "=" * 80)
        print("ğŸ’¬ å›ç­”ç”Ÿæˆä¸­...")
        print("=" * 80)

        prompt = f"""ä»¥ä¸‹ã®æ–‡æ›¸æƒ…å ±ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€è³ªå•ã€‘
{query}

ã€å‚è€ƒæ–‡æ›¸ã€‘
{context}

ã€å›ç­”ã®æ¡ä»¶ã€‘
- å‚è€ƒæ–‡æ›¸ã®æƒ…å ±ã‚’åŸºã«ã€æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„
- æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€ãã®æ—¨ã‚’ä¼ãˆã¦ãã ã•ã„
- ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„
- å›ç­”ã®æœ€å¾Œã«ã€å‚è€ƒã«ã—ãŸæ–‡æ›¸ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’åˆ—æŒ™ã—ã¦ãã ã•ã„

ã€å›ç­”ã€‘
"""

        response = llm_client.call_model(
            tier="ui_response",
            prompt=prompt
        )

        if not response.get('success'):
            print(f"âŒ å›ç­”ç”Ÿæˆã«å¤±æ•—: {response.get('error')}")
            return

        print(f"\nâœ… å›ç­”ç”Ÿæˆå®Œäº† (ãƒ¢ãƒ‡ãƒ«: {response.get('model')}, ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {response.get('provider')})")
        print("\n" + "=" * 80)
        print("ğŸ¤– ç”Ÿæˆã•ã‚ŒãŸå›ç­”:")
        print("=" * 80)
        print(response.get('content', ''))
        print("\n" + "=" * 80)

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python scripts/test_search_query.py <è³ªå•>")
        print("ä¾‹: python scripts/test_search_query.py 'é»’å§«ç§»å‹•æ•™å®¤ã®æŒã¡ç‰©ã¯ï¼Ÿ'")
        sys.exit(1)

    query = sys.argv[1]
    workspace = sys.argv[2] if len(sys.argv) > 2 else None

    # éåŒæœŸé–¢æ•°ã‚’å®Ÿè¡Œ
    asyncio.run(search_and_answer(query, workspace))


if __name__ == "__main__":
    main()
