#!/usr/bin/env python3
"""
ä¸­åˆ†é¡ï¼å°åˆ†é¡ã®å•†å“ã‚’é©åˆ‡ãªå°åˆ†é¡ã«è‡ªå‹•æŒ¯ã‚Šåˆ†ã‘
Gemini AIã‚’ä½¿ç”¨ã—ã¦é©åˆ‡ãªåˆ†é¡ã‚’ææ¡ˆã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
"""
import sys
from pathlib import Path
import time
import json
import os
from collections import defaultdict

root_dir = Path(__file__).resolve().parent
sys.path.insert(0, str(root_dir))

from shared.common.database.client import DatabaseClient
import logging
import google.generativeai as genai
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Geminiè¨­å®š
genai.configure(api_key=os.getenv('GOOGLE_AI_API_KEY'))
model = genai.GenerativeModel('gemini-2.0-flash-exp')

def get_duplicate_categories(db):
    """ä¸­åˆ†é¡ï¼å°åˆ†é¡ã®ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—"""
    logger.info("ä¸­åˆ†é¡ï¼å°åˆ†é¡ã®ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—ä¸­...")

    all_categories = []
    offset = 0

    while True:
        result = db.client.table('MASTER_Categories_product')\
            .select('id', 'name', 'large_category', 'medium_category', 'small_category')\
            .range(offset, offset + 999)\
            .execute()

        if not result.data:
            break

        all_categories.extend(result.data)
        offset += 1000

        if len(result.data) < 1000:
            break

    # ä¸­åˆ†é¡ï¼å°åˆ†é¡ã‚’æŠ½å‡º
    duplicate_cats = []
    for cat in all_categories:
        medium = cat.get('medium_category', '')
        small = cat.get('small_category', '')

        if medium and small and medium == small:
            # å•†å“æ•°ã‚’ç¢ºèª
            count_result = db.client.table('Rawdata_NETSUPER_items')\
                .select('id', count='exact')\
                .eq('category_id', cat['id'])\
                .execute()
            product_count = count_result.count if count_result.count else 0

            if product_count > 0:
                cat['product_count'] = product_count
                duplicate_cats.append(cat)

    logger.info(f"å•†å“ãŒã‚ã‚‹ä¸­åˆ†é¡ï¼å°åˆ†é¡ã®ã‚«ãƒ†ã‚´ãƒª: {len(duplicate_cats)}ä»¶")
    return duplicate_cats, all_categories

def get_available_small_categories(all_categories, medium_category):
    """æŒ‡å®šã—ãŸä¸­åˆ†é¡ã§åˆ©ç”¨å¯èƒ½ãªåˆ¥ã®å°åˆ†é¡ã‚’å–å¾—"""
    available = []
    for cat in all_categories:
        if (cat.get('medium_category') == medium_category and
            cat.get('small_category') != medium_category):
            small = cat.get('small_category')
            if small and small not in available:
                available.append(small)
    return sorted(available)

def get_products_for_category(db, category_id, limit=50):
    """æŒ‡å®šã‚«ãƒ†ã‚´ãƒªã®å•†å“ã‚’å–å¾—"""
    result = db.client.table('Rawdata_NETSUPER_items')\
        .select('id', 'product_name', 'general_name')\
        .eq('category_id', category_id)\
        .limit(limit)\
        .execute()
    return result.data

def classify_with_gemini(products, medium_category, available_small_categories):
    """Geminiã§å•†å“ã‚’åˆ†é¡"""

    products_text = "\n".join([
        f"{i+1}. {p['product_name']} (ä¸€èˆ¬åè©: {p.get('general_name', 'ãªã—')})"
        for i, p in enumerate(products)
    ])

    categories_text = "\n".join([f"- {cat}" for cat in available_small_categories]) if available_small_categories else "ï¼ˆåˆ©ç”¨å¯èƒ½ãªå°åˆ†é¡ãªã—ï¼‰"

    prompt = f"""ä»¥ä¸‹ã®å•†å“ã¯ã€ä¸­åˆ†é¡ã€Œ{medium_category}ã€ã§å°åˆ†é¡ã‚‚ã€Œ{medium_category}ã€ã«ãªã£ã¦ã„ã¾ã™ã€‚
ã“ã‚Œã¯ä¸é©åˆ‡ãªã®ã§ã€ã‚ˆã‚Šå…·ä½“çš„ãªå°åˆ†é¡ã«æŒ¯ã‚Šåˆ†ã‘ã¦ãã ã•ã„ã€‚

åˆ©ç”¨å¯èƒ½ãªæ—¢å­˜ã®å°åˆ†é¡:
{categories_text}

å•†å“ãƒªã‚¹ãƒˆ:
{products_text}

å„å•†å“ã«ã¤ã„ã¦:
1. æ—¢å­˜ã®å°åˆ†é¡ã§é©åˆ‡ãªã‚‚ã®ãŒã‚ã‚Œã°ãã‚Œã‚’é¸æŠ
2. ãªã‘ã‚Œã°æ–°ã—ã„é©åˆ‡ãªå°åˆ†é¡åã‚’ææ¡ˆï¼ˆå¿…ãšä¸­åˆ†é¡ã€Œ{medium_category}ã€ã®ä¸‹ä½æ¦‚å¿µã¨ã—ã¦é©åˆ‡ãªã‚‚ã®ã«ã™ã‚‹ï¼‰

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”:
{{
  "classifications": [
    {{
      "product_name": "å•†å“å",
      "small_category": "å°åˆ†é¡å",
      "is_new": false
    }}
  ]
}}

å¿…ãšæœ‰åŠ¹ãªJSONã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"""

    try:
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.2,
                max_output_tokens=8000,
            )
        )

        result_text = response.text.strip()

        # JSONã‚’æŠ½å‡º
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0].strip()
        elif "```" in result_text:
            result_text = result_text.split("```")[1].split("```")[0].strip()

        result = json.loads(result_text)
        return result['classifications']

    except Exception as e:
        logger.error(f"Geminiåˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
        if 'response' in locals():
            logger.error(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}")
        return None

def get_or_create_category(db, large_category, medium_category, small_category):
    """ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—ã¾ãŸã¯ä½œæˆ"""
    category_name = f"{large_category}>{medium_category}>{small_category}"

    # æ—¢å­˜ã‚«ãƒ†ã‚´ãƒªã‚’æ¤œç´¢
    result = db.client.table('MASTER_Categories_product')\
        .select('id')\
        .eq('name', category_name)\
        .execute()

    if result.data:
        return result.data[0]['id']

    # æ–°è¦ä½œæˆ
    new_cat = {
        'name': category_name,
        'large_category': large_category,
        'medium_category': medium_category,
        'small_category': small_category,
        'parent_id': None
    }

    result = db.client.table('MASTER_Categories_product').insert(new_cat).execute()

    if result.data:
        logger.info(f"âœ… æ–°è¦ã‚«ãƒ†ã‚´ãƒªä½œæˆ: {category_name}")
        return result.data[0]['id']

    raise Exception(f"ã‚«ãƒ†ã‚´ãƒªä½œæˆå¤±æ•—: {category_name}")

def main():
    db = DatabaseClient(use_service_role=True)

    # 1. é‡è¤‡ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—
    duplicate_cats, all_categories = get_duplicate_categories(db)

    if not duplicate_cats:
        logger.info("âœ… ä¸­åˆ†é¡ï¼å°åˆ†é¡ã®ã‚«ãƒ†ã‚´ãƒªã¯ã‚ã‚Šã¾ã›ã‚“")
        return

    logger.info(f"\nå‡¦ç†å¯¾è±¡: {len(duplicate_cats)}ã‚«ãƒ†ã‚´ãƒª")
    for cat in duplicate_cats[:10]:
        logger.info(f"  {cat['name']}: {cat['product_count']}ä»¶")

    # çµ±è¨ˆ
    total_products = sum(cat['product_count'] for cat in duplicate_cats)
    logger.info(f"\nç·å•†å“æ•°: {total_products}ä»¶")

    # 2. ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«å‡¦ç†
    updated_count = 0
    error_count = 0

    for cat_idx, dup_cat in enumerate(duplicate_cats, 1):
        medium_category = dup_cat['medium_category']
        large_category = dup_cat['large_category']
        category_id = dup_cat['id']
        product_count = dup_cat['product_count']

        logger.info(f"\n{'='*80}")
        logger.info(f"[{cat_idx}/{len(duplicate_cats)}] {dup_cat['name']} ({product_count}ä»¶)")
        logger.info(f"{'='*80}")

        # åˆ©ç”¨å¯èƒ½ãªå°åˆ†é¡ã‚’å–å¾—
        available_small = get_available_small_categories(all_categories, medium_category)
        logger.info(f"æ—¢å­˜å°åˆ†é¡: {len(available_small)}å€‹")
        if available_small:
            logger.info(f"  {', '.join(available_small[:5])}" + (f" ... ä»–{len(available_small)-5}å€‹" if len(available_small) > 5 else ""))

        # å•†å“ã‚’å–å¾—ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
        batch_size = 30
        offset = 0

        while offset < product_count:
            products = db.client.table('Rawdata_NETSUPER_items')\
                .select('id', 'product_name', 'general_name')\
                .eq('category_id', category_id)\
                .range(offset, offset + batch_size - 1)\
                .execute()

            if not products.data:
                break

            logger.info(f"\n  ãƒãƒƒãƒ {offset+1}~{min(offset+batch_size, product_count)}/{product_count}")

            # Geminiã§åˆ†é¡ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã§6ç§’å¾…æ©Ÿï¼‰
            time.sleep(6)
            classifications = classify_with_gemini(products.data, medium_category, available_small)

            if not classifications:
                logger.error("  åˆ†é¡å¤±æ•—ã€ã‚¹ã‚­ãƒƒãƒ—")
                offset += batch_size
                error_count += len(products.data)
                continue

            # å„å•†å“ã‚’æ›´æ–°
            for i, classification in enumerate(classifications):
                if i >= len(products.data):
                    break

                product = products.data[i]
                new_small_category = classification.get('small_category', medium_category)
                is_new = classification.get('is_new', False)

                # ã‚«ãƒ†ã‚´ãƒªIDã‚’å–å¾—/ä½œæˆ
                try:
                    new_category_id = get_or_create_category(
                        db, large_category, medium_category, new_small_category
                    )

                    # å•†å“ã‚’æ›´æ–°
                    db.client.table('Rawdata_NETSUPER_items').update({
                        'category_id': new_category_id,
                        'small_category': new_small_category
                    }).eq('id', product['id']).execute()

                    updated_count += 1

                    marker = "ğŸ†•" if is_new else "âœ…"
                    logger.info(f"    {marker} {product['product_name'][:40]} â†’ {new_small_category}")

                except Exception as e:
                    logger.error(f"    âŒ æ›´æ–°ã‚¨ãƒ©ãƒ¼: {product['product_name'][:40]} - {e}")
                    error_count += 1

            offset += batch_size

    logger.info(f"\n{'='*80}")
    logger.info(f"å®Œäº†")
    logger.info(f"æ›´æ–°æˆåŠŸ: {updated_count}ä»¶")
    logger.info(f"ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
    logger.info(f"{'='*80}")

if __name__ == "__main__":
    # âš ï¸ å®‰å…¨ã‚¬ãƒ¼ãƒ‰ï¼šèª¤å®Ÿè¡Œé˜²æ­¢
    print("\n" + "="*70)
    print("âš ï¸  è­¦å‘Š: ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯é«˜é¡ãªAPIæ–™é‡‘ãŒç™ºç”Ÿã—ã¾ã™")
    print("âš ï¸  å‡¦ç†å†…å®¹: ä¸­åˆ†é¡=å°åˆ†é¡ã®å•†å“ã‚’30ä»¶ãƒãƒƒãƒã§Geminiåˆ†é¡")
    print("âš ï¸  max_output_tokens: 8000ãƒˆãƒ¼ã‚¯ãƒ³/ãƒãƒƒãƒ")
    print("âš ï¸  æ¨å®šã‚³ã‚¹ãƒˆ: å•†å“æ•°ã«ã‚ˆã‚Šå¤‰å‹•ï¼ˆ100ä»¶ã§ç´„50-100å††ï¼‰")
    print("="*70)
    confirm = input("\næœ¬å½“ã«å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (YES ã¨å¤§æ–‡å­—ã§å…¥åŠ›): ")
    if confirm != "YES":
        print("âŒ å®Ÿè¡Œã‚’ä¸­æ­¢ã—ã¾ã—ãŸ")
        sys.exit(0)
    print("\nâœ… å®Ÿè¡Œã‚’é–‹å§‹ã—ã¾ã™...\n")

    try:
        main()
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
