"""
æ¥½å¤©è¥¿å‹ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼ã®å®Ÿéš›ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼æƒ…å ±ã‚’å–å¾—

å®Ÿè¡Œæ–¹æ³•:
    python discover_categories.py
"""

import asyncio
import os
import sys
import json
import logging
from pathlib import Path
from dotenv import load_dotenv

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
root_dir = Path(__file__).parent
sys.path.insert(0, str(root_dir))

from B_ingestion.rakuten_seiyu.rakuten_seiyu_scraper_playwright import RakutenSeiyuScraperPlaywright

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def discover_categories():
    """å®Ÿéš›ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼æƒ…å ±ã‚’å–å¾—"""

    rakuten_id = os.getenv("RAKUTEN_ID")
    password = os.getenv("RAKUTEN_PASSWORD")

    if not rakuten_id or not password:
        logger.error("âŒ ç’°å¢ƒå¤‰æ•° RAKUTEN_ID ã¨ RAKUTEN_PASSWORD ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³
    async with RakutenSeiyuScraperPlaywright() as scraper:
        await scraper.start(headless=False)

        # ãƒ­ã‚°ã‚¤ãƒ³
        logger.info("ğŸ” ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        success = await scraper.login(rakuten_id, password)
        if not success:
            logger.error("âŒ ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return

        page = scraper.page

        # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«æˆ»ã‚‹
        logger.info("ğŸŒ ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
        await page.goto("https://netsuper.rakuten.co.jp/seiyu", wait_until="domcontentloaded")
        await page.wait_for_timeout(3000)

        # ãƒšãƒ¼ã‚¸ã®HTMLã‚’ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        html_content = await page.content()
        with open("homepage_debug.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        logger.info("ğŸ“„ ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸HTMLã‚’ä¿å­˜: homepage_debug.html")

        # ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        logger.info("ğŸ” ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒªãƒ³ã‚¯ã‚’æ¢ç´¢ä¸­...")

        # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        category_selectors = [
            'a[href*="/category/"]',
            'nav a[href*="/seiyu/"]',
            '.category-link',
            '[class*="category"] a',
        ]

        all_categories = []

        for selector in category_selectors:
            try:
                links = await page.query_selector_all(selector)
                logger.info(f"ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ '{selector}': {len(links)}ä»¶ã®ãƒªãƒ³ã‚¯ç™ºè¦‹")

                for link in links:
                    try:
                        href = await link.get_attribute('href')
                        text = await link.inner_text()

                        # ç©ºã®ãƒªãƒ³ã‚¯ã‚„JavaScriptãƒªãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        if not href or href.startswith('javascript:') or not text:
                            continue

                        # ã‚«ãƒ†ã‚´ãƒªãƒ¼IDã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
                        category_id = None
                        if '/search/' in href:
                            parts = href.split('/search/')
                            if len(parts) > 1:
                                category_id = parts[1].split('?')[0].split('/')[0]
                        elif '/category/' in href:
                            parts = href.split('/category/')
                            if len(parts) > 1:
                                category_id = parts[1].split('?')[0].split('/')[0]
                        elif '/c/' in href:
                            parts = href.split('/c/')
                            if len(parts) > 1:
                                category_id = parts[1].split('?')[0].split('/')[0]

                        # å®Œå…¨ãªURLã‚’æ§‹ç¯‰
                        full_url = href if href.startswith('http') else f"https://netsuper.rakuten.co.jp{href}"

                        category_info = {
                            'name': text.strip(),
                            'url': full_url,
                            'category_id': category_id,
                            'selector': selector
                        }

                        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆURLã§åˆ¤å®šï¼‰
                        if not any(cat['url'] == full_url for cat in all_categories):
                            all_categories.append(category_info)

                    except Exception as e:
                        continue

            except Exception as e:
                logger.debug(f"ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ '{selector}' ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        # çµæœã‚’è¡¨ç¤º
        logger.info("\n" + "=" * 60)
        logger.info(f"âœ… ç™ºè¦‹ã—ãŸã‚«ãƒ†ã‚´ãƒªãƒ¼æ•°: {len(all_categories)}ä»¶")
        logger.info("=" * 60)

        for i, cat in enumerate(all_categories, 1):
            logger.info(f"{i}. {cat['name']}")
            logger.info(f"   URL: {cat['url']}")
            logger.info(f"   ID: {cat['category_id']}")
            logger.info("")

        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_file = "_runtime/data/categories/discovered_categories.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump({
                "discovered_at": "2025-12-19",
                "total_categories": len(all_categories),
                "categories": all_categories
            }, f, indent=2, ensure_ascii=False)

        logger.info(f"ğŸ’¾ ã‚«ãƒ†ã‚´ãƒªãƒ¼æƒ…å ±ã‚’ä¿å­˜: {output_file}")

        # 10ç§’å¾…æ©Ÿã—ã¦ã‹ã‚‰çµ‚äº†ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç”»é¢ã‚’ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ï¼‰
        logger.info("\n10ç§’å¾Œã«ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã¾ã™...")
        await page.wait_for_timeout(10000)


if __name__ == "__main__":
    asyncio.run(discover_categories())
