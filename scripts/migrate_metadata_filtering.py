"""
æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã®ã‚«ãƒ©ãƒ ã‚’è¿½åŠ ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å®Ÿè¡Œæ–¹æ³•:
    python scripts/migrate_metadata_filtering.py

æ©Ÿèƒ½:
    - æ—¢å­˜ã® documents ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ metadata ã¨ document_date ã‚’èª­ã¿è¾¼ã¿
    - year, month, amount, event_dates, grade_level, school_name ã‚’æŠ½å‡º
    - documents ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
"""
import asyncio
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.database.client import DatabaseClient
from core.utils.metadata_extractor import MetadataExtractor
from loguru import logger

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stdout, format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}", level="INFO")


class MetadataFilteringMigration:
    """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã‚«ãƒ©ãƒ ã®ç§»è¡Œã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.db_client = DatabaseClient()

    async def migrate_all_documents(self, batch_size: int = 10, skip_existing: bool = True):
        """
        å…¨æ–‡æ›¸ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã‚«ãƒ©ãƒ ã‚’è¿½åŠ 

        Args:
            batch_size: ä¸€åº¦ã«å‡¦ç†ã™ã‚‹æ–‡æ›¸æ•°
            skip_existing: æ—¢ã«yearãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹æ–‡æ›¸ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹
        """
        logger.info("==================== ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç§»è¡Œé–‹å§‹ ====================")

        # å‡¦ç†å¯¾è±¡ã®æ–‡æ›¸ã‚’å–å¾—
        documents = self.db_client.get_documents_for_review(limit=1000)

        if not documents:
            logger.warning("å‡¦ç†å¯¾è±¡ã®æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return

        logger.info(f"å‡¦ç†å¯¾è±¡: {len(documents)} ä»¶ã®æ–‡æ›¸")

        success_count = 0
        skip_count = 0
        error_count = 0

        for i, doc in enumerate(documents, 1):
            doc_id = doc.get('id')
            file_name = doc.get('file_name', 'Unknown')

            logger.info(f"\n[{i}/{len(documents)}] å‡¦ç†ä¸­: {file_name}")

            # æ—¢ã«yearãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if skip_existing and doc.get('year') is not None:
                logger.info(f"  â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: æ—¢ã« year={doc.get('year')} ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™")
                skip_count += 1
                continue

            # ç§»è¡Œå®Ÿè¡Œ
            success = await self.migrate_document(doc)

            if success:
                success_count += 1
                logger.info(f"  âœ… ç§»è¡ŒæˆåŠŸ")
            else:
                error_count += 1
                logger.error(f"  âŒ ç§»è¡Œå¤±æ•—")

            # é€²æ—è¡¨ç¤º
            if i % 10 == 0:
                logger.info(f"\n--- é€²æ—: {i}/{len(documents)} å®Œäº† (æˆåŠŸ: {success_count}, ã‚¹ã‚­ãƒƒãƒ—: {skip_count}, ã‚¨ãƒ©ãƒ¼: {error_count}) ---\n")

        logger.info("\n==================== ç§»è¡Œå®Œäº† ====================")
        logger.info(f"æˆåŠŸ: {success_count} ä»¶")
        logger.info(f"ã‚¹ã‚­ãƒƒãƒ—: {skip_count} ä»¶")
        logger.info(f"ã‚¨ãƒ©ãƒ¼: {error_count} ä»¶")

    async def migrate_document(self, doc: dict) -> bool:
        """
        1ã¤ã®æ–‡æ›¸ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã‚«ãƒ©ãƒ ã‚’è¿½åŠ 

        Args:
            doc: æ–‡æ›¸ãƒ¬ã‚³ãƒ¼ãƒ‰

        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        doc_id = doc.get('id')
        metadata = doc.get('metadata', {})
        document_date = doc.get('document_date')

        try:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡º
            filtering_metadata = MetadataExtractor.extract_filtering_metadata(
                metadata=metadata,
                document_date=document_date
            )

            # ãƒ­ã‚°å‡ºåŠ›
            extracted_info = []
            if filtering_metadata.get("year"):
                extracted_info.append(f"å¹´={filtering_metadata['year']}")
            if filtering_metadata.get("month"):
                extracted_info.append(f"æœˆ={filtering_metadata['month']}")
            if filtering_metadata.get("grade_level"):
                extracted_info.append(f"å­¦å¹´={filtering_metadata['grade_level']}")
            if filtering_metadata.get("school_name"):
                extracted_info.append(f"å­¦æ ¡={filtering_metadata['school_name']}")

            info_str = ", ".join(extracted_info) if extracted_info else "ãªã—"
            logger.info(f"  ğŸ“Š æŠ½å‡ºæƒ…å ±: {info_str}")

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
            update_data = {}

            for key, value in filtering_metadata.items():
                if value is not None:
                    update_data[key] = value

            if not update_data:
                logger.warning(f"  âš ï¸  æ›´æ–°ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return False

            response = (
                self.db_client.client.table('documents')
                .update(update_data)
                .eq('id', doc_id)
                .execute()
            )

            if response.data:
                return True
            else:
                logger.error(f"  âŒ æ›´æ–°å¤±æ•—")
                return False

        except Exception as e:
            logger.error(f"  âŒ ç§»è¡Œã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    migration = MetadataFilteringMigration()
    await migration.migrate_all_documents(batch_size=10, skip_existing=True)


if __name__ == "__main__":
    asyncio.run(main())
