"""
æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å½¢å¼ã«ç§»è¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å®Ÿè¡Œæ–¹æ³•:
    python scripts/migrate_to_chunks.py

æ©Ÿèƒ½:
    - æ—¢å­˜ã® documents ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ full_text ã‚’èª­ã¿è¾¼ã¿
    - ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦embeddingã‚’ç”Ÿæˆ
    - document_chunks ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
"""
import asyncio
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.database.client import DatabaseClient
from core.ai.embeddings import EmbeddingClient
from core.utils.chunking import chunk_document
from loguru import logger

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stdout, format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}", level="INFO")


class ChunkMigration:
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ£ãƒ³ã‚¯ç§»è¡Œã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.db_client = DatabaseClient()
        self.embedding_client = EmbeddingClient()

    async def migrate_all_documents(self, batch_size: int = 10, skip_existing: bool = True):
        """
        å…¨æ–‡æ›¸ã‚’ãƒãƒ£ãƒ³ã‚¯å½¢å¼ã«ç§»è¡Œ

        Args:
            batch_size: ä¸€åº¦ã«å‡¦ç†ã™ã‚‹æ–‡æ›¸æ•°
            skip_existing: æ—¢ã«ãƒãƒ£ãƒ³ã‚¯ãŒå­˜åœ¨ã™ã‚‹æ–‡æ›¸ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹
        """
        logger.info("==================== ãƒãƒ£ãƒ³ã‚¯ç§»è¡Œé–‹å§‹ ====================")

        # å‡¦ç†å¯¾è±¡ã®æ–‡æ›¸ã‚’å–å¾—
        documents = self.db_client.get_documents_for_review(limit=1000)

        if not documents:
            logger.warning("å‡¦ç†å¯¾è±¡ã®æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return

        logger.info(f"å‡¦ç†å¯¾è±¡: {len(documents)} ä»¶ã®æ–‡æ›¸")

        success_count = 0
        skip_count = 0
        error_count = 0

        for i, doc in enumerate(documents, 1):
            doc_id = doc.get('id')
            file_name = doc.get('file_name', 'Unknown')

            logger.info(f"\n[{i}/{len(documents)}] å‡¦ç†ä¸­: {file_name}")

            # æ—¢ã«ãƒãƒ£ãƒ³ã‚¯ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if skip_existing:
                existing_chunks = self.db_client.get_document_chunks(doc_id)
                if existing_chunks:
                    logger.info(f"  â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: æ—¢ã« {len(existing_chunks)} ãƒãƒ£ãƒ³ã‚¯ãŒå­˜åœ¨ã—ã¾ã™")
                    skip_count += 1
                    continue

            # ç§»è¡Œå®Ÿè¡Œ
            success = await self.migrate_document(doc)

            if success:
                success_count += 1
                logger.info(f"  âœ… ç§»è¡ŒæˆåŠŸ")
            else:
                error_count += 1
                logger.error(f"  âŒ ç§»è¡Œå¤±æ•—")

            # é€²æ—è¡¨ç¤º
            if i % 10 == 0:
                logger.info(f"\n--- é€²æ—: {i}/{len(documents)} å®Œäº† (æˆåŠŸ: {success_count}, ã‚¹ã‚­ãƒƒãƒ—: {skip_count}, ã‚¨ãƒ©ãƒ¼: {error_count}) ---\n")

        logger.info("\n==================== ç§»è¡Œå®Œäº† ====================")
        logger.info(f"æˆåŠŸ: {success_count} ä»¶")
        logger.info(f"ã‚¹ã‚­ãƒƒãƒ—: {skip_count} ä»¶")
        logger.info(f"ã‚¨ãƒ©ãƒ¼: {error_count} ä»¶")

    async def migrate_document(self, doc: dict) -> bool:
        """
        1ã¤ã®æ–‡æ›¸ã‚’ãƒãƒ£ãƒ³ã‚¯å½¢å¼ã«ç§»è¡Œ

        Args:
            doc: æ–‡æ›¸ãƒ¬ã‚³ãƒ¼ãƒ‰

        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        doc_id = doc.get('id')
        full_text = doc.get('full_text')

        if not full_text or not full_text.strip():
            logger.warning(f"  âš ï¸  full_text ãŒç©ºã§ã™ï¼ˆdoc_id: {doc_id}ï¼‰")
            return False

        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            logger.info(f"  ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(full_text)} æ–‡å­—")
            chunks = chunk_document(full_text, chunk_size=800, chunk_overlap=100)

            if not chunks:
                logger.warning(f"  âš ï¸  ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²çµæœãŒç©ºã§ã™")
                return False

            logger.info(f"  âœ‚ï¸  ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²: {len(chunks)} ãƒãƒ£ãƒ³ã‚¯")

            # ã‚¹ãƒ†ãƒƒãƒ—2: å„ãƒãƒ£ãƒ³ã‚¯ã®embeddingç”Ÿæˆ
            logger.info(f"  ğŸ”¢ Embeddingç”Ÿæˆä¸­...")
            embeddings = []

            for chunk in chunks:
                chunk_text = chunk["chunk_text"]
                try:
                    embedding = self.embedding_client.generate_embedding(chunk_text)
                    embeddings.append(embedding)
                except Exception as e:
                    logger.error(f"  âŒ Embeddingç”Ÿæˆã‚¨ãƒ©ãƒ¼ (chunk {chunk['chunk_index']}): {e}")
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨
                    embeddings.append([0.0] * 768)

            logger.info(f"  âœ… Embeddingç”Ÿæˆå®Œäº†: {len(embeddings)} ä»¶")

            # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            success = await self.db_client.insert_document_chunks(
                document_id=doc_id,
                chunks=chunks,
                embeddings=embeddings
            )

            if not success:
                logger.error(f"  âŒ ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å¤±æ•—")
                return False

            # ã‚¹ãƒ†ãƒƒãƒ—4: æ–‡æ›¸ã®ãƒãƒ£ãƒ³ã‚¯çµ±è¨ˆã‚’æ›´æ–°
            await self.db_client.update_document_chunk_count(
                document_id=doc_id,
                chunk_count=len(chunks),
                strategy="overlap"
            )

            return True

        except Exception as e:
            logger.error(f"  âŒ ç§»è¡Œã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    migration = ChunkMigration()
    await migration.migrate_all_documents(batch_size=10, skip_existing=True)


if __name__ == "__main__":
    asyncio.run(main())
