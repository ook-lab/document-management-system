"""
ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã‚­ãƒ¥ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ã‚ãšã€Rawdata_FILE_AND_MAIL.processing_status ã§ç›´æ¥ç®¡ç†

å‡¦ç†å†…å®¹:
1. processing_status='pending' ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
2. çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆStage E-Kï¼‰ã§å‡¦ç†
3. æˆåŠŸ: processing_status='completed'
4. å¤±æ•—: processing_status='failed'

ä½¿ã„æ–¹:
    # å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‡¦ç†
    python process_queued_documents_v3.py --limit=100

    # ç‰¹å®šã®ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®ã¿
    python process_queued_documents_v3.py --workspace=ema_classroom --limit=20

    # pendingã«ãƒªã‚»ãƒƒãƒˆï¼ˆå†å‡¦ç†ç”¨ï¼‰
    python process_queued_documents_v3.py --reset-to-pending --workspace=all
"""

import asyncio
from typing import List, Dict, Any, Optional
import sys
from datetime import datetime
from pathlib import Path
import mimetypes

from loguru import logger
from shared.common.database.client import DatabaseClient
from shared.common.connectors.google_drive import GoogleDriveConnector
from shared.pipeline import UnifiedDocumentPipeline


class DocumentProcessor:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""

    VIDEO_EXTENSIONS = ['.mp4', '.mov', '.avi', '.mkv', '.webm', '.flv', '.wmv', '.m4v', '.mpeg', '.mpg']

    def __init__(self):
        self.db = DatabaseClient()
        self.pipeline = UnifiedDocumentPipeline(db_client=self.db)
        self.drive = GoogleDriveConnector()
        self.temp_dir = Path("./temp")
        self.temp_dir.mkdir(parents=True, exist_ok=True)

    def get_pending_documents(self, workspace: str = 'all', limit: int = 100) -> List[Dict[str, Any]]:
        """
        processing_status='pending' ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—

        Args:
            workspace: å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ ('all' ã§å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹)
            limit: å–å¾—ã™ã‚‹æœ€å¤§ä»¶æ•°

        Returns:
            ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        """
        query = self.db.client.table('Rawdata_FILE_AND_MAIL').select('*').eq('processing_status', 'pending')

        if workspace != 'all':
            query = query.eq('workspace', workspace)

        result = query.limit(limit).execute()
        return result.data if result.data else []

    def mark_as_processing(self, document_id: str):
        """å‡¦ç†ä¸­ã«ãƒãƒ¼ã‚¯"""
        try:
            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                'processing_status': 'processing',
                'processing_stage': 'é–‹å§‹',
                'processing_progress': 0.0
            }).eq('id', document_id).execute()
        except Exception as e:
            logger.error( f"å‡¦ç†ä¸­ãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

    def update_progress(self, document_id: str, stage: str, progress: float):
        """é€²æ—ã‚’æ›´æ–°"""
        try:
            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                'processing_stage': stage,
                'processing_progress': progress
            }).eq('id', document_id).execute()
            logger.debug(f"é€²æ—æ›´æ–°: {stage} ({progress*100:.0f}%)")
        except Exception as e:
            logger.error(f"é€²æ—æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def mark_as_completed(self, document_id: str):
        """å®Œäº†ã«ãƒãƒ¼ã‚¯"""
        try:
            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                'processing_status': 'completed',
                'processing_stage': 'å®Œäº†',
                'processing_progress': 1.0
            }).eq('id', document_id).execute()
        except Exception as e:
            logger.error( f"å®Œäº†ãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

    def mark_as_failed(self, document_id: str, error_message: str = ""):
        """ã‚¨ãƒ©ãƒ¼ã«ãƒãƒ¼ã‚¯"""
        try:
            update_data = {
                'processing_status': 'failed',
                'processing_stage': 'ã‚¨ãƒ©ãƒ¼',
                'processing_progress': 0.0
            }

            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜
            if error_message:
                # æ—¢å­˜ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                doc_result = self.db.client.table('Rawdata_FILE_AND_MAIL').select('metadata').eq('id', document_id).execute()
                if doc_result.data and len(doc_result.data) > 0:
                    metadata = doc_result.data[0].get('metadata', {}) or {}
                else:
                    metadata = {}

                metadata['last_error'] = error_message
                metadata['last_error_time'] = datetime.now().isoformat()
                update_data['metadata'] = metadata

            self.db.client.table('Rawdata_FILE_AND_MAIL').update(update_data).eq('id', document_id).execute()
        except Exception as e:
            logger.error( f"å¤±æ•—ãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

    def get_queue_stats(self, workspace: str = 'all') -> Dict[str, int]:
        """
        çµ±è¨ˆæƒ…å ±ã‚’å–å¾—

        Args:
            workspace: å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ ('all' ã§å…¨ã¦)

        Returns:
            çµ±è¨ˆæƒ…å ±ã®è¾æ›¸
        """
        try:
            query = self.db.client.table('Rawdata_FILE_AND_MAIL').select('processing_status, workspace')

            if workspace != 'all':
                query = query.eq('workspace', workspace)

            response = query.execute()

            stats = {
                'pending': 0,
                'processing': 0,
                'completed': 0,
                'failed': 0,
                'null': 0  # æœªå‡¦ç†ï¼ˆprocessing_statusãŒnullï¼‰
            }

            for doc in response.data:
                status = doc.get('processing_status')
                if status is None:
                    stats['null'] += 1
                else:
                    stats[status] = stats.get(status, 0) + 1

            stats['total'] = len(response.data)

            # æˆåŠŸç‡ã‚’è¨ˆç®—
            processed = stats['completed'] + stats['failed']
            if processed > 0:
                stats['success_rate'] = round(stats['completed'] / processed * 100, 1)
            else:
                stats['success_rate'] = 0.0

            return stats

        except Exception as e:
            logger.error(f" çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def print_queue_stats(self, workspace: str = 'all'):
        """
        çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º

        Args:
            workspace: å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ ('all' ã§å…¨ã¦)
        """
        stats = self.get_queue_stats(workspace)

        if not stats:
            logger.info("çµ±è¨ˆæƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return

        logger.info("\n" + "="*80)
        if workspace == 'all':
            logger.info("ğŸ“Š å…¨ä½“çµ±è¨ˆ")
        else:
            logger.info(f"ğŸ“Š çµ±è¨ˆ (workspace: {workspace})")
        logger.info("="*80)
        logger.info(f"å¾…æ©Ÿä¸­ (pending):      {stats.get('pending', 0):>5}ä»¶")
        logger.info(f"å‡¦ç†ä¸­ (processing):   {stats.get('processing', 0):>5}ä»¶")
        logger.info(f"å®Œäº†   (completed):    {stats.get('completed', 0):>5}ä»¶")
        logger.info(f"å¤±æ•—   (failed):       {stats.get('failed', 0):>5}ä»¶")
        logger.info(f"æœªå‡¦ç† (null):         {stats.get('null', 0):>5}ä»¶")
        logger.info("-" * 80)
        logger.info(f"åˆè¨ˆ:                  {stats.get('total', 0):>5}ä»¶")

        # æˆåŠŸç‡ã‚’è¡¨ç¤º
        processed = stats.get('completed', 0) + stats.get('failed', 0)
        if processed > 0:
            logger.info(f"æˆåŠŸç‡:                {stats.get('success_rate', 0):>5.1f}% ({stats.get('completed', 0)}/{processed})")

        logger.info("="*80 + "\n")

    async def process_document(
        self,
        doc: Dict[str, Any],
        preserve_workspace: bool = True
    ) -> bool:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†

        Args:
            doc: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
            preserve_workspace: workspaceã‚’ä¿æŒã™ã‚‹ã‹

        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        document_id = doc['id']
        file_name = doc.get('file_name', 'unknown')
        title = doc.get('title', '')
        display_name = title if title else '(ã‚¿ã‚¤ãƒˆãƒ«æœªç”Ÿæˆ)'
        source_type = doc.get('source_type', '')

        completed_or_failed = False  # å‡¦ç†ãŒå®Œäº†ã¾ãŸã¯å¤±æ•—ã—ãŸã‹ã®ãƒ•ãƒ©ã‚°

        try:
            # å‡¦ç†ä¸­ã«ãƒãƒ¼ã‚¯
            self.mark_as_processing(document_id)

            # source_idã®æœ‰ç„¡ã§åˆ¤æ–­ï¼ˆsource_typeã«ã¯ä¾å­˜ã—ãªã„ï¼‰
            drive_file_id = doc.get('source_id')

            if drive_file_id:
                # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚ã‚Šï¼ˆDrive File IDãŒå­˜åœ¨ï¼‰
                result = await self._process_with_attachment(doc, preserve_workspace)
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼ˆæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ï¼‰
                result = await self._process_text_only(doc, preserve_workspace)

            # çµæœãŒboolã®å ´åˆï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
            if isinstance(result, bool):
                success = result
                error_msg = "å‡¦ç†å¤±æ•—ï¼ˆè©³ç´°ãªã—ï¼‰" if not success else None
            else:
                # çµæœãŒdictã®å ´åˆï¼ˆè©³ç´°ã‚¨ãƒ©ãƒ¼ä»˜ãï¼‰
                success = result.get('success', False)
                error_msg = result.get('error', "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼") if not success else None

            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
            if success:
                # screenshot_url ãŒã‚ã‚‹å ´åˆï¼šPNGã‚’å‰Šé™¤ã—ã¦ã‚¯ãƒªã‚¢
                screenshot_url = doc.get('screenshot_url')
                if screenshot_url:
                    try:
                        # screenshot_url ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’æŠ½å‡º
                        import re
                        match = re.search(r'/d/([a-zA-Z0-9_-]+)', screenshot_url)
                        if match:
                            png_file_id = match.group(1)

                            # PNGã‚’ã‚´ãƒŸç®±ã«ç§»å‹•ï¼ˆå…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–ã§ã¯å®Œå…¨å‰Šé™¤ä¸å¯ï¼‰
                            from shared.common.connectors.google_drive import GoogleDriveConnector
                            drive = GoogleDriveConnector()
                            drive.trash_file(png_file_id)
                            logger.info(f"[OK] OCRç”¨PNGã‚’ã‚´ãƒŸç®±ã«ç§»å‹•: {png_file_id}")

                            # screenshot_url ã‚’ã‚¯ãƒªã‚¢
                            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                                'screenshot_url': None
                            }).eq('id', document_id).execute()
                            logger.info(f"[OK] screenshot_url ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")

                    except Exception as e:
                        logger.warning(f" PNGå‰Šé™¤å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ï¼ˆå‡¦ç†ã¯ç¶™ç¶šï¼‰: {e}")

                self.mark_as_completed(document_id)
                completed_or_failed = True
                logger.info(f"[OK] å‡¦ç†æˆåŠŸ: {display_name}")
            else:
                self.mark_as_failed(document_id, error_msg)
                completed_or_failed = True
                logger.error( f"[FAIL] å‡¦ç†ã‚¨ãƒ©ãƒ¼: {display_name} - {error_msg}")

            return success

        except Exception as e:
            # æ˜ç¢ºãªã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦è¨˜éŒ²
            error_msg = f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}"
            logger.error("=" * 80)
            logger.error(f"[FAIL] æ˜ç¢ºãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ â†’ ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦è¨˜éŒ²")
            logger.error(f"  â”œâ”€ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {display_name}")
            logger.error(f"  â”œâ”€ ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
            logger.error(f"  â””â”€ ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_msg}")
            logger.error("=" * 80)
            self.mark_as_failed(document_id, error_msg)
            completed_or_failed = True
            return False

        finally:
            # å¼·åˆ¶çµ‚äº†ã‚„ä¸­æ–­æ™‚ã¯pendingã«å·®ã—æˆ»ã—ï¼ˆcompleted_or_failedãŒFalseã®å ´åˆï¼‰
            # ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã„ãªã„ãŒcompletedã«ãªã£ã¦ã„ãªã„ â†’ pendingã«æˆ»ã™ï¼ˆã‚¨ãƒ©ãƒ¼ã«ã—ãªã„ï¼‰
            if not completed_or_failed:
                logger.warning("=" * 80)
                logger.warning(f"[ROLLBACK] å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ â†’ pendingã«å·®ã—æˆ»ã—ï¼ˆã‚¨ãƒ©ãƒ¼ã«ã—ã¾ã›ã‚“ï¼‰")
                logger.warning(f"  â”œâ”€ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {display_name}")
                logger.warning(f"  â””â”€ ç†ç”±: æ˜ç¢ºãªã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã„ãªã„ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ã§ã¯ãªãpendingã«æˆ»ã—ã¾ã™")
                logger.warning("=" * 80)
                try:
                    self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                        'processing_status': 'pending'
                    }).eq('id', document_id).execute()
                    logger.info(f"[OK] pendingã«å·®ã—æˆ»ã—ã¾ã—ãŸ: {display_name}")
                except Exception as e:
                    logger.error(f"å·®ã—æˆ»ã—ã‚¨ãƒ©ãƒ¼: {e}")

    async def _process_text_only(
        self,
        doc: Dict[str, Any],
        preserve_workspace: bool = True
    ) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ï¼ˆçµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®Stage H-Kéƒ¨åˆ†ã®ã¿ä½¿ç”¨ï¼‰"""
        from shared.common.processing.metadata_chunker import MetadataChunker

        document_id = doc['id']
        file_name = doc.get('file_name', 'text_only')
        workspace_to_use = doc.get('workspace', 'unknown') if preserve_workspace else 'unknown'

        display_subject = doc.get('display_subject', '')
        display_post_text = doc.get('display_post_text', '')
        attachment_text = doc.get('attachment_text', '')

        # ãƒ†ã‚­ã‚¹ãƒˆçµåˆ
        text_parts = []
        if display_subject:
            text_parts.append(f"ã€ä»¶åã€‘\n{display_subject}")
        if display_post_text:
            text_parts.append(f"ã€æœ¬æ–‡ã€‘\n{display_post_text}")
        if attachment_text:
            text_parts.append(f"ã€æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã€‘\n{attachment_text}")

        combined_text = '\n\n'.join(text_parts)

        if not combined_text.strip():
            error_msg = "ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        # çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã® Stage H-K ã‚’ä½¿ç”¨
        # config ã‹ã‚‰è¨­å®šã‚’å–å¾—
        stage_h_config = self.pipeline.config.get_stage_config('stage_h', doc.get('doc_type', 'other'), workspace_to_use)

        # Stage H: æ§‹é€ åŒ–
        self.update_progress(document_id, 'Stage H: æ§‹é€ åŒ–', 0.3)
        stageh_result = self.pipeline.stage_h.process(
            file_name=file_name,
            doc_type=doc.get('doc_type', 'unknown'),
            workspace=workspace_to_use,
            combined_text=combined_text,
            prompt=stage_h_config['prompt'],
            model=stage_h_config['model']
        )

        # Stage H ã®çµæœã‚’ãƒã‚§ãƒƒã‚¯
        if not stageh_result or not isinstance(stageh_result, dict):
            error_msg = "Stage Hå¤±æ•—: æ§‹é€ åŒ–çµæœãŒä¸æ­£ã§ã™"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        stageh_metadata = stageh_result.get('metadata', {})
        if stageh_metadata.get('extraction_failed'):
            error_msg = "Stage Hå¤±æ•—: JSONæŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        document_date = stageh_result.get('document_date')
        tags = stageh_result.get('tags', [])

        # Stage I ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãªã®ã§è¦ç´„ä¸è¦ï¼‰

        # Stage J: ãƒãƒ£ãƒ³ã‚¯åŒ–
        self.update_progress(document_id, 'Stage J: ãƒãƒ£ãƒ³ã‚¯åŒ–', 0.6)
        metadata_chunker = MetadataChunker()
        document_data = {
            'file_name': file_name,
            'summary': '',
            'document_date': document_date,
            'tags': tags,
            'doc_type': doc.get('doc_type'),
            'display_subject': display_subject,
            'display_post_text': display_post_text,
            'display_sender': doc.get('display_sender'),
            'display_type': doc.get('display_type'),
            'display_sent_at': doc.get('display_sent_at'),
            'classroom_sender_email': doc.get('classroom_sender_email'),
            'attachment_text': attachment_text,
            'persons': stageh_metadata.get('persons', []) if isinstance(stageh_metadata, dict) else [],
            'organizations': stageh_metadata.get('organizations', []) if isinstance(stageh_metadata, dict) else [],
            'people': stageh_metadata.get('people', []) if isinstance(stageh_metadata, dict) else [],
            # Stage H ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            'text_blocks': stageh_metadata.get('text_blocks', []) if isinstance(stageh_metadata, dict) else [],
            'structured_tables': stageh_metadata.get('structured_tables', []) if isinstance(stageh_metadata, dict) else [],
            'weekly_schedule': stageh_metadata.get('weekly_schedule', []) if isinstance(stageh_metadata, dict) else [],
            'other_text': stageh_metadata.get('other_text', []) if isinstance(stageh_metadata, dict) else []
        }

        chunks = metadata_chunker.create_metadata_chunks(document_data)

        # æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤
        try:
            self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
        except Exception as e:
            logger.warning( f"æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {e}")

        # Stage K: Embedding + ä¿å­˜
        self.update_progress(document_id, 'Stage K: Embedding', 0.8)
        stage_k_result = self.pipeline.stage_k.embed_and_save(document_id, chunks)

        if not stage_k_result.get('success'):
            error_msg = f"Stage Kå¤±æ•—: {stage_k_result.get('failed_count', 0)}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å¤±æ•—"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        # éƒ¨åˆ†çš„å¤±æ•—ã‚‚ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦æ‰±ã†ï¼ˆå³æ ¼ãƒ¢ãƒ¼ãƒ‰ï¼‰
        failed_count = stage_k_result.get('failed_count', 0)
        if failed_count > 0:
            error_msg = f"Stage Kéƒ¨åˆ†å¤±æ•—: {failed_count}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å¤±æ•—"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        logger.info(f"ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å®Œäº†: {stage_k_result.get('saved_count', 0)}/{len(chunks)}ä»¶")

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°
        try:
            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                'tags': tags,
                'document_date': document_date,
                'metadata': stageh_metadata
            }).eq('id', document_id).execute()
        except Exception as e:
            error_msg = f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"
            logger.error( error_msg)
            return {'success': False, 'error': error_msg}

        return {'success': True}

    async def _process_with_attachment(
        self,
        doc: Dict[str, Any],
        preserve_workspace: bool = True
    ) -> bool:
        """æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚ã‚Šãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†"""
        document_id = doc['id']
        file_name = doc.get('file_name', 'unknown')
        drive_file_id = doc.get('source_id')

        if not drive_file_id:
            logger.error( "source_idï¼ˆDrive File IDï¼‰ãŒã‚ã‚Šã¾ã›ã‚“")
            return False

        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
        file_extension = Path(file_name).suffix.lower()
        if file_extension in self.VIDEO_EXTENSIONS:
            logger.info(f"â­ï¸  å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {file_name}")
            # å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—æ‰±ã„ã§æˆåŠŸã¨ã™ã‚‹
            return True

        # screenshot_url ãŒã‚ã‚Œã°PNGã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆOCRç”¨ï¼‰ã€ãªã‘ã‚Œã°é€šå¸¸ãƒ•ã‚¡ã‚¤ãƒ«
        screenshot_url = doc.get('screenshot_url')
        screenshot_file_id = None
        download_file_id = drive_file_id
        download_file_name = file_name

        if screenshot_url:
            # screenshot_url ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’æŠ½å‡º
            import re
            match = re.search(r'/d/([a-zA-Z0-9_-]+)', screenshot_url)
            if match:
                screenshot_file_id = match.group(1)
                download_file_id = screenshot_file_id
                # PNGãƒ•ã‚¡ã‚¤ãƒ«åã«å¤‰æ›´
                base_name = Path(file_name).stem
                download_file_name = f"{base_name}.png"
                logger.info(f"[OCRç”¨] PNGã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {download_file_name} (screenshot_urlä½¿ç”¨)")
            else:
                logger.warning( f"screenshot_url ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’æŠ½å‡ºã§ãã¾ã›ã‚“: {screenshot_url}")

        # Driveã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        self.update_progress(document_id, 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­', 0.1)
        try:
            self.drive.download_file(download_file_id, download_file_name, str(self.temp_dir))
            local_path = self.temp_dir / download_file_name
        except Exception as e:
            # 404ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰ã®å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            error_str = str(e)
            if 'File not found' in error_str or '404' in error_str:
                logger.warning( f"Driveã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ãƒ†ã‚­ã‚¹ãƒˆã®ã¿å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {file_name}")
                return await self._process_text_only(doc, preserve_workspace)
            else:
                logger.error( f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                return False

        # MIMEã‚¿ã‚¤ãƒ—ã‚’æ¨æ¸¬
        mime_type = doc.get('mimeType')
        if not mime_type:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãªã„å ´åˆã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬
            mime_type, _ = mimetypes.guess_type(file_name)
        if not mime_type:
            # ãã‚Œã§ã‚‚ä¸æ˜ãªå ´åˆã¯æ±ç”¨ãƒã‚¤ãƒŠãƒªã¨ã—ã¦æ‰±ã†
            mime_type = 'application/octet-stream'

        # çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å‡¦ç†
        self.update_progress(document_id, 'Stage E-K: å‡¦ç†ä¸­', 0.3)
        try:
            workspace_to_use = doc.get('workspace', 'unknown') if preserve_workspace else 'unknown'

            result = await self.pipeline.process_document(
                file_path=Path(local_path),
                file_name=file_name,
                doc_type=doc.get('doc_type', 'other'),
                workspace=workspace_to_use,
                mime_type=mime_type,
                source_id=drive_file_id,
                existing_document_id=document_id,
                extra_metadata={
                    'display_subject': doc.get('display_subject'),
                    'display_post_text': doc.get('display_post_text'),
                    'attachment_text': doc.get('attachment_text'),
                    'display_sender': doc.get('display_sender'),
                    'display_sender_email': doc.get('display_sender_email'),
                    'display_type': doc.get('display_type'),
                    'display_sent_at': doc.get('display_sent_at'),
                    'classroom_sender_email': doc.get('classroom_sender_email')
                }
            )

            # çµæœå…¨ä½“ã‚’è¿”ã™ï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å«ã‚€ï¼‰
            return result

        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            if local_path.exists():
                local_path.unlink()
                logger.debug( f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {local_path}")

    async def run(
        self,
        workspace: str = 'all',
        limit: int = 100,
        preserve_workspace: bool = True
    ):
        """
        å‡¦ç†ã‚’å®Ÿè¡Œ

        Args:
            workspace: å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹
            limit: å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•°
            preserve_workspace: workspaceã‚’ä¿æŒã™ã‚‹ã‹
        """
        logger.info("="*80)
        logger.info("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰")
        logger.info("="*80)

        # pending ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        docs = self.get_pending_documents(workspace, limit)

        if not docs:
            logger.info("å‡¦ç†å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            return

        logger.info(f"å‡¦ç†å¯¾è±¡: {len(docs)}ä»¶")
        logger.info("")

        # çµ±è¨ˆ
        stats = {'success': 0, 'failed': 0, 'total': len(docs)}

        # é †æ¬¡å‡¦ç†
        for i, doc in enumerate(docs, 1):
            file_name = doc.get('file_name', 'unknown')
            title = doc.get('title', '')
            # ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚Œã°ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤ºã€ãªã‘ã‚Œã°ã€Œã‚¿ã‚¤ãƒˆãƒ«æœªç”Ÿæˆã€
            display_name = title if title else '(ã‚¿ã‚¤ãƒˆãƒ«æœªç”Ÿæˆ)'
            logger.info(f"\n{'='*80}")
            logger.info(f"[{i}/{len(docs)}] å‡¦ç†é–‹å§‹: {display_name}")
            logger.info(f"Document ID: {doc['id']}")

            success = await self.process_document(doc, preserve_workspace)

            if success:
                stats['success'] += 1
            else:
                stats['failed'] += 1

            logger.info(f"é€²æ—: æˆåŠŸ={stats['success']}, ã‚¨ãƒ©ãƒ¼={stats['failed']}, æ®‹ã‚Š={len(docs)-i}")

        # æœ€çµ‚çµæœ
        logger.info("\n" + "="*80)
        logger.info("å‡¦ç†å®Œäº†")
        logger.info("="*80)
        logger.info(f"[OK] æˆåŠŸ: {stats['success']}ä»¶")
        logger.error(f"[FAIL] ã‚¨ãƒ©ãƒ¼: {stats['failed']}ä»¶")
        logger.info(f"[TOTAL] åˆè¨ˆ: {stats['total']}ä»¶")
        logger.info("="*80)


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰')
    parser.add_argument('--workspace', default='all', help='å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: all)')
    parser.add_argument('--limit', type=int, default=100, help='å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100)')
    parser.add_argument('--no-preserve-workspace', action='store_true', help='workspaceã‚’ä¿æŒã—ãªã„')
    parser.add_argument('--stats', action='store_true', help='çµ±è¨ˆæƒ…å ±ã®ã¿ã‚’è¡¨ç¤º')

    args = parser.parse_args()

    processor = DocumentProcessor()

    # çµ±è¨ˆæƒ…å ±ã®ã¿è¡¨ç¤º
    if args.stats:
        processor.print_queue_stats(workspace=args.workspace)
        return

    # é€šå¸¸ã®å‡¦ç†
    await processor.run(
        workspace=args.workspace,
        limit=args.limit,
        preserve_workspace=not args.no_preserve_workspace
    )


async def continuous_processing_loop():
    """ç¶™ç¶šçš„ãªå‡¦ç†ãƒ«ãƒ¼ãƒ—ï¼ˆè‡ªå‹•å‡¦ç†ç”¨ï¼‰"""
    processor = DocumentProcessor()

    logger.info("="*80)
    logger.info("è‡ªå‹•å‡¦ç†ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹ã—ã¾ã™")
    logger.info("="*80)

    while True:
        try:
            # pending ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
            docs = processor.get_pending_documents(workspace='all', limit=10)

            if docs:
                logger.info(f"\nå‡¦ç†å¯¾è±¡: {len(docs)}ä»¶")

                # é †æ¬¡å‡¦ç†
                for i, doc in enumerate(docs, 1):
                    title = doc.get('title', '')
                    display_name = title if title else '(ã‚¿ã‚¤ãƒˆãƒ«æœªç”Ÿæˆ)'
                    logger.info(f"\n[{i}/{len(docs)}] å‡¦ç†ä¸­: {display_name}")

                    await processor.process_document(doc, preserve_workspace=True)
            else:
                logger.debug("å‡¦ç†å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆ5ç§’å¾Œã«å†ãƒã‚§ãƒƒã‚¯ï¼‰")

            # 5ç§’å¾…æ©Ÿã—ã¦ã‹ã‚‰æ¬¡ã®ãƒ«ãƒ¼ãƒ—
            await asyncio.sleep(5)

        except Exception as e:
            logger.error(f"å‡¦ç†ãƒ«ãƒ¼ãƒ—ã§ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚10ç§’å¾…æ©Ÿã—ã¦ç¶™ç¶š
            await asyncio.sleep(10)


if __name__ == '__main__':
    import sys

    # --loop ãƒ•ãƒ©ã‚°ãŒã‚ã‚‹å ´åˆã¯ç¶™ç¶šãƒ«ãƒ¼ãƒ—ãƒ¢ãƒ¼ãƒ‰
    if '--loop' in sys.argv:
        asyncio.run(continuous_processing_loop())
    else:
        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼ˆ1å›å®Ÿè¡Œï¼‰
        asyncio.run(main())
