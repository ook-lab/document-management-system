"""
ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã‚­ãƒ¥ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ã‚ãšã€Rawdata_FILE_AND_MAIL.processing_status ã§ç›´æ¥ç®¡ç†

å‡¦ç†å†…å®¹:
1. processing_status='pending' ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
2. çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆStage E-Kï¼‰ã§å‡¦ç†
3. æˆåŠŸ: processing_status='completed'
4. å¤±æ•—: processing_status='failed'

ä½¿ã„æ–¹:
    # å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‡¦ç†
    python process_queued_documents_v3.py --limit=100

    # ç‰¹å®šã®ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®ã¿
    python process_queued_documents_v3.py --workspace=ema_classroom --limit=20

    # pendingã«ãƒªã‚»ãƒƒãƒˆï¼ˆå†å‡¦ç†ç”¨ï¼‰
    python process_queued_documents_v3.py --reset-to-pending --workspace=all
"""

import asyncio
from typing import List, Dict, Any, Optional
from loguru import logger
import sys
from datetime import datetime
from pathlib import Path

from A_common.database.client import DatabaseClient
from A_common.connectors.google_drive import GoogleDriveConnector
from G_unified_pipeline import UnifiedDocumentPipeline


class DocumentProcessor:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""

    VIDEO_EXTENSIONS = ['.mp4', '.mov', '.avi', '.mkv', '.webm', '.flv', '.wmv', '.m4v', '.mpeg', '.mpg']

    def __init__(self):
        self.db = DatabaseClient()
        self.pipeline = UnifiedDocumentPipeline(db_client=self.db)
        self.drive = GoogleDriveConnector()
        self.temp_dir = Path("./temp")
        self.temp_dir.mkdir(parents=True, exist_ok=True)

    def get_pending_documents(self, workspace: str = 'all', limit: int = 100) -> List[Dict[str, Any]]:
        """
        processing_status='pending' ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—

        Args:
            workspace: å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ ('all' ã§å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹)
            limit: å–å¾—ã™ã‚‹æœ€å¤§ä»¶æ•°

        Returns:
            ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        """
        query = self.db.client.table('Rawdata_FILE_AND_MAIL').select('*').eq('processing_status', 'pending')

        if workspace != 'all':
            query = query.eq('workspace', workspace)

        result = query.limit(limit).execute()
        return result.data if result.data else []

    def mark_as_processing(self, document_id: str):
        """å‡¦ç†ä¸­ã«ãƒãƒ¼ã‚¯"""
        try:
            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                'processing_status': 'processing'
            }).eq('id', document_id).execute()
        except Exception as e:
            logger.error(f"å‡¦ç†ä¸­ãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

    def mark_as_completed(self, document_id: str):
        """å®Œäº†ã«ãƒãƒ¼ã‚¯"""
        try:
            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                'processing_status': 'completed'
            }).eq('id', document_id).execute()
        except Exception as e:
            logger.error(f"å®Œäº†ãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

    def mark_as_failed(self, document_id: str, error_message: str = ""):
        """å¤±æ•—ã«ãƒãƒ¼ã‚¯"""
        try:
            update_data = {'processing_status': 'failed'}

            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜
            if error_message:
                # æ—¢å­˜ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                doc_result = self.db.client.table('Rawdata_FILE_AND_MAIL').select('metadata').eq('id', document_id).execute()
                if doc_result.data and len(doc_result.data) > 0:
                    metadata = doc_result.data[0].get('metadata', {}) or {}
                else:
                    metadata = {}

                metadata['last_error'] = error_message
                metadata['last_error_time'] = datetime.now().isoformat()
                update_data['metadata'] = metadata

            self.db.client.table('Rawdata_FILE_AND_MAIL').update(update_data).eq('id', document_id).execute()
        except Exception as e:
            logger.error(f"å¤±æ•—ãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

    async def process_document(
        self,
        doc: Dict[str, Any],
        preserve_workspace: bool = True
    ) -> bool:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†

        Args:
            doc: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
            preserve_workspace: workspaceã‚’ä¿æŒã™ã‚‹ã‹

        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        document_id = doc['id']
        file_name = doc.get('file_name', 'unknown')
        source_type = doc.get('source_type', '')

        try:
            # å‡¦ç†ä¸­ã«ãƒãƒ¼ã‚¯
            self.mark_as_processing(document_id)

            # source_idã®æœ‰ç„¡ã§åˆ¤æ–­ï¼ˆsource_typeã«ã¯ä¾å­˜ã—ãªã„ï¼‰
            drive_file_id = doc.get('source_id')

            if drive_file_id:
                # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚ã‚Šï¼ˆDrive File IDãŒå­˜åœ¨ï¼‰
                success = await self._process_with_attachment(doc, preserve_workspace)
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼ˆæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ï¼‰
                success = await self._process_text_only(doc, preserve_workspace)

            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
            if success:
                self.mark_as_completed(document_id)
                logger.info(f"âœ… å‡¦ç†æˆåŠŸ: {file_name}")
            else:
                self.mark_as_failed(document_id, "å‡¦ç†å¤±æ•—")
                logger.error(f"âŒ å‡¦ç†å¤±æ•—: {file_name}")

            return success

        except Exception as e:
            error_msg = f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.exception(e)
            self.mark_as_failed(document_id, error_msg)
            return False

    async def _process_text_only(
        self,
        doc: Dict[str, Any],
        preserve_workspace: bool = True
    ) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ï¼ˆçµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®Stage H-Kéƒ¨åˆ†ã®ã¿ä½¿ç”¨ï¼‰"""
        from A_common.processing.metadata_chunker import MetadataChunker

        document_id = doc['id']
        file_name = doc.get('file_name', 'text_only')
        workspace_to_use = doc.get('workspace', 'unknown') if preserve_workspace else 'unknown'

        display_subject = doc.get('display_subject', '')
        display_post_text = doc.get('display_post_text', '')
        attachment_text = doc.get('attachment_text', '')

        # ãƒ†ã‚­ã‚¹ãƒˆçµåˆ
        text_parts = []
        if display_subject:
            text_parts.append(f"ã€ä»¶åã€‘\n{display_subject}")
        if display_post_text:
            text_parts.append(f"ã€æœ¬æ–‡ã€‘\n{display_post_text}")
        if attachment_text:
            text_parts.append(f"ã€æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã€‘\n{attachment_text}")

        combined_text = '\n\n'.join(text_parts)

        if not combined_text.strip():
            logger.error("ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™")
            return False

        # çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã® Stage H-K ã‚’ä½¿ç”¨
        # config ã‹ã‚‰è¨­å®šã‚’å–å¾—
        stage_h_config = self.pipeline.config.get_stage_config('stage_h', doc.get('doc_type', 'other'), workspace_to_use)

        # Stage H: æ§‹é€ åŒ–
        stageh_result = self.pipeline.stage_h.process(
            file_name=file_name,
            doc_type=doc.get('doc_type', 'unknown'),
            workspace=workspace_to_use,
            combined_text=combined_text,
            prompt=stage_h_config['prompt'],
            model=stage_h_config['model']
        )

        # Stage H ã®çµæœã‚’ãƒã‚§ãƒƒã‚¯
        if not stageh_result or not isinstance(stageh_result, dict):
            logger.error("Stage Hå¤±æ•—: æ§‹é€ åŒ–çµæœãŒä¸æ­£ã§ã™")
            return False

        stageh_metadata = stageh_result.get('metadata', {})
        if stageh_metadata.get('extraction_failed'):
            logger.error("Stage Hå¤±æ•—: JSONæŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False

        document_date = stageh_result.get('document_date')
        tags = stageh_result.get('tags', [])

        # Stage I ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãªã®ã§è¦ç´„ä¸è¦ï¼‰

        # Stage J: ãƒãƒ£ãƒ³ã‚¯åŒ–
        metadata_chunker = MetadataChunker()
        document_data = {
            'file_name': file_name,
            'summary': '',
            'document_date': document_date,
            'tags': tags,
            'doc_type': doc.get('doc_type'),
            'display_subject': display_subject,
            'display_post_text': display_post_text,
            'display_sender': doc.get('display_sender'),
            'display_type': doc.get('display_type'),
            'display_sent_at': doc.get('display_sent_at'),
            'classroom_sender_email': doc.get('classroom_sender_email'),
            'attachment_text': attachment_text,
            'persons': stageh_metadata.get('persons', []) if isinstance(stageh_metadata, dict) else [],
            'organizations': stageh_metadata.get('organizations', []) if isinstance(stageh_metadata, dict) else [],
            'people': stageh_metadata.get('people', []) if isinstance(stageh_metadata, dict) else []
        }

        chunks = metadata_chunker.create_metadata_chunks(document_data)

        # æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤
        try:
            self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
        except Exception as e:
            logger.warning(f"æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {e}")

        # Stage K: Embedding + ä¿å­˜
        stage_k_result = self.pipeline.stage_k.embed_and_save(document_id, chunks)

        if not stage_k_result.get('success'):
            logger.error(f"Stage Kå¤±æ•—: {stage_k_result.get('failed_count', 0)}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å¤±æ•—")
            return False

        logger.info(f"ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å®Œäº†: {stage_k_result.get('saved_count', 0)}/{len(chunks)}ä»¶")

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°
        try:
            self.db.client.table('Rawdata_FILE_AND_MAIL').update({
                'tags': tags,
                'document_date': document_date,
                'metadata': stageh_metadata
            }).eq('id', document_id).execute()
        except Exception as e:
            logger.error(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return False

        return True

    async def _process_with_attachment(
        self,
        doc: Dict[str, Any],
        preserve_workspace: bool = True
    ) -> bool:
        """æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚ã‚Šãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†"""
        document_id = doc['id']
        file_name = doc.get('file_name', 'unknown')
        drive_file_id = doc.get('source_id')

        if not drive_file_id:
            logger.error("source_idï¼ˆDrive File IDï¼‰ãŒã‚ã‚Šã¾ã›ã‚“")
            return False

        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
        file_extension = Path(file_name).suffix.lower()
        if file_extension in self.VIDEO_EXTENSIONS:
            logger.info(f"â­ï¸  å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {file_name}")
            return False

        # Driveã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        try:
            self.drive.download_file(drive_file_id, file_name, str(self.temp_dir))
            local_path = self.temp_dir / file_name
        except Exception as e:
            # 404ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰ã®å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            error_str = str(e)
            if 'File not found' in error_str or '404' in error_str:
                logger.warning(f"Driveã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ãƒ†ã‚­ã‚¹ãƒˆã®ã¿å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {file_name}")
                return await self._process_text_only(doc, preserve_workspace)
            else:
                logger.error(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                return False

        # MIMEã‚¿ã‚¤ãƒ—ã‚’æ¨æ¸¬
        mime_type = doc.get('mimeType', 'application/octet-stream')

        # çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å‡¦ç†
        try:
            workspace_to_use = doc.get('workspace', 'unknown') if preserve_workspace else 'unknown'

            result = await self.pipeline.process_document(
                file_path=Path(local_path),
                file_name=file_name,
                doc_type=doc.get('doc_type', 'other'),
                workspace=workspace_to_use,
                mime_type=mime_type,
                source_id=drive_file_id,
                existing_document_id=document_id,
                extra_metadata={
                    'display_subject': doc.get('display_subject'),
                    'display_post_text': doc.get('display_post_text'),
                    'display_sender': doc.get('display_sender'),
                    'display_type': doc.get('display_type'),
                    'display_sent_at': doc.get('display_sent_at'),
                    'classroom_sender_email': doc.get('classroom_sender_email')
                }
            )

            return result.get('success', False)

        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            if local_path.exists():
                local_path.unlink()
                logger.debug(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {local_path}")

    async def run(
        self,
        workspace: str = 'all',
        limit: int = 100,
        preserve_workspace: bool = True
    ):
        """
        å‡¦ç†ã‚’å®Ÿè¡Œ

        Args:
            workspace: å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹
            limit: å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•°
            preserve_workspace: workspaceã‚’ä¿æŒã™ã‚‹ã‹
        """
        logger.info("="*80)
        logger.info("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰")
        logger.info("="*80)

        # pending ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        docs = self.get_pending_documents(workspace, limit)

        if not docs:
            logger.info("å‡¦ç†å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            return

        logger.info(f"å‡¦ç†å¯¾è±¡: {len(docs)}ä»¶")
        logger.info("")

        # çµ±è¨ˆ
        stats = {'success': 0, 'failed': 0, 'total': len(docs)}

        # é †æ¬¡å‡¦ç†
        for i, doc in enumerate(docs, 1):
            file_name = doc.get('file_name', 'unknown')
            logger.info(f"\n{'='*80}")
            logger.info(f"[{i}/{len(docs)}] å‡¦ç†é–‹å§‹: {file_name}")
            logger.info(f"Document ID: {doc['id']}")

            success = await self.process_document(doc, preserve_workspace)

            if success:
                stats['success'] += 1
            else:
                stats['failed'] += 1

            logger.info(f"é€²æ—: æˆåŠŸ={stats['success']}, å¤±æ•—={stats['failed']}, æ®‹ã‚Š={len(docs)-i}")

        # æœ€çµ‚çµæœ
        logger.info("\n" + "="*80)
        logger.info("å‡¦ç†å®Œäº†")
        logger.info("="*80)
        logger.info(f"âœ… æˆåŠŸ: {stats['success']}ä»¶")
        logger.info(f"âŒ å¤±æ•—: {stats['failed']}ä»¶")
        logger.info(f"ğŸ“Š åˆè¨ˆ: {stats['total']}ä»¶")
        logger.info("="*80)


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰')
    parser.add_argument('--workspace', default='all', help='å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: all)')
    parser.add_argument('--limit', type=int, default=100, help='å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100)')
    parser.add_argument('--no-preserve-workspace', action='store_true', help='workspaceã‚’ä¿æŒã—ãªã„')

    args = parser.parse_args()

    processor = DocumentProcessor()
    await processor.run(
        workspace=args.workspace,
        limit=args.limit,
        preserve_workspace=not args.no_preserve_workspace
    )


if __name__ == '__main__':
    asyncio.run(main())
