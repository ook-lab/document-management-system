"""
2æ®µéšå–ã‚Šè¾¼ã¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆv4.0: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰AIç‰ˆï¼‰
Stage 2ï¼ˆClaudeè©³ç´°æŠ½å‡ºï¼‰å®Ÿè£…ç‰ˆ
"""

import os
import asyncio
from typing import Dict, Any, Optional
from pathlib import Path
from datetime import datetime
from loguru import logger
import hashlib
import json
import traceback

from core.connectors.google_drive import GoogleDriveConnector
from core.processors.pdf import PDFProcessor
from core.processors.office import OfficeProcessor
from core.ai.stage1_classifier import Stage1Classifier
from core.ai.stage2_extractor import Stage2Extractor
from core.ai.confidence_calculator import calculate_total_confidence
from core.ai.json_validator import validate_metadata
# from core.ai.embeddings import EmbeddingClient  # 768æ¬¡å…ƒ - ä½¿ç”¨ã—ãªã„
from core.database.client import DatabaseClient
from core.ai.llm_client import LLMClient
from core.utils.chunking import chunk_document, chunk_document_parent_child
from core.utils.synthetic_chunks import create_all_synthetic_chunks
from config.yaml_loader import get_classification_yaml_string


def flatten_metadata_to_text(metadata: Dict[str, Any]) -> str:
    """
    ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢å¯èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
    weekly_scheduleã€text_blocksã€special_eventsãªã©ã‚’å¹³å¦åŒ–ã—ã¦æ¤œç´¢å¯¾è±¡ã«ã™ã‚‹
    """
    searchable_parts = []

    # weekly_schedule ã®å±•é–‹
    if 'weekly_schedule' in metadata and metadata['weekly_schedule']:
        for day_schedule in metadata['weekly_schedule']:
            # æ—¥ä»˜ã¨æ›œæ—¥
            if 'date' in day_schedule:
                searchable_parts.append(day_schedule['date'])
            if 'day_of_week' in day_schedule:
                searchable_parts.append(day_schedule['day_of_week'])
            if 'day' in day_schedule:
                searchable_parts.append(f"{day_schedule['day']}æ›œæ—¥")

            # ã‚¤ãƒ™ãƒ³ãƒˆ
            if 'events' in day_schedule and day_schedule['events']:
                searchable_parts.extend(day_schedule['events'])

            # ãƒãƒ¼ãƒˆ
            if 'note' in day_schedule and day_schedule['note']:
                searchable_parts.append(day_schedule['note'])

            # ã‚¯ãƒ©ã‚¹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
            if 'class_schedules' in day_schedule:
                for class_schedule in day_schedule['class_schedules']:
                    if 'class' in class_schedule:
                        searchable_parts.append(class_schedule['class'])

                    # subjectsé…åˆ—
                    if 'subjects' in class_schedule and class_schedule['subjects']:
                        searchable_parts.extend(class_schedule['subjects'])

                    # periodsé…åˆ—
                    if 'periods' in class_schedule and class_schedule['periods']:
                        for period in class_schedule['periods']:
                            if 'subject' in period:
                                searchable_parts.append(period['subject'])
                            if 'time' in period:
                                searchable_parts.append(period['time'])

    # text_blocks ã®å±•é–‹
    if 'text_blocks' in metadata and metadata['text_blocks']:
        for block in metadata['text_blocks']:
            if 'title' in block and block['title']:
                searchable_parts.append(block['title'])
            if 'content' in block and block['content']:
                searchable_parts.append(block['content'])

    # special_events ã®å±•é–‹
    if 'special_events' in metadata and metadata['special_events']:
        searchable_parts.extend(metadata['special_events'])

    # basic_info ã®å±•é–‹
    if 'basic_info' in metadata and metadata['basic_info']:
        basic_info = metadata['basic_info']
        for key, value in basic_info.items():
            if value:
                searchable_parts.append(str(value))

    return ' '.join(searchable_parts)

PROCESSING_STATUS = {
    "PENDING": "pending",
    "COMPLETED": "completed",
    "FAILED": "failed",
    "SKIPPED": "skipped"
}

class TwoStageIngestionPipeline:
    """2æ®µéšå–ã‚Šè¾¼ã¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, temp_dir: str = "./temp"):
        
        self.llm_client = LLMClient()
        self.drive = GoogleDriveConnector()
        self.db = DatabaseClient()
        self.yaml_string = get_classification_yaml_string()

        self.pdf_processor = PDFProcessor(llm_client=self.llm_client)
        self.office_processor = OfficeProcessor()

        self.stage1_classifier = Stage1Classifier(llm_client=self.llm_client)
        self.stage2_extractor = Stage2Extractor(llm_client=self.llm_client)
        # Embeddingã¯LLMClientçµŒç”±ã§ç”Ÿæˆï¼ˆ1536æ¬¡å…ƒï¼‰
        # self.embeddings = EmbeddingClient()  # å‰Šé™¤
        
        self.temp_dir = Path(temp_dir)
        self.temp_dir.mkdir(parents=True, exist_ok=True)

        # Stage 2ã¯å®Œå…¨ãƒªãƒ¬ãƒ¼æ–¹å¼ï¼ˆåˆ¤å®šãªã—ã€Stage 1ã®çµæœã‚’å¿…ãšStage 2ã¸ï¼‰

        logger.info("TwoStageIngestionPipelineåˆæœŸåŒ–å®Œäº† (å®Œå…¨ãƒªãƒ¬ãƒ¼æ–¹å¼: Geminiâ†’Haiku)")
    
    def _extract_text(self, local_path: str, mime_type: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
        
        logger.debug(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºé–‹å§‹: {local_path}, mime_type={mime_type}")
        logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª: {Path(local_path).exists()}")
        logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {Path(local_path).stat().st_size if Path(local_path).exists() else 'N/A'} bytes")
        
        mime_map = {
            "application/pdf": self.pdf_processor.extract_text,
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document": self.office_processor.extract_from_docx,
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet": self.office_processor.extract_from_xlsx,
            "application/vnd.openxmlformats-officedocument.presentationml.presentation": self.office_processor.extract_from_pptx,
        }
        
        processor = mime_map.get(mime_type)
        if processor:
            result = processor(local_path)
            logger.debug(f"æŠ½å‡ºçµæœ: success={result.get('success')}, content_length={len(result.get('content', ''))}")
            return result
        
        return {"content": "", "metadata": {}, "success": False, "error_message": f"Unsupported MIME Type: {mime_type}"}

    def _get_file_type(self, mime_type: str) -> str:
        """MIME Typeã‹ã‚‰file_typeã‚’åˆ¤å®š"""
        mapping = {
            "application/pdf": "pdf",
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document": "docx",
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet": "xlsx",
            "application/vnd.openxmlformats-officedocument.presentationml.presentation": "pptx",
        }
        return mapping.get(mime_type, "other")
    
    def _should_run_stage2(self, stage1_result: Dict[str, Any], extracted_text: str) -> bool:
        """
        Stage 2ã‚’å®Ÿè¡Œã™ã¹ãã‹ã©ã†ã‹åˆ¤å®šï¼ˆå®Œå…¨ãƒªãƒ¬ãƒ¼æ–¹å¼ï¼‰

        ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‘
        - Stage 1 (Gemini): æ–‡æ›¸ã®åˆ†é¡ã¨åŸºæœ¬æƒ…å ±ã®æŠ½å‡º
        - Stage 2 (Haiku): Stage 1ã®çµæœã‚’å—ã‘ã¦æ§‹é€ åŒ–ãƒ»æ„å‘³ä»˜ã‘

        ãƒ†ã‚­ã‚¹ãƒˆãŒå­˜åœ¨ã™ã‚‹é™ã‚Šã€Stage 1ã®çµæœã¯å¿…ãšStage 2ï¼ˆHaikuï¼‰ã«æ¸¡ã—ã¦æ§‹é€ åŒ–ã™ã‚‹ã€‚
        ä¿¡é ¼åº¦ã«é–¢ä¿‚ãªãã€åˆ¤å®šãªã—ã®å®Œå…¨ãƒªãƒ¬ãƒ¼æ–¹å¼ã§å‹•ä½œã™ã‚‹ã€‚
        """

        # æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®å ´åˆã€ã¾ãŸã¯æ¥µç«¯ã«çŸ­ã„å ´åˆã®ã¿ã‚¹ã‚­ãƒƒãƒ—
        if not extracted_text or len(extracted_text.strip()) < 50:
            logger.info("[Stage 2] ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            return False

        # ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹é™ã‚Šã€ç„¡æ¡ä»¶ã§Stage 2ï¼ˆæ§‹é€ åŒ–ãƒ—ãƒ­ã‚»ã‚¹ï¼‰ã¸
        doc_type = stage1_result.get('doc_type', 'other')
        logger.info(f"[Stage 2] æ§‹é€ åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã¸ç§»è¡Œ ({doc_type})")
        return True

    async def process_file(
        self,
        file_meta: Dict[str, Any],
        workspace: str = "personal",
        force_reprocess: bool = False
    ) -> Optional[Dict[str, Any]]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’2æ®µéšã§å‡¦ç†"""
        file_id = file_meta['id']
        file_name = file_meta['name']
        mime_type = file_meta.get('mimeType', 'application/octet-stream')

        logger.info(f"=== 2æ®µéšå‡¦ç†é–‹å§‹: {file_name} ===")

        existing = self.db.get_document_by_source_id(file_id)
        if existing and not force_reprocess:
            logger.warning(f"æ—¢ã«å‡¦ç†æ¸ˆã¿ (Source ID): {file_name}")
            return existing

        if existing and force_reprocess:
            logger.info(f"ğŸ”„ å†å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä¸Šæ›¸ãã—ã¾ã™")
        
        local_path = None
        # extraction_resultã‚’åˆæœŸåŒ–ï¼ˆNameErrorå›é¿ï¼‰
        extraction_result = {"success": False, "content": "", "metadata": {}, "error_message": "æœªå®Ÿè¡Œ"}

        try:
            # ============================================
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            # ============================================
            local_path = self.drive.download_file(file_id, file_name, self.temp_dir)
            logger.debug(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {local_path}")

            # ============================================
            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆStage 1ã®å‰ã«å®Ÿè¡Œï¼‰
            # ============================================
            extraction_result = self._extract_text(local_path, mime_type)

            if not extraction_result["success"]:
                logger.warning(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: {file_name}")
                logger.warning(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {extraction_result.get('error_message')}")
                extracted_text = ""
            else:
                extracted_text = extraction_result["content"]

            base_metadata = extraction_result.get("metadata", {})

            # ============================================
            # Stage 1: Geminiåˆ†é¡ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã™ï¼‰
            # ============================================
            logger.info("[Stage 1] Geminiåˆ†é¡é–‹å§‹...")
            stage1_result = await self.stage1_classifier.classify(
                file_path=Path(local_path),
                doc_types_yaml=self.yaml_string,
                mime_type=mime_type,
                text_content=extracted_text
            )

            # Stage1ã¯doc_typeã¨workspaceã‚’è¿”ã•ãªã„ï¼ˆå…¥åŠ›å…ƒã§æ±ºå®šã•ã‚Œã‚‹ãŸã‚ï¼‰
            # workspaceã¯å¼•æ•°ã§æ¸¡ã•ã‚ŒãŸå€¤ã‚’ãã®ã¾ã¾ä½¿ç”¨
            summary = stage1_result.get('summary', '')
            relevant_date = stage1_result.get('relevant_date')
            stage1_confidence = stage1_result.get('confidence', 0.0)

            logger.info(f"[Stage 1] å®Œäº†: summary={summary[:50] if summary else ''}..., confidence={stage1_confidence:.2f}")

            # ============================================
            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãŒå¤±æ•—ã—ãŸå ´åˆã§ã‚‚summaryã‚’ä½¿ç”¨
            # ============================================
            if not extraction_result["success"]:
                logger.warning("[ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º] å¤±æ•— â†’ summaryã‚’full_textã¨ã—ã¦ä½¿ç”¨")
                extracted_text = summary

            # ============================================
            # Stage 2åˆ¤å®šãƒ»å®Ÿè¡Œï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—ã§ã‚‚å®Ÿè¡Œï¼‰
            # ============================================
            if self._should_run_stage2(stage1_result, extracted_text):
                logger.info("[Stage 2] Claudeè©³ç´°æŠ½å‡ºé–‹å§‹...")
                try:
                    stage2_result = self.stage2_extractor.extract_metadata(
                        full_text=extracted_text,
                        file_name=file_name,
                        stage1_result=stage1_result,
                        workspace=workspace  # å¼•æ•°ã§æ¸¡ã•ã‚ŒãŸå…ƒã®workspaceã‚’ä½¿ç”¨
                    )

                    # Stage 2ã®çµæœã‚’åæ˜ ï¼ˆdoc_typeã¯ä½¿ã‚ãªã„ï¼‰
                    summary = stage2_result.get('summary', summary)
                    document_date = stage2_result.get('document_date')
                    tags = stage2_result.get('tags', [])
                    tables = stage2_result.get('tables', [])  # è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                    stage2_metadata = stage2_result.get('metadata', {})
                    stage2_confidence = stage2_result.get('extraction_confidence', 0.0)

                    # metadataã‚’ãƒãƒ¼ã‚¸ï¼ˆStage 2å„ªå…ˆï¼‰
                    metadata = {
                        **base_metadata,
                        **stage2_metadata,
                        'stage2_attempted': True
                    }
                    if tags:
                        metadata['tags'] = tags
                    if document_date:
                        metadata['document_date'] = document_date
                    if tables:
                        metadata['tables'] = tables  # è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’metadataã«è¿½åŠ 

                    # æœ€çµ‚çš„ãªä¿¡é ¼åº¦ï¼ˆStage 1ã¨Stage 2ã®åŠ é‡å¹³å‡ï¼‰
                    confidence = (stage1_confidence * 0.3 + stage2_confidence * 0.7)
                    processing_stage = 'stage1_and_stage2'
                    stage2_model = 'claude-haiku-4-5-20251001'  # æœ€æ–°ã®Haiku 4.5ãƒ¢ãƒ‡ãƒ«

                    logger.info(f"[Stage 2] å®Œäº†: confidence={stage2_confidence:.2f}, metadata_fields={len(stage2_metadata)}")

                    # ============================================
                    # JSON Schemaæ¤œè¨¼ï¼ˆPhase 2 - Track 1ï¼‰
                    # ============================================
                    logger.info("[JSONæ¤œè¨¼] ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼é–‹å§‹...")
                    is_valid, validation_error = validate_metadata(
                        metadata=stage2_metadata,
                        doc_type=doc_type
                    )

                    if not is_valid:
                        # æ¤œè¨¼å¤±æ•—æ™‚ã®å‡¦ç†
                        # KeyErrorå›é¿: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®‰å…¨ã«æ–‡å­—åˆ—åŒ–
                        safe_validation_error = str(validation_error).replace('{', '{{').replace('}', '}}')
                        logger.error(f"[JSONæ¤œè¨¼] æ¤œè¨¼å¤±æ•—: {safe_validation_error}")

                        # metadataã«æ¤œè¨¼å¤±æ•—æƒ…å ±ã‚’è¨˜éŒ²
                        metadata['schema_validation'] = {
                            'is_valid': False,
                            'error_message': validation_error,
                            'validated_at': datetime.now().isoformat()
                        }

                        # ä¿¡é ¼åº¦ã‚’æ¸›ç‚¹ï¼ˆæ¤œè¨¼å¤±æ•—ã¯é‡å¤§ãªå“è³ªå•é¡Œï¼‰
                        confidence = confidence * 0.8  # 20%æ¸›ç‚¹
                        logger.warning(f"[JSONæ¤œè¨¼] ä¿¡é ¼åº¦ã‚’æ¸›ç‚¹: {confidence:.2f} (æ¤œè¨¼å¤±æ•—ã®ãŸã‚)")
                    else:
                        logger.info("[JSONæ¤œè¨¼] [OK] æ¤œè¨¼æˆåŠŸ")
                        metadata['schema_validation'] = {
                            'is_valid': True,
                            'validated_at': datetime.now().isoformat()
                        }

                except Exception as e:
                    error_msg = str(e)
                    error_traceback = traceback.format_exc()
                    # KeyErrorå›é¿: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®‰å…¨ã«æ–‡å­—åˆ—åŒ–
                    safe_error_msg = error_msg.replace('{', '{{').replace('}', '}}')
                    safe_traceback = error_traceback.replace('{', '{{').replace('}', '}}')
                    logger.error(f"[Stage 2] å‡¦ç†ã‚¨ãƒ©ãƒ¼: {safe_error_msg}\n{safe_traceback}")

                    # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’metadataã«è¨˜éŒ²
                    metadata = {
                        **base_metadata,
                        'stage2_attempted': True,
                        'stage2_error': str(e),
                        'stage2_error_type': type(e).__name__,
                        'stage2_error_timestamp': datetime.now().isoformat()
                    }

                    confidence = stage1_confidence
                    processing_stage = 'stage2_failed'
                    stage2_model = None
            else:
                # Stage 1ã®ã¿ã§å®Œçµ
                confidence = stage1_confidence
                processing_stage = 'stage1_only'
                metadata = {**base_metadata, 'stage2_attempted': False}
                stage2_model = None

            # ============================================
            # è¤‡åˆä¿¡é ¼åº¦è¨ˆç®—ï¼ˆPhase 2 - Track 1ï¼‰
            # ============================================
            logger.info("[è¤‡åˆä¿¡é ¼åº¦] ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—é–‹å§‹...")
            confidence_scores = calculate_total_confidence(
                model_confidence=confidence,
                text=extracted_text,
                metadata=metadata,
                doc_type=doc_type
            )

            total_confidence = confidence_scores['total_confidence']
            keyword_match_score = confidence_scores['keyword_match_score']
            metadata_completeness = confidence_scores['metadata_completeness']
            data_consistency = confidence_scores['data_consistency']

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«å„ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ï¼ˆåˆ†æç”¨ï¼‰
            metadata['quality_scores'] = {
                'keyword_match': keyword_match_score,
                'metadata_completeness': metadata_completeness,
                'data_consistency': data_consistency
            }

            logger.info(f"[è¤‡åˆä¿¡é ¼åº¦] å®Œäº†: total_confidence={total_confidence:.3f}")

            # ============================================
            # Embeddingç”Ÿæˆï¼ˆOpenAI text-embedding-3-smallã€1536æ¬¡å…ƒï¼‰
            # ============================================
            if extracted_text:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢å¯èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
                metadata_text = flatten_metadata_to_text(metadata) if metadata else ""

                # æœ¬æ–‡ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¦embeddingç”Ÿæˆ
                combined_text = extracted_text[:7000]  # æœ¬æ–‡ã‚’7000æ–‡å­—ã«åˆ¶é™
                if metadata_text:
                    combined_text += "\n\n[ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿]\n" + metadata_text[:1000]  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’1000æ–‡å­—è¿½åŠ 

                embedding = self.llm_client.generate_embedding(combined_text)
                logger.info(f"[Embedding] ç”Ÿæˆå®Œäº†: æœ¬æ–‡{len(extracted_text[:7000])}æ–‡å­— + ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿{len(metadata_text[:1000])}æ–‡å­—")
            else:
                embedding = None
            
            # ============================================
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
            # ============================================
            content_hash = hashlib.sha256(extracted_text.encode('utf-8')).hexdigest() if extracted_text else None
            
            # ============================================
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ï¼ˆNullæ–‡å­—é™¤å» + é‡è¤‡ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼‰
            # ============================================
            # Nullæ–‡å­—ã‚’é™¤å»
            if extracted_text:
                extracted_text = extracted_text.replace('\x00', '')
            if summary:
                summary = summary.replace('\x00', '')

            # metadata.tables ã‹ã‚‰ extracted_tables ã‚’æŠ½å‡º
            extracted_tables = None
            if 'tables' in metadata and metadata['tables']:
                extracted_tables = metadata['tables']

            document_data = {
                "source_type": "drive",
                "source_id": file_id,
                "source_url": f"https://drive.google.com/file/d/{file_id}/view",
                "drive_file_id": file_id,
                "file_name": file_name,
                "file_type": self._get_file_type(mime_type),
                "doc_type": workspace,  # doc_typeã¯å…¥åŠ›å…ƒã§æ±ºå®šï¼ˆworkspaceã¨åŒã˜å€¤ã‚’ä½¿ç”¨ï¼‰
                "workspace": workspace,  # å¼•æ•°ã§æ¸¡ã•ã‚ŒãŸå€¤ã‚’ä½¿ç”¨ï¼ˆå…¥åŠ›å…ƒã§æ±ºå®šï¼‰
                "full_text": extracted_text,
                "summary": summary,
                "embedding": embedding,
                "metadata": metadata,
                "extracted_tables": extracted_tables,  # UIã§ã®è¡¨è¡¨ç¤ºç”¨
                "content_hash": content_hash,
                "confidence": confidence,  # AIãƒ¢ãƒ‡ãƒ«ã®ç¢ºä¿¡åº¦
                "total_confidence": total_confidence,  # è¤‡åˆä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
                "processing_status": PROCESSING_STATUS["COMPLETED"],
                "processing_stage": processing_stage,
                "stage1_model": "gemini-2.5-flash",
                "stage2_model": stage2_model,
                "relevant_date": relevant_date,
            }

            try:
                # upsertã‚’ä½¿ç”¨
                # force_reprocess=Trueæ™‚ã¯å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ›´æ–°ã™ã‚‹ãŒã€GASç”±æ¥ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ä¿æŒ
                preserve_fields = ['doc_type', 'workspace'] if force_reprocess else []
                result = await self.db.upsert_document(
                    'documents',
                    document_data,
                    conflict_column='source_id',
                    force_update=force_reprocess,
                    preserve_fields=preserve_fields
                )
                document_id = result.get('id')
                logger.info(f"Documentä¿å­˜å®Œäº†ï¼ˆupsert, force_update={force_reprocess}ï¼‰: {document_id}")

                # ============================================
                # ãƒãƒ£ãƒ³ã‚¯åŒ–å‡¦ç†ï¼ˆ2éšå±¤ï¼šå°ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ç”¨ + å¤§ãƒãƒ£ãƒ³ã‚¯å›ç­”ç”¨ + åˆæˆãƒãƒ£ãƒ³ã‚¯ï¼‰
                # ============================================
                if extracted_text and document_id:
                    logger.info(f"  ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®2éšå±¤ãƒãƒ£ãƒ³ã‚¯åŒ–é–‹å§‹ï¼ˆå°ãƒ»å¤§ãƒ»åˆæˆï¼‰")
                    try:
                        # å°ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆæ¤œç´¢ç”¨ï¼‰
                        small_chunks = chunk_document(
                            text=extracted_text,
                            chunk_size=300,
                            chunk_overlap=50
                        )

                        logger.info(f"  å°ãƒãƒ£ãƒ³ã‚¯ä½œæˆå®Œäº†: {len(small_chunks)}å€‹")

                        # ã‚¹ãƒ†ãƒƒãƒ—1: å°ãƒãƒ£ãƒ³ã‚¯ï¼ˆæ¤œç´¢ç”¨ï¼‰ã‚’ä¿å­˜
                        logger.info(f"  å°ãƒãƒ£ãƒ³ã‚¯ï¼ˆæ¤œç´¢ç”¨ï¼‰ã®ä¿å­˜é–‹å§‹: {len(small_chunks)}å€‹")
                        small_chunk_success_count = 0
                        for small_chunk in small_chunks:
                            try:
                                small_text = small_chunk.get('chunk_text', '')
                                small_embedding = self.llm_client.generate_embedding(small_text)

                                small_doc = {
                                    'document_id': document_id,
                                    'chunk_index': small_chunk.get('chunk_index', 0),
                                    'chunk_text': small_text,
                                    'chunk_size': small_chunk.get('chunk_size', len(small_text)),
                                    'embedding': small_embedding
                                }

                                chunk_result = await self.db.insert_document('document_chunks', small_doc)
                                if chunk_result:
                                    small_chunk_success_count += 1
                            except Exception as chunk_insert_error:
                                logger.error(f"  å°ãƒãƒ£ãƒ³ã‚¯ä¿å­˜ã‚¨ãƒ©ãƒ¼: {type(chunk_insert_error).__name__}: {chunk_insert_error}")
                                logger.debug(f"  ã‚¨ãƒ©ãƒ¼è©³ç´°: {repr(chunk_insert_error)}", exc_info=True)

                        logger.info(f"  å°ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å®Œäº†: {small_chunk_success_count}/{len(small_chunks)}å€‹")

                        # ã‚¹ãƒ†ãƒƒãƒ—2: å¤§ãƒãƒ£ãƒ³ã‚¯ï¼ˆå…¨æ–‡ãƒ»å›ç­”ç”Ÿæˆç”¨ï¼‰ã‚’ä¿å­˜
                        logger.info(f"  å¤§ãƒãƒ£ãƒ³ã‚¯ï¼ˆå…¨æ–‡ï¼‰ã®ä¿å­˜é–‹å§‹")
                        large_chunk_success_count = 0
                        current_chunk_index = len(small_chunks)
                        try:
                            # å…¨æ–‡ã‚’1ã¤ã®å¤§ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦ä¿å­˜
                            large_doc = {
                                'document_id': document_id,
                                'chunk_index': current_chunk_index,
                                'chunk_text': extracted_text,  # å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆ
                                'chunk_size': len(extracted_text),
                                'embedding': embedding  # å…¨æ–‡ã®embedding ã‚’ä½¿ç”¨
                            }

                            chunk_result = await self.db.insert_document('document_chunks', large_doc)
                            if chunk_result:
                                large_chunk_success_count = 1
                                current_chunk_index += 1
                        except Exception as chunk_insert_error:
                            logger.error(f"  å¤§ãƒãƒ£ãƒ³ã‚¯ä¿å­˜ã‚¨ãƒ©ãƒ¼: {type(chunk_insert_error).__name__}: {chunk_insert_error}")
                            logger.debug(f"  ã‚¨ãƒ©ãƒ¼è©³ç´°: {repr(chunk_insert_error)}", exc_info=True)

                        logger.info(f"  å¤§ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å®Œäº†: {large_chunk_success_count}/1å€‹")

                        # ã‚¹ãƒ†ãƒƒãƒ—3: åˆæˆãƒãƒ£ãƒ³ã‚¯ï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»è­°é¡Œç­‰ï¼‰ã‚’ç”Ÿæˆãƒ»ä¿å­˜
                        logger.info(f"  åˆæˆãƒãƒ£ãƒ³ã‚¯ï¼ˆæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å°‚ç”¨ï¼‰ã®ç”Ÿæˆé–‹å§‹")
                        synthetic_chunk_success_count = 0
                        synthetic_chunks = create_all_synthetic_chunks(metadata, file_name)

                        if synthetic_chunks:
                            logger.info(f"  åˆæˆãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆå®Œäº†: {len(synthetic_chunks)}å€‹")
                            for synthetic in synthetic_chunks:
                                try:
                                    synthetic_text = synthetic.get('content', '')
                                    synthetic_type = synthetic.get('type', 'unknown')

                                    if not synthetic_text:
                                        continue

                                    # åˆæˆãƒãƒ£ãƒ³ã‚¯ç”¨ã®embeddingã‚’ç”Ÿæˆ
                                    synthetic_embedding = self.llm_client.generate_embedding(synthetic_text)

                                    synthetic_doc = {
                                        'document_id': document_id,
                                        'chunk_index': current_chunk_index,
                                        'chunk_text': synthetic_text,
                                        'chunk_size': len(synthetic_text),
                                        'embedding': synthetic_embedding,
                                        'section_title': f'[åˆæˆãƒãƒ£ãƒ³ã‚¯: {synthetic_type}]'  # è­˜åˆ¥ç”¨
                                    }

                                    chunk_result = await self.db.insert_document('document_chunks', synthetic_doc)
                                    if chunk_result:
                                        synthetic_chunk_success_count += 1
                                        current_chunk_index += 1
                                        logger.info(f"  âœ… åˆæˆãƒãƒ£ãƒ³ã‚¯ä¿å­˜æˆåŠŸ: {synthetic_type} ({len(synthetic_text)}æ–‡å­—)")
                                except Exception as chunk_insert_error:
                                    logger.error(f"  åˆæˆãƒãƒ£ãƒ³ã‚¯ä¿å­˜ã‚¨ãƒ©ãƒ¼: {type(chunk_insert_error).__name__}: {chunk_insert_error}")
                                    logger.debug(f"  ã‚¨ãƒ©ãƒ¼è©³ç´°: {repr(chunk_insert_error)}", exc_info=True)
                        else:
                            logger.info(f"  åˆæˆãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ: å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")

                        total_chunks = small_chunk_success_count + large_chunk_success_count + synthetic_chunk_success_count
                        logger.info(f"  ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å®Œäº†ï¼ˆåˆè¨ˆï¼‰: {total_chunks}å€‹ï¼ˆå°{small_chunk_success_count}å€‹ + å¤§{large_chunk_success_count}å€‹ + åˆæˆ{synthetic_chunk_success_count}å€‹ï¼‰")

                        # ã‚¹ãƒ†ãƒƒãƒ—4: document ã® chunk_count ã‚’æ›´æ–°
                        try:
                            update_data = {
                                'chunk_count': total_chunks,
                                'chunking_strategy': 'small_large_synthetic'  # å°ãƒãƒ£ãƒ³ã‚¯ + å¤§ãƒãƒ£ãƒ³ã‚¯ + åˆæˆãƒãƒ£ãƒ³ã‚¯
                            }
                            response = (
                                self.db.client.table('documents')
                                .update(update_data)
                                .eq('id', document_id)
                                .execute()
                            )
                            logger.info(f"  Document chunk_countæ›´æ–°å®Œäº†: {total_chunks}å€‹")
                        except Exception as update_error:
                            logger.error(f"  Document chunk_countæ›´æ–°ã‚¨ãƒ©ãƒ¼: {update_error}")

                    except Exception as chunk_error:
                        logger.error(f"  ãƒãƒ£ãƒ³ã‚¯åŒ–ã‚¨ãƒ©ãƒ¼: {chunk_error}", exc_info=True)
                else:
                    logger.warning(f"  ãƒãƒ£ãƒ³ã‚¯åŒ–ã‚¹ã‚­ãƒƒãƒ—: extracted_text={bool(extracted_text)}, document_id={bool(document_id)}")

                logger.info(f"=== å‡¦ç†å®Œäº†: {file_name} ({doc_type}, {processing_stage}) ===")
                return {"success": True, "document_id": document_id, "doc_type": doc_type}
            except Exception as db_error:
                # é‡è¤‡ã‚¨ãƒ©ãƒ¼ï¼ˆ23505ï¼‰ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                error_str = str(db_error)
                if '23505' in error_str or 'duplicate' in error_str.lower():
                    logger.warning(f"é‡è¤‡ã‚¨ãƒ©ãƒ¼æ¤œå‡ºï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {file_name} - {error_str}")
                    return {"status": "skipped", "reason": "duplicate"}
                else:
                    # ãã®ä»–ã®DBã‚¨ãƒ©ãƒ¼ã¯å†ã‚¹ãƒ­ãƒ¼
                    raise
            
        except Exception as e:
            error_msg = str(e)
            error_traceback = traceback.format_exc()
            # KeyErrorå›é¿: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®‰å…¨ã«æ–‡å­—åˆ—åŒ–
            safe_error_msg = error_msg.replace('{', '{{').replace('}', '}}')
            safe_traceback = error_traceback.replace('{', '{{').replace('}', '}}')
            logger.error(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_name} - {safe_error_msg}\n{safe_traceback}")
            
            error_data = {
                "source_type": "drive",
                "source_id": file_id,
                "file_name": file_name,
                "workspace": workspace,
                "processing_status": PROCESSING_STATUS["FAILED"],
                "error_message": str(e),
                "file_type": self._get_file_type(mime_type),
            }

            try:
                await self.db.insert_document('documents', error_data)
            except Exception as db_error:
                db_error_traceback = traceback.format_exc()
                # KeyErrorå›é¿: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®‰å…¨ã«æ–‡å­—åˆ—åŒ–
                safe_db_error = str(db_error).replace('{', '{{').replace('}', '}}')
                safe_db_traceback = db_error_traceback.replace('{', '{{').replace('}', '}}')
                logger.critical(f"DBä¿å­˜å¤±æ•—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰: {file_name} - DB Error: {safe_db_error}\n{safe_db_traceback}")

                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                fallback_dir = Path('logs/db_errors')
                fallback_dir.mkdir(parents=True, exist_ok=True)

                fallback_file = fallback_dir / f"db_error_{datetime.now():%Y%m%d_%H%M%S}_{file_id}.json"

                try:
                    with open(fallback_file, 'w', encoding='utf-8') as f:
                        json.dump({
                            'error_data': error_data,
                            'db_error': str(db_error),
                            'db_error_traceback': traceback.format_exc(),
                            'timestamp': datetime.now().isoformat()
                        }, f, ensure_ascii=False, indent=2)
                    logger.warning(f"ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜: {fallback_file}")
                except Exception as file_error:
                    # KeyErrorå›é¿: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®‰å…¨ã«æ–‡å­—åˆ—åŒ–
                    safe_file_error = str(file_error).replace('{', '{{').replace('}', '}}')
                    logger.critical(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ä¿å­˜ã‚‚å¤±æ•—: {safe_file_error}") 
                
            return None
            
        finally:
            if local_path and Path(local_path).exists():
                os.remove(local_path)
                logger.debug(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {local_path}")