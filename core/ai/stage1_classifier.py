"""
Stage 1 åˆ†é¡å™¨ (Gemini 2.5 Flash)

Phase 2: 50ç¨®é¡ã®æ›¸é¡ã‚¿ã‚¤ãƒ—åˆ†é¡ä½“ç³»ã«å¯¾å¿œ
"""
from typing import Dict, Any, Optional
from pathlib import Path
import json
from core.ai.llm_client import LLMClient
from config.DOC_TYPE_CONSTANTS import (
    ALL_DOC_TYPES,
    DOC_TYPE_METADATA,
    FOLDER_MAPPINGS,
    get_display_name
)


class Stage1Classifier:
    """Stage 1: Gemini 2.5 Flashã«ã‚ˆã‚‹åˆæœŸåˆ†é¡ (50ç¨®é¡å¯¾å¿œ)"""

    def __init__(self, llm_client: LLMClient):
        self.client = llm_client
        self.tier = "stage1_classification"

    def _generate_doc_types_list(self) -> str:
        """
        50ç¨®é¡ã®æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’ãƒ•ã‚©ãƒ«ãƒ€åˆ¥ã«æ•´ç†ã—ãŸãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ

        Returns:
            ãƒ•ã‚©ãƒ«ãƒ€åˆ¥ã«æ•´ç†ã•ã‚ŒãŸæ–‡æ›¸ã‚¿ã‚¤ãƒ—ãƒªã‚¹ãƒˆï¼ˆæ–‡å­—åˆ—ï¼‰
        """
        doc_types_text = []

        for folder_key, folder_types in FOLDER_MAPPINGS.items():
            # ãƒ•ã‚©ãƒ«ãƒ€åã‚’æ—¥æœ¬èªã«å¤‰æ›
            folder_names = {
                "ikuya_school": "ğŸ“š è‚²å“‰-å­¦æ ¡",
                "work": "ğŸ’¼ ä»•äº‹",
                "finance": "ğŸ’° å®¶è¨ˆãƒ»é‡‘è",
                "medical": "ğŸ¥ åŒ»ç™‚ãƒ»å¥åº·",
                "housing": "ğŸ  ä½ã¾ã„ãƒ»ä¸å‹•ç”£",
                "legal_admin": "âš–ï¸ æ³•å¾‹ãƒ»è¡Œæ”¿",
                "lifestyle": "ğŸ¨ è¶£å‘³ãƒ»ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«",
                "other": "ğŸ“ ãã®ä»–",
            }

            folder_display = folder_names.get(folder_key, folder_key)
            doc_types_text.append(f"\n{folder_display}:")

            for doc_type in folder_types:
                display_name = get_display_name(doc_type)
                doc_types_text.append(f"  - {doc_type}: {display_name}")

        return "\n".join(doc_types_text)

    def generate_classification_prompt(self, doc_types_yaml: str = None) -> str:
        """
        åˆ†é¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆï¼ˆå˜ä¸€Doc Typeç‰ˆ: ikuya_schoolçµ±åˆï¼‰

        Args:
            doc_types_yaml: å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã—ã¦ã„ã‚‹ãŒã€ä½¿ç”¨ã—ãªã„

        Returns:
            åˆ†é¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        return f"""ã‚ãªãŸã¯æ–‡æ›¸åˆ†é¡ã®å°‚é–€å®¶ã§ã™ã€‚ã“ã®æ–‡æ›¸ã‚’åˆ†æã—ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:

{{
  "folder_category": "8ã¤ã®æœ€çµ‚ãƒ•ã‚©ãƒ«ãƒ€ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰1ã¤é¸æŠ",
  "doc_type": "ikuya_school",
  "workspace": "family/personal/work ã®ã„ãšã‚Œã‹",
  "relevant_date": "é‡è¦ãªæ—¥ä»˜ (YYYY-MM-DDå½¢å¼ã€ãªã‘ã‚Œã°null)",
  "summary": "æ–‡æ›¸ã®è¦ç´„ (100æ–‡å­—ä»¥å†…)",
  "confidence": 0.0ã‹ã‚‰1.0ã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
}}

**ã€Phase 4ã€‘8ã¤ã®æœ€çµ‚ãƒ•ã‚©ãƒ«ãƒ€ã‚«ãƒ†ã‚´ãƒª:**
1. **è‚²å“‰-å­¦æ ¡** - è‚²å“‰ã®å­¦æ ¡é–¢é€£ï¼ˆæ™‚é–“å‰²ã€å­¦ç´šé€šä¿¡ã€ã»ã‘ã‚“ã ã‚ˆã‚Šã€å­¦å¹´é€šä¿¡ãªã©ï¼‰
2. **è‚²å“‰-å¡¾** - è‚²å“‰ã®å¡¾é–¢é€£ï¼ˆå¡¾ã®æ¡ˆå†…ã€å®¿é¡Œã€ãƒ†ã‚­ã‚¹ãƒˆãªã©ï¼‰
3. **è‚²å“‰-å—é¨“** - è‚²å“‰ã®å—é¨“é–¢é€£ï¼ˆéå»å•ã€æ¨¡è©¦çµæœã€å—é¨“è¦é …ãªã©ï¼‰
4. **çµµéº»-å­¦æ ¡** - çµµéº»ã®å­¦æ ¡é–¢é€£ï¼ˆæ™‚é–“å‰²ã€å­¦ç´šé€šä¿¡ã€ã»ã‘ã‚“ã ã‚ˆã‚Šã€å­¦å¹´é€šä¿¡ãªã©ï¼‰
5. **å®¶-ç”Ÿæ´»** - å®¶æ—å…¨ä½“ã®ç”Ÿæ´»é–¢é€£ï¼ˆãƒãƒ³ã‚·ãƒ§ãƒ³ç†äº‹ä¼šã€å…¬å…±æ–™é‡‘ã€ä¿é™ºã€åŒ»ç™‚ãªã©ï¼‰
6. **å®¶-æ–™ç†æœ¬** - æ–™ç†æœ¬ãƒ»ãƒ¬ã‚·ãƒ”é›†
7. **å®œç´€-ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆ** - å®œç´€å€‹äººã®æ–‡æ›¸ï¼ˆä»•äº‹ä»¥å¤–ï¼‰
8. **ä»•äº‹** - å®œç´€ã®ä»•äº‹é–¢é€£æ›¸é¡

**æ–‡æ›¸ã‚¿ã‚¤ãƒ—:**
- **å…¨ã¦ã®æ–‡æ›¸ã¯ `ikuya_school` ã¨ã—ã¦åˆ†é¡ã•ã‚Œã¾ã™**ï¼ˆå˜ä¸€ã‚¹ã‚­ãƒ¼ãƒçµ±åˆï¼‰

**ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹åŸºæº–:**
- family: å­¦æ ¡ã€ãƒãƒ³ã‚·ãƒ§ãƒ³ç†äº‹ä¼šãªã©å®¶æ—å…¨ä½“ã®æ–‡æ›¸
- personal: åŒ»ç™‚ã€é‡‘èãªã©å€‹äººã®æ–‡æ›¸
- work: ä»•äº‹é–¢é€£ã®æ–‡æ›¸

**é‡è¦ãªæŒ‡ç¤º:**
1. folder_categoryã¯ä¸Šè¨˜8ã¤ã®ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰å¿…ãš1ã¤é¸æŠã—ã¦ãã ã•ã„
2. doc_typeã¯å¿…ãš `ikuya_school` ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼ˆå›ºå®šå€¤ï¼‰
3. å¿…ãšJSONå½¢å¼ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜ã¯ä¸è¦ï¼‰
4. confidenceã¯åˆ†é¡ã®ç¢ºä¿¡åº¦ã‚’0.0ã€œ1.0ã§ç¤ºã—ã¦ãã ã•ã„

å¿…ãšJSONå½¢å¼ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

    async def classify(
        self,
        file_path: Path,
        doc_types_yaml: str,
        mime_type: Optional[str] = None,
        text_content: Optional[str] = None
    ) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†é¡ï¼ˆPDFä»¥å¤–ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ï¼‰"""
        prompt = self.generate_classification_prompt(doc_types_yaml)

        # PDFä»¥å¤–ã®å ´åˆï¼ˆExcelã€Wordç­‰ï¼‰ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã‚€
        if mime_type and mime_type != "application/pdf" and text_content:
            # ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
            prompt += f"\n\n**ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹:**\n{text_content[:5000]}"  # æœ€å¤§5000æ–‡å­—
            response = self.client.call_model(
                tier=self.tier,
                prompt=prompt,
                file_path=None  # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
            )
        else:
            # PDFã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            response = self.client.call_model(
                tier=self.tier,
                prompt=prompt,
                file_path=file_path
            )

        if not response.get("success"):
            raise ValueError(f"Stage1åˆ†é¡ã«å¤±æ•—: {response.get('error')}")

        # JSONå¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹
        try:
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
            content = response["content"]
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            result = json.loads(content)
            return result
        except json.JSONDecodeError as e:
            raise ValueError(f"Stage1åˆ†é¡çµæœã®JSONè§£æã«å¤±æ•—: {e}\nå¿œç­”: {response['content']}")