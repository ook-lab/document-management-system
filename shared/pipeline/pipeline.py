"""
çµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (Stage E-K) - è¨­å®šãƒ™ãƒ¼ã‚¹ç‰ˆ

è¨­è¨ˆæ›¸: DESIGN_UNIFIED_PIPELINE.md v2.0 ã«æº–æ‹ 
å‡¦ç†é †åº: Stage E â†’ F â†’ G â†’ H1 â†’ H2 â†’ J â†’ K

Stageæ¦‚è¦:
- Stage E: Pre-processingï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼‰
- Stage F: Visual Analysisï¼ˆè¦–è¦šè§£æã€gemini-2.5-proï¼‰
         - ç‰©ç†çš„OCRæŠ½å‡ºã€JSONå‡ºåŠ›ï¼ˆã‚«ãƒ©ãƒ ãƒŠå½¢å¼ï¼‰
- Stage G: Logical Refinementï¼ˆè«–ç†çš„ç²¾éŒ¬ã€gemini-2.0-flash-liteï¼‰
         - é‡è¤‡æ’é™¤ã€REF_IDä»˜ä¸ã€unified_textç”Ÿæˆ
- Stage H1: Table Specialistï¼ˆè¡¨å‡¦ç†å°‚é–€ï¼‰
         - å®šå‹è¡¨ãƒ»æ§‹é€ åŒ–è¡¨ã‚’å…ˆã«å‡¦ç†
         - ã‚«ãƒ©ãƒ ãƒŠå½¢å¼â†’è¾æ›¸ãƒªã‚¹ãƒˆå¤‰æ›
         - H2ã¸ã®å…¥åŠ›é‡å‰Šæ¸›ã®ãŸã‚è¡¨ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
- Stage H2: Text Specialistï¼ˆãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†å°‚é–€ã€gemini-2.0-flashï¼‰
         - è»½é‡åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã§æ§‹é€ åŒ– + è¦ç´„
         - calendar_events, tasks, title, summary ã‚’ç”Ÿæˆ
         - audit_canonical_textï¼ˆç›£æŸ»ç”¨æ­£æœ¬ï¼‰ã‚’ç”Ÿæˆ
- Stage J: Chunkingï¼ˆãƒãƒ£ãƒ³ã‚¯åŒ–ï¼‰
- Stage K: Embeddingï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰

ç‰¹å¾´:
- doc_type / workspace ã«å¿œã˜ã¦è‡ªå‹•çš„ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆ
- config/ å†…ã® YAML ã¨ Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã§è¨­å®šç®¡ç†
- Stage G ã§ REF_IDä»˜ãç›®éŒ²ã‚’ç”Ÿæˆã—ã€å¾Œç¶šã‚¹ãƒ†ãƒ¼ã‚¸ãŒå‚ç…§å¯èƒ½
- H1 + H2 åˆ†å‰²ã«ã‚ˆã‚Šãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ã‚’å‰Šæ¸›
"""
import asyncio
import json
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger

from shared.ai.llm_client.llm_client import LLMClient
from shared.common.database.client import DatabaseClient
from shared.common.connectors.google_drive import GoogleDriveConnector

from .config_loader import ConfigLoader
from .stage_e import (
    StageEPreprocessor,          # E1
    E2TableDetector,             # E2
    E3OpenCVBlocks,              # E3
    E4CoordinateIntegrator,      # E4
    E5MaskGenerator,             # E5
    E6VisionOCR,                 # E6
    E7TextAggregator,            # E7
    E8VisionAggregator,          # E8
    E9TextReplacer,              # E9
    E11BboxNormalizer,           # E11
    StageEOrchestrator           # E1-E2-E3-E4-E5-E6-E7-E8-E9-E11çµ±åˆ
)
from .stage_f import StageFVisualAnalyzer  # ã€Ver 11.0ã€‘F1â†’F2â†’F3â†’G3â†’G4â†’G5â†’G6ï¼ˆE6-E8ã¯Stage Eã«ç§»å‹•ï¼‰
from .stage_h import StageH1Table, StageH2Text  # Stage H1/H2
from .stage_h.h_kakeibo import StageHKakeibo  # å®¶è¨ˆç°¿å°‚ç”¨
from .stage_j_chunking import StageJChunking
from .stage_k_embedding import StageKEmbedding

# Phase 5: Execution versioning
from shared.processing.execution_manager import ExecutionManager, ExecutionContext

# å®¶è¨ˆç°¿å°‚ç”¨ã®DBä¿å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ (ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«)
try:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from shared.kakeibo.kakeibo_db_handler import KakeiboDBHandler
    KAKEIBO_AVAILABLE = True
except ImportError:
    logger.warning("K_kakeibo module not available, kakeibo features will be disabled")
    KakeiboDBHandler = None
    KAKEIBO_AVAILABLE = False


# ============================================
# v1.1 å¥‘ç´„: post_body ã¯ Rawdata_FILE_AND_MAIL.display_post_text ã‹ã‚‰å–å¾—
# ============================================
def _build_post_body(raw_doc: dict | None) -> dict:
    """
    post_body ã‚’ Rawdata_FILE_AND_MAIL.display_post_text ã‹ã‚‰ç›´æ¥å–å¾—ã€‚
    GAS ã§ classroom/gmail/drive å…¨ã¦ã“ã®ã‚«ãƒ©ãƒ ã«æœ¬æ–‡ã‚’ä¿å­˜ã—ã¦ã„ã‚‹ã€‚

    Returns:
        { "text": str, "source": str, "char_count": int }
    """
    if not isinstance(raw_doc, dict):
        return {"text": "", "source": "no_raw_doc", "char_count": 0}

    text = (raw_doc.get("display_post_text") or "").strip()
    if text:
        return {"text": text, "source": "rawdata.display_post_text", "char_count": len(text)}

    return {"text": "", "source": "empty", "char_count": 0}


class UnifiedDocumentPipeline:
    """çµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (Stage E-K) - è¨­å®šãƒ™ãƒ¼ã‚¹ç‰ˆ"""

    @staticmethod
    def _sanitize_text(text: str) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰nullæ–‡å­—ã‚’é™¤å»

        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            ã‚µãƒ‹ã‚¿ã‚¤ã‚ºæ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not text:
            return text
        # nullæ–‡å­— (\u0000) ã‚’é™¤å»
        return text.replace('\u0000', '')

    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        db_client: Optional[DatabaseClient] = None,
        config_dir: Optional[Path] = None,
        enable_hybrid_ocr: Optional[bool] = None
    ):
        """
        Args:
            llm_client: LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            db_client: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            config_dir: è¨­å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: G_unified_pipeline/config/ï¼‰
            enable_hybrid_ocr: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰OCRï¼ˆSurya + PaddleOCRï¼‰ã‚’æœ‰åŠ¹åŒ–ï¼ˆNoneã®å ´åˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼‰
        """
        self.llm_client = llm_client or LLMClient()
        self.db = db_client or DatabaseClient(use_service_role=True)  # RLSãƒã‚¤ãƒ‘ã‚¹ã®ãŸã‚Service Roleä½¿ç”¨
        self.drive_connector = GoogleDriveConnector()  # Google Drive ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°ç”¨

        # è¨­å®šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
        self.config = ConfigLoader(config_dir)

        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰OCRã®æœ‰åŠ¹/ç„¡åŠ¹ã‚’æ±ºå®š
        if enable_hybrid_ocr is None:
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è¨­å®šï¼‰
            enable_hybrid_ocr = self.config.get_hybrid_ocr_enabled('default')

        # å„ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’åˆæœŸåŒ–
        # Stage Eï¼ˆE1-E2-E3-E4-E5-E6-E7-E8-E9-E11çµ±åˆï¼‰
        self.stage_e = StageEOrchestrator(
            llm_client=self.llm_client,
            stage_e_preprocessor=StageEPreprocessor(),          # E1
            e2_table_detector=E2TableDetector(),                # E2
            e3_opencv_blocks=E3OpenCVBlocks(),                  # E3
            e4_coordinate_integrator=E4CoordinateIntegrator(),  # E4
            e5_mask_generator=E5MaskGenerator(),                # E5
            e6_ocr=E6VisionOCR(),                               # E6
            e7_text_aggregator=E7TextAggregator(),              # E7
            e8_vision_aggregator=E8VisionAggregator(),          # E8
            e9_text_replacer=E9TextReplacer(),                  # E9
            e11_normalizer=E11BboxNormalizer()                  # E11
        )
        # Stage Fï¼ˆF1-F3 + G3-G6ã€E6-E8ã‚’å‰Šé™¤ï¼‰
        self.stage_f = StageFVisualAnalyzer(self.llm_client, enable_surya=enable_hybrid_ocr)
        # Stage H
        self.stage_h1 = StageH1Table(self.llm_client)  # Stage H1: è¡¨å‡¦ç†å°‚é–€
        self.stage_h2 = StageH2Text(self.llm_client)  # Stage H2: ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†å°‚é–€
        self.stage_h_kakeibo = StageHKakeibo(self.db)  # å®¶è¨ˆç°¿å°‚ç”¨
        self.stage_j = StageJChunking()
        self.stage_k = StageKEmbedding(self.llm_client, self.db)

        # å®¶è¨ˆç°¿å°‚ç”¨ã®DBä¿å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        self.kakeibo_db_handler = KakeiboDBHandler(self.db) if KAKEIBO_AVAILABLE else None

        logger.info(f"âœ… UnifiedDocumentPipeline åˆæœŸåŒ–å®Œäº†ï¼ˆEâ†’F(Ver9.0)â†’H1â†’H2â†’Jâ†’K, ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰OCR={'æœ‰åŠ¹' if enable_hybrid_ocr else 'ç„¡åŠ¹'}ï¼‰")

    async def process_document(
        self,
        file_path: Path,
        file_name: str,
        doc_type: str,
        workspace: str,
        mime_type: str,
        source_id: str,
        existing_document_id: Optional[str] = None,
        extra_metadata: Optional[Dict[str, Any]] = None,
        progress_callback=None,
        owner_id: Optional[str] = None,
        enable_execution_tracking: bool = False
    ) -> Dict[str, Any]:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ï¼ˆStage E-Kï¼‰

        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«å
            doc_type: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆè¨­å®šãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«ä½¿ç”¨ï¼‰
            workspace: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹
            mime_type: MIMEã‚¿ã‚¤ãƒ—
            source_id: ã‚½ãƒ¼ã‚¹ID
            existing_document_id: æ›´æ–°ã™ã‚‹æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            extra_metadata: è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆClassroomå›ºæœ‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãªã©ï¼‰
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            owner_id: ã‚ªãƒ¼ãƒŠãƒ¼IDï¼ˆPhase 3 å¿…é ˆ for kakeiboï¼‰
            enable_execution_tracking: Phase 5 execution versioning ã‚’æœ‰åŠ¹åŒ–

        Returns:
            å‡¦ç†çµæœ {'success': bool, 'document_id': str, ...}
        """
        # Phase 5: Execution tracking åˆæœŸåŒ–
        execution_context: Optional[ExecutionContext] = None
        execution_manager: Optional[ExecutionManager] = None
        start_time = None

        if enable_execution_tracking:
            import time
            start_time = time.time()
            execution_manager = ExecutionManager(self.db)

        try:
            logger.info(f"ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†é–‹å§‹: {file_name} (doc_type={doc_type}, workspace={workspace})")

            # ============================================
            # Stage E: E1-E8çµ±åˆå‡¦ç†ï¼ˆPDFæŠ½å‡º + Vision OCRï¼‰
            # ============================================
            logger.info("[Stage E] E1-E8çµ±åˆå‡¦ç†é–‹å§‹...")
            if progress_callback:
                progress_callback("E1")

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ¤å®šï¼ˆPDFã‹ã©ã†ã‹ï¼‰
            is_document = mime_type and mime_type.startswith('application/pdf')

            # Stage E: E1-E8ã‚’å®Ÿè¡Œ
            stage_e_result = self.stage_e.process(
                file_path=file_path,
                mime_type=mime_type,
                is_document=is_document,
                progress_callback=progress_callback
            )

            # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã«åˆ¶å¾¡ã‚’è¿”ã™ï¼ˆä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã®ãŸã‚ï¼‰
            await asyncio.sleep(0)

            # Stage E ã®çµæœã‚’ãƒã‚§ãƒƒã‚¯
            if not stage_e_result.get('success'):
                error_msg = f"Stage Eå¤±æ•—: {stage_e_result.get('error', 'E1-E8å‡¦ç†ã‚¨ãƒ©ãƒ¼')}"
                logger.error(f"[Stage Eå¤±æ•—] {error_msg}")
                return {'success': False, 'error': error_msg}

            # Stage E ã®å‡ºåŠ›ã‚’å–å¾—
            normalized_tokens = stage_e_result.get('normalized_tokens', [])
            e_physical_chars = stage_e_result.get('e_physical_chars', [])
            extracted_text = stage_e_result.get('extracted_text', '')
            page_images = stage_e_result.get('page_images', [])
            stage_e_metadata = stage_e_result.get('metadata', {})
            e2_table_bboxes = stage_e_metadata.get('table_bboxes', [])

            logger.info(f"[Stage Eå®Œäº†] normalized_tokens={len(normalized_tokens)}, "
                       f"e_physical_chars={len(e_physical_chars)}, "
                       f"extracted_text={len(extracted_text)}æ–‡å­—, "
                       f"page_images={len(page_images)}ãƒšãƒ¼ã‚¸")
            # ãƒ­ã‚°å‡ºåŠ›ã¯ Stage E å†…ã§æ—¢ã«å®Ÿæ–½æ¸ˆã¿

            # ============================================
            # Stage F: Visual Analysis (gemini-2.5-pro ã§å®Œç’§ã«ä»•ä¸Šã’ã‚‹)
            # ============================================
            # post_body ä½œæˆï¼ˆæŠ•ç¨¿æœ¬æ–‡ = Stage H æœ€å„ªå…ˆæ–‡è„ˆï¼‰
            # ã€v1.1å¥‘ç´„ã€‘Rawdata_FILE_AND_MAIL ã‹ã‚‰æœ¬æ–‡ã‚’å„ªå…ˆå–å¾—
            raw_doc = None
            if existing_document_id:
                try:
                    r = self.db.client.table("Rawdata_FILE_AND_MAIL").select(
                        "id, display_post_text, attachment_text"
                    ).eq("id", existing_document_id).limit(1).execute()
                    if r and getattr(r, "data", None):
                        raw_doc = r.data[0]
                        logger.info(f"[Stage F] raw_docå–å¾—: id={existing_document_id}")
                except Exception as e:
                    logger.warning(f"[Stage F] raw_docå–å¾—å¤±æ•—: {e.__class__.__name__}: {e}")

            post_body = _build_post_body(raw_doc)
            logger.info(f"[Stage F] post_bodyä½œæˆ: {post_body['char_count']}æ–‡å­— (source: {post_body['source']})")

            # P0-4: Stage F ç›´å‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
            if file_path is not None and not file_path.exists():
                error_msg = f"[P0-4] TEMP_PDF_MISSING: Stage F å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_path}"
                logger.error(error_msg)
                return {
                    'success': False,
                    'error': error_msg,
                    'failure_stage': 'F',
                    'failure_reason': 'TEMP_PDF_MISSING'
                }

            # è¨­å®šã‹ã‚‰ Stage F ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            stage_f_config = self.config.get_stage_config('stage_f', doc_type, workspace)
            prompt_f = stage_f_config['prompt']
            model_f = stage_f_config['model']

            # P0-1: æ˜ç¤ºçš„ã« file_path ã‚’æ¸¡ã™ï¼ˆstate å‚ç…§ç¦æ­¢ï¼‰
            logger.info(f"[Stage F] Visual Analysisé–‹å§‹... (model={model_f})")
            if file_path is not None:
                logger.info(f"[P0-1] å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {file_path} (exists={file_path.exists()})")
            else:
                logger.info("[P0-1] å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: ãªã—ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼‰")
            # P2-2: E-2ã®table_bboxesæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
            if e2_table_bboxes:
                logger.info(f"[P2-2] Stage Fã¸æ¸¡ã™ E-2 table_bboxes: {len(e2_table_bboxes)}å€‹")

            if progress_callback:
                progress_callback("F")

            # Stage E ãŒæ—¢ã« Vision å‡¦ç†ã‚’å®Œäº†ã—ã¦ã„ã‚‹ãŸã‚ã€å¸¸ã« vision ã‚’å®Ÿè¡Œ
            requires_vision = True
            requires_transcription = False

            # stage_e_metadata ã« physical_chars ã‚’è¿½åŠ 
            stage_e_metadata['physical_chars'] = e_physical_chars

            # Stage F å‘¼ã³å‡ºã—ï¼ˆVer 11.0: E6-E8ã®å‡ºåŠ›ã‚’æ¸¡ã™ï¼‰
            stage_f_result = self.stage_f.process(
                file_path=file_path,
                mime_type=mime_type or '',
                normalized_tokens=normalized_tokens,  # Stage Eï¼ˆE6-E8ï¼‰ã®å‡ºåŠ›
                page_images=page_images,  # ãƒšãƒ¼ã‚¸ç”»åƒ
                requires_vision=requires_vision,
                requires_transcription=requires_transcription,
                post_body=post_body,
                progress_callback=progress_callback,
                # YAMLã‹ã‚‰èª­ã¿è¾¼ã‚“ã è¨­å®šã‚’æ¸¡ã™
                prompt=prompt_f,
                model=model_f,
                extracted_text=extracted_text,
                workspace=workspace,
                e2_table_bboxes=e2_table_bboxes,
                stage_e_metadata=stage_e_metadata  # ã€Ver 6.4ã€‘åº§æ¨™ä»˜ãæ–‡å­—æƒ…å ±ï¼ˆphysical_charså«ã‚€ï¼‰
            )
            logger.info(f"[Stage Få®Œäº†] Visionçµæœ: {type(stage_f_result).__name__}")

            # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã«åˆ¶å¾¡ã‚’è¿”ã™ï¼ˆä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã®ãŸã‚ï¼‰
            await asyncio.sleep(0)

            # ============================================
            # Stage F çµæœå‡¦ç†ï¼ˆDictå‹ã‚’ç›´æ¥ä½¿ç”¨ - dumps/loadsæ’é™¤ï¼‰
            # ============================================
            try:
                # Stage F ã¯ Dict ã‚’ç›´æ¥è¿”ã™ï¼ˆJSONã®å¾€å¾©å¤‰æ›ã‚’æ’é™¤ï¼‰
                vision_json = stage_f_result
                # DBä¿å­˜ç”¨ã«JSONæ–‡å­—åˆ—ã‚‚ä¿æŒï¼ˆvision_rawï¼‰
                vision_raw = json.dumps(stage_f_result, ensure_ascii=False)

                # Stage F payload ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆå†æ§‹æˆç¦æ­¢ï¼‰
                stage_f_structure = vision_json
                schema_ver = vision_json.get('schema_version', '')

                # full_text ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆæ··ãœç‰©åˆæˆç¦æ­¢ï¼‰
                # Ver 14.0: G5å‡ºåŠ›ã¯ path_a_result.full_text_ordered ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æ ¼ç´
                _pa = vision_json.get('path_a_result', {})
                combined_text = _pa.get('full_text_ordered', '') or vision_json.get('full_text', '')
                post_body = vision_json.get('post_body', {})
                text_blocks = vision_json.get('text_blocks', [])

                _tables = _pa.get('tables', [])
                logger.info(f"[Stage Fâ†’H] ãƒ‡ãƒ¼ã‚¿å—ã‘æ¸¡ã—:")
                logger.info(f"  â”œâ”€ schema_version: {schema_ver}")
                logger.info(f"  â”œâ”€ full_text_ordered: {len(combined_text)}æ–‡å­—")
                logger.info(f"  â”œâ”€ post_body: {post_body.get('char_count', 0)}æ–‡å­— (source: {post_body.get('source', 'unknown')})")
                logger.info(f"  â”œâ”€ text_blocks: {len(text_blocks)}ãƒ–ãƒ­ãƒƒã‚¯")
                logger.info(f"  â”œâ”€ tables: {len(_tables)}å€‹")
                for _t in _tables:
                    _rid = _t.get('ref_id', '?')
                    _hm = _t.get('header_map', {})
                    _ce = _t.get('cells_enriched', [])
                    _cf = _t.get('cells_flat', [])
                    _panels = _hm.get('panels', {})
                    logger.info(f"  â”‚   {_rid}: cells_enriched={len(_ce)}, cells_flat={len(_cf)}, header_map panels={len(_panels)}")
                    # G7: ãƒ‘ãƒãƒ«ã”ã¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼ä½ç½®
                    for _pk, _pcfg in _panels.items():
                        logger.info(f"  â”‚     {_pk}: col_header_rows={_pcfg.get('col_header_rows', [])}, row_header_cols={_pcfg.get('row_header_cols', [])}")
                    # G8: ãƒ‘ãƒãƒ«ã”ã¨ã®enrichmentç´ä»˜ã‘ç‡
                    if _ce:
                        _by_panel = {}
                        for _c in _ce:
                            _pid = f"P{_c.get('panel_id', 0) or 0}"
                            if _pid not in _by_panel:
                                _by_panel[_pid] = {'total': 0, 'data': 0, 'col': 0, 'row': 0}
                            _by_panel[_pid]['total'] += 1
                            if not _c.get('is_header', False) and str(_c.get('text', '')).strip():
                                _by_panel[_pid]['data'] += 1
                                if _c.get('col_header'):
                                    _by_panel[_pid]['col'] += 1
                                if _c.get('row_header'):
                                    _by_panel[_pid]['row'] += 1
                        for _pid in sorted(_by_panel.keys()):
                            _s = _by_panel[_pid]
                            logger.info(f"  â”‚     {_pid} enrichment: data={_s['data']}, col_header={_s['col']}/{_s['data']}, row_header={_s['row']}/{_s['data']}")
                logger.info(f"  â””â”€ (G7/G8 enrichment {'æ¸ˆ' if _tables and _tables[0].get('cells_enriched') else 'æœª'})")
            except json.JSONDecodeError as e:
                logger.warning(f"[Stage Fâ†’H] JSONè§£æå¤±æ•—: {e}")
                combined_text = vision_raw
                stage_f_structure = None

            # ç©ºã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç©ºã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯è­¦å‘Šã®ã¿ã€ã‚¨ãƒ©ãƒ¼ã§ã¯ãªã„ï¼‰
            if not combined_text or not combined_text.strip():
                logger.warning(f"[Stage Fâ†’H] çµ±åˆãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ãªã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å¯èƒ½æ€§ï¼‰")
                combined_text = ""  # ç©ºæ–‡å­—åˆ—ã¨ã—ã¦ç¶™ç¶š

            # ============================================
            # Stage G: Ver 9.0 ã§ã¯ Stage F å†…éƒ¨ã§å‡¦ç†æ¸ˆã¿
            # ============================================
            # G3(Scrub)â†’G4(Assemble)â†’G5(Audit)â†’G6(Packager) ã¯ orchestrator.py å†…ã§å®Ÿè¡Œ
            # stage_f_result ã«ã¯ scrubbed_data (G5å‡ºåŠ›) ãŒå«ã¾ã‚Œã‚‹
            logger.info("[Stage G] Ver 9.0: Stage F å†…éƒ¨ã§å‡¦ç†æ¸ˆã¿ï¼ˆG3â†’G4â†’G5â†’G6ï¼‰")

            # Stage F ã® path_a_result ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
            path_a_result = stage_f_structure.get('path_a_result', {})

            # è­¦å‘ŠãŒã‚ã‚Œã°å‡ºåŠ›
            for warning in stage_f_structure.get('warnings', []):
                logger.warning(f"[Stage F/Gè­¦å‘Š] {warning}")

            # ============================================
            # Stage H+I: æ§‹é€ åŒ– + çµ±åˆãƒ»è¦ç´„
            # ============================================
            # custom_handler ã®ç¢ºèªï¼ˆãƒ«ãƒ¼ãƒˆè¨­å®šã‹ã‚‰ç›´æ¥å–å¾—ã€model ã¯å–å¾—ã—ãªã„ï¼‰
            route_config = self.config.get_route_config(doc_type, workspace)
            stage_h_routing = route_config.get('stages', {}).get('stage_h', {})
            custom_handler = stage_h_routing.get('custom_handler')

            # å®¶è¨ˆç°¿å°‚ç”¨å‡¦ç†ã®å ´åˆï¼ˆçµ±åˆç‰ˆã¯ä½¿ã‚ãªã„ï¼‰
            if custom_handler == 'kakeibo':
                # å®¶è¨ˆç°¿ã®å ´åˆã®ã¿ stage_h_config ã‚’å–å¾—
                stage_h_config = self.config.get_stage_config('stage_h', doc_type, workspace)
                logger.info(f"[Stage H] å®¶è¨ˆç°¿æ§‹é€ åŒ–é–‹å§‹... (custom_handler=kakeibo)")
                if progress_callback:
                    progress_callback("H")

                # Stage F ã®å‡ºåŠ›ã‚’è¾æ›¸ã«å¤‰æ›ï¼ˆcombined_text ãŒ JSON æ–‡å­—åˆ—ã®å ´åˆï¼‰
                # â€» json, re ã¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿
                try:
                    # Markdownã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ (```json ... ```) ã‚’é™¤å»
                    json_text = combined_text.strip()
                    if json_text.startswith('```'):
                        # æœ€åˆã¨æœ€å¾Œã®```ã‚’é™¤å»
                        json_text = re.sub(r'^```(?:json)?\s*\n', '', json_text)
                        json_text = re.sub(r'\n```\s*$', '', json_text)

                    logger.debug(f"[Stage H] JSON ãƒ‘ãƒ¼ã‚¹å‰ã®æœ€åˆã®500æ–‡å­—:\n{json_text[:500]}")
                    stage_f_output = json.loads(json_text)
                except (json.JSONDecodeError, TypeError) as e:
                    logger.error(f"[Stage H] combined_text ãŒ JSON å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {e}")
                    logger.error(f"[Stage H] combined_text ã®å†…å®¹:\n{combined_text[:1000]}")
                    raise ValueError("Stage F output must be JSON for kakeibo processing")

                # å®¶è¨ˆç°¿å°‚ç”¨ Stage H ã§å‡¦ç†
                stageH_result = self.stage_h_kakeibo.process(stage_f_output)

                # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã«åˆ¶å¾¡ã‚’è¿”ã™ï¼ˆä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã®ãŸã‚ï¼‰
                await asyncio.sleep(0)

                # å®¶è¨ˆç°¿å°‚ç”¨ã®DBä¿å­˜
                if self.kakeibo_db_handler:
                    # Phase 3: owner_id å¿…é ˆãƒã‚§ãƒƒã‚¯
                    if not owner_id:
                        raise ValueError("owner_id is required for kakeibo processing (Phase 3)")

                    logger.info("[DBä¿å­˜] å®¶è¨ˆç°¿ãƒ‡ãƒ¼ã‚¿ã‚’DBã«ä¿å­˜...")
                    kakeibo_save_result = self.kakeibo_db_handler.save_receipt(
                        stage_h_output=stageH_result,
                        file_name=file_name,
                        drive_file_id=source_id,
                        model_name=stage_h_config['model'],
                        source_folder=workspace,
                        owner_id=owner_id
                    )
                    logger.info(f"[DBä¿å­˜å®Œäº†] receipt_id={kakeibo_save_result['receipt_id']}")
                else:
                    logger.warning("K_kakeibo module not available, skipping kakeibo DB save")

                # å®¶è¨ˆç°¿ã¯ Rawdata_FILE_AND_MAIL ã«ä¿å­˜ã›ãšã€ã“ã“ã§çµ‚äº†
                return {
                    'success': True,
                    'receipt_id': kakeibo_save_result['receipt_id'],
                    'transaction_ids': kakeibo_save_result['transaction_ids'],
                    'log_id': kakeibo_save_result['log_id'],
                    'doc_type': 'kakeibo'
                }

            # ============================================
            # Stage H1 + H2: åˆ†å‰²å‡¦ç†ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»å‰Šæ¸›ç‰ˆï¼‰
            # ============================================
            else:
                stage_hi_config = self.config.get_stage_config('stage_hi', doc_type, workspace)
                prompt_hi = stage_hi_config['prompt']
                model_hi = stage_hi_config['model']

                # -----------------------------------------
                # Ver 9.0: Stage F ã‹ã‚‰ç›´æ¥ãƒ‡ãƒ¼ã‚¿å–å¾—
                # -----------------------------------------
                # ã‚¢ãƒ³ã‚«ãƒ¼é…åˆ—ã‚’å–å¾—ï¼ˆG5å‡ºåŠ›ï¼‰
                anchors = stage_f_structure.get('anchors', []) if stage_f_structure else []
                # è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆpath_a_resultå†…ï¼‰
                tables = path_a_result.get('tables', [])

                logger.info(f"[Stage Fâ†’H] Ver 9.0: anchors={len(anchors)}ä»¶, tables={len(tables)}ä»¶")

                # -----------------------------------------
                # Stage H1: è¡¨å‡¦ç†å°‚é–€
                # -----------------------------------------
                logger.info(f"[Stage H1] è¡¨å‡¦ç†é–‹å§‹... (è¡¨: {len(tables)}ä»¶)")
                if progress_callback:
                    progress_callback("H1")

                # G4ã®èª­ã¿é †æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³æ¤œå‡ºç”¨ï¼‰
                all_tagged_texts = path_a_result.get('tagged_texts', [])
                logger.info(f"[Stage H1] all_tagged_texts: {len(all_tagged_texts)}ä»¶")

                h1_result = self.stage_h1.process(
                    table_inventory=tables,
                    all_tagged_texts=all_tagged_texts,
                    doc_type=doc_type,
                    workspace=workspace,
                    unified_text=combined_text
                )

                # H1ã®çµæœã‚’ãƒ­ã‚°
                h1_stats = h1_result.get('statistics', {})
                logger.info(f"[Stage H1å®Œäº†] processed={h1_stats.get('processed', 0)}, skipped={h1_stats.get('skipped', 0)}")
                for _pt in h1_result.get('processed_tables', []):
                    _cols = _pt.get('columns', [])
                    _rows = _pt.get('rows', [])
                    logger.info(f"  â”œâ”€ {_pt.get('ref_id')}: columns={_cols}, rows={len(_rows)}è¡Œ")
                    for _ri, _row in enumerate(_rows[:3]):
                        logger.info(f"  â”‚   row[{_ri}]: {_row}")
                    if len(_rows) > 3:
                        logger.info(f"  â”‚   ... æ®‹ã‚Š{len(_rows) - 3}è¡Œ")
                logger.info(f"  â””â”€ reduced_text: {len(h1_result.get('reduced_text', ''))}æ–‡å­—")

                # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã«åˆ¶å¾¡ã‚’è¿”ã™
                await asyncio.sleep(0)

                # -----------------------------------------
                # H2ç”¨ãƒ†ã‚­ã‚¹ãƒˆï¼ˆVer 9.0: full_text_orderedä½¿ç”¨ï¼‰
                # -----------------------------------------
                reduced_text = path_a_result.get('full_text_ordered', '') or combined_text

                logger.info(f"[Stage H1â†’H2] ãƒ†ã‚­ã‚¹ãƒˆ: {len(reduced_text)}æ–‡å­—")

                # -----------------------------------------
                # Stage H2: ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†å°‚é–€
                # -----------------------------------------
                logger.info(f"[Stage H2] ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†é–‹å§‹... (model={model_hi})")
                if progress_callback:
                    progress_callback("H2")

                stageHI_result = self.stage_h2.process(
                    file_name=file_name,
                    doc_type=doc_type,
                    workspace=workspace,
                    reduced_text=reduced_text,
                    prompt=prompt_hi,
                    model=model_hi,
                    h1_result=h1_result,
                    stage_f_structure=stage_f_structure,
                    stage_g_result=None  # Ver 9.0: æ—§Stage Gå‰Šé™¤
                )

                # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã«åˆ¶å¾¡ã‚’è¿”ã™ï¼ˆä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã®ãŸã‚ï¼‰
                await asyncio.sleep(0)

                # Stage H2 ã®çµæœã‚’ãƒã‚§ãƒƒã‚¯
                if not stageHI_result or not isinstance(stageHI_result, dict):
                    error_msg = "Stage H2å¤±æ•—: çµæœãŒä¸æ­£ã§ã™"
                    logger.error(f"[Stage H2å¤±æ•—] {error_msg}")
                    return {'success': False, 'error': error_msg}

                # çµæœã‚’å¤‰æ•°ã«å±•é–‹
                document_date = stageHI_result.get('document_date')
                tags = stageHI_result.get('tags', [])
                stageH_metadata = stageHI_result.get('metadata', {})
                title = stageHI_result.get('title', '')
                summary = stageHI_result.get('summary', '')
                relevant_date = stageHI_result.get('document_date')

                # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆã¨ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                calendar_events = stageHI_result.get('calendar_events', [])
                tasks = stageHI_result.get('tasks', [])

                # metadataã«è¿½åŠ 
                stageH_metadata['calendar_events'] = calendar_events
                stageH_metadata['tasks'] = tasks

                # audit_canonical_text ãŒã‚ã‚Œã° metadata ã«è¿½åŠ 
                audit_text = stageHI_result.get('audit_canonical_text', '')
                if audit_text:
                    stageH_metadata['audit_canonical_text'] = audit_text

                # H1å‡¦ç†çµ±è¨ˆã‚’è¿½åŠ 
                stageH_metadata['_h1_h2_split'] = True
                stageH_metadata['_h1_statistics'] = h1_stats

                # ã€Ver 9.0ã€‘ç›£æŸ»ãƒ­ã‚°ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ ï¼ˆchange_log from G5ï¼‰
                if stage_f_structure.get('change_log'):
                    stageH_metadata['_v90_change_log'] = stage_f_structure['change_log']
                if stage_f_structure.get('anomaly_report'):
                    stageH_metadata['_v90_anomaly_report'] = stage_f_structure['anomaly_report']

                logger.info(f"[Stage H1+H2å®Œäº†] title={title[:30] if title else 'N/A'}..., "
                           f"calendar_events={len(calendar_events)}ä»¶, tasks={len(tasks)}ä»¶")

                # Stage H ã®çµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                stageH_result = {
                    'document_date': document_date,
                    'tags': tags,
                    'metadata': stageH_metadata
                }

            # ============================================
            # Google Drive ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã«åŸºã¥ãï¼‰
            # ============================================
            if title and source_id and file_name:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’æŠ½å‡º
                import os
                file_extension = os.path.splitext(file_name)[1]  # ä¾‹: ".pdf"

                # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒˆãƒ« + æ‹¡å¼µå­ï¼‰
                new_file_name = title + file_extension

                # Google Drive ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ›´æ–°
                try:
                    self.drive_connector.rename_file(source_id, new_file_name)
                    logger.info(f"[Google Drive] ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°æˆåŠŸ: {new_file_name}")
                except Exception as e:
                    # ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°å¤±æ•—ã¯ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ã¿ï¼ˆå‡¦ç†ã¯ç¶™ç¶šï¼‰
                    logger.warning(f"[Google Drive] ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°å¤±æ•—: {e}")

            # ============================================
            # Stage J: Chunking
            # ============================================
            logger.info("[Stage J] ãƒãƒ£ãƒ³ã‚¯åŒ–é–‹å§‹...")
            if progress_callback:
                progress_callback("J")
            chunks = self.stage_j.process(
                display_subject=extra_metadata.get('display_subject', file_name) if extra_metadata else file_name,
                summary=summary,
                tags=tags,
                document_date=document_date,
                metadata=stageH_metadata
            )
            logger.info(f"[Stage Jå®Œäº†] ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")

            # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã«åˆ¶å¾¡ã‚’è¿”ã™ï¼ˆä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã®ãŸã‚ï¼‰
            await asyncio.sleep(0)

            # ============================================
            # DBä¿å­˜: Rawdata_FILE_AND_MAIL
            # ============================================
            document_id = existing_document_id
            try:
                # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã® attachment_text, metadata, display_* ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—ï¼ˆnullã§ä¸Šæ›¸ãã—ãªã„ãŸã‚ï¼‰
                existing_attachment_text = None
                existing_metadata = {}
                existing_display_fields = {}
                if existing_document_id:
                    try:
                        existing_doc = self.db.client.table('Rawdata_FILE_AND_MAIL').select(
                            'attachment_text, metadata, display_sender, display_sender_email, display_subject, display_sent_at, display_post_text'
                        ).eq('id', existing_document_id).execute()
                        if existing_doc.data and len(existing_doc.data) > 0:
                            doc = existing_doc.data[0]
                            existing_attachment_text = doc.get('attachment_text', '')
                            # æ—¢å­˜ metadata ã‚’ä¿æŒï¼ˆmessage_id, thread_id, subject ãªã©ï¼‰
                            existing_metadata = doc.get('metadata', {})
                            if isinstance(existing_metadata, str):
                                existing_metadata = json.loads(existing_metadata)
                            # display_* ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä¿æŒ
                            existing_display_fields = {
                                'display_sender': doc.get('display_sender'),
                                'display_sender_email': doc.get('display_sender_email'),
                                'display_subject': doc.get('display_subject'),
                                'display_sent_at': doc.get('display_sent_at'),
                                'display_post_text': doc.get('display_post_text')
                            }
                            logger.debug(f"[DBä¿å­˜] æ—¢å­˜attachment_textå–å¾—: {len(existing_attachment_text or '')}æ–‡å­—")
                            logger.debug(f"[DBä¿å­˜] æ—¢å­˜metadataå–å¾—: {list(existing_metadata.keys())}")
                            logger.debug(f"[DBä¿å­˜] æ—¢å­˜display_*ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å–å¾—: sender={existing_display_fields.get('display_sender')}, subject={existing_display_fields.get('display_subject')}")
                    except Exception as e:
                        logger.warning(f"[DBä¿å­˜è­¦å‘Š] æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å–å¾—å¤±æ•—: {e}")

                # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼ˆnullæ–‡å­—ã‚’é™¤å»ï¼‰
                sanitized_combined_text = self._sanitize_text(combined_text)
                sanitized_summary = self._sanitize_text(summary)
                sanitized_extracted_text = self._sanitize_text(extracted_text)

                # Stage F ã®å‡ºåŠ›ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆJSONã‹ã‚‰å„è¦ç´ ã‚’æŠ½å‡ºï¼‰
                stage_f_text_ocr = None
                stage_f_layout_ocr = None
                stage_f_visual_elements = None
                try:
                    if vision_raw and stage_f_structure:
                        # full_text ã‚’ text OCR ã¨ã—ã¦ä¿å­˜
                        stage_f_text_ocr = self._sanitize_text(stage_f_structure.get('full_text', ''))

                        # sections/tables ã®å–å¾—
                        layout_info = stage_f_structure.get('layout_info', {})
                        sections = layout_info.get('sections', [])
                        tables = stage_f_structure.get('tables', [])

                        stage_f_layout_ocr = json.dumps({
                            'sections': sections,
                            'tables': tables
                        }, ensure_ascii=False, indent=2)

                        # visual_elements ã‚’ãã®ã¾ã¾ä¿å­˜
                        stage_f_visual_elements = json.dumps(
                            stage_f_structure.get('visual_elements', {}),
                            ensure_ascii=False,
                            indent=2
                        )
                except Exception as e:
                    logger.warning(f"[DBä¿å­˜è­¦å‘Š] Stage Få‡ºåŠ›ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {e}")

                # Stage EãŒç©ºã®å ´åˆã€Stage Fã®full_textã‚’ä½¿ç”¨
                if not sanitized_extracted_text and stage_f_text_ocr:
                    logger.info("[DBä¿å­˜] Stage EãŒç©ºã®ãŸã‚ã€Stage Fã®full_textã‚’ä½¿ç”¨")
                    sanitized_extracted_text = stage_f_text_ocr

                # Stage F ã‚¢ãƒ³ã‚«ãƒ¼é…åˆ—ã‚’å–å¾—
                stage_f_anchors = None
                if stage_f_structure and 'anchors' in stage_f_structure:
                    stage_f_anchors = stage_f_structure.get('anchors', [])

                # Ver 9.0: Stage Gçµæœã¯Stage Få†…éƒ¨ã§å‡¦ç†æ¸ˆã¿ï¼ˆG3â†’G4â†’G5â†’G6ï¼‰
                # quality_detail ã¨ anomaly_report ã‚’ä¿å­˜
                stage_g_result_json = {
                    'quality_detail': stage_f_structure.get('quality_detail', {}),
                    'anomaly_report': stage_f_structure.get('anomaly_report', []),
                    'change_log': stage_f_structure.get('change_log', []),
                    'schema_version': stage_f_structure.get('schema_version', '')
                }

                # Stage H1 çµæœã‚’å–å¾—
                stage_h1_tables_json = None
                if 'h1_result' in dir() and h1_result:
                    stage_h1_tables_json = {
                        'processed_tables': h1_result.get('processed_tables', []),
                        'extracted_metadata': h1_result.get('extracted_metadata', {}),
                        'statistics': h1_result.get('statistics', {})
                    }

                # titleã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
                sanitized_title = self._sanitize_text(title)

                # attachment_text ã®æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯
                # - Stage EãŒæ­£å½“ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ãŸå ´åˆï¼ˆsanitized_combined_text ãŒç©ºã§ãªã„ï¼‰â†’ ä½¿ç”¨ï¼ˆæ­£å½“ãªä¸Šæ›¸ãï¼‰
                # - Stage EãŒå¤±æ•—ã—ãŸå ´åˆï¼ˆsanitized_combined_text ãŒç©ºï¼‰â†’ æ—¢å­˜å€¤ã‚’ä¿æŒï¼ˆnullã§ä¸Šæ›¸ãã—ãªã„ï¼‰
                final_attachment_text = sanitized_combined_text
                if not sanitized_combined_text and existing_attachment_text:
                    final_attachment_text = existing_attachment_text
                    logger.info(f"[DBä¿å­˜] Stage EãŒç©ºã®ãŸã‚ã€æ—¢å­˜attachment_textã‚’ä¿æŒ: {len(final_attachment_text)}æ–‡å­—")

                # metadata ã®ãƒãƒ¼ã‚¸ãƒ­ã‚¸ãƒƒã‚¯
                # æ—¢å­˜ã® metadataï¼ˆmessage_id, thread_id, subject ãªã©ï¼‰ã‚’ä¿æŒã—ã¤ã¤ã€
                # Stage H ã® metadataï¼ˆLLMãŒç”Ÿæˆã—ãŸæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’è¿½åŠ 
                final_metadata = {}
                if existing_document_id and existing_metadata:
                    # æ—¢å­˜ã® metadata ã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹
                    final_metadata = existing_metadata.copy()
                    logger.info(f"[DBä¿å­˜] æ—¢å­˜metadataã‚’ä¿æŒ: {list(existing_metadata.keys())}")
                # Stage H ã® metadata ã‚’è¿½åŠ ãƒ»æ›´æ–°
                if stageH_metadata:
                    final_metadata.update(stageH_metadata)
                    logger.info(f"[DBä¿å­˜] Stage H metadataã‚’ãƒãƒ¼ã‚¸: {list(stageH_metadata.keys())}")

                doc_data = {
                    'source_id': source_id,
                    'source_type': 'unified_pipeline',
                    'file_name': file_name,
                    'workspace': workspace,
                    'doc_type': doc_type,
                    'title': sanitized_title,
                    'attachment_text': final_attachment_text,
                    'summary': sanitized_summary,
                    'tags': tags,
                    'document_date': document_date,
                    'metadata': final_metadata,
                    'processing_status': 'completed',
                    # å„ã‚¹ãƒ†ãƒ¼ã‚¸ã®å‡ºåŠ›ã‚’ä¿å­˜ï¼ˆæ–°ã‚¹ã‚­ãƒ¼ãƒ 2026-01-27ï¼‰
                    'stage_e_text': sanitized_extracted_text,  # Stage E: ç‰©ç†æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆï¼ˆE-1ã€œE-3çµ±åˆï¼‰
                    'stage_f_text_ocr': stage_f_text_ocr,        # Stage F: Path A ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                    'stage_f_layout_ocr': stage_f_layout_ocr,    # Stage F: ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæƒ…å ±
                    'stage_f_visual_elements': stage_f_visual_elements,  # Stage F: è¦–è¦šè¦ç´ 
                    'stage_f_anchors': json.dumps(stage_f_anchors, ensure_ascii=False) if stage_f_anchors else None,  # Stage F: ã‚¢ãƒ³ã‚«ãƒ¼é…åˆ—
                    'stage_g_result': json.dumps(stage_g_result_json, ensure_ascii=False) if stage_g_result_json else None,  # Stage G: çµ±åˆç²¾éŒ¬çµæœ
                    'stage_h_normalized': reduced_text if 'reduced_text' in dir() else sanitized_combined_text,  # Stage H2: è»½é‡åŒ–æ¸ˆã¿å…¥åŠ›
                    'stage_h1_tables': json.dumps(stage_h1_tables_json, ensure_ascii=False) if stage_h1_tables_json else None,  # Stage H1: å‡¦ç†æ¸ˆã¿è¡¨
                    'stage_h_result': json.dumps(stageH_result, ensure_ascii=False, indent=2) if stageH_result else None,  # Stage H2: æ§‹é€ åŒ–çµæœ
                    'stage_j_chunks_json': json.dumps(chunks, ensure_ascii=False, indent=2)  # Stage J: ãƒãƒ£ãƒ³ã‚¯åŒ–çµæœ
                }

                # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆã€display_* ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä¿æŒï¼ˆGmail ingestionæ™‚ã«è¨­å®šã•ã‚ŒãŸå€¤ã‚’ä¸Šæ›¸ãã—ãªã„ãŸã‚ï¼‰
                if existing_document_id and existing_display_fields:
                    for key, value in existing_display_fields.items():
                        if value is not None:  # Noneã§ãªã„å€¤ã®ã¿ä¿æŒ
                            doc_data[key] = value
                    logger.debug(f"[DBä¿å­˜] display_*ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä¿æŒ: {list(existing_display_fields.keys())}")

                # extra_metadata ã‚’ãƒãƒ¼ã‚¸
                if extra_metadata:
                    # display_*ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯æœ€ä¸Šä½ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ã—ã¦ä¿å­˜
                    display_fields = ['display_subject', 'display_sender', 'display_sender_email', 'display_sent_at', 'display_post_text', 'display_type']
                    for field in display_fields:
                        if field in extra_metadata and extra_metadata[field] is not None:
                            doc_data[field] = extra_metadata[field]
                            logger.debug(f"[DBä¿å­˜] extra_metadataã‹ã‚‰{field}ã‚’è¨­å®š: {extra_metadata[field]}")

                    # display_*ä»¥å¤–ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯metadataã«ãƒãƒ¼ã‚¸
                    other_metadata = {k: v for k, v in extra_metadata.items() if k not in display_fields}
                    if other_metadata:
                        if isinstance(doc_data['metadata'], dict):
                            doc_data['metadata'].update(other_metadata)
                        else:
                            doc_data['metadata'] = other_metadata

                # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ›´æ–° or æ–°è¦ä½œæˆ
                if existing_document_id:
                    logger.info(f"[DBæ›´æ–°] æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°: {existing_document_id}")
                    # IDã‚’é™¤å¤–ã—ã¦UPDATEï¼ˆIDã¯å¤‰æ›´ä¸å¯ï¼‰
                    update_data = {k: v for k, v in doc_data.items() if k != 'id'}
                    result = self.db.client.table('Rawdata_FILE_AND_MAIL').update(update_data).eq('id', existing_document_id).execute()
                    if result.data and len(result.data) > 0:
                        document_id = result.data[0]['id']
                        logger.info(f"[DBæ›´æ–°å®Œäº†] Rawdata_FILE_AND_MAIL ID: {document_id}")
                    else:
                        logger.error("[DBæ›´æ–°ã‚¨ãƒ©ãƒ¼] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°å¤±æ•—")
                        return {'success': False, 'error': 'Document update failed'}
                else:
                    logger.info("[DBä¿å­˜] æ–°è¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ")
                    result = self.db.client.table('Rawdata_FILE_AND_MAIL').insert(doc_data).execute()
                    if result.data and len(result.data) > 0:
                        document_id = result.data[0]['id']
                        logger.info(f"[DBä¿å­˜] Rawdata_FILE_AND_MAIL ID: {document_id}")
                    else:
                        logger.error("[DBä¿å­˜ã‚¨ãƒ©ãƒ¼] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆå¤±æ•—")
                        return {'success': False, 'error': 'Document creation failed'}

            except Exception as e:
                logger.error(f"[DBä¿å­˜ã‚¨ãƒ©ãƒ¼] {e}")
                return {'success': False, 'error': str(e)}

            # ============================================
            # Stage K: Embedding
            # ============================================
            logger.info("[Stage K] ãƒ™ã‚¯ãƒˆãƒ«åŒ–é–‹å§‹...")
            if progress_callback:
                progress_callback("K")

            # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆã¯ã€å¤ã„ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤
            if existing_document_id:
                try:
                    logger.info(f"[Stage K] æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤: document_id={document_id}")
                    self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
                except Exception as e:
                    logger.warning(f"[Stage K è­¦å‘Š] æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {e}")

            # æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
            stage_k_result = self.stage_k.embed_and_save(document_id, chunks)

            # Stage K ã®çµæœã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå³æ ¼ãƒ¢ãƒ¼ãƒ‰: 1ã¤ã§ã‚‚å¤±æ•—ã—ãŸã‚‰å…¨ä½“å¤±æ•—ï¼‰
            if not stage_k_result.get('success'):
                error_msg = f"Stage Kå¤±æ•—: {stage_k_result.get('failed_count', 0)}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å¤±æ•—"
                logger.error(f"[Stage Kå¤±æ•—] {error_msg}")
                return {'success': False, 'error': error_msg}

            # éƒ¨åˆ†çš„å¤±æ•—ã¯è­¦å‘Šã¨ã—ã¦æ‰±ã†ï¼ˆä¸€éƒ¨ã®ãƒãƒ£ãƒ³ã‚¯ã¯ä¿å­˜æ¸ˆã¿ï¼‰
            failed_count = stage_k_result.get('failed_count', 0)
            saved_count = stage_k_result.get('saved_count', 0)
            if failed_count > 0:
                logger.warning(f"[Stage Kè­¦å‘Š] éƒ¨åˆ†çš„ãªå¤±æ•—: {failed_count}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å¤±æ•—ï¼ˆ{saved_count}ãƒãƒ£ãƒ³ã‚¯ã¯ä¿å­˜æ¸ˆã¿ï¼‰")
                # å¤±æ•—ã—ãŸãŒã€ä¸€éƒ¨ã¯æˆåŠŸã—ã¦ã„ã‚‹ã®ã§ç¶™ç¶š

            logger.info(f"[Stage Kå®Œäº†] {stage_k_result.get('saved_count', 0)}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜")

            # Phase 5: Execution tracking - æˆåŠŸæ™‚
            if enable_execution_tracking and execution_manager and owner_id and document_id:
                import time
                duration_ms = int((time.time() - start_time) * 1000) if start_time else None

                # execution ä½œæˆï¼ˆå‡¦ç†å®Œäº†å¾Œã«ä½œæˆã€å³åº§ã« succeededï¼‰
                try:
                    exec_ctx = execution_manager.create_execution(
                        document_id=document_id,
                        owner_id=owner_id,
                        input_text=combined_text if 'combined_text' in dir() else '',
                        model_version=stage_h_config.get('model') if 'stage_h_config' in dir() else None,
                        normalized_text=combined_text if 'combined_text' in dir() else ''
                    )
                    execution_manager.mark_succeeded(
                        execution_id=exec_ctx.execution_id,
                        result_data={
                            'summary': summary,
                            'tags': tags,
                            'document_date': document_date if 'document_date' in dir() else None,
                            'metadata': stageH_metadata if 'stageH_metadata' in dir() else {},
                            'chunks_count': stage_k_result.get('saved_count', 0)
                        },
                        processing_duration_ms=duration_ms
                    )
                    logger.info(f"[Phase 5] Execution è¨˜éŒ²å®Œäº†: {exec_ctx.execution_id[:8]}...")
                except Exception as exec_e:
                    logger.warning(f"[Phase 5] Execution è¨˜éŒ²ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {exec_e}")

            return {
                'success': True,
                'document_id': document_id,
                'summary': summary,
                'tags': tags,
                'chunks_count': stage_k_result.get('saved_count', 0)
            }

        except Exception as e:
            logger.error(f"[ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼] {e}", exc_info=True)

            # Phase 5: Execution tracking - å¤±æ•—æ™‚
            if enable_execution_tracking and execution_manager and owner_id:
                import time
                duration_ms = int((time.time() - start_time) * 1000) if start_time else None
                try:
                    # æ—¢å­˜ document_id ãŒã‚ã‚‹å ´åˆã®ã¿ execution ã‚’è¨˜éŒ²
                    doc_id = existing_document_id or (document_id if 'document_id' in dir() else None)
                    if doc_id:
                        exec_ctx = execution_manager.create_execution(
                            document_id=doc_id,
                            owner_id=owner_id,
                            input_text='',  # å¤±æ•—æ™‚ã¯å…¥åŠ›ãŒä¸æ˜ãªå ´åˆãŒã‚ã‚‹
                            model_version=None
                        )
                        execution_manager.mark_failed(
                            execution_id=exec_ctx.execution_id,
                            error_code='PIPELINE_ERROR',
                            error_message=str(e),
                            processing_duration_ms=duration_ms
                        )
                        logger.info(f"[Phase 5] å¤±æ•— Execution è¨˜éŒ²: {exec_ctx.execution_id[:8]}...")
                except Exception as exec_e:
                    logger.warning(f"[Phase 5] å¤±æ•— Execution è¨˜éŒ²ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {exec_e}")

            return {'success': False, 'error': str(e)}
