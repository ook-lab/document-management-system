"""
çµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (Stage E-K) - è¨­å®šãƒ™ãƒ¼ã‚¹ç‰ˆ

è¨­è¨ˆæ›¸: DESIGN_UNIFIED_PIPELINE.md v2.0 ã«æº–æ‹ 
å‡¦ç†é †åº: Stage E â†’ F â†’ G â†’ H+I â†’ J â†’ K

Stageæ¦‚è¦:
- Stage E: Pre-processingï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼‰
- Stage F: Visual Analysisï¼ˆè¦–è¦šè§£æã€gemini-2.5-proï¼‰
         - ç‰©ç†çš„OCRæŠ½å‡ºã€JSONå‡ºåŠ›
- Stage G: Logical Refinementï¼ˆè«–ç†çš„ç²¾éŒ¬ã€gemini-2.0-flash-liteï¼‰
         - é‡è¤‡æ’é™¤ã€REF_IDä»˜ä¸ã€unified_textç”Ÿæˆ
- Stage H+I: Combined Structuring & Synthesisï¼ˆçµ±åˆç‰ˆã€gemini-2.0-flashï¼‰
         - æ§‹é€ åŒ– + çµ±åˆãƒ»è¦ç´„ã‚’1å›ã®LLMå‘¼ã³å‡ºã—ã§å®Ÿè¡Œ
         - calendar_events, tasks, title, summary ã‚’ç”Ÿæˆ
         - audit_canonical_textï¼ˆç›£æŸ»ç”¨æ­£æœ¬ï¼‰ã‚’ç”Ÿæˆ
- Stage J: Chunkingï¼ˆãƒãƒ£ãƒ³ã‚¯åŒ–ï¼‰
- Stage K: Embeddingï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰

ç‰¹å¾´:
- doc_type / workspace ã«å¿œã˜ã¦è‡ªå‹•çš„ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆ
- config/ å†…ã® YAML ã¨ Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã§è¨­å®šç®¡ç†
- Stage G ã§ REF_IDä»˜ãç›®éŒ²ã‚’ç”Ÿæˆã—ã€å¾Œç¶šã‚¹ãƒ†ãƒ¼ã‚¸ãŒå‚ç…§å¯èƒ½
"""
import asyncio
import json
from pathlib import Path
from typing import Dict, Any, Optional
from loguru import logger

from shared.ai.llm_client.llm_client import LLMClient
from shared.common.database.client import DatabaseClient
from shared.common.connectors.google_drive import GoogleDriveConnector

from .config_loader import ConfigLoader
from .stage_e_preprocessing import StageEPreprocessor
from .stage_f_visual import StageFVisualAnalyzer
from .stage_g_refiner import StageGRefiner  # Stage G: è«–ç†çš„ç²¾éŒ¬
from .stage_hi_combined import StageHICombined  # Stage H+I: çµ±åˆç‰ˆ
from .stage_h_kakeibo import StageHKakeibo  # å®¶è¨ˆç°¿å°‚ç”¨
from .stage_j_chunking import StageJChunking
from .stage_k_embedding import StageKEmbedding
from .constants import STAGE_H_INPUT_SCHEMA_VERSION

# Phase 5: Execution versioning
from shared.processing.execution_manager import ExecutionManager, ExecutionContext

# å®¶è¨ˆç°¿å°‚ç”¨ã®DBä¿å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ (ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«)
try:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from shared.kakeibo.kakeibo_db_handler import KakeiboDBHandler
    KAKEIBO_AVAILABLE = True
except ImportError:
    logger.warning("K_kakeibo module not available, kakeibo features will be disabled")
    KakeiboDBHandler = None
    KAKEIBO_AVAILABLE = False


# ============================================
# v1.1 å¥‘ç´„: post_body ã¯ Rawdata_FILE_AND_MAIL.display_post_text ã‹ã‚‰å–å¾—
# ============================================
def _build_post_body(raw_doc: dict | None) -> dict:
    """
    post_body ã‚’ Rawdata_FILE_AND_MAIL.display_post_text ã‹ã‚‰ç›´æ¥å–å¾—ã€‚
    GAS ã§ classroom/gmail/drive å…¨ã¦ã“ã®ã‚«ãƒ©ãƒ ã«æœ¬æ–‡ã‚’ä¿å­˜ã—ã¦ã„ã‚‹ã€‚

    Returns:
        { "text": str, "source": str, "char_count": int }
    """
    if not isinstance(raw_doc, dict):
        return {"text": "", "source": "no_raw_doc", "char_count": 0}

    text = (raw_doc.get("display_post_text") or "").strip()
    if text:
        return {"text": text, "source": "rawdata.display_post_text", "char_count": len(text)}

    return {"text": "", "source": "empty", "char_count": 0}


class UnifiedDocumentPipeline:
    """çµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (Stage E-K) - è¨­å®šãƒ™ãƒ¼ã‚¹ç‰ˆ"""

    @staticmethod
    def _sanitize_text(text: str) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰nullæ–‡å­—ã‚’é™¤å»

        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            ã‚µãƒ‹ã‚¿ã‚¤ã‚ºæ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not text:
            return text
        # nullæ–‡å­— (\u0000) ã‚’é™¤å»
        return text.replace('\u0000', '')

    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        db_client: Optional[DatabaseClient] = None,
        config_dir: Optional[Path] = None,
        enable_hybrid_ocr: Optional[bool] = None
    ):
        """
        Args:
            llm_client: LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            db_client: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            config_dir: è¨­å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: G_unified_pipeline/config/ï¼‰
            enable_hybrid_ocr: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰OCRï¼ˆSurya + PaddleOCRï¼‰ã‚’æœ‰åŠ¹åŒ–ï¼ˆNoneã®å ´åˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼‰
        """
        self.llm_client = llm_client or LLMClient()
        self.db = db_client or DatabaseClient(use_service_role=True)  # RLSãƒã‚¤ãƒ‘ã‚¹ã®ãŸã‚Service Roleä½¿ç”¨
        self.drive_connector = GoogleDriveConnector()  # Google Drive ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°ç”¨

        # è¨­å®šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
        self.config = ConfigLoader(config_dir)

        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰OCRã®æœ‰åŠ¹/ç„¡åŠ¹ã‚’æ±ºå®š
        if enable_hybrid_ocr is None:
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è¨­å®šï¼‰
            enable_hybrid_ocr = self.config.get_hybrid_ocr_enabled('default')

        # å„ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’åˆæœŸåŒ–
        self.stage_e = StageEPreprocessor(self.llm_client)
        self.stage_f = StageFVisualAnalyzer(self.llm_client, enable_hybrid_ocr=enable_hybrid_ocr)
        self.stage_g = StageGRefiner(self.llm_client)  # Stage G: è«–ç†çš„ç²¾éŒ¬
        self.stage_hi = StageHICombined(self.llm_client)  # Stage H+I: çµ±åˆç‰ˆ
        self.stage_h_kakeibo = StageHKakeibo(self.db)  # å®¶è¨ˆç°¿å°‚ç”¨
        self.stage_j = StageJChunking()
        self.stage_k = StageKEmbedding(self.llm_client, self.db)

        # å®¶è¨ˆç°¿å°‚ç”¨ã®DBä¿å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        self.kakeibo_db_handler = KakeiboDBHandler(self.db) if KAKEIBO_AVAILABLE else None

        logger.info(f"âœ… UnifiedDocumentPipeline åˆæœŸåŒ–å®Œäº†ï¼ˆEâ†’Fâ†’Gâ†’Hâ†’Iâ†’Jâ†’K, ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰OCR={'æœ‰åŠ¹' if enable_hybrid_ocr else 'ç„¡åŠ¹'}ï¼‰")

    async def process_document(
        self,
        file_path: Path,
        file_name: str,
        doc_type: str,
        workspace: str,
        mime_type: str,
        source_id: str,
        existing_document_id: Optional[str] = None,
        extra_metadata: Optional[Dict[str, Any]] = None,
        progress_callback=None,
        owner_id: Optional[str] = None,
        enable_execution_tracking: bool = False
    ) -> Dict[str, Any]:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ï¼ˆStage E-Kï¼‰

        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«å
            doc_type: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆè¨­å®šãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«ä½¿ç”¨ï¼‰
            workspace: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹
            mime_type: MIMEã‚¿ã‚¤ãƒ—
            source_id: ã‚½ãƒ¼ã‚¹ID
            existing_document_id: æ›´æ–°ã™ã‚‹æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            extra_metadata: è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆClassroomå›ºæœ‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãªã©ï¼‰
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            owner_id: ã‚ªãƒ¼ãƒŠãƒ¼IDï¼ˆPhase 3 å¿…é ˆ for kakeiboï¼‰
            enable_execution_tracking: Phase 5 execution versioning ã‚’æœ‰åŠ¹åŒ–

        Returns:
            å‡¦ç†çµæœ {'success': bool, 'document_id': str, ...}
        """
        # Phase 5: Execution tracking åˆæœŸåŒ–
        execution_context: Optional[ExecutionContext] = None
        execution_manager: Optional[ExecutionManager] = None
        start_time = None

        if enable_execution_tracking:
            import time
            start_time = time.time()
            execution_manager = ExecutionManager(self.db)

        try:
            logger.info(f"ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†é–‹å§‹: {file_name} (doc_type={doc_type}, workspace={workspace})")

            # ============================================
            # Stage E: Pre-processing
            # ============================================
            logger.info("[Stage E] Pre-processingé–‹å§‹...")
            if progress_callback:
                progress_callback("E1")

            # extra_metadata ã‹ã‚‰æ—¢ã«æŠ½å‡ºæ¸ˆã¿ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆattachment_textï¼‰ã‚’å–å¾—
            # HTMLãƒ•ã‚¡ã‚¤ãƒ«ç­‰ã€Ingestionæ™‚ã«ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæ¸ˆã¿ã®å ´åˆã«ä½¿ç”¨
            pre_extracted_text = extra_metadata.get('attachment_text', '') if extra_metadata else ''

            stage_e_result = self.stage_e.extract_text(
                file_path,
                mime_type,
                pre_extracted_text=pre_extracted_text,
                workspace=workspace,
                progress_callback=progress_callback
            )

            # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã«åˆ¶å¾¡ã‚’è¿”ã™ï¼ˆä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã®ãŸã‚ï¼‰
            await asyncio.sleep(0)

            # Stage E ã®çµæœã‚’ãƒã‚§ãƒƒã‚¯
            if not stage_e_result.get('success'):
                error_msg = f"Stage Eå¤±æ•—: {stage_e_result.get('error', 'ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼')}"
                logger.error(f"[Stage Eå¤±æ•—] {error_msg}")
                return {'success': False, 'error': error_msg}

            extracted_text = stage_e_result.get('content', '')
            # P2-2: E-2ã§æ¤œå‡ºã—ãŸè¡¨ã®bboxæƒ…å ±ã‚’å–å¾—
            stage_e_metadata = stage_e_result.get('metadata', {})
            e2_table_bboxes = stage_e_metadata.get('table_bboxes', [])
            # ãƒ­ã‚°å‡ºåŠ›ã¯ Stage E å†…ã§æ—¢ã«å®Ÿæ–½æ¸ˆã¿

            # ============================================
            # Stage F: Visual Analysis (gemini-2.5-pro ã§å®Œç’§ã«ä»•ä¸Šã’ã‚‹)
            # ============================================
            # post_body ä½œæˆï¼ˆæŠ•ç¨¿æœ¬æ–‡ = Stage H æœ€å„ªå…ˆæ–‡è„ˆï¼‰
            # ã€v1.1å¥‘ç´„ã€‘Rawdata_FILE_AND_MAIL ã‹ã‚‰æœ¬æ–‡ã‚’å„ªå…ˆå–å¾—
            raw_doc = None
            if existing_document_id:
                try:
                    r = self.db.client.table("Rawdata_FILE_AND_MAIL").select(
                        "id, display_post_text, attachment_text"
                    ).eq("id", existing_document_id).limit(1).execute()
                    if r and getattr(r, "data", None):
                        raw_doc = r.data[0]
                        logger.info(f"[Stage F] raw_docå–å¾—: id={existing_document_id}")
                except Exception as e:
                    logger.warning(f"[Stage F] raw_docå–å¾—å¤±æ•—: {e.__class__.__name__}: {e}")

            post_body = _build_post_body(raw_doc)
            logger.info(f"[Stage F] post_bodyä½œæˆ: {post_body['char_count']}æ–‡å­— (source: {post_body['source']})")

            # P0-4: Stage F ç›´å‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
            if file_path is not None and not file_path.exists():
                error_msg = f"[P0-4] TEMP_PDF_MISSING: Stage F å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_path}"
                logger.error(error_msg)
                return {
                    'success': False,
                    'error': error_msg,
                    'failure_stage': 'F',
                    'failure_reason': 'TEMP_PDF_MISSING'
                }

            # è¨­å®šã‹ã‚‰ Stage F ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            stage_f_config = self.config.get_stage_config('stage_f', doc_type, workspace)
            prompt_f = stage_f_config['prompt']
            model_f = stage_f_config['model']

            # P0-1: æ˜ç¤ºçš„ã« file_path ã‚’æ¸¡ã™ï¼ˆstate å‚ç…§ç¦æ­¢ï¼‰
            logger.info(f"[Stage F] Visual Analysisé–‹å§‹... (model={model_f})")
            if file_path is not None:
                logger.info(f"[P0-1] å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {file_path} (exists={file_path.exists()})")
            else:
                logger.info("[P0-1] å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: ãªã—ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼‰")
            # P2-2: E-2ã®table_bboxesæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
            if e2_table_bboxes:
                logger.info(f"[P2-2] Stage Fã¸æ¸¡ã™ E-2 table_bboxes: {len(e2_table_bboxes)}å€‹")

            if progress_callback:
                progress_callback("F")
            vision_raw = self.stage_f.process(
                file_path=file_path,
                prompt=prompt_f,
                model=model_f,
                extracted_text=extracted_text,
                workspace=workspace,
                progress_callback=progress_callback,
                e2_table_bboxes=e2_table_bboxes,  # P2-2: E-2ã®table_bboxes
                post_body=post_body,  # æŠ•ç¨¿æœ¬æ–‡ï¼ˆStage Hæœ€å„ªå…ˆæ–‡è„ˆï¼‰
                mime_type=mime_type  # MIMEã‚¿ã‚¤ãƒ—ï¼ˆéŸ³å£°/æ˜ åƒåˆ¤å®šç”¨ï¼‰
            )
            logger.info(f"[Stage Få®Œäº†] Visionçµæœ: {len(vision_raw)}æ–‡å­—")

            # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã«åˆ¶å¾¡ã‚’è¿”ã™ï¼ˆä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã®ãŸã‚ï¼‰
            await asyncio.sleep(0)

            # ============================================
            # Stage F çµæœãƒ‘ãƒ¼ã‚¹: JSON ã‹ã‚‰æ§‹é€ åŒ–æƒ…å ±ã‚’å–å¾—
            # ============================================
            import json
            try:
                vision_json = json.loads(vision_raw)

                # v1.1å¥‘ç´„: Stage F payload ã‚’ãã®ã¾ã¾ stage_f_structure ã¨ã—ã¦ä½¿ç”¨ï¼ˆå†æ§‹æˆç¦æ­¢ï¼‰
                stage_f_structure = vision_json
                schema_ver = vision_json.get('schema_version', '')
                is_v1_1 = (schema_ver == STAGE_H_INPUT_SCHEMA_VERSION)

                if is_v1_1:
                    # v1.1: full_text ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆæ··ãœç‰©åˆæˆç¦æ­¢ï¼‰
                    combined_text = vision_json.get('full_text', '')
                    post_body = vision_json.get('post_body', {})
                    text_blocks = vision_json.get('text_blocks', [])

                    logger.info(f"[Stage Fâ†’H] v1.1å¥‘ç´„ãƒ¢ãƒ¼ãƒ‰:")
                    logger.info(f"  â”œâ”€ schema_version: {schema_ver}")
                    logger.info(f"  â”œâ”€ full_text: {len(combined_text)}æ–‡å­—")
                    logger.info(f"  â”œâ”€ post_body: {post_body.get('char_count', 0)}æ–‡å­— (source: {post_body.get('source', 'unknown')})")
                    logger.info(f"  â”œâ”€ text_blocks: {len(text_blocks)}ãƒ–ãƒ­ãƒƒã‚¯")
                    logger.info(f"  â”œâ”€ text_blocks[0]: {text_blocks[0].get('block_type') if text_blocks else 'N/A'}")
                    logger.info(f"  â””â”€ tables: {len(vision_json.get('tables', []))}å€‹")
                else:
                    # ãƒ¬ã‚¬ã‚·ãƒ¼: å¾“æ¥ã®åˆæˆãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå¾Œæ–¹äº’æ›ï¼‰
                    ocr_text = vision_json.get('full_text', '')
                    text_parts = []

                    # 1. æŠ•ç¨¿æ–‡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆClassroomç­‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
                    if extra_metadata:
                        display_post_text = extra_metadata.get('display_post_text', '')
                        if display_post_text and display_post_text.strip():
                            text_parts.append(f"[æŠ•ç¨¿æ–‡]\n{display_post_text}")
                            logger.info(f"[Stage Fâ†’H] display_post_textè¿½åŠ : {len(display_post_text)}æ–‡å­—")

                    # 2. OCRæŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ
                    if ocr_text and ocr_text.strip():
                        text_parts.append(f"[OCRæŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ]\n{ocr_text}")

                    # 3. ç”»åƒã®è¦–è¦šçš„èª¬æ˜ï¼ˆvisual_elements.notesï¼‰
                    visual_elements = vision_json.get('visual_elements', {})
                    notes = visual_elements.get('notes', [])
                    if notes:
                        notes_text = '\n'.join(notes)
                        text_parts.append(f"[ç”»åƒã®è¦–è¦šçš„èª¬æ˜]\n{notes_text}")
                        logger.info(f"[Stage Fâ†’H] visual_elements.notesè¿½åŠ : {len(notes_text)}æ–‡å­—")

                    combined_text = '\n\n'.join(text_parts)

                    logger.info(f"[Stage Fâ†’H] ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ¢ãƒ¼ãƒ‰:")
                    logger.info(f"  â”œâ”€ combined_text: {len(combined_text)}æ–‡å­—")
                    logger.info(f"  â”œâ”€ OCR full_text: {len(ocr_text)}æ–‡å­—")
                    logger.info(f"  â”œâ”€ sections: {len(vision_json.get('layout_info', {}).get('sections', []))}å€‹")
                    logger.info(f"  â””â”€ tables: {len(vision_json.get('layout_info', {}).get('tables', []))}å€‹")
            except json.JSONDecodeError as e:
                logger.warning(f"[Stage Fâ†’H] JSONè§£æå¤±æ•—: {e}")
                combined_text = vision_raw
                stage_f_structure = None

            # ç©ºã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç©ºã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯è­¦å‘Šã®ã¿ã€ã‚¨ãƒ©ãƒ¼ã§ã¯ãªã„ï¼‰
            if not combined_text or not combined_text.strip():
                logger.warning(f"[Stage Fâ†’H] çµ±åˆãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ãªã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å¯èƒ½æ€§ï¼‰")
                combined_text = ""  # ç©ºæ–‡å­—åˆ—ã¨ã—ã¦ç¶™ç¶š

            # ============================================
            # Stage G: è«–ç†çš„ç²¾éŒ¬ï¼ˆLogical Refinementï¼‰
            # ============================================
            # Stage F ã®å‡ºåŠ›ã‚’æ•´ç†ã—ã€REF_IDä»˜ãç›®éŒ²ã‚’ä½œæˆ
            stage_g_result = None
            stage_g_config = self.config.get_stage_config('stage_g', doc_type, workspace)

            # Stage G ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®šï¼ˆå®¶è¨ˆç°¿ã‚„ skip è¨­å®šãŒã‚ã‚‹å ´åˆï¼‰
            skip_stage_g = stage_g_config.get('skip', False) or doc_type == 'kakeibo'

            if stage_f_structure and not skip_stage_g:
                model_g = stage_g_config.get('model', 'gemini-2.0-flash-lite')
                logger.info(f"[Stage G] è«–ç†çš„ç²¾éŒ¬é–‹å§‹... (model={model_g})")
                if progress_callback:
                    progress_callback("G")

                try:
                    # Stage F payload ã‚’ Stage G ã«æ¸¡ã™
                    stage_g_result = self.stage_g.process(
                        stage_f_payload=stage_f_structure,
                        model=model_g,
                        workspace=workspace
                    )

                    # Stage G ã®å‡ºåŠ›ã‚’ãƒ­ã‚°
                    logger.info(f"[Stage Gå®Œäº†] ref_count={stage_g_result.get('ref_count', 0)}, mode={stage_g_result.get('processing_mode', 'unknown')}")

                    # Stage G ã® unified_text ã‚’ combined_text ã¨ã—ã¦ä½¿ç”¨ï¼ˆå¾Œç¶šã«æ¸¡ã™ï¼‰
                    if stage_g_result.get('unified_text'):
                        combined_text = stage_g_result['unified_text']
                        logger.info(f"[Stage Gâ†’H] unified_text: {len(combined_text)}æ–‡å­—")

                    # Stage G ã® source_inventory ã‚’ stage_f_structure ã«è¿½åŠ ï¼ˆStage Hã§å‚ç…§å¯èƒ½ã«ï¼‰
                    if stage_g_result.get('source_inventory'):
                        stage_f_structure['source_inventory'] = stage_g_result['source_inventory']
                        logger.info(f"[Stage Gâ†’H] source_inventory: {len(stage_g_result['source_inventory'])}ä»¶")

                    if stage_g_result.get('table_inventory'):
                        stage_f_structure['table_inventory'] = stage_g_result['table_inventory']
                        logger.info(f"[Stage Gâ†’H] table_inventory: {len(stage_g_result['table_inventory'])}ä»¶")

                    # è­¦å‘ŠãŒã‚ã‚Œã°å‡ºåŠ›
                    for warning in stage_g_result.get('warnings', []):
                        logger.warning(f"[Stage Gè­¦å‘Š] {warning}")

                except Exception as e:
                    logger.warning(f"[Stage G] å‡¦ç†å¤±æ•—ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œ: {e}")
                    # Stage G ãŒå¤±æ•—ã—ã¦ã‚‚ Stage H ã¯ç¶šè¡Œå¯èƒ½

                # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã«åˆ¶å¾¡ã‚’è¿”ã™ï¼ˆä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã®ãŸã‚ï¼‰
                await asyncio.sleep(0)
            elif skip_stage_g:
                logger.info(f"[Stage G] ã‚¹ã‚­ãƒƒãƒ— (doc_type={doc_type}, skip={stage_g_config.get('skip', False)})")

            # ============================================
            # Stage H+I: æ§‹é€ åŒ– + çµ±åˆãƒ»è¦ç´„
            # ============================================
            # custom_handler ã®ç¢ºèªï¼ˆãƒ«ãƒ¼ãƒˆè¨­å®šã‹ã‚‰ç›´æ¥å–å¾—ã€model ã¯å–å¾—ã—ãªã„ï¼‰
            route_config = self.config.get_route_config(doc_type, workspace)
            stage_h_routing = route_config.get('stages', {}).get('stage_h', {})
            custom_handler = stage_h_routing.get('custom_handler')

            # å®¶è¨ˆç°¿å°‚ç”¨å‡¦ç†ã®å ´åˆï¼ˆçµ±åˆç‰ˆã¯ä½¿ã‚ãªã„ï¼‰
            if custom_handler == 'kakeibo':
                # å®¶è¨ˆç°¿ã®å ´åˆã®ã¿ stage_h_config ã‚’å–å¾—
                stage_h_config = self.config.get_stage_config('stage_h', doc_type, workspace)
                logger.info(f"[Stage H] å®¶è¨ˆç°¿æ§‹é€ åŒ–é–‹å§‹... (custom_handler=kakeibo)")
                if progress_callback:
                    progress_callback("H")

                # Stage F ã®å‡ºåŠ›ã‚’è¾æ›¸ã«å¤‰æ›ï¼ˆcombined_text ãŒ JSON æ–‡å­—åˆ—ã®å ´åˆï¼‰
                import json
                import re
                try:
                    # Markdownã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ (```json ... ```) ã‚’é™¤å»
                    json_text = combined_text.strip()
                    if json_text.startswith('```'):
                        # æœ€åˆã¨æœ€å¾Œã®```ã‚’é™¤å»
                        json_text = re.sub(r'^```(?:json)?\s*\n', '', json_text)
                        json_text = re.sub(r'\n```\s*$', '', json_text)

                    logger.debug(f"[Stage H] JSON ãƒ‘ãƒ¼ã‚¹å‰ã®æœ€åˆã®500æ–‡å­—:\n{json_text[:500]}")
                    stage_f_output = json.loads(json_text)
                except (json.JSONDecodeError, TypeError) as e:
                    logger.error(f"[Stage H] combined_text ãŒ JSON å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {e}")
                    logger.error(f"[Stage H] combined_text ã®å†…å®¹:\n{combined_text[:1000]}")
                    raise ValueError("Stage F output must be JSON for kakeibo processing")

                # å®¶è¨ˆç°¿å°‚ç”¨ Stage H ã§å‡¦ç†
                stageH_result = self.stage_h_kakeibo.process(stage_f_output)

                # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã«åˆ¶å¾¡ã‚’è¿”ã™ï¼ˆä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã®ãŸã‚ï¼‰
                await asyncio.sleep(0)

                # å®¶è¨ˆç°¿å°‚ç”¨ã®DBä¿å­˜
                if self.kakeibo_db_handler:
                    # Phase 3: owner_id å¿…é ˆãƒã‚§ãƒƒã‚¯
                    if not owner_id:
                        raise ValueError("owner_id is required for kakeibo processing (Phase 3)")

                    logger.info("[DBä¿å­˜] å®¶è¨ˆç°¿ãƒ‡ãƒ¼ã‚¿ã‚’DBã«ä¿å­˜...")
                    kakeibo_save_result = self.kakeibo_db_handler.save_receipt(
                        stage_h_output=stageH_result,
                        file_name=file_name,
                        drive_file_id=source_id,
                        model_name=stage_h_config['model'],
                        source_folder=workspace,
                        owner_id=owner_id
                    )
                    logger.info(f"[DBä¿å­˜å®Œäº†] receipt_id={kakeibo_save_result['receipt_id']}")
                else:
                    logger.warning("K_kakeibo module not available, skipping kakeibo DB save")

                # å®¶è¨ˆç°¿ã¯ Rawdata_FILE_AND_MAIL ã«ä¿å­˜ã›ãšã€ã“ã“ã§çµ‚äº†
                return {
                    'success': True,
                    'receipt_id': kakeibo_save_result['receipt_id'],
                    'transaction_ids': kakeibo_save_result['transaction_ids'],
                    'log_id': kakeibo_save_result['log_id'],
                    'doc_type': 'kakeibo'
                }

            # ============================================
            # Stage H+I: æ§‹é€ åŒ– + çµ±åˆãƒ»è¦ç´„ï¼ˆçµ±åˆç‰ˆï¼‰
            # ============================================
            else:
                stage_hi_config = self.config.get_stage_config('stage_hi', doc_type, workspace)
                prompt_hi = stage_hi_config['prompt']
                model_hi = stage_hi_config['model']

                logger.info(f"[Stage H+I] çµ±åˆç‰ˆ æ§‹é€ åŒ–+çµ±åˆé–‹å§‹... (model={model_hi})")
                if progress_callback:
                    progress_callback("H+I")

                stageHI_result = self.stage_hi.process(
                    file_name=file_name,
                    doc_type=doc_type,
                    workspace=workspace,
                    combined_text=combined_text,
                    prompt=prompt_hi,
                    model=model_hi,
                    stage_f_structure=stage_f_structure,
                    stage_g_result=stage_g_result
                )

                # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã«åˆ¶å¾¡ã‚’è¿”ã™ï¼ˆä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã®ãŸã‚ï¼‰
                await asyncio.sleep(0)

                # Stage H+I ã®çµæœã‚’ãƒã‚§ãƒƒã‚¯
                if not stageHI_result or not isinstance(stageHI_result, dict):
                    error_msg = "Stage H+Iå¤±æ•—: çµæœãŒä¸æ­£ã§ã™"
                    logger.error(f"[Stage H+Iå¤±æ•—] {error_msg}")
                    return {'success': False, 'error': error_msg}

                # çµæœã‚’å¤‰æ•°ã«å±•é–‹ï¼ˆå¾Œç¶šå‡¦ç†ã¨ã®äº’æ›æ€§ã®ãŸã‚ï¼‰
                document_date = stageHI_result.get('document_date')
                tags = stageHI_result.get('tags', [])
                stageH_metadata = stageHI_result.get('metadata', {})
                title = stageHI_result.get('title', '')
                summary = stageHI_result.get('summary', '')
                relevant_date = stageHI_result.get('document_date')  # H+Iã§ã¯ document_date = relevant_date

                # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆã¨ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                calendar_events = stageHI_result.get('calendar_events', [])
                tasks = stageHI_result.get('tasks', [])

                # metadataã«è¿½åŠ 
                stageH_metadata['calendar_events'] = calendar_events
                stageH_metadata['tasks'] = tasks

                # audit_canonical_text ãŒã‚ã‚Œã° metadata ã«è¿½åŠ 
                audit_text = stageHI_result.get('audit_canonical_text', '')
                if audit_text:
                    stageH_metadata['audit_canonical_text'] = audit_text

                logger.info(f"[Stage H+Iå®Œäº†] title={title[:30] if title else 'N/A'}..., calendar_events={len(calendar_events)}ä»¶, tasks={len(tasks)}ä»¶")

                # Stage H äº’æ›ã®çµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                stageH_result = {
                    'document_date': document_date,
                    'tags': tags,
                    'metadata': stageH_metadata
                }

            # ============================================
            # Google Drive ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã«åŸºã¥ãï¼‰
            # ============================================
            if title and source_id and file_name:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’æŠ½å‡º
                import os
                file_extension = os.path.splitext(file_name)[1]  # ä¾‹: ".pdf"

                # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒˆãƒ« + æ‹¡å¼µå­ï¼‰
                new_file_name = title + file_extension

                # Google Drive ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ›´æ–°
                try:
                    self.drive_connector.rename_file(source_id, new_file_name)
                    logger.info(f"[Google Drive] ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°æˆåŠŸ: {new_file_name}")
                except Exception as e:
                    # ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°å¤±æ•—ã¯ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ã¿ï¼ˆå‡¦ç†ã¯ç¶™ç¶šï¼‰
                    logger.warning(f"[Google Drive] ãƒ•ã‚¡ã‚¤ãƒ«åæ›´æ–°å¤±æ•—: {e}")

            # ============================================
            # Stage J: Chunking
            # ============================================
            logger.info("[Stage J] ãƒãƒ£ãƒ³ã‚¯åŒ–é–‹å§‹...")
            if progress_callback:
                progress_callback("J")
            chunks = self.stage_j.process(
                display_subject=extra_metadata.get('display_subject', file_name) if extra_metadata else file_name,
                summary=summary,
                tags=tags,
                document_date=document_date,
                metadata=stageH_metadata
            )
            logger.info(f"[Stage Jå®Œäº†] ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")

            # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã«åˆ¶å¾¡ã‚’è¿”ã™ï¼ˆä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã®ãŸã‚ï¼‰
            await asyncio.sleep(0)

            # ============================================
            # DBä¿å­˜: Rawdata_FILE_AND_MAIL
            # ============================================
            document_id = existing_document_id
            try:
                # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã® attachment_text, metadata, display_* ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—ï¼ˆnullã§ä¸Šæ›¸ãã—ãªã„ãŸã‚ï¼‰
                existing_attachment_text = None
                existing_metadata = {}
                existing_display_fields = {}
                if existing_document_id:
                    try:
                        existing_doc = self.db.client.table('Rawdata_FILE_AND_MAIL').select(
                            'attachment_text, metadata, display_sender, display_sender_email, display_subject, display_sent_at, display_post_text'
                        ).eq('id', existing_document_id).execute()
                        if existing_doc.data and len(existing_doc.data) > 0:
                            doc = existing_doc.data[0]
                            existing_attachment_text = doc.get('attachment_text', '')
                            # æ—¢å­˜ metadata ã‚’ä¿æŒï¼ˆmessage_id, thread_id, subject ãªã©ï¼‰
                            existing_metadata = doc.get('metadata', {})
                            if isinstance(existing_metadata, str):
                                import json
                                existing_metadata = json.loads(existing_metadata)
                            # display_* ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä¿æŒ
                            existing_display_fields = {
                                'display_sender': doc.get('display_sender'),
                                'display_sender_email': doc.get('display_sender_email'),
                                'display_subject': doc.get('display_subject'),
                                'display_sent_at': doc.get('display_sent_at'),
                                'display_post_text': doc.get('display_post_text')
                            }
                            logger.debug(f"[DBä¿å­˜] æ—¢å­˜attachment_textå–å¾—: {len(existing_attachment_text or '')}æ–‡å­—")
                            logger.debug(f"[DBä¿å­˜] æ—¢å­˜metadataå–å¾—: {list(existing_metadata.keys())}")
                            logger.debug(f"[DBä¿å­˜] æ—¢å­˜display_*ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å–å¾—: sender={existing_display_fields.get('display_sender')}, subject={existing_display_fields.get('display_subject')}")
                    except Exception as e:
                        logger.warning(f"[DBä¿å­˜è­¦å‘Š] æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å–å¾—å¤±æ•—: {e}")

                # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼ˆnullæ–‡å­—ã‚’é™¤å»ï¼‰
                sanitized_combined_text = self._sanitize_text(combined_text)
                sanitized_summary = self._sanitize_text(summary)
                sanitized_extracted_text = self._sanitize_text(extracted_text)

                # Stage F ã®å‡ºåŠ›ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆJSONã‹ã‚‰å„è¦ç´ ã‚’æŠ½å‡ºï¼‰
                stage_f_text_ocr = None
                stage_f_layout_ocr = None
                stage_f_visual_elements = None
                try:
                    if vision_raw and stage_f_structure:
                        # full_text ã‚’ text OCR ã¨ã—ã¦ä¿å­˜
                        stage_f_text_ocr = self._sanitize_text(stage_f_structure.get('full_text', ''))

                        # v1.1å¥‘ç´„: sections/tables ã®å–å¾—å ´æ‰€ã‚’é©åˆ‡ã«
                        import json
                        sf_schema = stage_f_structure.get('schema_version', '')
                        if sf_schema == STAGE_H_INPUT_SCHEMA_VERSION:
                            # v1.1: layout_info.sections, ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ« tables
                            layout_info = stage_f_structure.get('layout_info', {})
                            sections = layout_info.get('sections', [])
                            tables = stage_f_structure.get('tables', [])
                        else:
                            # ãƒ¬ã‚¬ã‚·ãƒ¼: ç›´ä¸‹ã¾ãŸã¯ layout_info ã‹ã‚‰å–å¾—ï¼ˆå¾Œæ–¹äº’æ›ï¼‰
                            sections = stage_f_structure.get('sections', []) or stage_f_structure.get('layout_info', {}).get('sections', [])
                            tables = stage_f_structure.get('tables', []) or stage_f_structure.get('layout_info', {}).get('tables', [])

                        stage_f_layout_ocr = json.dumps({
                            'sections': sections,
                            'tables': tables
                        }, ensure_ascii=False, indent=2)

                        # visual_elements ã‚’ãã®ã¾ã¾ä¿å­˜
                        stage_f_visual_elements = json.dumps(
                            stage_f_structure.get('visual_elements', {}),
                            ensure_ascii=False,
                            indent=2
                        )
                except Exception as e:
                    logger.warning(f"[DBä¿å­˜è­¦å‘Š] Stage Få‡ºåŠ›ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {e}")

                # Stage EãŒç©ºã®å ´åˆã€Stage Fã®full_textã‚’E4/E5ã«ä½¿ç”¨
                if not sanitized_extracted_text and stage_f_text_ocr:
                    logger.info("[DBä¿å­˜] Stage EãŒç©ºã®ãŸã‚ã€Stage Fã®full_textã‚’E4/E5ã«ä½¿ç”¨")
                    sanitized_extracted_text = stage_f_text_ocr

                # titleã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
                sanitized_title = self._sanitize_text(title)

                # attachment_text ã®æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯
                # - Stage EãŒæ­£å½“ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ãŸå ´åˆï¼ˆsanitized_combined_text ãŒç©ºã§ãªã„ï¼‰â†’ ä½¿ç”¨ï¼ˆæ­£å½“ãªä¸Šæ›¸ãï¼‰
                # - Stage EãŒå¤±æ•—ã—ãŸå ´åˆï¼ˆsanitized_combined_text ãŒç©ºï¼‰â†’ æ—¢å­˜å€¤ã‚’ä¿æŒï¼ˆnullã§ä¸Šæ›¸ãã—ãªã„ï¼‰
                final_attachment_text = sanitized_combined_text
                if not sanitized_combined_text and existing_attachment_text:
                    final_attachment_text = existing_attachment_text
                    logger.info(f"[DBä¿å­˜] Stage EãŒç©ºã®ãŸã‚ã€æ—¢å­˜attachment_textã‚’ä¿æŒ: {len(final_attachment_text)}æ–‡å­—")

                # metadata ã®ãƒãƒ¼ã‚¸ãƒ­ã‚¸ãƒƒã‚¯
                # æ—¢å­˜ã® metadataï¼ˆmessage_id, thread_id, subject ãªã©ï¼‰ã‚’ä¿æŒã—ã¤ã¤ã€
                # Stage H ã® metadataï¼ˆLLMãŒç”Ÿæˆã—ãŸæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’è¿½åŠ 
                final_metadata = {}
                if existing_document_id and existing_metadata:
                    # æ—¢å­˜ã® metadata ã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹
                    final_metadata = existing_metadata.copy()
                    logger.info(f"[DBä¿å­˜] æ—¢å­˜metadataã‚’ä¿æŒ: {list(existing_metadata.keys())}")
                # Stage H ã® metadata ã‚’è¿½åŠ ãƒ»æ›´æ–°
                if stageH_metadata:
                    final_metadata.update(stageH_metadata)
                    logger.info(f"[DBä¿å­˜] Stage H metadataã‚’ãƒãƒ¼ã‚¸: {list(stageH_metadata.keys())}")

                doc_data = {
                    'source_id': source_id,
                    'source_type': 'unified_pipeline',
                    'file_name': file_name,
                    'workspace': workspace,
                    'doc_type': doc_type,
                    'title': sanitized_title,
                    'attachment_text': final_attachment_text,
                    'summary': sanitized_summary,
                    'tags': tags,
                    'document_date': document_date,
                    'metadata': final_metadata,
                    'processing_status': 'completed',
                    # å„ã‚¹ãƒ†ãƒ¼ã‚¸ã®å‡ºåŠ›ã‚’ä¿å­˜
                    # E1-E3: ç¾åœ¨ã¯æœªå®Ÿè£…ã®ãŸã‚ã€E4ã¨åŒã˜å€¤ã‚’ä¿å­˜ï¼ˆå°†æ¥çš„ã«å€‹åˆ¥ã‚¨ãƒ³ã‚¸ãƒ³ã‚’å®Ÿè£…äºˆå®šï¼‰
                    'stage_e1_text': sanitized_extracted_text,  # Stage E-1: PyPDF2ï¼ˆæœªå®Ÿè£…ã€E4ã®å€¤ã‚’ä½¿ç”¨ï¼‰
                    'stage_e2_text': sanitized_extracted_text,  # Stage E-2: pdfminerï¼ˆæœªå®Ÿè£…ã€E4ã®å€¤ã‚’ä½¿ç”¨ï¼‰
                    'stage_e3_text': sanitized_extracted_text,  # Stage E-3: PyMuPDFï¼ˆæœªå®Ÿè£…ã€E4ã®å€¤ã‚’ä½¿ç”¨ï¼‰
                    'stage_e4_text': sanitized_extracted_text,  # Stage E-4: pdfplumber/ç”»åƒOCR
                    'stage_e5_text': sanitized_extracted_text,  # Stage E-5: æœ€çµ‚çµ±åˆï¼ˆç¾åœ¨ã¯E4ã¨åŒã˜ï¼‰
                    'stage_f_text_ocr': stage_f_text_ocr,        # Stage F: Text OCR
                    'stage_f_layout_ocr': stage_f_layout_ocr,    # Stage F: Layout OCR
                    'stage_f_visual_elements': stage_f_visual_elements,  # Stage F: Visual Elements
                    'stage_h_normalized': sanitized_combined_text,  # Stage H ã¸ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
                    'stage_i_structured': json.dumps(stageH_result, ensure_ascii=False, indent=2) if stageH_result else None,  # Stage H ã®å‡ºåŠ›
                    'stage_j_chunks_json': json.dumps(chunks, ensure_ascii=False, indent=2)  # Stage J ã®å‡ºåŠ›
                }

                # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆã€display_* ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä¿æŒï¼ˆGmail ingestionæ™‚ã«è¨­å®šã•ã‚ŒãŸå€¤ã‚’ä¸Šæ›¸ãã—ãªã„ãŸã‚ï¼‰
                if existing_document_id and existing_display_fields:
                    for key, value in existing_display_fields.items():
                        if value is not None:  # Noneã§ãªã„å€¤ã®ã¿ä¿æŒ
                            doc_data[key] = value
                    logger.debug(f"[DBä¿å­˜] display_*ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä¿æŒ: {list(existing_display_fields.keys())}")

                # extra_metadata ã‚’ãƒãƒ¼ã‚¸
                if extra_metadata:
                    # display_*ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯æœ€ä¸Šä½ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ã—ã¦ä¿å­˜
                    display_fields = ['display_subject', 'display_sender', 'display_sender_email', 'display_sent_at', 'display_post_text', 'display_type']
                    for field in display_fields:
                        if field in extra_metadata and extra_metadata[field] is not None:
                            doc_data[field] = extra_metadata[field]
                            logger.debug(f"[DBä¿å­˜] extra_metadataã‹ã‚‰{field}ã‚’è¨­å®š: {extra_metadata[field]}")

                    # display_*ä»¥å¤–ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯metadataã«ãƒãƒ¼ã‚¸
                    other_metadata = {k: v for k, v in extra_metadata.items() if k not in display_fields}
                    if other_metadata:
                        if isinstance(doc_data['metadata'], dict):
                            doc_data['metadata'].update(other_metadata)
                        else:
                            doc_data['metadata'] = other_metadata

                # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ›´æ–° or æ–°è¦ä½œæˆ
                if existing_document_id:
                    logger.info(f"[DBæ›´æ–°] æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°: {existing_document_id}")
                    # IDã‚’é™¤å¤–ã—ã¦UPDATEï¼ˆIDã¯å¤‰æ›´ä¸å¯ï¼‰
                    update_data = {k: v for k, v in doc_data.items() if k != 'id'}
                    result = self.db.client.table('Rawdata_FILE_AND_MAIL').update(update_data).eq('id', existing_document_id).execute()
                    if result.data and len(result.data) > 0:
                        document_id = result.data[0]['id']
                        logger.info(f"[DBæ›´æ–°å®Œäº†] Rawdata_FILE_AND_MAIL ID: {document_id}")
                    else:
                        logger.error("[DBæ›´æ–°ã‚¨ãƒ©ãƒ¼] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°å¤±æ•—")
                        return {'success': False, 'error': 'Document update failed'}
                else:
                    logger.info("[DBä¿å­˜] æ–°è¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ")
                    result = self.db.client.table('Rawdata_FILE_AND_MAIL').insert(doc_data).execute()
                    if result.data and len(result.data) > 0:
                        document_id = result.data[0]['id']
                        logger.info(f"[DBä¿å­˜] Rawdata_FILE_AND_MAIL ID: {document_id}")
                    else:
                        logger.error("[DBä¿å­˜ã‚¨ãƒ©ãƒ¼] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆå¤±æ•—")
                        return {'success': False, 'error': 'Document creation failed'}

            except Exception as e:
                logger.error(f"[DBä¿å­˜ã‚¨ãƒ©ãƒ¼] {e}")
                return {'success': False, 'error': str(e)}

            # ============================================
            # Stage K: Embedding
            # ============================================
            logger.info("[Stage K] ãƒ™ã‚¯ãƒˆãƒ«åŒ–é–‹å§‹...")
            if progress_callback:
                progress_callback("K")

            # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆã¯ã€å¤ã„ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤
            if existing_document_id:
                try:
                    logger.info(f"[Stage K] æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤: document_id={document_id}")
                    self.db.client.table('10_ix_search_index').delete().eq('document_id', document_id).execute()
                except Exception as e:
                    logger.warning(f"[Stage K è­¦å‘Š] æ—¢å­˜ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {e}")

            # æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
            stage_k_result = self.stage_k.embed_and_save(document_id, chunks)

            # Stage K ã®çµæœã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå³æ ¼ãƒ¢ãƒ¼ãƒ‰: 1ã¤ã§ã‚‚å¤±æ•—ã—ãŸã‚‰å…¨ä½“å¤±æ•—ï¼‰
            if not stage_k_result.get('success'):
                error_msg = f"Stage Kå¤±æ•—: {stage_k_result.get('failed_count', 0)}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å¤±æ•—"
                logger.error(f"[Stage Kå¤±æ•—] {error_msg}")
                return {'success': False, 'error': error_msg}

            # éƒ¨åˆ†çš„å¤±æ•—ã¯è­¦å‘Šã¨ã—ã¦æ‰±ã†ï¼ˆä¸€éƒ¨ã®ãƒãƒ£ãƒ³ã‚¯ã¯ä¿å­˜æ¸ˆã¿ï¼‰
            failed_count = stage_k_result.get('failed_count', 0)
            saved_count = stage_k_result.get('saved_count', 0)
            if failed_count > 0:
                logger.warning(f"[Stage Kè­¦å‘Š] éƒ¨åˆ†çš„ãªå¤±æ•—: {failed_count}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜å¤±æ•—ï¼ˆ{saved_count}ãƒãƒ£ãƒ³ã‚¯ã¯ä¿å­˜æ¸ˆã¿ï¼‰")
                # å¤±æ•—ã—ãŸãŒã€ä¸€éƒ¨ã¯æˆåŠŸã—ã¦ã„ã‚‹ã®ã§ç¶™ç¶š

            logger.info(f"[Stage Kå®Œäº†] {stage_k_result.get('saved_count', 0)}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ä¿å­˜")

            # Phase 5: Execution tracking - æˆåŠŸæ™‚
            if enable_execution_tracking and execution_manager and owner_id and document_id:
                import time
                duration_ms = int((time.time() - start_time) * 1000) if start_time else None

                # execution ä½œæˆï¼ˆå‡¦ç†å®Œäº†å¾Œã«ä½œæˆã€å³åº§ã« succeededï¼‰
                try:
                    exec_ctx = execution_manager.create_execution(
                        document_id=document_id,
                        owner_id=owner_id,
                        input_text=combined_text if 'combined_text' in dir() else '',
                        model_version=stage_h_config.get('model') if 'stage_h_config' in dir() else None,
                        normalized_text=combined_text if 'combined_text' in dir() else ''
                    )
                    execution_manager.mark_succeeded(
                        execution_id=exec_ctx.execution_id,
                        result_data={
                            'summary': summary,
                            'tags': tags,
                            'document_date': document_date if 'document_date' in dir() else None,
                            'metadata': stageH_metadata if 'stageH_metadata' in dir() else {},
                            'chunks_count': stage_k_result.get('saved_count', 0)
                        },
                        processing_duration_ms=duration_ms
                    )
                    logger.info(f"[Phase 5] Execution è¨˜éŒ²å®Œäº†: {exec_ctx.execution_id[:8]}...")
                except Exception as exec_e:
                    logger.warning(f"[Phase 5] Execution è¨˜éŒ²ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {exec_e}")

            return {
                'success': True,
                'document_id': document_id,
                'summary': summary,
                'tags': tags,
                'chunks_count': stage_k_result.get('saved_count', 0)
            }

        except Exception as e:
            logger.error(f"[ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼] {e}", exc_info=True)

            # Phase 5: Execution tracking - å¤±æ•—æ™‚
            if enable_execution_tracking and execution_manager and owner_id:
                import time
                duration_ms = int((time.time() - start_time) * 1000) if start_time else None
                try:
                    # æ—¢å­˜ document_id ãŒã‚ã‚‹å ´åˆã®ã¿ execution ã‚’è¨˜éŒ²
                    doc_id = existing_document_id or (document_id if 'document_id' in dir() else None)
                    if doc_id:
                        exec_ctx = execution_manager.create_execution(
                            document_id=doc_id,
                            owner_id=owner_id,
                            input_text='',  # å¤±æ•—æ™‚ã¯å…¥åŠ›ãŒä¸æ˜ãªå ´åˆãŒã‚ã‚‹
                            model_version=None
                        )
                        execution_manager.mark_failed(
                            execution_id=exec_ctx.execution_id,
                            error_code='PIPELINE_ERROR',
                            error_message=str(e),
                            processing_duration_ms=duration_ms
                        )
                        logger.info(f"[Phase 5] å¤±æ•— Execution è¨˜éŒ²: {exec_ctx.execution_id[:8]}...")
                except Exception as exec_e:
                    logger.warning(f"[Phase 5] å¤±æ•— Execution è¨˜éŒ²ã‚¨ãƒ©ãƒ¼ï¼ˆç¶™ç¶šï¼‰: {exec_e}")

            return {'success': False, 'error': str(e)}
