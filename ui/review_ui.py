"""
Document Review UI (v2.1 - Database Migration Edition)
äººé–“ãŒAIã®æŠ½å‡ºçµæœã‚’ç¢ºèªãƒ»ä¿®æ­£ã™ã‚‹ãŸã‚ã®ç®¡ç†ç”»é¢

æ–°æ©Ÿèƒ½:
- ã‚¿ãƒ–ãƒ™ãƒ¼ã‚¹UI (ãƒ•ã‚©ãƒ¼ãƒ ç·¨é›† / è¡¨ã‚¨ãƒ‡ã‚£ã‚¿ / JSONãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼)
- ã‚¹ã‚­ãƒ¼ãƒãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚©ãƒ¼ãƒ ç·¨é›†
- ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã‚ˆã‚‹è¡¨å½¢å¼ç·¨é›†
- JSONå·®åˆ†è¡¨ç¤º
"""
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

import streamlit as st
import json
import tempfile
from typing import Dict, Any, Optional, List
import pandas as pd
from loguru import logger

from A_common.database.client import DatabaseClient
from A_common.connectors.google_drive import GoogleDriveConnector

# æ–°ã—ã„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ui.utils.schema_detector import SchemaDetector
from ui.components.form_editor import render_form_editor
from ui.components.table_editor import render_table_editor, _render_array_table, _format_field_name
from ui.components.json_preview import render_json_preview, render_json_diff


def detect_structured_fields(metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è‡ªå‹•æ¤œå‡º

    Args:
        metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¾æ›¸

    Returns:
        æ§‹é€ åŒ–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒªã‚¹ãƒˆ [{"key": str, "label": str, "data": list}, ...]
    """
    structured_fields = []

    logger.info("=" * 60)
    logger.info("ğŸ” æ§‹é€ åŒ–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ¤œå‡ºã‚’é–‹å§‹")
    logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼æ•°: {len(metadata)}")
    logger.info("=" * 60)

    for key, value in metadata.items():
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: å…¨ã¦ã®ã‚­ãƒ¼ã¨å€¤ã®å‹ã‚’å‡ºåŠ›
        logger.debug(f"Key: {key}, Type: {type(value)}, Value start: {str(value)[:50]}")

        # extracted_tablesã®ç‰¹åˆ¥å‡¦ç†
        if key == "extracted_tables":
            logger.info(f"ğŸ¯ FOUND extracted_tables! Type: {type(value)}, Length: {len(value) if isinstance(value, list) else 'N/A'}")
            if isinstance(value, list):
                logger.info(f"  First element type: {type(value[0]) if len(value) > 0 else 'empty'}")

        # _list, _blocks, _matrix, _tables ã§çµ‚ã‚ã‚‹ã‚­ãƒ¼ã€ã¾ãŸã¯ structured_tables, weekly_schedule, extracted_tables ã‚’æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦èªè­˜
        # ãŸã ã— text_blocks ã¯é™¤å¤–ï¼ˆãƒ•ã‚©ãƒ¼ãƒ ç·¨é›†ã‚¿ãƒ–ã§ç·¨é›†å¯èƒ½ã«ã™ã‚‹ãŸã‚ï¼‰
        if key == "text_blocks":
            logger.info(f"âœ“ '{key}' ã¯ text_blocks ã¨ã—ã¦æ¤œå‡ºã•ã‚Œã¾ã—ãŸãŒã€ãƒ•ã‚©ãƒ¼ãƒ ç·¨é›†ã‚¿ãƒ–ã§è¡¨ç¤ºã™ã‚‹ãŸã‚é™¤å¤–ã—ã¾ã™")
            continue

        if (key.endswith("_list") or key.endswith("_blocks") or
            key.endswith("_matrix") or key.endswith("_tables") or
            key == "structured_tables" or key == "weekly_schedule" or key == "extracted_tables"):
            logger.info(f"âœ“ '{key}' ã¯æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ã—ã¦æ¤œå‡º")

            if not isinstance(value, list):
                logger.warning(f"  âš ï¸ '{key}' ã¯ãƒªã‚¹ãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚Type: {type(value)}")
                continue

            if len(value) == 0:
                logger.warning(f"  âš ï¸ '{key}' ã¯ç©ºã®ãƒªã‚¹ãƒˆã§ã™")
                continue

            logger.info(f"  âœ“ '{key}' ã¯ãƒªã‚¹ãƒˆã§ã€è¦ç´ æ•°: {len(value)}")

            # extracted_tablesã¯ç‰¹åˆ¥å‡¦ç†ï¼ˆæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ï¼‰
            if key == "extracted_tables":
                logger.info(f"  âœ“ '{key}' ã¯ extracted_tables ã¨ã—ã¦æ¤œå‡º - ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚’å®Ÿè¡Œ")
                from ui.utils.table_parser import parse_extracted_tables
                parsed_tables = parse_extracted_tables(value)
                if parsed_tables:
                    logger.info(f"  âœ“ {len(parsed_tables)} å€‹ã®è¡¨ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¾ã—ãŸ")
                    structured_fields.append({
                        "key": key,
                        "label": _format_field_name(key),
                        "data": parsed_tables
                    })
                else:
                    logger.warning(f"  âš ï¸ '{key}' ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ")
                continue

            # é…åˆ—ã®æœ€åˆã®è¦ç´ ãŒè¾æ›¸ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®è¨¼æ‹ ï¼‰
            if isinstance(value[0], dict):
                logger.info(f"  âœ“ '{key}' ã®æœ€åˆã®è¦ç´ ã¯è¾æ›¸ã§ã™ â†’ æ§‹é€ åŒ–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ã—ã¦æ¤œå‡º!")
                structured_fields.append({
                    "key": key,
                    "label": _format_field_name(key),
                    "data": value
                })
            else:
                logger.warning(f"  âš ï¸ '{key}' ã®æœ€åˆã®è¦ç´ ã¯è¾æ›¸ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚Type: {type(value[0])}")

    logger.info("=" * 60)
    logger.info(f"ğŸ¯ æ¤œå‡ºã•ã‚ŒãŸæ§‹é€ åŒ–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°: {len(structured_fields)}")
    for field in structured_fields:
        logger.info(f"  - {field['key']} ({field['label']}) - {len(field['data'])} ä»¶")
    logger.info("=" * 60)

    return structured_fields


def download_file_from_drive(source_id: str, file_name: str) -> Optional[str]:
    """
    Google Driveã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

    Args:
        source_id: Google Driveã®ãƒ•ã‚¡ã‚¤ãƒ«ID
        file_name: ãƒ•ã‚¡ã‚¤ãƒ«å

    Returns:
        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€å¤±æ•—æ™‚ã¯None
    """
    try:
        drive_connector = GoogleDriveConnector()
        temp_dir = tempfile.gettempdir()
        file_path = drive_connector.download_file(source_id, file_name, temp_dir)
        return file_path
    except Exception as e:
        error_type = type(e).__name__
        error_msg = str(e)
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: source_id={source_id}, file_name={file_name}", exc_info=True)
        st.error(f"âŒ Google Driveã‚¨ãƒ©ãƒ¼")
        st.error(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {error_type}")
        st.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {error_msg}")
        st.code(f"file_id: {source_id}\nfile_name: {file_name}")
        return None


def pdf_review_ui():
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ“ãƒ¥ãƒ¼UIãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—å¯¾å¿œï¼‰"""
    st.markdown("#### ğŸ“‹ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ“ãƒ¥ãƒ¼")
    st.caption("AIãŒæŠ½å‡ºã—ãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªãƒ»ä¿®æ­£ã§ãã¾ã™ï¼ˆPDFã€ãƒ†ã‚­ã‚¹ãƒˆã€ãƒ¡ãƒ¼ãƒ«ç­‰ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—å¯¾å¿œï¼‰")

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ã‚¹ã‚­ãƒ¼ãƒæ¤œå‡ºå™¨ã®åˆæœŸåŒ–
    try:
        db_client = DatabaseClient()
        schema_detector = SchemaDetector()
    except Exception as e:
        st.error(f"åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼: æ¤œç´¢ã¨ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š
    st.sidebar.header("ğŸ” æ¤œç´¢ & ãƒ•ã‚£ãƒ«ã‚¿")

    # Workspaceãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå‹•çš„ã«å–å¾—ï¼‰
    available_workspaces = db_client.get_available_workspaces()
    workspace_options = ["å…¨ã¦"] + available_workspaces
    workspace_filter = st.sidebar.selectbox(
        "Workspace",
        options=workspace_options,
        index=0,
        help="ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"
    )

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿
    file_type_options = ["å…¨ã¦", "pdf", "email", "text", "markdown", "csv", "json"]
    file_type_filter = st.sidebar.selectbox(
        "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—",
        options=file_type_options,
        index=0,
        help="ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"
    )

    # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ•ã‚£ãƒ«ã‚¿
    review_status_options = ["å…¨ã¦", "æœªç¢ºèª", "ç¢ºèªæ¸ˆã¿"]
    review_status_filter = st.sidebar.selectbox(
        "ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹",
        options=review_status_options,
        index=0,
        help="ãƒ¬ãƒ“ãƒ¥ãƒ¼çŠ¶æ…‹ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"
    )

    # æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹
    search_query = st.sidebar.text_input(
        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢",
        placeholder="ä¾‹: å­¦å¹´é€šä¿¡, abc123...",
        help="æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼çŠ¶æ…‹ã«é–¢ä¿‚ãªãå…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¤œç´¢ã—ã¾ã™"
    )

    # å–å¾—ä»¶æ•°
    limit = st.sidebar.number_input(
        "å–å¾—ä»¶æ•°",
        min_value=10,
        max_value=500,
        value=50,
        step=10,
        help="è¡¨ç¤ºã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æœ€å¤§ä»¶æ•°"
    )

    # ãƒ¢ãƒ¼ãƒ‰è¡¨ç¤º
    if search_query:
        st.sidebar.info("ğŸ” **æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰**: å…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¤œç´¢ä¸­")
    else:
        st.sidebar.success("ğŸ“ **é€šå¸¸ãƒ¢ãƒ¼ãƒ‰**: æœªãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã¿è¡¨ç¤º")

    # é€²æ—è¡¨ç¤º
    st.sidebar.markdown("---")
    st.sidebar.header("ğŸ“Š ãƒ¬ãƒ“ãƒ¥ãƒ¼é€²æ—")
    progress_data = db_client.get_review_progress()

    col_p1, col_p2 = st.sidebar.columns(2)
    with col_p1:
        st.metric("æœªãƒ¬ãƒ“ãƒ¥ãƒ¼", f"{progress_data['unreviewed']} ä»¶")
    with col_p2:
        st.metric("å®Œäº†", f"{progress_data['reviewed']} ä»¶")

    st.sidebar.progress(progress_data['progress_percent'] / 100)
    st.sidebar.caption(f"é€²æ—ç‡: {progress_data['progress_percent']}%")

    # ãƒªã‚¹ãƒˆæ›´æ–°ãƒœã‚¿ãƒ³
    st.sidebar.markdown("---")
    if st.sidebar.button("ğŸ”„ ãƒªã‚¹ãƒˆã‚’æ›´æ–°", use_container_width=True, key="refresh_pdf_list"):
        st.rerun()

    # ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ï¼ˆå…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ï¼‰
    with st.spinner("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ä¸­..."):
        # Workspaceãƒ•ã‚£ãƒ«ã‚¿ã®å€¤ã‚’å¤‰æ›ï¼ˆ"å…¨ã¦"ã®å ´åˆã¯Noneï¼‰
        workspace_value = workspace_filter if workspace_filter != "å…¨ã¦" else None

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿ã®å€¤ã‚’å¤‰æ›ï¼ˆ"å…¨ã¦"ã®å ´åˆã¯Noneï¼‰
        file_type_value = file_type_filter if file_type_filter != "å…¨ã¦" else None

        # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ã®å€¤ã‚’å¤‰æ›
        if review_status_filter == "ç¢ºèªæ¸ˆã¿":
            review_status_value = "reviewed"
        elif review_status_filter == "æœªç¢ºèª":
            review_status_value = "pending"
        else:  # "å…¨ã¦"
            review_status_value = "all"

        documents = db_client.get_documents_for_review(
            limit=limit,
            search_query=search_query if search_query else None,
            workspace=workspace_value,
            file_type=file_type_value,
            review_status=review_status_value
        )

    # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: å–å¾—å¾Œã®ç¢ºèª
    logger.info(f"DBã‹ã‚‰å–å¾—ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}ä»¶")

    if not documents:
        st.info("ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
        return

    st.sidebar.success(f"âœ… {len(documents)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’DataFrameã§è¡¨ç¤ºï¼ˆãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ä»˜ãï¼‰
    df_data = []
    for idx, doc in enumerate(documents):
        df_data.append({
            'é¸æŠ': False,  # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ç”¨
            'ID': doc.get('id', '')[:8],
            'ãƒ•ã‚¡ã‚¤ãƒ«å': doc.get('file_name', ''),
            'æ–‡æ›¸ã‚¿ã‚¤ãƒ—': doc.get('doc_type', ''),
            'ä½œæˆæ—¥æ™‚': doc.get('created_at', '')[:10]
        })

    df = pd.DataFrame(df_data)

    # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: DataFrameä½œæˆå¾Œã®ç¢ºèª
    logger.info(f"è¡¨ç¤ºç”¨DataFrameã®è¡Œæ•°: {len(df)}ä»¶")

    st.subheader("ğŸ“ ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§")

    # ã¾ã¨ã‚ã¦å‰Šé™¤æ©Ÿèƒ½
    col_list_header, col_bulk_delete = st.columns([3, 1])
    with col_list_header:
        st.markdown("ä¸€è¦§ã‹ã‚‰é¸æŠã—ã¦ã¾ã¨ã‚ã¦å‰Šé™¤ã§ãã¾ã™")
    with col_bulk_delete:
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã®çŠ¶æ…‹ã‚’ç®¡ç†
        if 'selected_docs' not in st.session_state:
            st.session_state.selected_docs = []

    # ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ‡ã‚£ã‚¿ã§ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ä»˜ãã®è¡¨ã‚’è¡¨ç¤º
    edited_df = st.data_editor(
        df,
        use_container_width=True,
        height=200,
        hide_index=True,
        column_config={
            "é¸æŠ": st.column_config.CheckboxColumn(
                "é¸æŠ",
                help="å‰Šé™¤ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é¸æŠ",
                default=False,
            )
        },
        disabled=["ID", "ãƒ•ã‚¡ã‚¤ãƒ«å", "æ–‡æ›¸ã‚¿ã‚¤ãƒ—", "ä½œæˆæ—¥æ™‚"],
        key="document_list_editor"
    )

    # é¸æŠã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
    selected_indices = edited_df[edited_df['é¸æŠ'] == True].index.tolist()
    selected_count = len(selected_indices)

    # ã¾ã¨ã‚ã¦å‰Šé™¤ãƒœã‚¿ãƒ³
    if selected_count > 0:
        col_bulk1, col_bulk2, col_spacer = st.columns([1, 1, 2])

        with col_bulk1:
            st.warning(f"âš ï¸ {selected_count}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒé¸æŠã•ã‚Œã¦ã„ã¾ã™")

        with col_bulk2:
            # ä¸€æ‹¬å‰Šé™¤ç¢ºèªç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹
            if 'bulk_delete_confirm' not in st.session_state:
                st.session_state.bulk_delete_confirm = False

            if not st.session_state.bulk_delete_confirm:
                if st.button(f"ğŸ—‘ï¸ {selected_count}ä»¶ã‚’ã¾ã¨ã‚ã¦å‰Šé™¤", use_container_width=True, type="secondary"):
                    st.session_state.bulk_delete_confirm = True
                    st.rerun()
            else:
                if st.button(f"âœ… {selected_count}ä»¶ã®å‰Šé™¤ã‚’å®Ÿè¡Œ", use_container_width=True, type="primary"):
                    with st.spinner(f"{selected_count}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ä¸­..."):
                        success_count = 0
                        fail_count = 0

                        for idx in selected_indices:
                            doc = documents[idx]
                            doc_id = doc.get('id')
                            file_id = doc.get('drive_file_id') or doc.get('source_id')

                            # Google Driveã‹ã‚‰å‰Šé™¤
                            if file_id:
                                try:
                                    drive_connector = GoogleDriveConnector()
                                    drive_connector.trash_file(file_id)
                                except Exception as e:
                                    logger.error(f"Google Driveå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

                            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å‰Šé™¤
                            if db_client.delete_document(doc_id):
                                success_count += 1
                            else:
                                fail_count += 1

                        if success_count > 0:
                            st.success(f"âœ… {success_count}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                        if fail_count > 0:
                            st.error(f"âŒ {fail_count}ä»¶ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ")

                        st.session_state.bulk_delete_confirm = False
                        st.balloons()
                        import time
                        time.sleep(1)
                        st.rerun()

                if st.button("âŒ ã‚­ãƒ£ãƒ³ã‚»ãƒ«", use_container_width=True):
                    st.session_state.bulk_delete_confirm = False
                    st.rerun()

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé¸æŠ
    st.subheader("ğŸ” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè©³ç´°")

    # ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹ã®ã‚­ãƒ¼ã«æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å«ã‚ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ¼ãƒ‰å¤‰æ›´æ™‚ã«ãƒªã‚»ãƒƒãƒˆ
    selector_key = f"document_selector_{search_query or 'normal'}"

    selected_index = st.selectbox(
        "ç·¨é›†ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é¸æŠ",
        range(len(documents)),
        format_func=lambda i: f"{documents[i].get('file_name', 'Unknown')}",
        key=selector_key
    )

    selected_doc = documents[selected_index]
    doc_id = selected_doc.get('id')

    # ãƒ‡ãƒãƒƒã‚°: é¸æŠã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèª
    logger.info(f"=== é¸æŠã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ===")
    logger.info(f"selected_index: {selected_index}")
    logger.info(f"doc_id: {doc_id}")
    logger.info(f"file_name: {selected_doc.get('file_name')}")

    # å…ˆã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—ï¼ˆst.rerun()ã®å‰ã«ï¼‰
    drive_file_id = selected_doc.get('drive_file_id')
    source_id = selected_doc.get('source_id')
    file_id = drive_file_id or source_id
    file_name = selected_doc.get('file_name') or 'unknown'
    doc_type = selected_doc.get('doc_type', '')

    # metadataã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆJSONæ–‡å­—åˆ—ã®å ´åˆã¨è¾æ›¸ã®å ´åˆã®ä¸¡æ–¹ã«å¯¾å¿œï¼‰
    metadata = selected_doc.get('metadata') or {}
    if isinstance(metadata, str):
        try:
            metadata = json.loads(metadata)
        except json.JSONDecodeError:
            logger.error(f"Failed to parse metadata JSON for doc_id: {doc_id}")
            metadata = {}

    # extracted_tables ã‚«ãƒ©ãƒ ã®å†…å®¹ã‚’ metadata ã«çµ±åˆ
    if 'extracted_tables' in selected_doc and selected_doc['extracted_tables']:
        extracted_tables = selected_doc['extracted_tables']
        if isinstance(extracted_tables, str):
            try:
                extracted_tables = json.loads(extracted_tables)
            except json.JSONDecodeError:
                logger.error(f"Failed to parse extracted_tables JSON for doc_id: {doc_id}")
                extracted_tables = None
        if extracted_tables:
            metadata['extracted_tables'] = extracted_tables
            logger.info(f"Added extracted_tables to metadata: {len(extracted_tables)} tables")

    # ãƒ‡ãƒãƒƒã‚°: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®çŠ¶æ…‹ã‚’ç¢ºèª
    logger.info(f"metadata keys: {list(metadata.keys())}")
    logger.info(f"metadata size: {len(str(metadata))} bytes")
    if 'extracted_tables' in metadata:
        logger.info(f"extracted_tables found in metadata: {len(metadata['extracted_tables'])} tables")

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå¤‰æ›´ã‚’æ¤œå‡ºã—ã¦ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
    if 'previous_doc_id' not in st.session_state:
        st.session_state.previous_doc_id = doc_id

    if st.session_state.previous_doc_id != doc_id:
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã€å…¨ã¦ã®ç·¨é›†é–¢é€£ã®ã‚­ãƒ¼ã‚’ã‚¯ãƒªã‚¢
        logger.info(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå¤‰æ›´ã‚’æ¤œå‡º: {st.session_state.previous_doc_id} -> {doc_id}")
        logger.info(f"æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«å: {file_name}")

        # ç·¨é›†é–¢é€£ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
        # å¤ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚­ãƒ¼ã‚’å‰Šé™¤
        old_doc_id = st.session_state.previous_doc_id
        keys_to_remove = [
            key for key in st.session_state.keys()
            if (key.startswith('form_') or
                key.startswith(f'json_editor_{old_doc_id}') or
                key.startswith(f'text_editor_{old_doc_id}') or
                key.startswith('table_editor_'))
        ]

        for key in keys_to_remove:
            del st.session_state[key]
            logger.debug(f"  å‰Šé™¤: {key}")

        st.session_state.previous_doc_id = doc_id
        logger.info(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢: {len(keys_to_remove)} keys removed")

        # ãƒšãƒ¼ã‚¸ã‚’å†ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
        st.rerun()

    # åŸºæœ¬æƒ…å ±è¡¨ç¤º
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown(f"**ãƒ•ã‚¡ã‚¤ãƒ«å**: {file_name}")
    with col2:
        st.markdown(f"**æ–‡æ›¸ã‚¿ã‚¤ãƒ—**: {doc_type}")

    st.markdown("---")

    # ä¿®æ­£å±¥æ­´ã¨ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ï¼ˆPhase 2ï¼‰
    latest_correction_id = selected_doc.get('latest_correction_id')
    if latest_correction_id:
        with st.expander("ğŸ“œ ä¿®æ­£å±¥æ­´ã¨ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯", expanded=False):
            correction_history = db_client.get_correction_history(doc_id, limit=5)

            if correction_history:
                st.markdown(f"**ä¿®æ­£å›æ•°**: {len(correction_history)}å›")

                # æœ€æ–°ã®ä¿®æ­£æƒ…å ±
                latest_correction = correction_history[0]
                st.markdown(f"**æœ€æ–°ã®ä¿®æ­£æ—¥æ™‚**: {latest_correction.get('corrected_at')}")
                if latest_correction.get('corrector_email'):
                    st.markdown(f"**ä¿®æ­£è€…**: {latest_correction.get('corrector_email')}")

                # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒœã‚¿ãƒ³
                col_rollback, col_spacer = st.columns([1, 2])
                with col_rollback:
                    if st.button("â®ï¸ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå…ƒã«æˆ»ã™ï¼‰", use_container_width=True, type="secondary"):
                        with st.spinner("ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸­..."):
                            rollback_success = db_client.rollback_document(doc_id)

                        if rollback_success:
                            st.success("âœ… ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«æˆåŠŸã—ã¾ã—ãŸï¼å‰ã®çŠ¶æ…‹ã«æˆ»ã‚Šã¾ã—ãŸã€‚")
                            st.rerun()
                        else:
                            st.error("âŒ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«å¤±æ•—ã—ã¾ã—ãŸ")

                # ä¿®æ­£å±¥æ­´ã®è©³ç´°è¡¨ç¤º
                with st.expander("ä¿®æ­£å±¥æ­´ã®è©³ç´°ã‚’è¡¨ç¤º", expanded=False):
                    for idx, correction in enumerate(correction_history):
                        st.markdown(f"### ä¿®æ­£ #{idx + 1}")
                        st.markdown(f"**æ—¥æ™‚**: {correction.get('corrected_at')}")
                        if correction.get('notes'):
                            st.markdown(f"**ãƒ¡ãƒ¢**: {correction.get('notes')}")

                        # ä¿®æ­£å‰å¾Œã®å·®åˆ†ã‚’è¡¨ç¤º
                        col_before, col_after = st.columns(2)
                        with col_before:
                            st.markdown("**ä¿®æ­£å‰**")
                            st.json(correction.get('old_metadata', {}), expanded=True)
                        with col_after:
                            st.markdown("**ä¿®æ­£å¾Œ**")
                            st.json(correction.get('new_metadata', {}), expanded=True)

                        st.markdown("---")
            else:
                st.info("ä¿®æ­£å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")

    st.markdown("---")

    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ: å·¦ã«PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€å³ã«ç·¨é›†ã‚¿ãƒ–
    col_left, col_right = st.columns([1, 1.2])

    with col_left:
        st.markdown("### ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        logger.info(f"ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æº–å‚™: file_id={file_id}, file_name={file_name}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨è¡¨ç¤º
        if file_id:
            with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                file_path = download_file_from_drive(file_id, file_name)

            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœã®ç¢ºèª
            if not file_path:
                st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
                st.info("Google Driveã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±"):
                    st.code(f"file_id: {file_id}\nfile_name: {file_name}")
            elif not Path(file_path).exists():
                st.error("âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                st.info("ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸãŒã€ä¿å­˜å…ˆã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±"):
                    st.code(f"ãƒ‘ã‚¹: {file_path}")

            if file_path and Path(file_path).exists():
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
                actual_file_name = Path(file_path).name
                if file_name == 'unknown' and actual_file_name:
                    file_name = actual_file_name
                    logger.info(f"å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—: {file_name}")

                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
                # ã¾ãšå®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰æ‹¡å¼µå­ã‚’å–å¾—ï¼ˆã‚ˆã‚Šç¢ºå®Ÿï¼‰
                file_extension = Path(file_path).suffix.lstrip('.').lower()

                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰å–å¾—ã§ããªã„å ´åˆã€file_nameã‹ã‚‰å–å¾—
                if not file_extension and file_name and '.' in file_name:
                    file_extension = file_name.lower().split('.')[-1]

                # æ‹¡å¼µå­ãŒå–å¾—ã§ããªã„å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‹ã‚‰åˆ¤å®šï¼ˆãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ï¼‰
                if not file_extension:
                    logger.info("æ‹¡å¼µå­ãªã—ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‹ã‚‰åˆ¤å®šã—ã¾ã™")
                    try:
                        with open(file_path, 'rb') as f:
                            header = f.read(16)

                        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ¤å®š
                        if header.startswith(b'%PDF-'):
                            file_extension = 'pdf'
                            logger.info("ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã‹ã‚‰PDFã¨åˆ¤å®š")
                        # ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚‚è¿½åŠ å¯èƒ½
                        elif header.startswith(b'\x50\x4B\x03\x04'):  # ZIP/DOCX/XLSXç­‰
                            file_extension = 'zip'
                            logger.info("ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã‹ã‚‰ZIPã¨åˆ¤å®š")
                        elif header.startswith(b'\xff\xd8\xff'):  # JPEG
                            file_extension = 'jpg'
                            logger.info("ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã‹ã‚‰JPEGã¨åˆ¤å®š")
                        elif header.startswith(b'\x89PNG'):  # PNG
                            file_extension = 'png'
                            logger.info("ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã‹ã‚‰PNGã¨åˆ¤å®š")
                    except Exception as e:
                        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")

                logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­: {file_extension}")

                # PDFã®å ´åˆ
                if file_extension == 'pdf' or file_name.lower().endswith('.pdf'):
                    import os
                    logger.info(f"PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼é–‹å§‹ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹: {file_path}")
                    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(file_path)} bytes")

                    try:
                        from streamlit_pdf_viewer import pdf_viewer
                        logger.info("streamlit_pdf_viewer ã‚’ä½¿ç”¨ã—ã¦PDFè¡¨ç¤ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ç›´æ¥æ¸¡ã—ï¼‰")
                        pdf_viewer(file_path, height=700)
                    except ImportError:
                        logger.warning("streamlit_pdf_viewer ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤ºã—ã¾ã™")
                        st.warning("PDFãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                        with open(file_path, 'rb') as f:
                            pdf_bytes = f.read()
                        st.download_button(
                            label="ğŸ“¥ PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=pdf_bytes,
                            file_name=file_name,
                            mime="application/pdf",
                            use_container_width=True
                        )
                    except Exception as e:
                        logger.error(f"PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                        st.warning(f"PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")

                # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼ˆtxt, md, csv, json, etc.ï¼‰
                elif file_extension in ['txt', 'md', 'markdown', 'csv', 'json', 'log', 'py', 'js', 'html', 'css']:
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            text_content = f.read()

                        st.markdown("#### ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")

                        # ã‚¿ãƒ–ã§è¡¨ç¤ºã‚’åˆ‡ã‚Šæ›¿ãˆ
                        text_tab1, text_tab2, text_tab3 = st.tabs(["åŸæ–‡", "æ§‹é€ åŒ–è¡¨ç¤º", "çµ±è¨ˆ"])

                        with text_tab1:
                            # åŸæ–‡è¡¨ç¤º
                            st.text_area(
                                "ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹",
                                value=text_content,
                                height=500,
                                disabled=True,
                                key=f"text_preview_{doc_id}"
                            )

                        with text_tab2:
                            # æ§‹é€ åŒ–è¡¨ç¤º
                            from ui.utils.text_structurer import TextStructurer
                            structured_blocks = TextStructurer.structure_text(text_content)

                            if structured_blocks:
                                # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã§è¡¨ç¤º
                                df_structured = pd.DataFrame(structured_blocks)

                                # ã‚¿ã‚¤ãƒ—ã‚’æ—¥æœ¬èªã«ç¿»è¨³
                                df_structured['type_ja'] = df_structured['type'].apply(
                                    lambda t: TextStructurer._translate_type(t)
                                )

                                st.dataframe(
                                    df_structured[['line_number', 'type_ja', 'content', 'length']],
                                    column_config={
                                        "line_number": st.column_config.NumberColumn("è¡Œç•ªå·", width="small"),
                                        "type_ja": st.column_config.TextColumn("ã‚¿ã‚¤ãƒ—", width="small"),
                                        "content": st.column_config.TextColumn("å†…å®¹", width="large"),
                                        "length": st.column_config.NumberColumn("æ–‡å­—æ•°", width="small")
                                    },
                                    height=500,
                                    use_container_width=True
                                )

                                # metadataã«æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                                if 'text_blocks' not in metadata:
                                    metadata['text_blocks'] = structured_blocks
                                    logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ : {len(structured_blocks)} ãƒ–ãƒ­ãƒƒã‚¯")

                        with text_tab3:
                            # çµ±è¨ˆæƒ…å ±
                            from ui.utils.text_structurer import TextStructurer
                            structured_blocks = TextStructurer.structure_text(text_content)
                            stats = TextStructurer.get_statistics(structured_blocks)

                            st.markdown("### ğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆçµ±è¨ˆ")
                            col_stat1, col_stat2, col_stat3 = st.columns(3)
                            with col_stat1:
                                st.metric("ç·è¡Œæ•°", stats['total_lines'])
                            with col_stat2:
                                st.metric("ãƒ–ãƒ­ãƒƒã‚¯ã‚¿ã‚¤ãƒ—æ•°", stats['unique_types'])
                            with col_stat3:
                                total_chars = sum(block['length'] for block in structured_blocks)
                                st.metric("ç·æ–‡å­—æ•°", total_chars)

                            st.markdown("### ğŸ“‹ ãƒ–ãƒ­ãƒƒã‚¯ã‚¿ã‚¤ãƒ—åˆ¥ä»¶æ•°")
                            for block_type, count in sorted(stats['type_counts'].items(), key=lambda x: x[1], reverse=True):
                                type_ja = TextStructurer._translate_type(block_type)
                                st.write(f"- **{type_ja}**: {count} è¡Œ")

                        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                        st.download_button(
                            label="ğŸ“¥ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=text_content,
                            file_name=file_name,
                            mime="text/plain",
                            use_container_width=True
                        )
                    except UnicodeDecodeError:
                        st.error("âŒ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã€‚UTF-8ã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã€‚")
                    except Exception as e:
                        logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                        st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

                # ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«
                else:
                    st.info(f"ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ï¼ˆ.{file_extension}ï¼‰ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«ã¯å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“")

                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
                    with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±"):
                        st.code(f"""
ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {file_path}
ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆDBï¼‰: {selected_doc.get('file_name')}
ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä½¿ç”¨ä¸­ï¼‰: {file_name}
ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­: '{file_extension}'
Path.suffix: '{Path(file_path).suffix}'
                        """.strip())
                    try:
                        with open(file_path, 'rb') as f:
                            file_bytes = f.read()
                        st.download_button(
                            label="ğŸ“¥ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=file_bytes,
                            file_name=file_name,
                            use_container_width=True
                        )
                    except Exception as e:
                        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                        st.error("ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            else:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚file_path={file_path}, exists={Path(file_path).exists() if file_path else False}")
                st.warning("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
        else:
            st.warning("âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            st.info("ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¯ãƒ•ã‚¡ã‚¤ãƒ«IDãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")
            with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±"):
                st.json({
                    "drive_file_id": selected_doc.get('drive_file_id'),
                    "source_id": selected_doc.get('source_id'),
                    "file_name": file_name,
                    "doc_type": doc_type
                })

        # ============================================
        # ã€æ–°æ©Ÿèƒ½ã€‘æ‰‹å‹•ãƒ†ã‚­ã‚¹ãƒˆè£œæ­£ï¼ˆHuman-in-the-loopï¼‰
        # ============================================
        # PDFã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€æ‰‹å‹•è£œæ­£æ©Ÿèƒ½ã‚’è¡¨ç¤º
        if file_path and Path(file_path).exists():
            from ui.components.manual_text_correction import (
                render_manual_text_correction,
                execute_stage2_reprocessing
            )

            # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            # å¿…ãšã“ã®é †ã§çµåˆ: 1. display_post_text (æŠ•ç¨¿æœ¬æ–‡) â†’ 2. attachment_text (æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«)
            display_text = selected_doc.get('display_post_text', '') or ''
            attachment_text = selected_doc.get('attachment_text', '') or ''

            # ä¸¡æ–¹ã‚’çµåˆï¼ˆç©ºã®å ´åˆã‚‚å«ã‚€ï¼‰
            parts = []
            if display_text:
                parts.append(display_text)
            if attachment_text:
                parts.append(attachment_text)

            extracted_text = '\n\n'.join(parts) if parts else ''

            # æ‰‹å‹•è£œæ­£UIã‚’è¡¨ç¤º
            corrected_text = render_manual_text_correction(
                doc_id=doc_id,
                file_name=file_name,
                extracted_text=extracted_text,
                metadata=metadata,
                doc_type=doc_type
            )

            # Stage 2å†å®Ÿè¡ŒãŒè¦æ±‚ã•ã‚ŒãŸå ´åˆ
            if corrected_text:
                with st.spinner("ğŸ”„ è£œæ­£ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã§Stage 2ï¼ˆæ§‹é€ åŒ–ï¼‰ã‚’å†å®Ÿè¡Œä¸­..."):
                    try:
                        # Stage 2å†å®Ÿè¡Œ
                        reprocessed_result = execute_stage2_reprocessing(
                            corrected_text=corrected_text,
                            file_name=file_name,
                            metadata=metadata,
                            workspace=selected_doc.get('workspace', 'personal')
                        )

                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
                        new_metadata = reprocessed_result['metadata']

                        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                        logger.info(f"[Stage 2å†å®Ÿè¡Œ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜é–‹å§‹: doc_id={doc_id}")
                        logger.info(f"[Stage 2å†å®Ÿè¡Œ] new_metadata keys: {list(new_metadata.keys())}")
                        logger.info(f"[Stage 2å†å®Ÿè¡Œ] new_doc_type: {doc_type}")

                        success = db_client.record_correction(
                            doc_id=doc_id,
                            new_metadata=new_metadata,
                            new_doc_type=doc_type,
                            corrector_email=None,
                            notes="æ‰‹å‹•ãƒ†ã‚­ã‚¹ãƒˆè£œæ­£ã«ã‚ˆã‚‹Stage 2å†å®Ÿè¡Œ"
                        )

                        if success:
                            st.success("âœ… Stage 2å†å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸï¼æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸã€‚")
                            logger.info(f"[Stage 2å†å®Ÿè¡Œ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜æˆåŠŸ")
                            st.balloons()

                            # è£œæ­£å‰å¾Œã®æ¯”è¼ƒã‚’è¡¨ç¤º
                            with st.expander("ğŸ“Š è£œæ­£å‰å¾Œã®æ¯”è¼ƒ", expanded=True):
                                col_before, col_after = st.columns(2)

                                with col_before:
                                    st.markdown("**è£œæ­£å‰**")
                                    st.metric("æ–‡å­—æ•°", len(extracted_text))

                                with col_after:
                                    st.markdown("**è£œæ­£å¾Œ**")
                                    st.metric("æ–‡å­—æ•°", len(corrected_text))

                            # ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰
                            import time
                            time.sleep(2)
                            st.rerun()
                        else:
                            logger.error(f"[Stage 2å†å®Ÿè¡Œ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜å¤±æ•—: doc_id={doc_id}")
                            logger.error(f"[Stage 2å†å®Ÿè¡Œ] metadata type: {type(new_metadata)}")
                            logger.error(f"[Stage 2å†å®Ÿè¡Œ] metadata sample: {str(new_metadata)[:500]}")
                            st.error("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

                    except Exception as e:
                        logger.error(f"Stage 2å†å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                        st.error(f"âŒ Stage 2å†å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

    with col_right:
        st.markdown("### âœï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç·¨é›†")

        # ã‚¹ã‚­ãƒ¼ãƒã‚’æ¤œå‡º
        detected_schema = schema_detector.detect_schema(doc_type, metadata)

        if detected_schema:
            st.info(f"ğŸ¯ æ¤œå‡ºã•ã‚ŒãŸã‚¹ã‚­ãƒ¼ãƒ: **{detected_schema}**")
            editable_fields = schema_detector.get_editable_fields(detected_schema)
        else:
            st.warning("âš ï¸ ã‚¹ã‚­ãƒ¼ãƒãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚JSONç·¨é›†ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
            editable_fields = []

        # ã€å‹•çš„ã‚¿ãƒ–ç”Ÿæˆã€‘æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è‡ªå‹•æ¤œå‡º
        structured_fields = detect_structured_fields(metadata)

        # æ§‹é€ åŒ–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã‚­ãƒ¼ã‚»ãƒƒãƒˆã‚’ä½œæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ ç·¨é›†ã‹ã‚‰é™¤å¤–ã™ã‚‹ãŸã‚ï¼‰
        structured_field_keys = {field["key"] for field in structured_fields}

        # ã‚¿ãƒ–ãƒªã‚¹ãƒˆã‚’å‹•çš„ã«æ§‹ç¯‰
        tab_names = ["ğŸ“ ãƒ•ã‚©ãƒ¼ãƒ ç·¨é›†"]  # ãƒ•ã‚©ãƒ¼ãƒ ç·¨é›†ã‚¿ãƒ–ã®ã¿

        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã”ã¨ã«ã‚¿ãƒ–ã‚’è¿½åŠ 
        logger.info(f"ğŸ·ï¸ ã‚¿ãƒ–ç”Ÿæˆ: æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚¿ãƒ–ã‚’ {len(structured_fields)} å€‹è¿½åŠ ã—ã¾ã™")
        for field in structured_fields:
            logger.info(f"  ã‚¿ãƒ–è¿½åŠ : {field['label']} (ã‚­ãƒ¼: {field['key']})")
            tab_names.append(field["label"])

        # å›ºå®šã‚¿ãƒ–ï¼šè¡¨ã‚’è¿½åŠ ã€JSONãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        tab_names.append("â• è¡¨ã‚’è¿½åŠ ")
        tab_names.append("ğŸ” JSONãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")

        logger.info(f"ğŸ“‘ ç”Ÿæˆã•ã‚Œã‚‹ã‚¿ãƒ–ä¸€è¦§ ({len(tab_names)} å€‹): {tab_names}")

        # ã‚¿ãƒ–ã‚’å‹•çš„ã«ç”Ÿæˆ
        tabs = st.tabs(tab_names)
        edited_metadata = None

        # ã‚¿ãƒ–1: ãƒ•ã‚©ãƒ¼ãƒ ç·¨é›†
        with tabs[0]:
            if editable_fields:
                # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ãƒ•ã‚©ãƒ¼ãƒ ã‹ã‚‰é™¤å¤–
                form_fields = [f for f in editable_fields if f["name"] not in structured_field_keys]

                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
                with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆé–‹ç™ºç”¨ï¼‰", expanded=False):
                    st.code(f"""
ç·¨é›†å¯èƒ½ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°: {len(editable_fields)}
ãƒ•ã‚©ãƒ¼ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°: {len(form_fields)}
æ§‹é€ åŒ–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°: {len(structured_fields)}

ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼: {list(metadata.keys())}
text_blocks ã®å­˜åœ¨: {'text_blocks' in metadata}
text_blocks ã®å€¤: {metadata.get('text_blocks', 'ãªã—')}

editable_fields:
{[f['name'] for f in editable_fields]}

form_fields:
{[f['name'] for f in form_fields]}

structured_field_keys:
{structured_field_keys}
                    """.strip())

                if form_fields:
                    edited_metadata = render_form_editor(metadata, form_fields, doc_id)
                else:
                    st.info("ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯å…¨ã¦å°‚ç”¨ã‚¿ãƒ–ã§ç·¨é›†ã§ãã¾ã™")
                    st.markdown("å„ãƒ‡ãƒ¼ã‚¿ã‚¿ãƒ–ã¾ãŸã¯JSONãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¿ãƒ–ã‚’ã”åˆ©ç”¨ãã ã•ã„")
            else:
                st.info("ãƒ•ã‚©ãƒ¼ãƒ ç·¨é›†ã«ã¯å¯¾å¿œã™ã‚‹ã‚¹ã‚­ãƒ¼ãƒãŒå¿…è¦ã§ã™")

        # ã‚¿ãƒ–2ä»¥é™: æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚¿ãƒ–ï¼ˆå‹•çš„ã«ç”Ÿæˆï¼‰
        for idx, field in enumerate(structured_fields):
            with tabs[idx + 1]:  # ãƒ•ã‚©ãƒ¼ãƒ ç·¨é›†ã®æ¬¡ã‹ã‚‰
                logger.info(f"ğŸ“Š ã‚¿ãƒ– {idx + 1} ã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°: {field['label']} ({field['key']})")
                logger.info(f"  ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(field['data'])} ä»¶")

                st.markdown(f"### {field['label']}")
                st.markdown("è¡¨å½¢å¼ã§ç·¨é›†ã§ãã¾ã™")
                st.markdown("---")

                # è¡¨ã‚¨ãƒ‡ã‚£ã‚¿ã§ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã«ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚­ãƒ¼ã‚’ä½¿ç”¨ï¼‰
                edited_value = _render_array_table(
                    f"{field['key']}_{doc_id}",
                    field["data"],
                    field["label"]
                )

                # edited_metadataã‚’åˆæœŸåŒ–ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
                if edited_metadata is None:
                    edited_metadata = metadata.copy()

                edited_metadata[field["key"]] = edited_value
                logger.info(f"  âœ“ {field['label']} ã‚¿ãƒ–ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº†")

        # æœ€å¾Œã‹ã‚‰2ç•ªç›®ã®ã‚¿ãƒ–: è¡¨ã‚’è¿½åŠ 
        with tabs[-2]:
            from ui.components.table_creator import render_table_creator

            updated_metadata = render_table_creator(doc_id, metadata.copy())

            if updated_metadata:
                edited_metadata = updated_metadata
                st.info("ğŸ’¡ è¿½åŠ ã—ãŸè¡¨ã‚’ä¿å­˜ã™ã‚‹ã«ã¯ã€ä¸‹ã®ã€ŒğŸ’¾ ä¿å­˜ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„")

        # æœ€å¾Œã®ã‚¿ãƒ–: JSONãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        with tabs[-1]:
            edited_metadata = render_json_preview(metadata, editable=True, key_suffix=doc_id)

        # ä¿å­˜ãƒœã‚¿ãƒ³ã‚¨ãƒªã‚¢
        st.markdown("---")

        # ãƒ¬ãƒ“ãƒ¥ãƒ¼çŠ¶æ…‹ã®è¡¨ç¤º
        is_reviewed = selected_doc.get('is_reviewed', False)
        if is_reviewed:
            reviewed_at = selected_doc.get('reviewed_at', '')
            reviewed_by = selected_doc.get('reviewed_by', '')
            st.info(f"âœ… ãƒ¬ãƒ“ãƒ¥ãƒ¼æ¸ˆã¿ï¼ˆ{reviewed_at[:10] if reviewed_at else 'æ—¥æ™‚ä¸æ˜'}ï¼‰" +
                   (f" by {reviewed_by}" if reviewed_by else ""))

        col_save, col_validate, col_review, col_cancel = st.columns([1, 1, 1, 1])

        with col_validate:
            if st.button("ğŸ” å¤‰æ›´ã‚’ç¢ºèª", use_container_width=True):
                if edited_metadata:
                    with st.expander("å¤‰æ›´å†…å®¹ã®è©³ç´°", expanded=True):
                        render_json_diff(metadata, edited_metadata)

        with col_save:
            if st.button("ğŸ’¾ ä¿å­˜", type="primary", use_container_width=True):
                if edited_metadata is None:
                    st.error("ç·¨é›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                else:
                    # ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼
                    if detected_schema:
                        is_valid, errors = schema_detector.validate_metadata(detected_schema, edited_metadata)
                        if not is_valid:
                            st.error("âŒ ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼ã‚¨ãƒ©ãƒ¼:")
                            for error in errors:
                                st.error(f"  - {error}")
                            st.stop()

                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°ï¼ˆä¿®æ­£å±¥æ­´ã‚’è¨˜éŒ²ï¼‰
                    success = db_client.record_correction(
                        doc_id=doc_id,
                        new_metadata=edited_metadata,
                        new_doc_type=doc_type,
                        corrector_email=None,  # å°†æ¥çš„ã«èªè¨¼æƒ…å ±ã‹ã‚‰å–å¾—
                        notes="Review UIã‹ã‚‰ã®æ‰‹å‹•ä¿®æ­£"
                    )

                    if success:
                        st.success("âœ… ä¿å­˜ã«æˆåŠŸã—ã¾ã—ãŸï¼ä¿®æ­£å±¥æ­´ãŒè¨˜éŒ²ã•ã‚Œã¾ã—ãŸã€‚")
                        st.balloons()
                        # ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰
                        st.rerun()
                    else:
                        st.error("âŒ ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")

        with col_review:
            # ãƒ¬ãƒ“ãƒ¥ãƒ¼çŠ¶æ…‹åˆ‡ã‚Šæ›¿ãˆãƒœã‚¿ãƒ³
            if is_reviewed:
                # ãƒ¬ãƒ“ãƒ¥ãƒ¼æ¸ˆã¿ â†’ æœªå®Œäº†ã«æˆ»ã™
                if st.button("â†©ï¸ æœªå®Œäº†ã«æˆ»ã™", use_container_width=True, type="secondary"):
                    success = db_client.mark_document_unreviewed(doc_id)
                    if success:
                        st.success("âœ… æœªå®Œäº†ã«æˆ»ã—ã¾ã—ãŸ")
                        st.rerun()
                    else:
                        st.error("âŒ æ“ä½œã«å¤±æ•—ã—ã¾ã—ãŸ")
            else:
                # æœªãƒ¬ãƒ“ãƒ¥ãƒ¼ â†’ ãƒã‚§ãƒƒã‚¯å®Œäº†
                if st.button("âœ… ãƒã‚§ãƒƒã‚¯å®Œäº†", use_container_width=True, type="primary"):
                    success = db_client.mark_document_reviewed(doc_id)
                    if success:
                        st.success("âœ… ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†ã¨ã—ã¦ãƒãƒ¼ã‚¯ã—ã¾ã—ãŸ")
                        st.balloons()
                        st.rerun()
                    else:
                        st.error("âŒ æ“ä½œã«å¤±æ•—ã—ã¾ã—ãŸ")

        with col_cancel:
            if st.button("ğŸ”„ ãƒªã‚»ãƒƒãƒˆ", use_container_width=True):
                st.rerun()

    # å‰Šé™¤æ©Ÿèƒ½ï¼ˆå±é™ºãªæ“ä½œã®ãŸã‚ã€åˆ¥ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«é…ç½®ï¼‰
    st.markdown("---")
    st.markdown("### âš ï¸ å±é™ºãªæ“ä½œ")

    # å‰Šé™¤ç¢ºèªç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹
    delete_confirm_key = f"delete_confirm_{doc_id}"
    if delete_confirm_key not in st.session_state:
        st.session_state[delete_confirm_key] = False

    col_delete1, col_delete2, col_spacer = st.columns([1, 1, 2])

    with col_delete1:
        if not st.session_state[delete_confirm_key]:
            if st.button("ğŸ—‘ï¸ ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤", use_container_width=True, type="secondary"):
                st.session_state[delete_confirm_key] = True
                st.rerun()
        else:
            st.warning("æœ¬å½“ã«å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ")

    with col_delete2:
        if st.session_state[delete_confirm_key]:
            if st.button("âœ… å‰Šé™¤ã‚’å®Ÿè¡Œ", use_container_width=True, type="primary"):
                with st.spinner("å‰Šé™¤ä¸­..."):
                    # 1. ã¾ãšGoogle Driveã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚´ãƒŸç®±ã«ç§»å‹•
                    drive_success = False
                    if file_id:
                        try:
                            drive_connector = GoogleDriveConnector()
                            drive_success = drive_connector.trash_file(file_id)
                            if drive_success:
                                st.success(f"âœ… Google Driveã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚´ãƒŸç®±ã«ç§»å‹•ã—ã¾ã—ãŸ")
                            else:
                                st.warning(f"âš ï¸ Google Driveãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã¯å‰Šé™¤ã—ã¾ã™")
                        except Exception as e:
                            st.error(f"Google Driveå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
                            st.warning(f"âš ï¸ Google Driveãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã¯å‰Šé™¤ã—ã¾ã™")
                    else:
                        st.warning("Google Driveã®ãƒ•ã‚¡ã‚¤ãƒ«IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã¿å‰Šé™¤ã—ã¾ã™ã€‚")

                    # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å‰Šé™¤
                    db_success = db_client.delete_document(doc_id)

                    if db_success:
                        st.success("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                        st.balloons()
                        # å‰Šé™¤ç¢ºèªçŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
                        st.session_state[delete_confirm_key] = False
                        # å°‘ã—å¾…ã£ã¦ã‹ã‚‰ãƒªãƒ­ãƒ¼ãƒ‰
                        import time
                        time.sleep(1)
                        st.rerun()
                    else:
                        st.error("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ")
                        st.session_state[delete_confirm_key] = False

            if st.button("âŒ ã‚­ãƒ£ãƒ³ã‚»ãƒ«", use_container_width=True):
                st.session_state[delete_confirm_key] = False
                st.rerun()

    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    col_footer1, col_footer2 = st.columns([3, 1])
    with col_footer1:
        st.caption("Document Management System - Review UI v2.0 (Tab Edition)")
    with col_footer2:
        st.caption(f"ğŸ¨ æ¤œå‡ºã‚¹ã‚­ãƒ¼ãƒ: {detected_schema or 'N/A'}")


def main():
    """ãƒ¡ã‚¤ãƒ³UIãƒ­ã‚¸ãƒƒã‚¯ - ã‚¿ãƒ–åˆ‡ã‚Šæ›¿ãˆ"""
    st.set_page_config(
        page_title="Document Management System",
        page_icon="ğŸ“š",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.title("ğŸ“š Document Management System")
    st.markdown("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ãƒ¡ãƒ¼ãƒ«å—ä¿¡ãƒˆãƒ¬ã‚¤ã‚’çµ±åˆç®¡ç†ï¼ˆå…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—å¯¾å¿œï¼‰")

    # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®ã‚¿ãƒ–
    tab1, tab2 = st.tabs(["ğŸ“‹ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ“ãƒ¥ãƒ¼", "ğŸ“¬ ãƒ¡ãƒ¼ãƒ«å—ä¿¡ãƒˆãƒ¬ã‚¤"])

    with tab1:
        pdf_review_ui()

    with tab2:
        # ãƒ¡ãƒ¼ãƒ«å—ä¿¡ãƒˆãƒ¬ã‚¤æ©Ÿèƒ½ï¼ˆé–‹ç™ºä¸­ï¼‰
        st.info("ğŸ“¬ ãƒ¡ãƒ¼ãƒ«å—ä¿¡ãƒˆãƒ¬ã‚¤æ©Ÿèƒ½ã¯ç¾åœ¨é–‹ç™ºä¸­ã§ã™ã€‚")
        st.markdown("""
        ã“ã®æ©Ÿèƒ½ã§ã¯ä»¥ä¸‹ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ï¼š
        - ãƒ¡ãƒ¼ãƒ«ã®ä¸€è¦§è¡¨ç¤º
        - ãƒ¡ãƒ¼ãƒ«ã®å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        - ãƒ¡ãƒ¼ãƒ«ã®åˆ†é¡ã¨ç®¡ç†
        """)


if __name__ == "__main__":
    main()
